[
  {
    "objectID": "vector_spaces_and_subspaces.html",
    "href": "vector_spaces_and_subspaces.html",
    "title": "Vector spaces and subspaces",
    "section": "",
    "text": "The operation of linear combination allow us to construct vector spaces and vector subspaces. In this section we want to look at some figures and from them deduce how to construct spaces. In later sections we will adopt a new approach: to describe the space by an equation or system of equations \\(A\\mathbf{x}=\\mathbf{0}\\).\nKey concepts: vectors space, subspace, span, form of the elements, direct sum.",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Vector spaces and subspaces"
    ]
  },
  {
    "objectID": "vector_spaces_and_subspaces.html#example-1",
    "href": "vector_spaces_and_subspaces.html#example-1",
    "title": "Vector spaces and subspaces",
    "section": "Example 1",
    "text": "Example 1\n\n\n\n\n\n\nVideo\n\n\n\nR2 plane as a vector space\n\n\nConsider the vectors \\(\\begin{pmatrix} 1\\\\2\\end{pmatrix}\\) and \\(\\begin{pmatrix} 3\\\\0\\end{pmatrix}\\). Now multiply both by every possible scalar and then add them; the end result is a set of vectors. All these vectors constitute a two dimensional plane, which we call \\(\\mathbb{R}^2\\):\n\nFrom the picture, it is easy to convince yourself that any linear combination of two vectors of the plane \\(\\mathbb{R}^2\\) is another vector in \\(\\mathbb{R}^2\\), in particular from the l.c. \\(0\\begin{pmatrix} 1\\\\2\\end{pmatrix}+0\\begin{pmatrix} 3\\\\0\\end{pmatrix}\\) we get the origin. Hence we write:\n\\[\n\\mathbb{R}^2 = span\\{\\begin{pmatrix} 1\\\\2\\end{pmatrix},\\begin{pmatrix} 3\\\\0\\end{pmatrix}\\}\n\\tag{1}\\] Another important way to write this set is to identify the form of its elements, the form of its elements is:\n\\[\na(1,2)+b(3,0)\n\\]\nor written more compactly \\((a+3b,2a)\\); the set of this elements is\n\\[\n\\{(a+3b,2a)\\,\\,|\\,\\,a,b\\in\\mathbb{R}\\}=\\mathbb{R}^2\n\\]\nThe idea of a set of things (in this case column vectors) where any l.c. of its elements gives us again an element of the set (we say the set is closed under l.c.) is very important and will appear countless times in this course. Hence we give it a special name: vector space.\n\nDefinition 1 [Vector space] := a set of vectors with the property of being closed under linear combinations\n\n\\(\\mathbb{R}^2\\) is a vector space.\nNotice the same space comes about if we consider \\(\\begin{pmatrix} 1\\\\0\\end{pmatrix}\\) and \\(\\begin{pmatrix} 0\\\\1\\end{pmatrix}\\) as our starting vectors. More, any other pair of vectors of \\(\\mathbb{R}^2\\) that point in distinct directions, do the job. Hence we write\n\\[\n\\mathbb{R}^2 = span\\{\\begin{pmatrix} 1\\\\2\\end{pmatrix},\\begin{pmatrix} 3\\\\0\\end{pmatrix}\\} = span\\{\\begin{pmatrix} 1\\\\0\\end{pmatrix},\\begin{pmatrix} 0\\\\1\\end{pmatrix}\\}=\\dots\n\\]\nThe form of elements of \\(\\mathbb{R}^2\\) appears in many way, either \\((a+3b,2a)\\) or \\((a,b)\\), etc, depending on our choice of vectors to span.",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Vector spaces and subspaces"
    ]
  },
  {
    "objectID": "vector_spaces_and_subspaces.html#example-2",
    "href": "vector_spaces_and_subspaces.html#example-2",
    "title": "Vector spaces and subspaces",
    "section": "Example 2",
    "text": "Example 2\n\n\n\n\n\n\nVideo\n\n\n\na line in R2 is a subspace of R2\n\n\nConsider the vector \\(\\begin{pmatrix} 1\\\\2\\end{pmatrix}\\), this is our starting point. Now imagine multiplying it by every possible scalar; the end result is a set of vectors which lie along the same line:\n\nFrom the picture, it is easy to convince yourself that any linear combination of two (or more) vectors of the line gives us another vector in the line.\n\\[\n\\textbf{line} = span\\{\\begin{pmatrix}1\\\\2 \\end{pmatrix}\\}=\\{(a,2a)\\,\\,|\\,\\, a\\in\\mathbb{R}\\}\n\\tag{2}\\]\nIn particular if you multiply any of its vectors by \\(0\\), we get the origin \\(\\begin{pmatrix} 0\\\\0\\end{pmatrix}\\). Thus the line we are referring passes through the origin and we can say \\(\\mathbf{0} \\in \\textbf{line}\\).\nOnce again, the line is a set closed under l.c., which means it is a vector space.\nAdditionally, notice that the line lives inside the plane \\(\\mathbb{R}^2\\), because of that we say it is a subspace of \\(\\mathbb{R}^2\\).\n\nDefinition 2 [subspace (of a vector space \\(A\\))] := a vector space which is a subset of \\(A\\).\n(A subspace is also closed under linear combinations of its vectors.)\n\nNote that: the same line we see in the picture also results if you consider as your starting vector, the vector \\(\\begin{pmatrix} 2\\\\4\\end{pmatrix}\\), and then multiply it by all possible numbers (i.e. all l.c. of this vector). Thus this line can be created in many ways:\n\\[\n\\textbf{line} = span\\{\\begin{pmatrix}1\\\\2 \\end{pmatrix}\\} = span\\{\\begin{pmatrix}2\\\\4 \\end{pmatrix}\\}=\\dots\n\\]\n\n\n\n\n\n\nCommentary\n\n\n\n\nMultiplying a vector by a scalar is just a particular example of a linear combination:\n\n\\[ a\\begin{pmatrix}1\\\\2\\end{pmatrix}+b\\begin{pmatrix}0\\\\0\\end{pmatrix} \\]\n\nAny two vectors on the line are parallel, i.e., \\(u||v\\) if they are proportional.",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Vector spaces and subspaces"
    ]
  },
  {
    "objectID": "vector_spaces_and_subspaces.html#example-3",
    "href": "vector_spaces_and_subspaces.html#example-3",
    "title": "Vector spaces and subspaces",
    "section": "Example 3",
    "text": "Example 3\n\n\n\n\n\n\nVideo\n\n\n\nR3 and some of its subspaces\n\n\nConsider the vectors \\((1,2,0)^\\intercal\\) and \\((1,0,1)^\\intercal\\) and \\((0,0,1)^\\intercal\\). Linear combine them with every scalar and you obtain a set of vectors. All these vectors fill the three dimensional space \\(\\mathbb{R}^3\\).\n\nIn special notation we have:\n\\[\n\\mathbb{R}^3 = span\\{(1,2,0)^\\intercal,(1,0,1)^\\intercal,(0,0,1)^\\intercal\\}\n\\]From the picture here can identify some subspaces of \\(\\mathbb{R}^3\\), we also write the equation that describes them, take them as facts for now, later, when we introduce matrices we justify them.\n\\[\n[\\textbf{pink-blue plane}] = span\\{(1,2,0), (0,0,1)\\}=\\{(a,2a,b)\\,\\,|\\,\\,a,b\\in\\mathbb{R}\\}\n\\]\n\\[\n[\\textbf{xy plane}] = span \\{(1,0,0),(0,1,0\\}=\\{(a,b,0)\\,\\,|\\,\\,a,b\\in\\mathbb{R}\\}\n\\]\n\\[\n[\\textbf{yellow line}] = span \\{(1,0,1)\\}=\\{(a,0,a)\\,\\,|\\,\\, a\\in\\mathbb{R}\\}\n\\]\nObservation 1: The vector space \\(\\mathbb{R}^3\\) results from summing vectors from the x0y plane and z axis subspaces. Thus any vector in \\(\\mathbb{R}^3\\) can be written as a sum of a vector from the first plus one of the second. We have special notation for this:\n\nDefinition 3 Assume we have two subspaces \\(U\\) and \\(W\\) of \\(\\mathbb{R}^3\\) (eg. the xoy plane and z axis) where the only common element is the origin \\(\\mathbf{0}\\). Then any vector \\(\\mathbf{r}\\) in \\(\\mathbb{R}^3\\) can be written by some appropriate sum of \\(\\mathbf{u}\\) and \\(\\mathbf{w}\\):\n\\[\n\\mathbf{r} = \\mathbf{u}+\\mathbf{w}\n\\]\nIn math language:\n\\[\n\\mathbb{R}^3 = \\{\\mathbf{u}+\\mathbf{v}\\,\\,|\\,\\, \\mathbf{u}\\in U \\,\\,\\text{and} \\,\\,\\mathbf{v}\\in V\\}\n\\]\nThe nick-name of this set is: \\(U\\oplus V\\). The direct sum. Reading \\(\\mathbb{R}^3=U\\oplus V\\) from right to left we see what we said above; reading from left to right we see the space being decomposed in subspaces, clearly this can be done in many ways:\n\\[\n\\mathbb{R}^3 = [\\textbf{xy plane}] \\oplus [\\textbf{0z line}] = [\\textbf{yellow line}]\\oplus [\\textbf{plane perpendicular to yellow line}]=\\dots\n\\]\n\nObservation 2: Let \\(A\\) and \\(B\\) be two (non parallel) planes containing the yellow-line, then the intersection of both yields the yellow line:\n\\[\n[\\textbf{yellow-line}] = A \\cap B\n\\]\nThis is important because it tells us that interception of vectors spaces yields new vector spaces.",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Vector spaces and subspaces"
    ]
  },
  {
    "objectID": "vector_spaces_and_subspaces.html#example-4",
    "href": "vector_spaces_and_subspaces.html#example-4",
    "title": "Vector spaces and subspaces",
    "section": "Example 4",
    "text": "Example 4\nConsider the vectors \\(\\begin{pmatrix} 1\\\\2\\end{pmatrix}\\) , \\(\\begin{pmatrix} 3\\\\0\\end{pmatrix}\\) and \\(\\begin{pmatrix} 0\\\\1\\end{pmatrix}\\). All linear combinations of these three vectors always yield a vector in \\(\\mathbb{R}^2\\), a two-dimensional vector space and not three-dimensional!\n\nNotice one of the three vectors is redundant.\n\\[\n\\mathbb{R}^2 = span\\{\\begin{pmatrix} 1\\\\2\\end{pmatrix},\\begin{pmatrix} 3\\\\0\\end{pmatrix},\\begin{pmatrix} 0\\\\3\\end{pmatrix}\\}=span\\{\\begin{pmatrix} 1\\\\2\\end{pmatrix},\\begin{pmatrix} 3\\\\0\\end{pmatrix}\\}\n\\]\n\n\n\n\n\n\nCommentary\n\n\n\nIn general a vector space \\(\\mathbb{V}\\) is a set of things with which remains invariant under linear combinations. Those things are what we call vectors. A vector is simply a member of this set. An array of numbers, like a column vector, is such an example; matrices, functions are other examples as we’ll see later.",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Vector spaces and subspaces"
    ]
  },
  {
    "objectID": "vector_spaces_and_subspaces.html#example-3-cont",
    "href": "vector_spaces_and_subspaces.html#example-3-cont",
    "title": "Vector spaces and subspaces",
    "section": "Example 3 (cont)",
    "text": "Example 3 (cont)\nFrom the pictures above we deduced what vectors we need to span to generate that space and from this choice we identified the form of the elements. Now lets assume we are given some form of the vectors, how do we know they constitute a vector space?\nWhether they do or do not form one is a matter of checking the definition of space, recall that a space is a collection of things which remain invariant under l.c.\nTo see this concretely, consider pink-blue plane:\n\\[\n\\{(a,2a,b)^\\intercal\\,\\,|\\,\\, a,b\\in\\mathbb{R}\\}\n\\]\nIs it a vector space?\nLet \\((a_1,2a_1,b_1)\\) and \\((a_2,2a_2,b_2)\\) be two generic vectors of the set, now take a generic l.c. \\(\\alpha(a_1,2a_1,b_1)+\\beta(a_2,2a_2,b_2)\\). Is this again an element of that set? If it is, it must have the form \\((a,2a,b)\\). Lets check if this is true:\n\\[\n\\alpha(a_1,2a_1,b_1)+\\beta(a_2,2a_2,b_2) = (\\alpha a_1+\\beta a_2,2(\\alpha a_1 +\\beta a_2),\\alpha b_1+\\beta b_2) = (\\spadesuit,2\\spadesuit,\\clubsuit)\n\\]\nIt has.\nBecause, the chosen elements and the l.c. were arbitrary we can conclude that this set is closed under l.c. and therefore constitute a vector space.",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Vector spaces and subspaces"
    ]
  },
  {
    "objectID": "vector_spaces_and_subspaces.html#exercises",
    "href": "vector_spaces_and_subspaces.html#exercises",
    "title": "Vector spaces and subspaces",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1 Solve 1.1 &gt; 3: If a span of \\(3v\\), \\((2,-1)\\) and \\(1/2(-2,4)\\) what can you say?\nSolve 1.1 &gt; 4 &gt; (e) Same question.\nSolve 1.4 &gt; 1",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Vector spaces and subspaces"
    ]
  },
  {
    "objectID": "two_special_subspaces.html",
    "href": "two_special_subspaces.html",
    "title": "Two special subspaces",
    "section": "",
    "text": "From a system of equation we obtained a matrix. In this section we will see how matrices define two important subspaces: the column space and nullspace of a matrix.\n\nNullspace of a matrix (kernel of a linear function)\n\n\n\n\n\n\nVideo\n\n\n\nHow to compute the nullspace\n\n\nThe nullspace of the matrix is the vector space generated by all solutions \\(\\mathbf{x}_N\\) of the equation:\n\\[ A\\mathbf{x}_N=\\mathbf{0} \\]\nThis equation is exactly the same as out test for independence/dependence!\nConsider again the matrix\n\\[\nA=\\begin{pmatrix}1 & 2 & 2 & 2 \\\\2 & 4 & 6 & 8 \\\\3 & 6 & 8 & 10 \\end{pmatrix}\n\\]\nSolving we find:\n\\[ \\begin{align}&\\begin{pmatrix}1 & 2 & 2 & 2 &\\bigm|0\\\\2 & 4 & 6 & 8 &\\bigm|0\\\\3 & 6 & 8 & 10 &\\bigm| 0\\end{pmatrix}\\overset{l_2' = l_2-2l_1}{\\longrightarrow}\\begin{pmatrix}1 & 2 & 2 & 2 &\\bigm| 0\\\\0 & 0 & 2 & 4 &\\bigm| 0\\\\3 & 6 & 8 & 10 &\\bigm| 0\\end{pmatrix}\\overset{l_3' = l_3-3l_1}{\\longrightarrow}\\begin{pmatrix}1 & 2 & 2 & 2 &\\bigm| 0\\\\0 & 0 & 2 & 4 &\\bigm| 0\\\\0 & 0 & 2 & 4 &\\bigm| 0\\end{pmatrix}\\\\&\\overset{l_3'=l_3-l_2}{\\longrightarrow}\\begin{pmatrix}1 & 2 & 2 & 2 &\\bigm| 0\\\\0 & 0 & 2 & 4 &\\bigm| 0\\\\0 & 0 & 0 & 0 &\\bigm| 0\\end{pmatrix}\\overset{l_2'=1/2l_2}{\\longrightarrow}\\begin{pmatrix}1 & 2 & 2 & 2 &\\bigm| 0\\\\0 & 0 & 1 & 2 &\\bigm| 0\\\\0 & 0 & 0 & 0 &\\bigm| 0\\end{pmatrix}\\overset{l_1'=l_1-2l_2}{\\longrightarrow}\\begin{pmatrix}\\mathbf{1} & 2 & 0 & -2 &\\bigm| 0\\\\0 & 0 & \\mathbf{1} & 2 &\\bigm| 0\\\\0 & 0 & 0 & 0 &\\bigm| 0\\end{pmatrix}\\end{align}  \\tag{1}\\]\nBy looking at the simplified system \\(A'\\) we see column 2 and column 4 depend on column 1 and 3. (focus on the pivots)\nImportant observation: the same column dependence found in \\(A'\\) holds for the columns of \\(A\\).\nWe can solve this problem in two other ways:\n\nway 1: In summary, use back substitution and promote the necessary variables to parameters.As an example, translate Equation 1 into the old notation set the free variables \\(y_N\\) and \\(w_N\\) as the parameters \\(a\\) and \\(b\\) respectively then solve for the dependent variables \\(x_N\\) and \\(z_N\\):\n\\[ \\begin{cases} x_N+2y_N-2w_N=0\\\\ z_N+2w_N =0\\\\ y_N=a\\\\ w_N = b \\end{cases} \\implies \\begin{cases} x_N=-2a+2b\\\\ z_N = -2b\\\\y_N=a\\\\ w_N = b \\end{cases} \\implies \\begin{pmatrix} x_N\\\\y_N\\\\z_N\\\\w_N \\end{pmatrix} = a \\begin{pmatrix} -2\\\\1\\\\0\\\\0 \\end{pmatrix} + b \\begin{pmatrix} 2\\\\0\\\\-2\\\\1 \\end{pmatrix} \\]\nway 2: To find solutions of this system, choose (any) values for the free variables \\(y_N\\) and \\(w_N\\) and then solve for \\(x_N\\) and \\(z_N\\). Why is this a good strategy? Since the \\(col_2\\) and \\(col_4\\) are dependent on \\(col_1\\) and \\(col_3\\) then \\(y_N\\times col_1\\) and \\(w_n \\times col_4\\) are also dependent, and thus we can find the appropriate \\(x_N\\) and \\(z_N\\) to cancel them.\nWhen choosing freely, at least choose something that simplify your calculations, for example set \\(y_N=1\\) and \\(w_N=0\\) and guess what is \\(x_N\\) and \\(z_N\\), we get:\n\\[ \\mathbf{x}_N =\\begin{pmatrix}-2\\\\1\\\\0\\\\0 \\end{pmatrix} \\]\nNow substitute \\(y_N=0\\) and \\(w_N=1\\) and guess the corresponding \\(x_N\\) and \\(z_N\\); the answer gives us:\n\\[ \\mathbf{x}_N=\\begin{pmatrix}2\\\\0\\\\-2\\\\1\\end{pmatrix} \\]\nNotice we did this twice because there are two free columns (dependent columns to cancel with the independent one)\nThe nullspace of the matrix \\(A\\) is composed by all linear combinations:\n\\[ a\\begin{pmatrix}-2\\\\1\\\\0\\\\0 \\end{pmatrix} +b\\begin{pmatrix}2\\\\0\\\\-2\\\\1 \\end{pmatrix} \\]\nThese two vectors constitute a basis for it.\nEither way 1 or 2 gives the same answer:\n\n\\[ a\\begin{pmatrix}-2\\\\1\\\\0\\\\0\\end{pmatrix}+b\\begin{pmatrix}2\\\\0\\\\-2\\\\1\\end{pmatrix} \\]\nThe nullspace of \\(A\\) and its basis:\n\\[ N(A) = span\\{(-2,1,0,0),(2,0,-2,1)\\} \\]\n\n\n\n\n\n\nCommentary\n\n\n\nThe way 1 and 2 are essentially the same, on 2 we choose specific values for the free variables and compute the corresponding non-free variables to get specific solution, which are then multiplied by \\(a\\) and \\(b\\); on way 1 we choose arbitrary values for the free variables and then solve for the non-free.\n\n\n\n\nColumns Space of a Matrix (image of a linear function)\n\n\n\n\n\n\nVideo\n\n\n\nHow to compute the column space\n\n\nThe column space is the vector space that is generated by taking all linear combinations of the columns of a matrix. For example, the column space \\(C(A)\\) of the matrix:\n\\[\nA=\\begin{pmatrix}1 & 2 & 2 & 2 \\\\2 & 4 & 6 & 8 \\\\3 & 6 & 8 & 10 \\end{pmatrix}\n\\tag{2}\\]\nis generated by\n\\[\nC(A)=span\\{\\begin{pmatrix}\n1\\\\2\\\\3\n\\end{pmatrix}\n,\n\\begin{pmatrix}2\\\\4\\\\6\\end{pmatrix}\n,\n\\begin{pmatrix}2\\\\6\\\\8\\end{pmatrix}\n,\n\\begin{pmatrix}2\\\\8\\\\10\\end{pmatrix}\\}\n\\]\nIt may happen, and it is the case as we shall see, that we need not all the columns of \\(A\\) to generate \\(C(A)\\), the reason being, some of the column vectors may depend on other columns, thus not providing no additional information. In other words, the columns of \\(A\\) may or not constitute a basis for \\(C(A)\\). Thus we also do not know, yet, the dimensionality of this space.\nTo construct a basis for \\(C(A)\\) we have to identify which column or columns are dependent and which are independent, we found by inspecting the \\(\\text{rref} A\\) in Equation 1, we know the column 1 and 3 are independent, hence the basis for the column space:\n\\[\nC(A)=span\\{\n\\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix},\\begin{pmatrix}2\\\\6\\\\8\\end{pmatrix}\\}\n\\]\n\n\n\n\n\n\nCommentary\n\n\n\nThe column space of \\(A\\) is different from that of \\(\\text{rref}\\,A\\).\n\n\n\n\nConclusion\n\nThe nullspace is the set of solutions of \\(A\\mathbf{x}_N=\\mathbf{0}\\), which we compute by simplifying the matrix \\(A\\) to the point \\(A'\\) (=rref of \\(A\\)) where we can see the dependent columns.\nBy knowing in \\(A'\\) which columns are dependent, we know which ones are independent, the corresponding columns in \\(A\\) in turn span the column space.\n\n\n\n\n\n\n\nCommentary\n\n\n\n\n\\(\\dim C(A) +\\dim N(A) = m\\), where \\(m\\) is the number of columns. It will be better intuitively to write \\(\\dim C(A^\\intercal) +\\dim N(A) = m\\) as we shall see later.\n\n\nExercises: 1.5.11,12",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Column space and nullspace"
    ]
  },
  {
    "objectID": "systems_from_a_new_view.html",
    "href": "systems_from_a_new_view.html",
    "title": "Lines, planes and hyperplanes",
    "section": "",
    "text": "We now to return to the observations:\nGoal: To understand the reason behind these inequalities. In doing so we will simultaneously understand how \\(r=r^*&lt;n\\) is related with lines, planes and hyperplanes.\nHow? we’ll look carefully at the structure of the computations involved in the equation \\(A\\mathbf{x}=\\mathbf{b}\\) (or \\(A'\\mathbf{x}=\\mathbf{b}'\\), \\(A'=\\text{rref}\\, A\\))",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "System from a new point of view"
    ]
  },
  {
    "objectID": "systems_from_a_new_view.html#looking-at-amathbfxmathbfb-from-a-new-point-of-view",
    "href": "systems_from_a_new_view.html#looking-at-amathbfxmathbfb-from-a-new-point-of-view",
    "title": "Lines, planes and hyperplanes",
    "section": "Looking at \\(A\\mathbf{x}=\\mathbf{b}\\) from a new point of view",
    "text": "Looking at \\(A\\mathbf{x}=\\mathbf{b}\\) from a new point of view\n\n\n\n\n\n\nVideo\n\n\n\nA new view\n\n\nReturn to the equations:\n\\[\nA\\mathbf{x}=\\mathbf{b}\n\\leftrightsquigarrow\n\\begin{pmatrix}1 & 2 & 2 & 2 \\\\2 & 4 & 6 & 8 \\\\3 & 6 & 8 & 10 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\\\z\\\\w\\end{pmatrix}=\\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix}\n\\leftrightsquigarrow\n\\begin{pmatrix}1 & 2 & 2 & 2 &\\bigm|1\\\\2 & 4 & 6 & 8 &\\bigm|2\\\\3 & 6 & 8 & 10 &\\bigm| 3\\end{pmatrix}\n\\tag{1}\\]\nLooking at Equation 1 we would ask: what is the column vector \\(\\mathbf{x}\\) which when multiplied by the matrix \\(A\\) yields the column vector \\(\\mathbf{b}\\)?\nNow rewrite the system of equation in a different point of view:\n\\[\n\\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix}x+\\begin{pmatrix}2\\\\4\\\\6\\end{pmatrix}y+\\begin{pmatrix}2\\\\6\\\\8\\end{pmatrix}z+\\begin{pmatrix}2\\\\8\\\\10\\end{pmatrix}w = \\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix}\n\\tag{2}\\]\nLooking at Equation 2 we would ask: what is the linear combination of the columns that leads to the vector \\(\\mathbf{b}\\) on the rhs? [Not every columns is independent!]\nThis second perspective on a system of equations allow us to make the following:\n\nKey observation: this linear combination is only possible, provided the \\(\\mathbf{b}\\) vector is in the column space of \\(A\\)! Meaning \\(\\mathbf{b}\\in C(A)\\).\nKey observation paraphrased differently: going back to Equation 1, focus on the extended matrix version of the system of the equations and note, that, when we extend the matrix, i.e., when we append the \\(\\mathbf{b}\\) vector on the rhs, the column space must not change, for otherwise the \\(\\mathbf{b}\\) was not in the column space. This is to say:\n\n\\[\nr =r^*\n\\]\nThis same key observation can be obtained from \\(A'\\mathbf{x}=\\mathbf{b}'\\):\n\n\\[\n\\begin{align}\n\\begin{pmatrix}\n1 & 2 & 0 & -2 &\\bigm| 1\\\\\n0 & 0 & 1 & 2 &\\bigm| 0\\\\\n0 & 0 & 0 & 0 &\\bigm| 0\n\\end{pmatrix}\n\\end{align}\n\\tag{3}\\]\nIf \\(\\mathbf{b}'\\in C(A')\\), that is to say when \\(r=r^*\\), then there is a solution. Thus if \\(\\mathbf{b}\\in C(A)\\) then \\(\\mathbf{b}'\\in C(A')\\). Note how much easier is to check the later.\nHow would we find \\(x,y,z,w\\) that satisfy the equations above?\nWay 1: Use back substitution. And promoting the necessary variables to parameters.\nWay 2: Follow these three step procedure:\n\nFirst, find one particular solution \\(\\mathbf{x}_P\\) for the system.\nSecond, find the \\(N(A)\\) by solving \\(A\\mathbf{x}_N=\\mathbf{0}\\).\nThird, The complete solution is of the form \\(\\mathbf{x}_P+\\mathbf{x}_N\\).",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "System from a new point of view"
    ]
  },
  {
    "objectID": "systems_from_a_new_view.html#first-how-to-compute-mathbfx_p",
    "href": "systems_from_a_new_view.html#first-how-to-compute-mathbfx_p",
    "title": "Lines, planes and hyperplanes",
    "section": "First, how to compute \\(\\mathbf{x}_P\\)?",
    "text": "First, how to compute \\(\\mathbf{x}_P\\)?\n\nSimplify the system as much as you can using Elimination and the pivots, we already did this and obtained Equation 3.\nIdentify the dependent and independent columns of \\(A'\\). We do this by visual inspection of the simplified system: The independent column are where the pivots lie. The dependent columns (also known as free columns) are the remaining ones.\nThe unknowns multiplying the dependent columns in Equation 3 are \\(y\\) and \\(w\\); these are known as free unknowns.\nTo find a particular solution \\(\\mathbf{x}_P\\) of the system, set the free unknowns to zero:\n\\[\ny=0\\qquad w=0\n\\]\n\\[\n\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}x_P\n+\\begin{pmatrix}2\\\\0\\\\0\\end{pmatrix}0\n+\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}z_P\n+\\begin{pmatrix}-2\\\\2\\\\0\\end{pmatrix}0 = \\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}\n\\tag{4}\\]\nNow solve Equation 4 for \\(x_P\\) and \\(z_P\\); by inspection:\n\\[\nx_P=1 \\qquad z_P=0\n\\]\nThe particular solution is\n\\[ \\mathbf{x}_P=\\begin{pmatrix} 1\\\\0\\\\0\\\\0 \\end{pmatrix} \\]",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "System from a new point of view"
    ]
  },
  {
    "objectID": "systems_from_a_new_view.html#second-how-to-compute-mathbfx_n",
    "href": "systems_from_a_new_view.html#second-how-to-compute-mathbfx_n",
    "title": "Lines, planes and hyperplanes",
    "section": "Second, how to compute \\(\\mathbf{x}_N\\)?",
    "text": "Second, how to compute \\(\\mathbf{x}_N\\)?\nCompute the nullspace of \\(A\\) by solving \\(A\\mathbf{x}_N=\\mathbf{0}\\), after simplification (through elimination) we arrive at \\(A'\\mathbf{x}_N=\\mathbf{0}\\), i.e.\n\\[\n\\begin{pmatrix}1 & 2 & 0 & -2 &\\bigm| 0\\\\0 & 0 & 1 & 2 &\\bigm| 0\\\\0 & 0 & 0 & 0 &\\bigm| 0\\end{pmatrix}\n\\tag{5}\\]\nSolving (done previously) we obtain the nullspace of the matrix \\(A\\) which is composed by all linear combinations:\n\\[\na\\begin{pmatrix}-2\\\\1\\\\0\\\\0 \\end{pmatrix} +b\\begin{pmatrix}2\\\\0\\\\-2\\\\1 \\end{pmatrix}\n\\]\nGeometrically: this is plane containing the origin living in four-dimension space. This is a vector space.",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "System from a new point of view"
    ]
  },
  {
    "objectID": "systems_from_a_new_view.html#third-the-complete-solution-of-amathbfxmathbfb",
    "href": "systems_from_a_new_view.html#third-the-complete-solution-of-amathbfxmathbfb",
    "title": "Lines, planes and hyperplanes",
    "section": "Third, the complete solution of \\(A\\mathbf{x}=\\mathbf{b}\\)",
    "text": "Third, the complete solution of \\(A\\mathbf{x}=\\mathbf{b}\\)\nSo far we know a particular solution \\(A\\mathbf{x}_P=\\mathbf{b}\\) and we know the nullspace \\(A\\mathbf{x}_N=\\mathbf{0}\\).\nThus the solution of \\(A\\mathbf{x}=\\mathbf{b}\\) are of the form:\n\\[\n\\begin{pmatrix} 1\\\\0\\\\0\\\\0 \\end{pmatrix} +a\\begin{pmatrix}-2\\\\1\\\\0\\\\0 \\end{pmatrix} +b\\begin{pmatrix}2\\\\0\\\\-2\\\\1 \\end{pmatrix}\n\\tag{6}\\]\nand the solution set is:\n\\[\n\\begin{pmatrix} 1\\\\0\\\\0\\\\0 \\end{pmatrix} +span\\{\\begin{pmatrix}-2\\\\1\\\\0\\\\0 \\end{pmatrix} ,\\begin{pmatrix}2\\\\0\\\\-2\\\\1 \\end{pmatrix}\\}\n\\]\nGeometrically: this is a plane in four-dimension space that does not contain the origin, thus it is not a vector space.\nWhy is this the solution?\nIf \\(\\mathbf{x}_P\\) is a particular solution of \\(A\\mathbf{x}=\\mathbf{b}\\) then adding to it any \\(\\mathbf{x}_N\\) will not make a difference.\nJustification: \\(A(\\mathbf{x}_P+\\mathbf{x}_N)=A\\mathbf{x}_P+A\\mathbf{x}_N=\\mathbf{b}+\\mathbf{0}=\\mathbf{b}\\)\nThus, the system \\(A\\mathbf{x} =\\mathbf{b}\\) has an infinite number of solutions, all of the form \\(\\mathbf{x}=\\mathbf{x}_P+\\mathbf{x}_N\\).",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "System from a new point of view"
    ]
  },
  {
    "objectID": "systems_from_a_new_view.html#a-system-with-one-solution",
    "href": "systems_from_a_new_view.html#a-system-with-one-solution",
    "title": "Lines, planes and hyperplanes",
    "section": "A system with one solution",
    "text": "A system with one solution\nUsing elimination we showed:\n\\[\n\\begin{pmatrix}1 & 1 & -1 &\\bigm| & 1\\\\2 & -1 & 2 &\\bigm| & 9\\\\1 & 2 & -1 &\\bigm| & 0\\end{pmatrix} \\iff \\begin{pmatrix}1 & 0 & 0 &\\bigm| & 2\\\\0 & 1 & 0 &\\bigm| & -1\\\\0 & 0 & 1 &\\bigm| & 1\\end{pmatrix}\n\\]\nClearly there are no free column and thus no free coefficients to set to zero. The matrix has the form:\n\\[\n\\begin{pmatrix}I\\end{pmatrix}\n\\]Solving for the remaining we get:\n\\[\n\\mathbf{x}_P=\\begin{pmatrix}2\\\\-1\\\\1\\end{pmatrix}\n\\]\nThe nullspace only contains the \\(\\mathbf{0}\\) vector. There is no \\(F\\) block, since \\(n-r=0\\).\nThe solution of the system is just the particular solution.\n\\[\n\\mathbf{x}=\\mathbf{x}_P+\\mathbf{x}_N=\\begin{pmatrix}2\\\\-1\\\\1\\end{pmatrix}+\\begin{pmatrix}0\\\\0\\\\0\\end{pmatrix}=\\begin{pmatrix}2\\\\-1\\\\1\\end{pmatrix}\n\\]",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "System from a new point of view"
    ]
  },
  {
    "objectID": "systems_from_a_new_view.html#a-system-with-no-solution",
    "href": "systems_from_a_new_view.html#a-system-with-no-solution",
    "title": "Lines, planes and hyperplanes",
    "section": "A system with no solution",
    "text": "A system with no solution\nPreviously we computed:\n\\[\n\\begin{pmatrix}2 & -1 &\\bigm| & 8\\\\2 & 1  &\\bigm| & 4\\\\1 & 1  &\\bigm| & -1\\end{pmatrix}\\iff \\begin{pmatrix}1 & 0 &\\bigm| & 5\\\\0 & 1 &\\bigm| & -6\\\\0 & 0 &\\bigm| & -8\\end{pmatrix}\n\\]\nThere is no particular solution because \\(\\mathbf{b}'\\not\\in C(A)\\), meaning \\(r&lt;r^*\\). The null space is \\(\\{\\mathbf{0}\\}\\). The system as has no solution",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "System from a new point of view"
    ]
  },
  {
    "objectID": "sol_to_exer_withperequisites.html",
    "href": "sol_to_exer_withperequisites.html",
    "title": "solution",
    "section": "",
    "text": "\\(A(x)\\coloneqq [\\text{$x$ is a vowel}]\\) makes it true; \\(A(x)\\coloneqq [\\text{$x$ is a consonant}]\\) makes it false.\n\\(A(x)\\coloneqq [\\text{$x$ is a consonat}]\\) is a solution of \\(\\exists x : A(x)\\). but \\(A(x)\\coloneqq [\\text{$x$ is $v$}]\\) is not.\n\\([\\text{$x$ is below $i$}]\\) for true and \\([\\text{$x$ is above $a$}]\\) for false.\nFor the second part of the exercise we find:\n\\[\n\\begin{equation}\\begin{split}&\\forall x \\in \\{a,b,c\\}:\\exists ! y\\in \\{x,y\\}\\in F\\\\&\\iff\\\\&\\left(((a,1)\\in F)\\dot{\\lor}((a,2)\\in F)\\right) \\land \\left(((b,1)\\in F)\\dot{\\lor}((a,2)\\in F)\\right)\\land\\left(((c,1)\\in F)\\dot{\\lor}((c,2)\\in F)\\right)\\end{split}\\end{equation}\n\\]\nThe true values are: (1) True, (2) True, (3) False, (4) True"
  },
  {
    "objectID": "qu_131.html",
    "href": "qu_131.html",
    "title": "asymptotic to MARS",
    "section": "",
    "text": "Exercise 1  \n\nWith \\(X\\coloneqq \\{\\alpha,\\beta,\\gamma,\\delta\\}\\) and \\(Y\\coloneqq \\{1,2,3,4\\}\\) create relations, two that are not functions and two that are. Make sure you create function, which use all \\(X\\) and others that do not.\n\n\n\nSolution 1. The main point of this exercise is to show that \\(X\\) and \\(Y\\) are the starting assumptions from which many relations stem, some are functions while others are not."
  },
  {
    "objectID": "pratical_guide_cm.html",
    "href": "pratical_guide_cm.html",
    "title": "A Pratical Guide to Condensed Matter Physics",
    "section": "",
    "text": "Einstein Model"
  },
  {
    "objectID": "pratical_guide_cm.html#topics",
    "href": "pratical_guide_cm.html#topics",
    "title": "A Pratical Guide to Condensed Matter Physics",
    "section": "",
    "text": "Einstein Model"
  },
  {
    "objectID": "pol_division.html",
    "href": "pol_division.html",
    "title": "Polynomial division",
    "section": "",
    "text": "Look at the two divisions below:\n\\[\n\\begin{equation}\n\\frac{x^3+1}{x+2}\\qquad \\text{vs}\\qquad \\frac{1001}{12}\n\\end{equation}\n\\tag{1}\\]\nWhat do they have in common?\nIn this notes we see 9 important ideas behind division of polynomials and integers. We’ll start by integers since its easier.",
    "crumbs": [
      "Brief Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Polynomial division"
    ]
  },
  {
    "objectID": "pol_division.html#analogy-between-polynomial-notation-and-integer-notation",
    "href": "pol_division.html#analogy-between-polynomial-notation-and-integer-notation",
    "title": "Polynomial division",
    "section": "1) Analogy between polynomial notation and integer notation",
    "text": "1) Analogy between polynomial notation and integer notation\nMany of you may have forgotten, but the position of watch integer in the symbol \\(1001\\) has a meaning, specifically:\n\\[\n1001 = 1\\cdot 10 ^3+0\\cdot10^2+0\\cdot10^1+1\\cdot10^0\n\\]\nThe left hand side is just a super compact way of writing the right side.\nSimilarly, \\(12\\) is just a symbol for \\(1\\cdot 10^1+2\\cdot 10^0\\).\nWith this reminder we can see clearly the connection between the numerators and denominators in Equation 1 .\n\\[\n\\begin{cases}\nx^3+1 = 1\\cdot x^3+0\\cdot x^2 + 0 \\cdot x^1 + 1\\cdot x^0\\\\\n1001 = 1\\cdot 10 ^3+0\\cdot10^2+0\\cdot10^1+1\\cdot10^0\n\\end{cases}\n\\]\nand\n\\[\n\\begin{cases}\nx+2 = 1 \\cdot x^1 + 2\\cdot x^0\\\\\n12 = 1\\cdot10^1+2\\cdot10^0\n\\end{cases}\n\\]\nThe connection is: powers of \\(10\\) are replaced by powers of \\(x\\).\nWith such a bridge, we may expect the practical realization of this calculation will also be similar. You could have done polynomial division on the 4th grade.",
    "crumbs": [
      "Brief Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Polynomial division"
    ]
  },
  {
    "objectID": "pol_division.html#to-divide-two-integers-is-to-not-only-find-the-quotient-but-also-find-the-remainder",
    "href": "pol_division.html#to-divide-two-integers-is-to-not-only-find-the-quotient-but-also-find-the-remainder",
    "title": "Polynomial division",
    "section": "2) To divide two integers is to NOT ONLY find the quotient but also find the remainder!!",
    "text": "2) To divide two integers is to NOT ONLY find the quotient but also find the remainder!!\nLets see this with a picture.\n\nImagine a chocolate bar of length \\(p=22\\) with grooves at each unit. You want to divide it, cutting only at the grooves, by your \\(d=3\\) friends. You don’t want to cheat anyone (do you?), so you must give pieces of the same length and, of course, also give them the maximum possible piece-length.\nYou might immediately think to measure the chocolate bar length, pull a calculator and compute the division\n\\[\n\\frac{22}{3}\\approx 7.3\n\\]\nThen measure and mark the cuts at \\(7.3\\) and \\(14.6\\); then cut the thing and give then. See picture above.\nWith minimal error in the calculation, because the division is only approximately \\(7.3\\) and error in the actual measurements, you succeeded. Congratulations….but this is not what was being asked!\nI remind you: you can only cut at the grooves that already came with the chocolate, they are spaced by \\(1\\)unit.\nIn this case a question naturally arises, what is the maximum piece-length \\(q\\), you can give your friends? Certainly it is not the real number \\(7.3\\), the length \\(q\\) must be an integer.\nThe following picture answers the problem (think about it before looking at it)\n\nThe unique solution is to cut at \\(7\\) and \\(14\\) and \\(21\\) units. Give your \\(d=3\\) friends chocolate pieces of length \\(q=7\\); a small one of length \\(1\\) remains, you can eat that as a prize.\nThe takeaway is that when you divide two things and the result you desire must be an integer thing, then a remaining thing is left (for you).\nI would like now to make things more complicated and write the statement above in proper math language, its a theorem:\n\n\n\n\n\n\nTheorem\n\n\n\nLet \\(p\\) and \\(d\\) be two integers where \\(p\\geq d\\). Then there exists an integer \\(r\\) such that \\(0\\leq r&lt;d\\) and an integer \\(q\\geq 0\\) such that\n\\[ p=qd+r \\]\n\n\nLets break down the meaning of this theorem. We start by requiring that \\(p\\) and \\(d\\) be integers (a chocolate bar has an integer number of pieces and you have an integer number of friends - right?) and thus positive, we also require that \\(p\\) is larger or equal to \\(d\\).\nIf we imagine \\(p\\) as the length of a line segment (our chocolate bar), this length is the sum of two length\n\\[\np=qd + r\n\\]\nThe \\(qd\\) length, is the length of each piece you give your friends times the number of friends, thus \\(qd\\) is the part of the chocolate bar you give away.\nThe \\(r\\) length, is the length of the remaining piece, you keep that!\nSumming both, \\(qd +r\\) yields the original chocolate bar length \\(p\\).\n\nTo divide \\(p\\) by \\(d\\) subjected to the fact we want an integer result, means to find the integer result \\(q\\) and then the integer remainder \\(r\\).\nIn practice, when we seek for the value of \\(q\\), we seek for the highest possible \\(q\\), that way ensures the smallest length \\(r\\). By smallest I mean, its length \\(r\\) is not divisible by \\(d\\) - Please STOP here and ensure you understand this!\nWe can guarantee that \\(r\\) is not divisible by \\(d\\) by stating that \\(r\\) must be smaller than \\(d\\), thus the requirement \\(0\\leq r&lt;d\\) in the theorem above.",
    "crumbs": [
      "Brief Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Polynomial division"
    ]
  },
  {
    "objectID": "pol_division.html#the-key-strategy-behind-computing-the-division-of-integers-and-an-example",
    "href": "pol_division.html#the-key-strategy-behind-computing-the-division-of-integers-and-an-example",
    "title": "Polynomial division",
    "section": "3) The key strategy behind computing the division of integers and an example",
    "text": "3) The key strategy behind computing the division of integers and an example\nTo compute the division of the integer \\(1001\\) by \\(12\\) under the constrain that the result is also an integer is to compute the values of \\(q\\) and \\(r\\) that obey the following equation:\n\\[ 1001 = q \\times 12 + r \\qquad 0\\leq r&lt;12 \\]\nHow do we proceed in finding these numbers? The key strategy is very simple: guess them!\nIts not a random guess, this is not a lottery, its an educated guess; educated by clues provided by the problem itself.\nSee the following line of reasoning (and guesses) to get the idea:\n\nStep 1: decompose \\(1001\\) and \\(12\\) into powers of \\(10\\). This a clue that will to aid (educate) our guesses!\n\n\\[\n\\begin{cases}\n1001 = 1\\cdot 10^3+0\\cdot 10^2 +0\\cdot 10^1+1\\cdot 10^0 = 1\\cdot 10^3 +1\\cdot 10^0\\\\\n12 = 1\\cdot 10^1 + 2\\cdot 10^0\n\\end{cases}\n\\]\n\nStep 2: Observe both sides of the equation:\n\\[ 1\\cdot 10^3 +1 = q\\cdot(10+2)+r  \\tag{2}\\]\nIt is very useful to identify what are the largest numbers on both sides of the equation: the largest value on the left side is \\(1\\cdot 10^3\\), the largest value on the right side is \\(q\\cdot 10\\). Hence we expect that whatever \\(q\\) is, it must, when multiplied by \\(10\\), to give a number with is close or equal to \\(1\\cdot 10^3\\), for otherwise, balancing the sides of the equation is impossible. We should balance first these two terms, then we deal with the smaller ones.\nOur guess of \\(q\\) must be guided by this observation.\nStep 3: Guess a value of \\(q\\) which makes large terms similar or equal, i.e., \\(q\\cdot 10\\) close to \\(1\\cdot 10^3\\). There are many possible guesses and all of them would work! Here is one possibility \\(q=10^2\\).\nStep 4: Now that we have a value for \\(q\\) lets see how large is the remainder.\nSubstituting \\(q=10^2\\) into Equation 2 gives:\n\\[ \\begin{align} &1\\cdot 10^3 +1 = 1\\cdot 10^3 +2 \\cdot 10^2 + r\\\\ \\implies&r=-2\\cdot10^2+1 \\end{align} \\]\nThe remainder is negative and this presents no problem, what we really want is the final remainder to be positive. The remainder is also larger (in absolute value) than the divisor \\(12\\). This means the remainder is still divisible by \\(12\\), hence we proceed into computing the division of \\(-2\\cdot 10 +1\\) by \\(12\\), this is what we’ll do in step 5. To end step 4 let us summarize in a single expression what we know so far:\n\\[\n1\\cdot 10^3+1=\\overbrace{10^2}^q\\cdot(10+2)+\\overbrace{(-2\\cdot 10^2+1)}^r\n\\tag{3}\\]\nStep 5: Write the division of the remainder \\(-2\\cdot 10+1\\) by \\(12\\):\n\\[\n-2\\cdot 10^2+1 = q'\\cdot(10+2)+r'\n\\]\nand analyse the equation.\nOur goal now is to guess \\(q'\\) that makes \\(r'\\) as small as possible, we achieve that by balancing the largest terms on both sides of the equation, in this case we want to guess a \\(q'\\) that when multiplied by \\(10\\) is close to \\(-2\\cdot 10^2\\), the largest number on the left.\nStep 6: Guess \\(q'=-2\\cdot 10\\) and then substitute into the equation:\n\\[ \\begin{align} &-2\\cdot 10^2+1 = -2\\cdot 10^2 -4\\cdot 10 + r'\\\\ \\implies &r'=4\\cdot 10+1 \\end{align} \\]\nThe remainder is \\(4\\cdot 10+1\\) and is still larger than \\(12\\), thus divisible by \\(12\\), we’ll do this calculation in step 7. Let’s end this step by summarizing what we know so far:\n\\[\n\\begin{cases}\n1\\cdot 10^3+1=\\overbrace{10^2}^q\\cdot(10+2)+\\overbrace{(-2\\cdot 10^2+1)}^r\\\\\n-2\\cdot10^2+1 =\\overbrace{(-2\\cdot10)}^{q'}\\cdot(10+2)+\\overbrace{(4\\cdot10+1)}^{r'}\n\\end{cases}\n\\]\nStep 7: We repeat the process. Write the equation for the division of the current remainder \\(4\\cdot 10+1\\) by \\(12\\), analyse it and guess the quotient \\(q''\\) that makes \\(r''\\) as small as possible:\n\\[\n4\\cdot 10 +1 = q''\\cdot (10+2)+r''\n\\tag{4}\\]\nGuessing \\(q''=4\\) we balance the largest term on the lhs. The corresponding remainder is:\n\\[\nr'' = -7\n\\]\nSince the remainder is smaller than \\(12\\) the guessing process ends here.\nOnce again we summarize everything we know so far:\n\\[\n\\begin{cases}\n1\\cdot 10^3+1=\\overbrace{10^2}^q\\cdot(10+2)+\\overbrace{(-2\\cdot 10^2+1)}^r\\\\\n-2\\cdot10^2+1 =\\overbrace{(-2\\cdot10)}^{q'}\\cdot(10+2)+\\overbrace{(4\\cdot10+1)}^{r'}\\\\\n4\\cdot 10+1 = \\overbrace{4}^{q''}\\cdot(10+2)+\\overbrace{(-7)}^{r''}\n\\end{cases}\n\\tag{5}\\]\nStep 7: Use Equation 5 to write\n\\[\n1\\cdot 10^3 +1 = (10^2-2\\cdot 10+4)\\cdot 12 -7\n\\]\nwhich simplifies into:\n\\[\n1001=84\\cdot 12 -7\n\\tag{6}\\]\nStep 8: Our final remainder, the \\(-7\\) in Equation 6 is negative, to make it positive we resort to a trick: add \\(0=12-12\\) to the rhs:\n\\[ 1001=(84-1)\\cdot 12+(12-7) \\]\nIn conclusion:\n\\[ 1001=83\\cdot 12 +5 \\]\nThe division is complete, our quotient is \\(q=83\\) and the remainder is the positive number \\(5\\).\n\n\n\n\n\n\n\nComment\n\n\n\nThe division of \\(1001\\) by \\(12\\) can be done in many ways, all ending in the result \\(1001=83\\cdot 12 +5\\). What distinguishes the different ways are the guesses of \\(q\\), \\(q''\\), etc one makes. Among all this approaches we find one very well known by you, the long division of integers, which you learned in elementary school.\nThe particular sequence of steps we followed above was chosen so that it can be compared with the sequence of step in dividing \\(x^3+1\\) by \\(x+2\\).",
    "crumbs": [
      "Brief Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Polynomial division"
    ]
  },
  {
    "objectID": "pol_division.html#to-divide-two-polynomials-is-to-not-only-find-the-quotient-but-also-find-the-remainder",
    "href": "pol_division.html#to-divide-two-polynomials-is-to-not-only-find-the-quotient-but-also-find-the-remainder",
    "title": "Polynomial division",
    "section": "4) To divide two polynomials is to NOT ONLY find the quotient but also find the remainder!!",
    "text": "4) To divide two polynomials is to NOT ONLY find the quotient but also find the remainder!!\nThe division of polynomials is analogous to the division of integers, the division of \\(1001/12\\) made us to seek the \\(q\\) and \\(r\\) that satisfy the equation \\(1001 = q\\times 12 +r\\), while having the \\(r\\) as small as possible, which means \\(0\\leq r &lt;12\\), so that the remainder is not divisible by \\(12\\).\nThe division of polynomials, for example \\((x^3+1)/(x+2)\\) , requires to find the function \\(q(x)\\) and \\(r(x)\\) that obey\n\\[ x^3+1=q(x)\\times(x+2)+r(x) \\qquad 0\\leq\\deg r &lt; 1   \\tag{7}\\]\nThe constraint \\(0\\leq\\deg r&lt;1\\) ensures the degree of the remainder is the smallest, meaning in this case, not divisible by \\(x+2\\) whose degree is \\(1\\).\nThe following theorem summarizes the problem to solve:\n\n\n\n\n\n\nTheorem\n\n\n\nLet \\(f\\) and \\(g\\) be non-zero polynomials where \\(\\deg f \\geq \\deg g\\) (this assumption is analogous to saying \\(1001&gt;12\\)). Then there exist polynomials \\(q\\), \\(r\\) such that:\n\\[ f(x)=q(x)g(x)+r(x) \\qquad 0\\leq\\deg r &lt; \\deg g \\quad\\text{(Analogous to $0\\leq r &lt; 12$)} \\]\n\n\nThe ideas and strategies presented above for division of integers can be employed to the division of polynomials.",
    "crumbs": [
      "Brief Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Polynomial division"
    ]
  },
  {
    "objectID": "pol_division.html#the-key-strategy-behind-computing-the-division-of-polynomials-and-an-example",
    "href": "pol_division.html#the-key-strategy-behind-computing-the-division-of-polynomials-and-an-example",
    "title": "Polynomial division",
    "section": "5) The key strategy behind computing the division of polynomials and an example",
    "text": "5) The key strategy behind computing the division of polynomials and an example\nAs usual we guess what \\(q(x)\\) might be and then compute the corresponding remainder \\(r(x)\\). By checking the constraint \\(0\\leq \\deg r&lt;1\\) at each stage of the guessing process, we decide whether to terminate it or keep going. Let us see this in practice:\n\nStep 1: Observe both sides of the equation:\n\\[ x^3+1=q(x)\\times(x+2)+r(x) \\qquad 0\\leq\\deg r &lt; 1   \\tag{8}\\]\nand identify the largest powers: on the lhs we have \\(x^3\\) and on the rhs we have \\(q(x) \\cdot x\\). This is useful information, because we want to match the largest powers on either side of the equation. This is the only way the equal sign will hold.\nStep 2: We want to guess what \\(q(x)\\) based on this matching observation we did at step 1, hence \\(q(x)=x^2\\) :\nSubstituting into Equation 8, we compute the remainder of the current guess:\n\\[ \\begin{align} &x^3+1=x^2\\times(x+2)+r(x)\\\\ \\implies &r(x) = -2x^2+1 \\end{align} \\]\nIs the constraint that the remainder is as “small” as possible, \\(0\\leq \\deg r &lt;1\\), satisfied by this remainder? We clearly see from the \\(x^2\\) power that its degree is \\(2\\), the constraint is not satisfied and that means this remainder is divisible by \\(x+2\\).\nAs we did for the integer division we summarize our calculations:\n\\[\nx^3+1 = x^2\\cdot(x+2)+\\overbrace{(-2x^2+1)}^r\n\\]\n\n\n\nStep 3: The division \\((-2x^2+1)/(x+2)\\) is done by guessing the quotient \\(q'(x)\\) and the remainder \\(r'(x)\\):\n\\[ -2x^2+1 = q'(x)(x+2)+r'(x)  \\tag{9}\\]\nWe proceed as we always did above and analyze the equation before attempting any guess. The highest degree terms in either side of the equation are \\(-2x^2\\) and \\(q'(x)\\cdot x\\), we must match these, this will guide our guess.\nStep 4: To match the highest powers of the equation, set \\(q'(x)=-2x\\)\nSubstituting into Equation 9 we compute the remainder:\n\\[ \\begin{align} &-2x^2 +1 =-2x(x+2)+r'(x)\\\\ \\implies &r'(x) = 4x+1 \\end{align} \\]\nAgain, the degree of the current remainder still does not obey \\(0\\leq \\deg r' &lt;1\\), thus the remainder is divisible by \\(x+2\\). Before doing this calculation we summarize calculations:\n\\[\n\\begin{cases}\nx^3+1 = x^2\\cdot(x+2)+\\overbrace{(-2x^2+1)}^r\\\\\n-2x^2+1 = -2x(x+2)+\\overbrace{(4x+1)}^{r'}\n\\end{cases}\n\\]\nStep 5: Division of \\(4x+1\\) by \\(x+2\\) means to solve the equation:\n\\[ 4x+1 = q''(x)(x+2)+r''(x)  \\tag{10}\\]\nWe should match \\(q''(x)\\cdot x\\) with \\(4x\\) - the higher power term on the lhs.\nStep 6: Guess \\(q''(x)=4\\) and substitute into Equation 10 to get:\n\\[ \\begin{align} &4x+1 = 4(x+2)+r''(x)\\\\ \\implies & r''(x)=-7 \\end{align} \\]\nFinally we arrive at \\(\\deg(-7)=0\\) as we needed by the condition \\(0\\leq\\deg r &lt; 1\\), the process of division end here.\nCollecting results we have:\n\\[\n\\begin{cases}\nx^3+1 = x^2\\cdot(x+2)+\\overbrace{(-2x^2+1)}^r\\\\\n-2x^2+1 = -2x(x+2)+\\overbrace{(4x+1)}^{r'}\\\\\n4x+1 = 4(x+2)+\\overbrace{(-7)}^{r''}\n\\end{cases}\n\\tag{11}\\]\nStep 7: From Equation 11 we arrive at the result of our division:\n\\[ \\begin{align} x^3+1&=\\overbrace{x^2}^{q(x)}\\times(x+2)+\\overbrace{(-2x^2+1)}^{r(x)}\\\\ &=\\overbrace{x^2}^{q(x)}\\times (x+2) + \\overbrace{(-2x)}^{q'(x)}(x+2) +\\overbrace{(4x+1)}^{r'(x)}\\\\ &=\\overbrace{x^2}^{q(x)}\\times (x+2) + \\overbrace{(-2x)}^{q'(x)}(x+2)+\\overbrace{(+4)}^{q''(x)}(x+2) +\\overbrace{(-7)}^{r''(x)}\\\\ &=(x^2-2x+4)(x+2)-7 \\end{align} \\]\n\nWe conclude that:\n\\[ x^3+1 = (x^2-2x+4)(x+2)-7 \\]\nCompare this result with Equation 6 !!!",
    "crumbs": [
      "Brief Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Polynomial division"
    ]
  },
  {
    "objectID": "pol_division.html#polynomial-division-is-useful-in-polynomial-factoring",
    "href": "pol_division.html#polynomial-division-is-useful-in-polynomial-factoring",
    "title": "Polynomial division",
    "section": "6) Polynomial division is useful in polynomial factoring",
    "text": "6) Polynomial division is useful in polynomial factoring\nThe decomposition\n\\[ f(x)=q(x)g(x)+r(x) \\]is key in factoring polynomials.\n\nFactoring polynomials\nConsider \\(f(x) = x^2-1\\). We know by inspection that \\(f(1)=0\\) and using our division procedures above we find:\n\\[ \\overbrace{x^2-1}^f = \\overbrace{(x+1)}^q\\overbrace{(x-1)}^g+\\overbrace{0}^r \\]\nThis simple example is a particular case of the following fact:\n\nA very important observation: If somehow we know a zero of \\(f\\) the division of \\(f(x)\\) by \\(x-\\text{zero of $f$ }\\) yields a zero remainder!\nNow, consider a more complicated polynomial \\(f\\):\\[ f(x) = x^4 -x^3-3x^2+5x-2 \\]\nImagine, that we know it has a zero at \\(x=1\\), an observation which could be arrived by inspecting its graph. Using polynomial division we find, as expected, a complete division, nothing remains, \\(r=0\\):\n\\[ \\overbrace{x^4 -x^3-3x^2+5x-2}^f = \\overbrace{(x^3-3x+2)}^q(x-1)+\\overbrace{0}^r \\]\nBut inspection of \\(q\\) reveals that \\(q(1)=0\\), the quotient has a zero as well, using again polynomial division we find \\(r'=0\\):\n\\[ \\overbrace{x^3-3x+2}^q = \\overbrace{(x^2+x-2)}^{q'}(x-1)+\\overbrace{0}^{r'} \\]\nIn general if we know a zero of \\(q\\) then again, division of \\(q(x)\\) by \\(x-\\text{zero of $q$}\\) yields again a zero remainder:\n\nTherefore:\n\nAnd the same story continues with \\(q'\\).\n\n\n\n\n\n\nCommentary\n\n\n\nNotice that a zero of \\(q\\) is automatically a zero of \\(f\\). And a zero of \\(q'\\) is a zero of \\(f\\) as well.\n\n\nNow repeat the process and inspect the prime quotient \\(x^2+x-2\\), what do we find? We find that \\(q'(1)=0\\), using again polynomial division, yes! A zero remainder again:\n\\[ \\overbrace{x^2+x-2}^{q'} =\\overbrace{(x+2)}^{q''}(x-1)+\\overbrace{0}^{r''} \\]\nHere is a summary of the sequence of steps:\n\nCollecting terms we found:\n\\[ x^4-x^3-3x^2+5x-2 =(x+2)(x-1)(x-1)(x-1)+0 \\]\nThe lower the degree of the polynomial the easier it is to find its zeros by inspecting the function. Figuring out the original zero (the zero of \\(f\\)) is the hardest part, a graph might help in our guess work, if nothing else, just use trial and error. If it is found, congrats yourself, you just made one step of \\(n\\) steps.",
    "crumbs": [
      "Brief Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Polynomial division"
    ]
  },
  {
    "objectID": "pol_division.html#why-factoring-polynomials-is-cool",
    "href": "pol_division.html#why-factoring-polynomials-is-cool",
    "title": "Polynomial division",
    "section": "7) Why factoring polynomials is cool?",
    "text": "7) Why factoring polynomials is cool?\nOne key aspect one should keep in mind, is that finding the zeros of a polynomial and factored polynomial goes hand in hand. As we seen, the zeros of \\(x^4-x^3-3x^2+5x-2\\) are the singlet \\(-2\\), and the triplet of \\(1\\); knowing this allowed us to write this polynomial as \\((x+2)(x-1)^3\\). Going the other way around, starting from a polynomial given by \\((x+2)(x-1)^3\\) we know, just by looking at this formula, the zeros are \\(-2\\) and a triplet \\(1\\). Knowing the zeros is important or should I say knowing the factored form of a polynomial is important because, in-between those zeros, the polynomial must either positive (above the x-axis) or negative (below the x-axis). Additionally, note that the factored form of a polynomial is a product of atomic polynomials, in the current case it the product of four terms of degree one, \\(x+2\\) and three \\(x-1\\); the sign (positive or negative) of each is easily computed, they are just lines in the plane! As a result the sign of the product polynomial can also be easily computed from the sign of the a product of the terms. It goes without saying: The good polynomial is the factored polynomial.\nWhat we intend to do now is precisely to find the sign (positive or negative) of the product polynomial in-between the zeros (\\(-2\\) and \\(1\\)), in other words, we want to compute the following sets:\n\\[ \\begin{align} \\{x\\in\\mathbb{R}\\,\\,|\\,\\,x^4-x^3-3x^2+5x-1&gt;0\\}\\\\ \\{x\\in\\mathbb{R}\\,\\,|\\,\\,x^4-x^3-3x^2+5x-1&lt;0\\} \\end{align}  \\tag{12}\\] We already know the zeros of \\(x ^ 4-x ^ 3-3x^2+5x-1\\) which are embedded in the terms\n\n\\(x+2\\) is zero at \\(x=-2\\)\n\\(x-1\\) has a zero at \\(x=1\\)\n\nNow, to compute the sign of the term, draw a table as follows: in the first column put the factors, the first row is suppose to represent the real line, in it we highlight key points, which in the present case are the zeros of the factors, the point at \\(-2\\) and \\(1\\) , and as reference we should include the infinities (“end points” of the real line) as well:\n\n\n\nReal line\n\\(-\\infty\\)\n\n\\(-2\\)\n\n\\(1\\)\n\n\\(\\infty\\)\n\n\n\n\n\\(x+2\\)\n\\(-\\)\n\\(-\\)\n\\(0\\)\n\\(+\\)\n\\(+\\)\n\\(+\\)\n\\(+\\)\n\n\n\\(x-1\\)\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\\(0\\)\n\\(+\\)\n\\(+\\)\n\n\n\\(x-1\\)\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\\(0\\)\n\\(+\\)\n\\(+\\)\n\n\n\\(x-1\\)\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\\(0\\)\n\\(+\\)\n\\(+\\)\n\n\n\\((x+2)(x-1)^3\\)\n\\(+\\)\n\\(+\\)\n\\(0\\)\n\\(-\\)\n\\(0\\)\n\\(+\\)\n\\(+\\)\n\n\n\nHere an equivalent but clearer picture of this table, which I prefer:\n\nIn the rows \\(x+2\\) and \\(x-1\\) fill the zeros as \\(x=-2\\) and \\(x=1\\) respectively. Now, take the \\(x+2\\) row as an example, there is only one zero, hence before that zero we have a definite sign and after this zero we have also a definite sign, to find these signs we can just pick any two \\(x\\)-values, picking \\(x=-3\\) and \\(x=1\\) we compute \\(-3+2 = -1\\) which is negative and thus the region \\((-\\infty,-2)\\) must be negative; computing \\(1+2=3\\) we get a positive value, thus the region \\((-2,\\infty)\\) is positive. We repeat the same approach and pick two numbers, one before and other after \\(x=1\\) the zero of \\(x-1\\) to check its sign, the results are shown above.\nThe last row is also easy to fill from the signs of \\(x+2\\) and \\((x-1)^3\\). Notice the cube power, being an odd number, does not change the sign of \\(x-1\\).\nFrom the last row we can read the answer for our question Equation 12 , we conclude:\n\\[ \\begin{align} &\\{x\\in\\mathbb{R}\\,\\,|\\,\\,x^4-x^3-3x^2+5x-1&gt;0\\}=(-\\infty,-2)\\cup (1,+\\infty)\\\\ &\\{x\\in\\mathbb{R}\\,\\,|\\,\\,x^4-x^3-3x^2+5x-1&lt;0\\}=(-2,1) \\end{align} \\] The table method to find the signs of a complicated polynomial is fine and good, but I would like to emphasize the central idea behind it. What the table essentially is, is a representation of the graph of the factors with just the signs singled out. As such an alternative way, which I actually prefer, to find the answer to Equation 12 is to just graph the terms of the product polynomial:\n\nAnd from it read the signs from the lines:\nCool ! :)",
    "crumbs": [
      "Brief Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Polynomial division"
    ]
  },
  {
    "objectID": "pol_division.html#method-of-undetermined-coefficients-for-integer-division",
    "href": "pol_division.html#method-of-undetermined-coefficients-for-integer-division",
    "title": "Polynomial division",
    "section": "8) Method of undetermined coefficients for integer division",
    "text": "8) Method of undetermined coefficients for integer division\nA general strategy to solve the problem:\n\\[ 1001=q\\cdot 12 +r\\qquad 0\\leq q&lt;12  \\tag{13}\\]\nstarts by splitting the parameters of the equation into powers of \\(10\\)\n\\[ \\begin{split}&1001=1\\cdot 10^3+0\\cdot 10^2+0\\cdot 10^1+1\\cdot 10^0\\\\&12=1\\cdot 10^1+2\\cdot 10^0\\\\\\end{split} \\]\nand the unknowns \\(q\\) and \\(r\\) as well:\n\\[ \\begin{split}&q=a\\cdot 10^2+b\\cdot 10^2+c\\cdot 10^0\\\\&r=\\alpha \\cdot 10^0\\end{split} \\]\nfor some real numbers \\(a\\), \\(b\\), \\(c\\) and \\(\\alpha\\). Note that \\(q\\) must be at most a power \\(10^2\\) to guarantee a \\(10^3\\) of \\(1001\\), meanwhile the remainder \\(r\\) which must be between \\(0\\) and \\(12\\) has to be a number proportional to \\(10^0=1\\).\nSubstituting we find\n\\[ 1\\cdot 10^3+0\\cdot 10^2+0\\cdot 10^1+1\\cdot 10^0=(a\\cdot 10^2+b\\cdot 10^1+c\\cdot 10^0)\\cdot (1\\cdot 10^1+2\\cdot 10^0) +\\alpha \\cdot 10^0 \\]\nRearranging, we factor the same powers on the right side, which gives us:\n\\[ 1\\cdot 10^3+0\\cdot 10^2+0\\cdot 10^1+1\\cdot 10^0=a\\cdot 10^3+(2a+b)\\cdot 10^2+(2b+c)\\cdot 10^1+(2c+\\alpha)\\cdot 10^0  \\tag{14}\\]\nThe goal of determining what is \\(q\\) and \\(r\\) that satisfy the conditions Equation 13 becomes the goal of determining \\(a\\), \\(b\\), \\(c\\) and \\(\\alpha\\) that satisfy Equation 14.\nBoth sides of Equation 14 are equal provided the coefficients of the same powers of \\(10\\) are equal, that yields the following system of equations to solve:\n\\[ \\begin{cases}10^3:\\,\\,\\,1=a\\\\10^2:\\,\\,\\,0=2a+b\\\\10^1:\\,\\,\\,0=2b+c\\\\10^0:\\,\\,\\,1=2c+\\alpha\\end{cases} \\]\nwith the additional constraint \\(0\\leq \\alpha &lt;12\\).\nThe solution of this system of equations is \\(a=1\\), \\(b=-2\\), \\(c=4\\) and \\(\\alpha=-7\\) . Thus:\n\\[ \\begin{split}&q=1\\cdot 10^2-2\\cdot 10^1+4\\cdot 10^0=84\\\\&r=-7\\cdot 10^0=-7\\end{split} \\]\nHence, \\(1001=84\\cdot 12-7\\), observe the remainder is negative, therefore we modify the aesthetics of this result in order to make it positive and below \\(12\\). Adding \\(0=12-12\\) to the right hand side we find:\n\\[ \\begin{split}&1001=84\\cdot 12-7+12-12\\\\\\implies&1001=83\\cdot 12+5\\end{split} \\]\nThe final result is \\(1001=83\\cdot 12+5\\) as we computed through other methods.",
    "crumbs": [
      "Brief Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Polynomial division"
    ]
  },
  {
    "objectID": "pol_division.html#method-of-undetermined-coefficients-for-polynomials-division",
    "href": "pol_division.html#method-of-undetermined-coefficients-for-polynomials-division",
    "title": "Polynomial division",
    "section": "9) Method of undetermined coefficients for polynomials division",
    "text": "9) Method of undetermined coefficients for polynomials division\nHere we present our problem by casting it as a system of equations to solve. Some students might prefer this approach.\nComparing the lhs and rhs of Equation 7 we notice:\n\nthe polynomial \\(q(x)\\) must have a degree equal to \\(2\\)\nthe degree of \\(r(x)\\) is \\(0\\)\n\nFrom this we know the general from of these polynomials to be:\n\\[ \\begin{split}&q(x) :=  ax^2+bx+c\\\\&r(x) :=  \\alpha \\end{split} \\]\nSubstituting we find:\n\\[ x^3+1=(ax^2+bx+c)(x+2)+\\alpha  \\]\nExpanding the rhs and lumping the same powers of \\(x\\) we get:\n\\[ x^3+1=a x^3+(2a+b) x^2+ (2b +c)x^1+(2c+\\alpha) \\]\nThe polynomial on the right is equal to the the left if the coefficients of the same powers are the same, i.e., they satisfy the following system of equations\n\\[ \\begin{cases}x^3:\\,\\,\\,1=a\\\\x^2:\\,\\,\\,0=2a+b\\\\x^1:\\,\\,\\,0=2b+c\\\\x^0:\\,\\,\\,1=2c+\\alpha\\end{cases} \\]\nThe solution is \\(a=1\\), \\(b=-2\\), \\(c=4\\) and \\(\\alpha=-7\\):\n\\[ x^3+1=(x^2-2x+4)(x+2)-7 \\]\nThe similarities between the division of integers and polynomials should be obvious, since we just replace the powers of \\(10\\) by powers of \\(x\\).",
    "crumbs": [
      "Brief Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Polynomial division"
    ]
  },
  {
    "objectID": "matrix_multiplication.html",
    "href": "matrix_multiplication.html",
    "title": "Matrix multiplication and linear combination of rows",
    "section": "",
    "text": "Consider the following matrix operation (we found this operation in step 1 of Example 4):\n\\[\n\\begin{pmatrix}1 & 2 & 2 & 2\\\\2 & 4 & 6 & 8\\\\3 & 6 & 8 & 10 \\end{pmatrix}\\overset{l_2' = l_2-2l_1}{\\longrightarrow}\\begin{pmatrix}1 & 2 & 2 & 2 \\\\0 & 0 & 2 & 4 \\\\3 & 6 & 8 & 10 \\end{pmatrix}\n\\]\nand corresponding vector operation\n\\[\n\\begin{pmatrix}\n1\\\\2\\\\3\n\\end{pmatrix}\n\\overset{l_2' = l_2-2l_1}{\\longrightarrow}\n\\begin{pmatrix}\n1\\\\0\\\\3\n\\end{pmatrix}\n\\]\nImplicit was the following matrix multiplication:\n\\[\n\\begin{pmatrix}1 & 0 & 0 \\\\-2 & 1 & 0 \\\\0 & 0 & 1 \\end{pmatrix}\\begin{pmatrix}1 & 2 & 2 & 2 \\\\2 & 4 & 6 & 8 \\\\3 & 6 & 8 & 10\\end{pmatrix} = \\begin{pmatrix}1 & 2 & 2 & 2 \\\\0 & 0 & 2 & 4 \\\\3 & 6 & 8 & 10 \\end{pmatrix}\n\\]\nand\n\\[\n\\begin{pmatrix}1 & 0 & 0 \\\\-2 & 1 & 0 \\\\0 & 0 & 1 \\end{pmatrix}\\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix}=\\begin{pmatrix}1\\\\0\\\\3\\end{pmatrix}\n\\]\nHow do we do these calculations?\nThe general rule is:\n\n\n\n\n\n\nFigure 1: Matrix multiplication of a 3x4 matrix by a 3x3 matrix.\n\n\n\nwhere, for example:\n\\[\n\\begin{align}\na_{22}'= e_{21}a_{12} + e_{22}a_{22} +e_{23}a_{32}\\\\\na_{23}'= e_{21}a_{13} + e_{22}a_{23} +e_{23}a_{33}\\\\\n\\end{align}\n\\tag{1}\\]\nLets name the matrices involved, the \\(e\\)’s matrix is called \\(E_1\\) and the \\(a\\)’s matrix is called \\(A\\), the result of the multiplication is called \\(A'\\).\nObserving Figure 1 and Equation 1 we see two important aspects about \\(E_1 A\\)\n\nThe shape of \\(E_1\\) is \\(3\\times3\\),the shape of \\(A\\) is \\(3\\times 4\\) and the shape of \\(A'\\) is \\(3\\times4\\).\nThe matrix \\(E_1\\) needs to have as many columns as there are rows in \\(A\\) (in this case \\(3\\)), otherwise it is not possible to do matrix multiplication. In other words, the compatible shapes for matrix multiplication are of the form \\([m\\times r][r\\times n] = [m\\times n]\\), for any integers \\(m,n,r\\). For example, the case above is \\([3\\times 3][3\\times 4]\\) and thus \\(m=3\\), \\(r=3\\) and \\(n=4\\).\nThe general formula for the entries of \\(A'\\) is the complicated formula:\n\\[\na_{ij}'=\\sum_{k=1}^re_{ik}a_{kj}\n\\]"
  },
  {
    "objectID": "macros.html",
    "href": "macros.html",
    "title": "macros",
    "section": "",
    "text": "::: {.hidden} $$ \\newcommand{\\foo}{E=mc^{2}} $$ :::"
  },
  {
    "objectID": "linear_dependence_and_independence.html",
    "href": "linear_dependence_and_independence.html",
    "title": "Linear dependence and independence",
    "section": "",
    "text": "Video\n\n\n\nTesting linear independence\nFirst lets see what is linear (in)dependence, then how to check it. As we will see throughout the course there are many ways of doing it.\nLinear (in)dependence is seen best with examples.\nFrom these examples we conclude that the following statements are equivalent:\nThere two statements in English translates to Mathematical language:",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Linear dependence and independence"
    ]
  },
  {
    "objectID": "linear_dependence_and_independence.html#how-to-check-whether-a-set-of-vectors-is-or-not-independent",
    "href": "linear_dependence_and_independence.html#how-to-check-whether-a-set-of-vectors-is-or-not-independent",
    "title": "Linear dependence and independence",
    "section": "How to check whether a set of vectors is or not independent?",
    "text": "How to check whether a set of vectors is or not independent?\nConsider again example 2 above. We were given the vectors: \\(\\begin{pmatrix} 1\\\\2\\end{pmatrix}\\) , \\(\\begin{pmatrix} 1\\\\0\\end{pmatrix}\\) and \\(\\begin{pmatrix} 0\\\\1\\end{pmatrix}\\) . Are they dependent or independent?\nIn other words, using the definition: what is the solution \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\) of the equation:\n\\[\n\\alpha \\begin{pmatrix} 1\\\\2\\end{pmatrix} +\\beta \\begin{pmatrix} 1\\\\0\\end{pmatrix}+\\gamma\\begin{pmatrix} 0\\\\1\\end{pmatrix}=\\begin{pmatrix} 0\\\\0\\end{pmatrix}\n\\]\nWell, rewriting it, we obtain a system of equations to solve:\n\\[\n\\begin{cases}\n\\alpha + \\beta =0\\\\\n2\\alpha +\\gamma =0\n\\end{cases}\n\\implies\n\\begin{cases}\n\\alpha = \\beta =0\\\\\n\\alpha =\\gamma/2 =0\n\\end{cases}\n\\]\nThere are solutions for this system, for example \\(\\gamma=2\\), \\(\\alpha=1\\) and \\(\\beta= -1\\). This shows some of the three vectors can be constructed from the remaining.\n\n\n\n\n\n\nCommentary\n\n\n\nWhen the number of vectors in \\(\\mathbb{R}^n\\) being spanned is larger than the dimension \\(n\\) of the space, then some are dependent vectors. In the example above, \\(n=2\\) and there are \\(3\\) vectors being spanned.\n\n\n\nExercise 2 Solve 1.2 &gt; 5 &gt; (d)",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Linear dependence and independence"
    ]
  },
  {
    "objectID": "linear_dependence_and_independence.html#how-to-check-whether-a-vector-depends-on-two-other-vectors",
    "href": "linear_dependence_and_independence.html#how-to-check-whether-a-vector-depends-on-two-other-vectors",
    "title": "Linear dependence and independence",
    "section": "How to check whether a vector depends on two other vectors?",
    "text": "How to check whether a vector depends on two other vectors?\nIs \\(\\begin{pmatrix} 1\\\\2\\end{pmatrix}\\) dependent on \\(\\begin{pmatrix} 1\\\\0\\end{pmatrix}\\) and \\(\\begin{pmatrix} 0\\\\1\\end{pmatrix}\\) ? In other words, is it true or false that \\(\\begin{pmatrix} 1\\\\2\\end{pmatrix} \\in span \\{\\begin{pmatrix} 1\\\\0\\end{pmatrix},\\begin{pmatrix} 0\\\\1\\end{pmatrix}\\}\\)\nUsing the definition above we seek for solutions of\n\\[\n\\begin{pmatrix} 1\\\\2\\end{pmatrix}=\\beta\\begin{pmatrix} 1\\\\0\\end{pmatrix}+\\gamma\\begin{pmatrix} 0\\\\1\\end{pmatrix}\n\\]\n(we set \\(\\alpha=-1\\))\nThe solutions are obtained by solving the system:\n\\[\n\\begin{cases}\n1=\\beta\\\\\n2=\\gamma\n\\end{cases}\n\\]\nwhich is immediate! So yes, the vector is dependent.\n\nExercise 3 Solve 1.2 &gt; 3 &gt; (b), (c);",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Linear dependence and independence"
    ]
  },
  {
    "objectID": "linear_approximation.html",
    "href": "linear_approximation.html",
    "title": "Linear approximation",
    "section": "",
    "text": "The basic idea is this: start with the graph of a function \\(f\\), then draw a line tangent to a chosen point \\((x_0,y_0)\\) on the function. This line is special, because the points \\((x,y)\\) on the line around the point \\((x_0,y_0)\\) and the points \\((x,f(x))\\) also around \\((x_0,y_0)\\) are very close and as a result we can use them interchangeably, at the price of a small error. Lets express this mathematically, for \\(x\\) close to \\(x_0\\) we have:\n\\[\nf(x) \\sim m(x-x_0)+y_0\n\\tag{1}\\]\nThe equation of the line is \\(y=m(x-x_0)+y_0\\), we want to find it! Because the line is tangent to the function \\(f\\), somehow, the parameters \\(m\\) and \\(y_0\\) are related to that function (\\(x_0\\) is chosen by us). From the picture we know that:\n\\[\nm = f'(x_0)\\qquad y_0 = f(x_0)\n\\]\nIn conclusion, the images of \\(x\\)’s around \\(x_0\\) through the function \\(f\\) are mapped into the corresponding \\(f(x)\\)’s, and in turn this values are approximately of the form \\(f'(x_0)(x-x_0)+f(x_0)\\).\nYou may wonder, why this approximation is useful (for a physicist it is extremely useful!), only though example we’ll see that, but just from the formula Equation 1 you can see that for a complicated function \\(f(x)\\), the rhs, is just a simple and each to handle polynomial of degree one. I would rather use that than the exact value \\(f(x)\\), if the error committed is negligible for the problem at hand.\n\nDiferentials\nThe idea of differentials is closely connected to the idea of linear approximations.\nAbove we seen that the values of \\(f(x)\\) are very close to the values \\(f'(x_0)(x-x_0)+f(x_0)\\) as long as \\(x\\) is close to \\(x_0\\), the farther \\(x\\) is, the worst is the difference between both.\nAs the name differentials suggests, we want to compute differences, differences between points on the function, for example, we want to compute \\(f(x)-f(x_0)\\).\nThe idea behind linear approximations gives us a way to evaluate this diference easily, with better and better accuracy the closer and closer is \\(x\\) to the given \\(x_0\\); just evaluate \\(f(x)\\) approximately using \\(f'(x_0)(x-x_0)+f(x_0)\\). Thus:\n\\[\nf(x)-f(x_0) \\sim f'(x_0)(x-x_0)+f(x_0) -f(x_0) = f'(x_0)(x-x_0)\n\\tag{2}\\]\nOn the lhs we have the difference between the images of \\(x\\) and \\(x_0\\) thought the function, meanwhile on the rhs we find the corresponding difference by though the tangent line. If \\(x\\) is very close to \\(x_0\\), then \\(x-x_0\\) is close to zero and \\(f(x)-f(x_0)\\) is close to zero as well, as it should; see the picture.\nIn fact we have special notation for formula Equation 2:\n\n\\(f(x)-f(x_0) =: df\\) is called the differential of the function \\(f\\) around \\(x_0\\).\n\\(x-x_0=:dx\\) is called the differential of the independent variable \\(x\\), around, again, the given value of \\(x_0\\). The differential \\(dx\\) is any real number!\n\nThus,\n\\[\ndf \\sim f'(x_0) dx\n\\]\nThe close \\(x\\) is to \\(x_0\\), the closer \\(dx\\) is to zero and the better is the approximation. If \\(dx\\) is extremely small we would rather use the equal sign rather than the approximate sign: \\(df = f'(x_0) dx\\) if \\(dx\\longrightarrow 0\\)."
  },
  {
    "objectID": "inner_product.html",
    "href": "inner_product.html",
    "title": "Definition of inner product and its properties",
    "section": "",
    "text": "The inner product operation \\(\\cdot\\) is an operation between vectors that yields a number, it is defined as:\nProperties:\nExercises: 1.6.3,5,6,7\nUsefulness: The inner product operation is a tool to gather geometrical information about vectors and to describe planes and hyperplanes. I.e. it allows answering the following questions:",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Inner product and Orthogonal spaces"
    ]
  },
  {
    "objectID": "inner_product.html#what-is-the-length-of-a-vector",
    "href": "inner_product.html#what-is-the-length-of-a-vector",
    "title": "Definition of inner product and its properties",
    "section": "What is the length of a vector?",
    "text": "What is the length of a vector?\nAnswer: Compute the inner product of a vector with itself.\n\\[\n\\mathbf{u}^\\intercal\\mathbf{u} = u_1^2+\\dots+u_n^2\n\\]\nWhose rhs we interpret, using the Pythagoras theorem, as the length square of \\(\\mathbf{u}\\). The norm \\(|\\mathbf{u}|\\) of the vector is the square-root of this value:\n\\[\n|\\mathbf{u}|:=\\sqrt{u_1^2+\\dots+u_n^2}\n\\]\nNotation: we can call the norm \\(|\\mathbf{u}|\\) just by \\(u\\).\n\nExample\nThe length squared of the vector \\(\\mathbf{u}=(1,2)\\) is:\n\\[\n|\\mathbf{u}|^2 = \\mathbf{u}^\\intercal\\mathbf{u} = u_1^2+u_2^2 = 1^2+2^2=1+4=5\n\\]\nThe norm of the vector is obtained by taking the square root of \\(5\\):\n\\[\nu=\\sqrt{5}\n\\]\n\nAnother way to compute the norm-squared of a vector is to consider \\(\\mathbb{R}^2=[\\textbf{x axis}]\\oplus[\\textbf{y axis}]\\) and write \\(\\mathbf{u} = (0,1)+(2,0)\\) and then use the Pythagoras theorem \\(|\\mathbf{w}+\\mathbf{v}|^2=w^2+v^2\\) to compute:\n\\[\n|\\mathbf{u}|^2=|(0,1)+(2,0)|^2=(0^2+1^2)^2+(2^2+0^2)^2=1+4=5\n\\]\nThen take the square-root.\nExercises: 1.6.1a",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Inner product and Orthogonal spaces"
    ]
  },
  {
    "objectID": "inner_product.html#how-to-build-a-unit-vector",
    "href": "inner_product.html#how-to-build-a-unit-vector",
    "title": "Definition of inner product and its properties",
    "section": "How to build a unit vector?",
    "text": "How to build a unit vector?\nStarting with the vector \\(\\mathbf{u}=(u_x,u_y)\\) we can define a parallel unit vector by dividing it by it length:\n\\[\n\\mathbf{e}_u = \\frac{\\mathbf{u}}{|\\mathbf{u}|}\n\\]\nJustification: Compute its norm \\(|\\mathbf{e}_u|^2 = \\frac{\\mathbf{u}}{|\\mathbf{u}|}\\cdot \\frac{\\mathbf{u}}{|\\mathbf{u}|}=\\frac{|\\mathbf{u}|^2}{|\\mathbf{u}|^2}=1\\)\n\nExample\nThe unit vector along \\(\\mathbf{u}=-(1,2)\\) is:\n\\[\n\\mathbf{e}_u = \\frac{-(1,2)}{\\sqrt{1+2^2}}=-\\frac{1}{\\sqrt{5}}(1,2)\n\\]\nClearly: \\(|\\mathbf{e}_u|^2 = -\\frac{1}{\\sqrt{5}}(1,2)\\cdot (-\\frac{1}{\\sqrt{5}}(1,2))=\\frac{1}{5}(1+2^2)=1\\)\nExercise: 1.6.2.c,e",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Inner product and Orthogonal spaces"
    ]
  },
  {
    "objectID": "inner_product.html#how-to-build-a-vector-perpendicular-to-another-vector",
    "href": "inner_product.html#how-to-build-a-vector-perpendicular-to-another-vector",
    "title": "Definition of inner product and its properties",
    "section": "How to build a vector perpendicular to another vector?",
    "text": "How to build a vector perpendicular to another vector?\n\nExample\nGiven again the vector \\(\\mathbf{n}=(1,2)\\), a vector \\(\\mathbf{r}\\) perpendicular to it, obeys \\(\\mathbf{n}^\\intercal \\mathbf{r}=0\\). Solving this equation for \\(\\mathbf{r}\\) we find:\n\\[\n(1,2)\\cdot (x,y)=0 \\implies x+2y=0\n\\]\nOne equation with two unknowns, we must promote one unknown to a parameter, let \\(x=-2\\). Then \\(y=1\\). Thus \\((-2,1)\\) is perpendicular to \\((1,2)\\). Notice this is not the only solution!\n[Comment: This is a very simple \\(A\\mathbf{x}=\\mathbf{0}\\) type of problem with a matrix \\(A=(1,2)\\) with one pivot \\(1\\) and one dependent column \\(2\\).]\nExercises: 1.6.2.b,c,f",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Inner product and Orthogonal spaces"
    ]
  },
  {
    "objectID": "inner_product.html#how-to-define-a-circles-lines-planes-and-hyperplane",
    "href": "inner_product.html#how-to-define-a-circles-lines-planes-and-hyperplane",
    "title": "Definition of inner product and its properties",
    "section": "How to define a circles, lines, planes and hyperplane?",
    "text": "How to define a circles, lines, planes and hyperplane?\nCircle and spheres: A circle in \\(\\mathbb{R}^2\\) or a sphere in \\(\\mathbb{R}^3\\) centered at \\(\\mathbf{0}\\) satisfy the equation: \\(|\\mathbf{r}|^2=R^2\\) where \\(R\\) is the radius. Centered at \\(\\mathbf{r}_0\\) they obey \\(|\\mathbf{r}-\\mathbf{r}_0|^2=R^2\\). The geometry of the situation should clearly justify the equations.\nExercise: 1.6.8\nLine in \\(\\mathbb{R}^2\\): A line perpendicular to \\(\\mathbf{n}=(1,2)\\) is the set of all solutions to the equation \\(\\mathbf{n}^\\intercal \\mathbf{r}=0\\), the answer is:\n\\[\n[\\textbf{line}\\,\\perp\\,\\textbf{to}\\,\\,(1,2)]=\\{(-2c,c)\\,\\,|\\,\\,c\\in \\mathbb{R}\\}\n\\]\nLine in \\(\\mathbb{R}^3\\): A line (which crosses the origin) is the interception of two non-parallel planes (which cross the origin as well); a plane is the subspace perpendicular to a vector. Let \\(\\mathbf{n}_1=(1,0,1)\\) and \\(\\mathbf{n}_2=(1,2,0)\\) define the two planes \\(\\mathbf{n}_{1,2}^\\intercal \\mathbf{r}=0\\). The interception is the set of vectors common to both planes, this means we have to find the \\(\\mathbf{r}=(x,y,z)\\in\\mathbb{R}^3\\) such that both plane equations are satisfied, i.e.\n\\[\n\\begin{cases}\n\\mathbf{n}_{1}^\\intercal \\mathbf{r}=0\\\\\n\\mathbf{n}_{2}^\\intercal \\mathbf{r}=0\n\\end{cases}\n\\leftrightsquigarrow\n\\left(\\begin{matrix}1 & 0 & 1\\\\1 & 2 & 0 \\end{matrix}\\;\\middle|\\;\\begin{matrix}0\\\\0 \\end{matrix}\\right)\n\\overset{l_2'=l_2-l_1}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 0 & 1\\\\0 & 2 & -1 \\end{matrix}\\;\\middle|\\;\\begin{matrix}0\\\\0 \\end{matrix}\\right)\n\\overset{l_2'=1/2l_2}{\\longrightarrow}\\left(\\begin{matrix}1 & 0 & 1\\\\0 & 1 & -1/2 \\end{matrix}\\;\\middle|\\;\\begin{matrix}0\\\\0 \\end{matrix}\\right)\n\\tag{1}\\]\nThere are two pivots in the first two columns and a dependent column, one solution is \\((-1,1/2,1)\\), in general we have:\n\\[\n\\mathbf{x}_N = c\\left(\\begin{matrix}-1\\\\1/2\\\\1\\end{matrix}\\right)\n\\]\nwhich gives us the form of the elements along a line!\n\\[\n[\\textbf{line}\\,\\perp\\, \\textbf{to}\\,\\, \\mathbf{n}_{1,2}] = \\{(-c,c/2,c)\\,\\,|,\\, c\\in \\mathbb{R}\\}\n\\]\nA Captain obvious observation: If we know the process of finding all vector (the line) perpendicular to these two vector, I can find just one vector perpendicular to both.\nA plane in \\(\\mathbb{R}^3\\): The subspace perpendicular to \\((1,0,1)\\) is the solution of \\((1,0,1)\\cdot (x,y,z)=0\\), because there are two free columns and one pivot we expect null space \\(N(1,0,1)\\) with two dimensions. Setting the free variable \\(y=1\\) and \\(z=0\\) we find \\(x=-1\\), hence \\(\\mathbf{x}_{N,1}=(-1,1,0)\\); setting \\(y=0\\) and \\(z=1\\) we arrive at \\(\\mathbf{x}_{N,2}=(-1,0,1)\\). Any l.c. of these solutions is a solution of the equation:\n\\[\n\\mathbf{x}_N=\\alpha\\left(\\begin{matrix}-1\\\\1\\\\0 \\end{matrix}\\right)+\\beta\\left(\\begin{matrix}-1\\\\0\\\\1 \\end{matrix}\\right)\n\\]\nwhich defines a plane containing \\(\\mathbf{0}\\).\nHyperplane: The idea is similar, just define the appropriate interception of subspaces perpendicular to the given vectors, then arrange the equations as an \\(A\\mathbf{r}=\\mathbf{0}\\) problem.\nExercise: 1.6.9",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Inner product and Orthogonal spaces"
    ]
  },
  {
    "objectID": "inner_product.html#how-much-a-vector-looks-like-another-vector",
    "href": "inner_product.html#how-much-a-vector-looks-like-another-vector",
    "title": "Definition of inner product and its properties",
    "section": "How much a vector looks like another vector?",
    "text": "How much a vector looks like another vector?\n\nExample 1\nThe formula \\(\\mathbf{u}\\cdot\\mathbf{v} = uv\\cos\\theta\\) is of great importance because it allow us to compute the angle between \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\). Let \\(\\mathbf{u} = (1,2)\\) and \\(\\mathbf{v}=(-2,3)\\):\n\nThen the angle is obtained by solving the trigonometric equation:\n\\[\n\\cos \\theta = \\frac{\\mathbf{u}^\\intercal\\mathbf{v}}{uv}=\\frac{1\\cdot (-2)+2\\cdot 3}{\\sqrt{1+2^2}\\sqrt{2^2+3^2}}=\\frac{4}{\\sqrt{65}}\\implies \\theta \\approx 1\\,\\, rad\n\\]\nThe angle of \\(1\\,\\,rad\\) is quiet large, hence the two vectors directions are not close to each other.\nAnother way to evaluate similarity is through the value of the cosine rather than the angle itself, in this case \\(\\cos \\theta\\) is \\(4/\\sqrt{85}\\) which is about \\(1/2\\).\n\nSince the \\(\\theta\\) is smaller than \\(\\pi/2\\), then the angle acute which means the vectors are not very similar, but at least point roughly in the same direction.\nExercises: 1.6.4\n\n\nExample 2\nIf we now let \\(\\mathbf{u} = (1,-2)\\) and \\(\\mathbf{v}=(-2,3)\\), then the angle is: \\(\\theta \\approx 0.95 \\pi \\,\\,rad\\). Almost \\(180^\\circ\\), very different vectors. The inner product by being negative already tells us that the vectors point in almost opposite directions.\nComment: In example 1 and 2 above the calculation of the angle or the cosine is rather useless because we can easily draw both vectors and check whether they are or not similar. A harder task is when we try to do this in three, four, etc dimensions.\n\n\nExample 3:\nLet \\(\\mathbf{u} = (1,2)\\) and the unitary vector \\(\\mathbf{e}_v=(1,-1)/\\sqrt{2}\\):\n\nComputing the inner product we find:\n\\[\n\\mathbf{e}_v\\cdot \\mathbf{u} = -\\frac{1}{\\sqrt{2}}=5\\cos \\theta\n\\]\nOn the rhs we see using trigonometry, the projection (shadow) of the vector \\(\\mathbf{u}\\) along the vector \\(\\mathbf{e}_v\\), which is this case is negative, implying the vectors point in different directions.\n[Comment: The matrix \\(\\mathbf{v}\\mathbf{v}^\\intercal/v^2\\) is a projector along \\(\\mathbf{v}\\).]\n\n\nExample 4\nThis time we start with \\(\\mathbf{e}_x\\) and \\(\\mathbf{e}_x\\), the unitary vector \\((1,0)\\) or \\((0,1)\\), and keep the usual \\(\\mathbf{u}=(1,2)\\). The inner products yields:\n\\[\n\\begin{cases}\n\\mathbf{e}_x^\\intercal \\mathbf{u}=1\\\\\n\\mathbf{e}_y^\\intercal \\mathbf{u}=2\\\\\n\\end{cases}\n\\]\nMoreover:\n\\[\n\\begin{cases}\n\\mathbf{e}_x^\\intercal \\mathbf{u}=5\\cos\\theta_x\\\\\n\\mathbf{e}_y^\\intercal \\mathbf{u}=5\\cos\\theta_y=5 \\sin\\theta_x\n\\end{cases}\n\\]\nWhich means geometrically:\n\nDotting a vector with an unitary vector yields \\(5\\cos\\theta_x\\) and \\(5\\cos\\theta_y\\) which using trigonometry are the projections of \\(\\mathbf{u}\\) along those directions, which in turn is are the entries \\(1\\) and \\(2\\) of the vector along the unitary vectors.\nWe conclude that, given a basis \\(B=\\{\\mathbf{e}_x, \\mathbf{e}_y\\}\\) we can write the vector \\(\\mathbf{u}=(1,2)\\) as:\n\\[\n\\mathbf{u} = (\\mathbf{e}_x\\cdot\\mathbf{u})\\mathbf{e}_x+(\\mathbf{e}_y\\cdot\\mathbf{u})\\mathbf{e}_y= 1\\mathbf{e}_x+2 \\mathbf{e}_y\n\\tag{2}\\]\nWhich means: \\([\\mathbf{u}]_B=(\\mathbf{e}_x\\cdot\\mathbf{u},\\mathbf{e}_y\\cdot\\mathbf{u})\\)\n\n\n\n\n\n\nCommentary\n\n\n\nWe identify in Equation 2, the partition of the identity:\n\\[ I=\\mathbf{e}_x\\mathbf{e}_x^\\intercal+\\mathbf{e}_y\\mathbf{e}_y^\\intercal \\]\nwhich when it act on a vector \\(\\mathbf{u}\\) in \\(\\mathbb{R}^2\\) gives us:\n\\[\n\\mathbf{u}=I\\mathbf{u} = \\mathbf{e}_x\\mathbf{e}_x^\\intercal\\mathbf{u}+\\mathbf{e}_y\\mathbf{e}_y^\\intercal\\mathbf{u}=1\\mathbf{e}_x+2 \\mathbf{e}_y\n\\]\n\n\nExercises: 1.7.5",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Inner product and Orthogonal spaces"
    ]
  },
  {
    "objectID": "inner_product.html#how-to-define-a-basis-for-a-subspace-orthogonal-to-another-given-subspace",
    "href": "inner_product.html#how-to-define-a-basis-for-a-subspace-orthogonal-to-another-given-subspace",
    "title": "Definition of inner product and its properties",
    "section": "How to define a basis for a subspace orthogonal to another given subspace?",
    "text": "How to define a basis for a subspace orthogonal to another given subspace?\nEssentially what is being asked is to solve \\(U\\oplus U^\\perp=\\mathbb{R}^n\\) given \\(U\\). And since \\(U^\\perp\\) has all its vectors perpendicular to \\(U\\), it is the nullspace of a matrix whose rows are the basis of \\(U\\).\n\nExample\nConsider the subspace of \\(\\mathbb{R}^3\\) given by \\(U=span\\{(1,0,1),(1,2,0)\\}\\). Question: Define a basis for \\(U^\\perp\\).\nAnswer: If we find the set of vectors perpendicular to the basis vectors of \\(U\\), we found \\(U^\\perp\\). The basis of \\(U\\) is \\(\\{(1,0,1),(1,2,0)\\}\\), a set of vectors perpendicular to it is the solution of:\n\\[\n\\left(\\begin{matrix}1 & 0 & 1\\\\1 & 2 & 0 \\end{matrix}\\;\\middle|\\;\\begin{matrix}0\\\\0 \\end{matrix}\\right)\n\\]\nThis system is solved the usual manner, see Equation 1. Thus:\n\\[ U^\\perp=[\\textbf{line}\\,\\perp\\, \\textbf{to}\\,\\, \\mathbf{n}_{1,2}] = \\{(-c,c/2,c)\\,\\,|,\\, c\\in \\mathbb{R}\\} \\]\nNow that we have the set \\(U^\\perp\\) we can pick a basis for it, for example \\((-1,1/2,1)\\). One basis of \\(U^\\perp\\) is \\((-1,1/2,1)\\).\nSometimes we might want a normalized basis in which case, we pick from \\(U^\\perp\\) the vector \\((-1,1/2,1)/\\sqrt{(-1)^2+(1/2)^2+1^2}\\).\nExercise: 1.7.2,3",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Inner product and Orthogonal spaces"
    ]
  },
  {
    "objectID": "inner_product.html#four-very-important-subspaces",
    "href": "inner_product.html#four-very-important-subspaces",
    "title": "Definition of inner product and its properties",
    "section": "Four very important subspaces",
    "text": "Four very important subspaces\nFor a given matrix \\(A\\) with shape \\(n\\times m\\) we already defined the column space \\(C(A)\\) which is a subspace of \\(\\mathbb{R}^n\\) and the nullspace \\(N(A)\\) which is a subspace of \\(\\mathbb{R}^m\\). Recall the matrix\n\\[\nA=\\begin{pmatrix}1 & 2 & 2 & 2 \\\\2 & 4 & 6 & 8 \\\\3 & 6 & 8 & 10 \\end{pmatrix}\n\\]\nThe column space spanned by the first and third column (because they are independent), its dimension is the number of pivots, in this case \\(2\\); it is a subspace of \\(\\mathbb{R}^3\\):\n\\[\nC(A)=span\\{(1,2,3),(2,6,8)\\}\\subset_{\\text{vec}}\\mathbb{R}^3\n\\]\nWhile the nullspace was obtained by solving \\(A\\mathbf{x}_N=\\mathbf{0}\\), its dimension is the number of dependent columns, which is the number of columns \\(m=4\\) minus the number of pivots \\(r=2\\); it is a subspace of \\(\\mathbb{R}^4\\).\n\\[\nN(A) = span\\{(-2,1,0,0),(2,0,-2,1)\\}\\subset_{\\text{vec}}\\mathbb{R}^4\n\\]\nNow we’ll introduce their orthogonal complements in the \\(\\mathbb{R}^3\\) and \\(\\mathbb{R}^4\\) respectively.\n\nLeft Nullspace\n\\(C(A)^\\perp\\) is the set of all vectors perpendicular to every vector in \\(C(A)\\). It is the nullspace of \\(A^\\intercal\\), aka, left nullspace \\(N(A^\\intercal)\\).\nWhy is this so?\n\n\n\nRow space\n\\(N(A)^\\perp\\) is the set of all vectors perpendicular to every vector in \\(N(A)\\). It is the column space of \\(A^\\intercal\\), aka, row space.\nWhy is this so?\n\n\n\nObservations:\nWe know (\\(n=3\\) and \\(m=4\\)):\n\\[\n\\mathbb{R}^n = C(A) \\oplus N(A^\\intercal)\\qquad \\mathbb{R}^m=C(A^\\intercal)\\oplus N(A)\n\\]\nSince \\(\\dim C(A) = \\dim C(A^\\intercal) =r\\), \\(\\dim N(A) =n-r\\) and \\(\\dim N(A^\\intercal) =m-r\\) we can verify that:\n\\[\nn = \\dim C(A) + \\dim N(A^\\intercal) \\qquad m = \\dim C(A^\\intercal)+\\dim N(A)\n\\]\nPictorially we find:\n\nWith this subspaces in hand we can look again at \\(A\\mathbf{x}=\\mathbf{b}\\):\n\nAny vector \\(\\mathbf{x}\\) lives in \\(\\mathbb{R}^m\\), it has as many entries as there are columns in the matrix it multiplies. Since this space is broken down into two complements \\(\\mathbb{R}^m=C(A^\\intercal)\\oplus N(A)\\) we can also break \\(\\mathbf{x}\\) into \\(\\mathbf{x}=\\mathbf{x}_\\text{row}+\\mathbf{x}_\\text{null}\\).\nThe vector belong to the space \\(\\mathbf{b}\\in\\mathbb{R}^n\\) and can be broken in \\(\\mathbf{b} = \\mathbf{b}_{col}+\\mathbf{b}_{\\textit{left null}}\\).\n\n\n\n\n\n\n\nCommentary\n\n\n\nWhen solving \\(A\\mathbf{x}=\\mathbf{b}\\) we found \\(\\mathbf{x}=\\mathbf{x}_P+\\mathbf{x}_N\\). So it seem as if \\(x_P\\) and \\(x_{row}\\) must be the same thing. But need not be the same. The particular solution may have some component in the nullspace of \\(A\\). As an example, our favorite system of equation has general solution:\n\\[\n\\begin{pmatrix} 1\\\\0\\\\0\\\\0 \\end{pmatrix} +span\\{\\begin{pmatrix}-2\\\\1\\\\0\\\\0 \\end{pmatrix} ,\\begin{pmatrix}2\\\\0\\\\-2\\\\1 \\end{pmatrix}\\}\n\\]\nNotice \\((1,0,0,0)\\) has non zero projection along the \\((-2,1,0,0)\\) and \\((2,0,-2,1)\\). However this solution can be rewritten written as \\(\\mathbf{x}_\\text{row}+\\mathbf{x}_\\text{null}\\). To achieve that some they strategies already used in Gram-Schmidt might be useful.\n\n\nExercises: 1.5.11,12",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Inner product and Orthogonal spaces"
    ]
  },
  {
    "objectID": "implicit_derivative.html",
    "href": "implicit_derivative.html",
    "title": "Implicit derivatives",
    "section": "",
    "text": "A more suitable title for this section is “what can we say about the derivative of an a function defined implicitly?”\nWith that, the first step is to understand what is an implicit function, only then we bother with its derivative.",
    "crumbs": [
      "Brief Notes",
      "Calculus",
      "Implicit derivative"
    ]
  },
  {
    "objectID": "implicit_derivative.html#implicit-functions",
    "href": "implicit_derivative.html#implicit-functions",
    "title": "Implicit derivatives",
    "section": "Implicit functions",
    "text": "Implicit functions\nAn implicit function is a function defined by an equation. An example shows well the idea:\nConsider the equation \\(x^2 + y^2=1\\). The truthset of this statement is the circle of radius one:\n\\[\n\\{(x,y)\\in\\mathbb{R}^2\\,\\,|\\,\\,x^2+y^2=1\\}\n\\]\nWe can use it to define functions, after all functions are also sets of ordered pair albeit with the property that each \\(x\\) can only appear one time in the set (please note this is not the case for the circle, each \\(x\\) occurs twice.)\nWhat we are going to do is to slice, restrict, the set in such a way as a function comes out. There are many ways of doing so, but here is the more useful ones: Make the upper half of the circle one function and the bottom half another.\nIn mathematical notation, the upper half is defined as:\n\\[\n\\begin{align}\nf: &[-1,1] \\longrightarrow \\mathbb{R}\\\\\n&x\\longmapsto f(x):=[\\text{positive $y$ solution of $x^2+y^2=1$}]\n\\end{align}\n\\]\nthe bottom half is:\n\\[\n\\begin{align}\ng: &[-1,1] \\longrightarrow \\mathbb{R}\\\\\n&x\\longmapsto g(x):=[\\text{negative $y$ solution of $x^2+y^2=1$}]\n\\end{align}\n\\]\nThe functions \\(f\\) and \\(g\\) are defined implicitly, because the procedure that one must follow to assign \\(y\\) to the chosen \\(x\\) is implict in solving an equation, in this case \\(x^2 + y^2=1\\); by implicit I mean, the sequence of steps you must follow to solve it, is not given to you explicitly.\nA more practical way to look at the upper half (similarly for the bottom half) is to say that \\(y\\) is the positive function of \\(x\\) such that it satisfies the equation \\(x^2 +y^2=1\\), and to make that evident we can write:\n\\[\nx^2 +y(x)^2=1\n\\]This notation tell us: \\(y\\) is a function of \\(x\\), though how we get the value of \\(y(x)\\) from \\(x\\) requires to solve the equation with whatever means are necessary. It is precise this “with whatever means…” that promotes the \\(y(x)\\) function (equivalently the \\(f\\) function) to the class of implicit functions.",
    "crumbs": [
      "Brief Notes",
      "Calculus",
      "Implicit derivative"
    ]
  },
  {
    "objectID": "implicit_derivative.html#derivative-of-an-implicit-function",
    "href": "implicit_derivative.html#derivative-of-an-implicit-function",
    "title": "Implicit derivatives",
    "section": "Derivative of an implicit function",
    "text": "Derivative of an implicit function\nWe can almost antecipate, since the function is not given explicitly, chances are, its derivative is also not given explicitly.\nPlease note that when a function is given explicitly, we used the derivatives rules to obtain an explicit derivative function.\nWhen dealing with implicit function, i.e., functions on which we only know the statement - in the case above \\(x^2+y^2=1\\) - that it obeys, the same thing occurs with its derivative, we’ll only know a statement it obeys.\nThe way to obtain that statement is to use the chain rule.\nTake the derivative of the lhs and rhs of \\(x^2 +y^2=1\\), the result is:\n\\[\n(x^2+y^2)'=1'\n\\implies 2x+2y(x)y'(x) = 0\n\\]\nOn the rhs we find the statement: whatever the function \\(y'(x)\\) is, it obeys that equation, hence to find what is the value of \\(y'(x)\\) corresponding to the chosen \\(x\\), we have to solve the equation \\(2x+2y(x)y'(x) = 0\\). To do that, chose an \\(x\\), compute \\(y(x)\\) using whatever means necessary (the function \\(y(x)\\) is implicit!), plug in that result in the equation above and in turn solve it for \\(y'(x)\\).\nAs you can see the sequence of steps that one must follow to arrive at one evaluation is implicit!",
    "crumbs": [
      "Brief Notes",
      "Calculus",
      "Implicit derivative"
    ]
  },
  {
    "objectID": "functions_as_relations.html",
    "href": "functions_as_relations.html",
    "title": "asymptotic to MARS",
    "section": "",
    "text": "A relation is a general connection between elements of two sets, consider the sets\n\\[\\begin{equation}\\begin{split}&Friends\\coloneqq \\{\\text{Abby, Eve, John, Hugo}\\}\\\\&Heights\\coloneqq \\{\\dots,1.60,1.70,1.80,1.90,\\dots\\}\\end{split} \\end{equation}\\]\nA connection between elements of both sets can be established by making a list of ordered pairs of the form \\((friend,height)\\), this can be done in many ways, here are two examples:\n\\[ \\begin{align}\n&G\\coloneqq \\{(\\text{Abby,1.70}),(\\text{Eve,1.80}),(\\text{Eve,1.70}),(\\text{John,1.80}),(\\text{Hugo,1.60})\\}\\\\\n&F\\coloneqq \\{(\\text{Abby,1.70}),(\\text{Eve,1.70}),(\\text{John,1.80}),(\\text{Hugo,1.60})\\}\n\\end{align}  \\tag{1}\\]\nThe process of choosing elements of \\(Friends\\) and \\(Heigts\\) and pairing them to build \\(G\\) or \\(F\\) can be viewed another way, we start by constructing all possible pair and save them into large list we call the Cartesian product between Friends and Heights, denoted by\n\\[ \\begin{equation}Friends \\times Heights \\coloneqq \\{(\\text{friend,height})\\,\\,|\\,\\,\\text{friend} \\in \\text{Friends}\\,\\, \\textit{and}\\,\\, \\text{height} \\in \\text{Heights}\\}\\end{equation}  \\tag{2}\\]\nThen we choose from \\(Friends \\times Heights\\) then pairs we want for \\(G\\) or \\(F\\).\nFrom this point of view, every subset of \\(Friends \\times Heights\\) is a relation and in particular we have:\n\\[\n\\begin{equation}\\begin{split} G \\subset Friends \\times Heights\\\\ F \\subset Friends \\times Heights\\\\ \\end{split} \\end{equation}\n\\]\n\\(G\\) and \\(F\\) are relations but only the latter achieves the status of a function since \\(F\\) has a special property not share with \\(G\\). Close inspection of \\(F\\) reveals that if one chooses a friend, say Eve, the function \\(F\\) tells us immediately and unquestionably that her height is \\(1.70\\). On the other hand, \\(G\\) wont since we find two pairs, which assign Eve with \\(1.80\\) or \\(1.70\\). Observation of the remainder pairs in \\(F\\) shows that the pairing in is in such a way that for any \\(friend\\) we choose, we have an associated \\(height\\) that is unique, as a result each first entry of the pairs only occurs once in the list Equation 1, thus avoiding the ambiguity found in \\(G\\) of having two heights assigned to the same friend. This is the special property that a list of ordered pairs (i.e. relation) must have to be classified as a function; irrelevant to this is whether when we pick an height we have none, or one or more friends. For example the next subsets of Equation 2 assign the same height to two or all friends:\n\\[ \\begin{equation}\\begin{split}I\\coloneqq \\{(\\text{Abby,1.80}),(\\text{Eve,1.60}),(\\text{John,1.80}),(\\text{Hugo,1.60})\\}\\\\J\\coloneqq \\{(\\text{Abby,1.80}),(\\text{Eve,1.80}),(\\text{John,1.80}),(\\text{Hugo,1.80})\\}\\end{split}\\end{equation} \\]\nThe lists \\(I\\) and \\(J\\) are relation and also functions, as each friend only has one height, thus appearing only one time in the list.\nFrom this discussion we can distill the essence of what a function is in the following definition:\n\nDefinition 1  \n\nLet \\(X\\) and \\(Y\\) be two sets. A function \\(F\\) is a list of pairs \\((x,y)\\) with \\(x\\) belonging to \\(X\\) and \\(y\\) in \\(Y\\) with the special property that each \\(x\\) is associated uniquely with one \\(y\\) (the reverse may or may not be true). In other words, \\(F\\) is a special subset of \\(X\\times Y\\) that makes the following statement true\n\\[\n\\forall x : \\exists ! y : (x,y) \\in F\n\\tag{3}\\]\nThe domain of \\(F\\) is the set of all \\(x\\)’s appearing in the pairs of \\(F\\), while the range is the the set of \\(y\\)’s; these sets are named \\(D_F\\) and \\(R_F\\) respectively. The set \\(Y\\) is called the codomain of \\(F\\) and may contain or be equal to \\(R_F\\); \\(X\\) doesn’t have an official name, lets call it the starting set, and may contain or be equal to \\(D_F\\), many times \\(X=D_F\\).\n\n\nExercise\n\n\n\n\n\n\nCommentary\n\n\n\nThe statement in Equation 3 is neither true nor false until we specify what \\(F\\) is. It works like an “equation” where the unknown in \\(F\\). We read it as follows: For all \\(x\\) there is only one \\(y\\) that make true \\((x,y)\\in F\\). This can only occur provided we chose an \\(F\\) where all pairs have \\(x\\)’s assigned to unique \\(y\\)’s. The expression \\(\\forall x : \\exists ! y : (x,y) \\in\\) acts on \\(F\\), by checking its content.",
    "crumbs": [
      "Brief Notes",
      "Basic Maths",
      "Functions",
      "Functions as relations"
    ]
  },
  {
    "objectID": "functions_as_relations.html#functions-as-relations",
    "href": "functions_as_relations.html#functions-as-relations",
    "title": "asymptotic to MARS",
    "section": "",
    "text": "A relation is a general connection between elements of two sets, consider the sets\n\\[\\begin{equation}\\begin{split}&Friends\\coloneqq \\{\\text{Abby, Eve, John, Hugo}\\}\\\\&Heights\\coloneqq \\{\\dots,1.60,1.70,1.80,1.90,\\dots\\}\\end{split} \\end{equation}\\]\nA connection between elements of both sets can be established by making a list of ordered pairs of the form \\((friend,height)\\), this can be done in many ways, here are two examples:\n\\[ \\begin{align}\n&G\\coloneqq \\{(\\text{Abby,1.70}),(\\text{Eve,1.80}),(\\text{Eve,1.70}),(\\text{John,1.80}),(\\text{Hugo,1.60})\\}\\\\\n&F\\coloneqq \\{(\\text{Abby,1.70}),(\\text{Eve,1.70}),(\\text{John,1.80}),(\\text{Hugo,1.60})\\}\n\\end{align}  \\tag{1}\\]\nThe process of choosing elements of \\(Friends\\) and \\(Heigts\\) and pairing them to build \\(G\\) or \\(F\\) can be viewed another way, we start by constructing all possible pair and save them into large list we call the Cartesian product between Friends and Heights, denoted by\n\\[ \\begin{equation}Friends \\times Heights \\coloneqq \\{(\\text{friend,height})\\,\\,|\\,\\,\\text{friend} \\in \\text{Friends}\\,\\, \\textit{and}\\,\\, \\text{height} \\in \\text{Heights}\\}\\end{equation}  \\tag{2}\\]\nThen we choose from \\(Friends \\times Heights\\) then pairs we want for \\(G\\) or \\(F\\).\nFrom this point of view, every subset of \\(Friends \\times Heights\\) is a relation and in particular we have:\n\\[\n\\begin{equation}\\begin{split} G \\subset Friends \\times Heights\\\\ F \\subset Friends \\times Heights\\\\ \\end{split} \\end{equation}\n\\]\n\\(G\\) and \\(F\\) are relations but only the latter achieves the status of a function since \\(F\\) has a special property not share with \\(G\\). Close inspection of \\(F\\) reveals that if one chooses a friend, say Eve, the function \\(F\\) tells us immediately and unquestionably that her height is \\(1.70\\). On the other hand, \\(G\\) wont since we find two pairs, which assign Eve with \\(1.80\\) or \\(1.70\\). Observation of the remainder pairs in \\(F\\) shows that the pairing in is in such a way that for any \\(friend\\) we choose, we have an associated \\(height\\) that is unique, as a result each first entry of the pairs only occurs once in the list Equation 1, thus avoiding the ambiguity found in \\(G\\) of having two heights assigned to the same friend. This is the special property that a list of ordered pairs (i.e. relation) must have to be classified as a function; irrelevant to this is whether when we pick an height we have none, or one or more friends. For example the next subsets of Equation 2 assign the same height to two or all friends:\n\\[ \\begin{equation}\\begin{split}I\\coloneqq \\{(\\text{Abby,1.80}),(\\text{Eve,1.60}),(\\text{John,1.80}),(\\text{Hugo,1.60})\\}\\\\J\\coloneqq \\{(\\text{Abby,1.80}),(\\text{Eve,1.80}),(\\text{John,1.80}),(\\text{Hugo,1.80})\\}\\end{split}\\end{equation} \\]\nThe lists \\(I\\) and \\(J\\) are relation and also functions, as each friend only has one height, thus appearing only one time in the list.\nFrom this discussion we can distill the essence of what a function is in the following definition:\n\nDefinition 1  \n\nLet \\(X\\) and \\(Y\\) be two sets. A function \\(F\\) is a list of pairs \\((x,y)\\) with \\(x\\) belonging to \\(X\\) and \\(y\\) in \\(Y\\) with the special property that each \\(x\\) is associated uniquely with one \\(y\\) (the reverse may or may not be true). In other words, \\(F\\) is a special subset of \\(X\\times Y\\) that makes the following statement true\n\\[\n\\forall x : \\exists ! y : (x,y) \\in F\n\\tag{3}\\]\nThe domain of \\(F\\) is the set of all \\(x\\)’s appearing in the pairs of \\(F\\), while the range is the the set of \\(y\\)’s; these sets are named \\(D_F\\) and \\(R_F\\) respectively. The set \\(Y\\) is called the codomain of \\(F\\) and may contain or be equal to \\(R_F\\); \\(X\\) doesn’t have an official name, lets call it the starting set, and may contain or be equal to \\(D_F\\), many times \\(X=D_F\\).\n\n\nExercise\n\n\n\n\n\n\nCommentary\n\n\n\nThe statement in Equation 3 is neither true nor false until we specify what \\(F\\) is. It works like an “equation” where the unknown in \\(F\\). We read it as follows: For all \\(x\\) there is only one \\(y\\) that make true \\((x,y)\\in F\\). This can only occur provided we chose an \\(F\\) where all pairs have \\(x\\)’s assigned to unique \\(y\\)’s. The expression \\(\\forall x : \\exists ! y : (x,y) \\in\\) acts on \\(F\\), by checking its content.",
    "crumbs": [
      "Brief Notes",
      "Basic Maths",
      "Functions",
      "Functions as relations"
    ]
  },
  {
    "objectID": "favourites.html",
    "href": "favourites.html",
    "title": "Randomm",
    "section": "",
    "text": "beans\nseeds\ntofu\nlegumes\nmore"
  },
  {
    "objectID": "favourites.html#my-list-of-things",
    "href": "favourites.html#my-list-of-things",
    "title": "Randomm",
    "section": "",
    "text": "beans\nseeds\ntofu\nlegumes\nmore"
  },
  {
    "objectID": "favourites.html#fun-picture-1",
    "href": "favourites.html#fun-picture-1",
    "title": "Randomm",
    "section": "Fun Picture 1",
    "text": "Fun Picture 1\n\n\n\n\n\n\nFigure 1: When you keep using the same strategy, expecting …..\n\n\n\n… unless you are having fun; some music ."
  },
  {
    "objectID": "favourites.html#fun-picture-2",
    "href": "favourites.html#fun-picture-2",
    "title": "Randomm",
    "section": "Fun Picture 2",
    "text": "Fun Picture 2\n\n\n\n\n\n\nFigure 2: When you keep using the same strategy, expecting …..\n\n\n\nTrue, both pictures have the same caption"
  },
  {
    "objectID": "favourites.html#best-movies-of-all-time.",
    "href": "favourites.html#best-movies-of-all-time.",
    "title": "Randomm",
    "section": "Best Movies of ALL TIME.",
    "text": "Best Movies of ALL TIME.\n\nSucker Punch\nReturn of the King\nMatrix: Reloaded\nMatrix: Revolution\nMatrix\nAnything Rambo or Steven Seagel\nEtc.\n(no more questions)"
  },
  {
    "objectID": "favourites.html#useful-links",
    "href": "favourites.html#useful-links",
    "title": "Randomm",
    "section": "Useful links",
    "text": "Useful links\n\nPeace of Mind good!\nArt or whatever..\nFunFun Short"
  },
  {
    "objectID": "favourites.html#oracle-advice-for-2024",
    "href": "favourites.html#oracle-advice-for-2024",
    "title": "Randomm",
    "section": "Oracle advice for 2024",
    "text": "Oracle advice for 2024\n\n\\[ 2024 = 2^3 \\times 11 \\times 23 \\]\n\nI actually payed for this advice."
  },
  {
    "objectID": "favourites.html#the-secret-code",
    "href": "favourites.html#the-secret-code",
    "title": "Randomm",
    "section": "The Secret Code",
    "text": "The Secret Code\ndef Update(x):\n  return 10 * x\n…that solves all Earth’s problems according to the gospel of Marx."
  },
  {
    "objectID": "favourites.html#a-one-inch-long-equation",
    "href": "favourites.html#a-one-inch-long-equation",
    "title": "Randomm",
    "section": "A one inch-long Equation",
    "text": "A one inch-long Equation\n\\[ F = ma \\]\n\n\n\n\n\n\nTip\n\n\n\nThe equation is a differential vector equation. Aesthetically, it is a pleasing equation as written, but it is more meaningfully pleasing by switching the right and left hand sides."
  },
  {
    "objectID": "favourites.html#testing-the-dirac-notation",
    "href": "favourites.html#testing-the-dirac-notation",
    "title": "Randomm",
    "section": "Testing the Dirac Notation",
    "text": "Testing the Dirac Notation\n\\[ i\\hbar\\frac{\\partial |\\psi\\rangle}{\\partial t} = \\hat{H}|\\psi\\rangle  \\]\nIt works."
  },
  {
    "objectID": "favourites.html#the-song-for-2024",
    "href": "favourites.html#the-song-for-2024",
    "title": "Randomm",
    "section": "THE Song for 2024",
    "text": "THE Song for 2024\n\n\nReferences for this outstanding site:\nMaster-Cohen Tannoudji, Quantum Mechanics, f#ing-Wiley"
  },
  {
    "objectID": "Example_of_pol_division_5.html",
    "href": "Example_of_pol_division_5.html",
    "title": "example_of_pol_division_5",
    "section": "",
    "text": "Another useful decomposition is:\n\\[ 1001=600+401 \\]\nHow would we argue in this case?\nGuess 1:\nObserving that \\(1001=600+401\\) and \\(5\\times 12 =60\\) we guess that actual solution \\(q\\) is close to \\(10\\times 5\\):\n\\[ 600+401 = \\overbrace{10 \\times 5}^{q} \\times 12 +r \\]\nwith the remainder \\(r=401\\). Since it is not between \\(0\\) and \\(12\\) we can do better. Lets break \\(401\\) by guessing what is \\(q'\\) and \\(r'\\) such that:\n\\[ 401 = q'\\times 12 + r' \\qquad 0\\leq r'&lt;12 \\]\nWe think \\(q'=3\\times10\\) is a good solution since \\(q'\\times 12 = 360\\), as a result the remainder is \\(r'=401-360=41\\)\nNot ideal, we want it at most \\(12\\). Therefore we again break into pieces:\n\\[ 41 = q''\\times 12 + r'' \\qquad 0\\leq r''&lt;12 \\]\nWith the guess \\(q''=3\\) and the current remainder is just \\(r''=41-36=5\\).\nNote, finally!! We have a remainder smaller than \\(12\\).\nCollecting out calculations we have:\n\\[ 1001 = \\overbrace{(10 \\times 5 + 3\\times 10 + 3)}^{q=83}\\times 12 + 5 \\]"
  },
  {
    "objectID": "Example_of_pol_division_5.html#example-5-100112-using-basic-principles-b",
    "href": "Example_of_pol_division_5.html#example-5-100112-using-basic-principles-b",
    "title": "example_of_pol_division_5",
    "section": "",
    "text": "Another useful decomposition is:\n\\[ 1001=600+401 \\]\nHow would we argue in this case?\nGuess 1:\nObserving that \\(1001=600+401\\) and \\(5\\times 12 =60\\) we guess that actual solution \\(q\\) is close to \\(10\\times 5\\):\n\\[ 600+401 = \\overbrace{10 \\times 5}^{q} \\times 12 +r \\]\nwith the remainder \\(r=401\\). Since it is not between \\(0\\) and \\(12\\) we can do better. Lets break \\(401\\) by guessing what is \\(q'\\) and \\(r'\\) such that:\n\\[ 401 = q'\\times 12 + r' \\qquad 0\\leq r'&lt;12 \\]\nWe think \\(q'=3\\times10\\) is a good solution since \\(q'\\times 12 = 360\\), as a result the remainder is \\(r'=401-360=41\\)\nNot ideal, we want it at most \\(12\\). Therefore we again break into pieces:\n\\[ 41 = q''\\times 12 + r'' \\qquad 0\\leq r''&lt;12 \\]\nWith the guess \\(q''=3\\) and the current remainder is just \\(r''=41-36=5\\).\nNote, finally!! We have a remainder smaller than \\(12\\).\nCollecting out calculations we have:\n\\[ 1001 = \\overbrace{(10 \\times 5 + 3\\times 10 + 3)}^{q=83}\\times 12 + 5 \\]"
  },
  {
    "objectID": "Example_of_pol_division_5.html#commentary",
    "href": "Example_of_pol_division_5.html#commentary",
    "title": "example_of_pol_division_5",
    "section": "Commentary",
    "text": "Commentary\nBreaking \\(p\\) into powers of then has proven to be useful to give us clues about what values of \\(q\\) to guess. Another strategy is to break \\(q\\) (in the example above \\(p=1001\\)) into a large and a smaller part.\n\\[ p=[\\text{large part}]+[\\text{small part}]  \\]\nAnd then guess the best you can the \\(q\\) that cancels the large part.\nFor example \\(1001=100\\cdot 10+1\\) where the large part is \\(100\\cdot10\\), additionally \\(100\\) is divisible by \\(12\\) since its larger.\nThere are many ways to break \\(p\\), experience will tell you what is ideal or not."
  },
  {
    "objectID": "examples_of_pol_part_5.html",
    "href": "examples_of_pol_part_5.html",
    "title": "Focus-diretrix formulation",
    "section": "",
    "text": "Observing Figure 1 we observe two points: the vertex with coordinates \\((x_0,y_0)\\) and the focus \\(c\\) units above at \\((x_0,y_0+c)\\). A parabola is by definition the set of points which:\nWhen \\(c&gt;0\\) the polynomials describes a cup whose minimum is the vertex; when \\(c&lt;0\\) it describes a cap whose maximum is the vertex.\nLets express condition 1. and 2. using mathematical notation.\nUsing the distance formula we seen above, we can express both distances in terms of the coordinates of the vertex and focus points as\n\\[\n\\begin{cases}d_1^2=(x-x_0)^2+(y-(y_0+c))^2\\\\d_2^2=(y-(y_0-c))^2\\end{cases}\n\\]\nCondition 2. becomes:\n\\[\n\\begin{align}\n&(x-x_0)^2+(y-(y_0+c))^2 =(y-(y_0-c))^2\\\\\n\\iff &(x-x_0)^2+y^2-2(y_0+c)y+(y_0+c)^2 = y^2-2(y_0-c)y+(y_0-c)^2\\\\\n\\iff &(x-x_0)^2+y^2-2(y_0+c)y+y_0^2+2y_0c+c^2 = y^2-2(y_0-c)y+y_0^2-2y_0c+c^2\\\\\n\\iff&(x-x_0)^2-4cy+4y_0c=0\\\\\n\\iff&y=y_0 + \\frac{1}{4c}(x-x_0)^2\n\\end{align}\n\\]\nThis is the element-hood test for the points in the parabola allowing us describe the set the parabola as the set of pairs\n\\[\n\\begin{equation}P_2:= \\{(x,y)\\,\\,|\\,\\,d_1=d_2\\}=\\{(x,y)\\,\\,|\\,\\,y-y_0=\\frac{1}{4c}(x-x_0)^2\\}\\end{equation}\n\\tag{1}\\]\nUsing the procedure point of view we write:\n\\[\n\\begin{equation}\\begin{split}p_2:\\,\\, &\\mathbb{R}\\longrightarrow \\mathbb{R}\\\\&x \\longrightarrow p_2(x):= \\frac{1}{4c}(x-x_0)^2+y_0\\end{split}\\end{equation}\n\\tag{2}\\]\nThese formulas are equivalent to ?@eq-poldeg2, the difference lies in what the parameters mean! While in ?@eq-poldeg2, the \\(a_0\\), \\(a_1\\) and \\(a_2\\) are just the weights of \\(1\\), \\(x\\) and \\(x^2\\); in Equation 1, the \\(x_0\\) and \\(y_0\\) are the vertex point of the function and \\(c\\) is distance from the focus. An analogous situation occurred in polynomials of degree one when we introduced the mediatrix equation.",
    "crumbs": [
      "Brief Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Focus-diretrix formulation"
    ]
  },
  {
    "objectID": "examples_of_pol_part_5.html#zeros-of-these-polynomials",
    "href": "examples_of_pol_part_5.html#zeros-of-these-polynomials",
    "title": "Focus-diretrix formulation",
    "section": "Zeros of these polynomials:",
    "text": "Zeros of these polynomials:\nWe wish now to zoom into polynomials given as Equation 1 or Equation 2 and see the zeros of these function are expressed in term of the diretrix and focus properties.\nTo find vertical axis interception is to find the point \\((x,y)\\) in \\(P_2\\) whose \\(x=0\\):\n\\[\n\\begin{equation}\\begin{cases}y-y_0=\\frac{1}{4c}(x-x_0)^2\\\\x=0\\end{cases}\\implies y-y_0=\\frac{1}{4c}(0-x_0)^2\\implies y=\\frac{1}{4c}x_0^2+y_0\\end{equation}\n\\]\nwhich tells us the interception point coordinates \\((0,\\frac{1}{4c}x_0^2+y_0)\\) belonging to \\(P_2\\).\nWhen seeking for the points with \\(y=0\\) we must be more careful since they may not exist (the parabola might be completely above the x-axis). By substituting again \\(y=0\\) in the element-hood test we get:\n\\[\n\\begin{equation}\\begin{cases}y-y_0=\\frac{1}{4c}(x-x_0)^2\\\\y=0\\end{cases}\\implies(x-x_0)^2=-4cy_0\\end{equation}\n\\]\nWe must be careful when solving for \\(x\\) the later equation since we must have a positive right hand side, this is only possible if the parameter \\(y_0\\leq 0\\). This is the condition for the existence of zeros, i.e., it is the condition that ensures us the parabola is not completely above the x-axis.\n\n\n\nA parabola and the x-axis (diretrix and focus NOT shown).\n\n\nSaying \\(y_0\\leq 0\\) either means, \\(y_0=0\\), in which case we immediately know \\(x=x_0\\) and thus \\((x_0,0)\\) is the ONLY horizontal axis interception (the vertex sits exactly at the x-axis); or it means \\(y_0&lt;0\\), in this case we solve the equation for \\(x\\) as follows\n\\[\n\\begin{equation}\\begin{split}(x-x_0)^2&=4c|y_0|\\\\&\\implies x-x_0=\\pm\\sqrt{4c|y_0|}\\\\&\\implies x=x_0\\pm \\sqrt{4c|y_0|}\\end{split}\\end{equation}\n\\]\nThere are two interception, at \\((x_0+ \\sqrt{4c|y_0|},0)\\) and another at \\((x_0-\\sqrt{4c|y_0|},0)\\), which is expected when part of the parabola is below the x-axis.",
    "crumbs": [
      "Brief Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Focus-diretrix formulation"
    ]
  },
  {
    "objectID": "examples_of_pol_part_5.html#when-x-is-large",
    "href": "examples_of_pol_part_5.html#when-x-is-large",
    "title": "Focus-diretrix formulation",
    "section": "When \\(x\\) is large:",
    "text": "When \\(x\\) is large:\nTo understand the behavior when \\(x\\) is very large, the extreme ends of \\(P_2\\), we have to note the following:\n\nObserve the term \\((x-x_0)^2\\), it is a squared of a number \\(x-x_0\\), hence always positive. This implies the smallest value it takes is zero.\nWhen \\(c\\) is positive, then \\((x-x_0)^2/4c\\) is positive for any \\(x\\), the minimum is zero when we set \\(x-x_0=0\\).\n\nThe procedure \\((x-x_0)^2/4c+y_0\\) results from the addition of a strictly positive number \\((x-x_0)^2/4c\\) to \\(y_0\\), hence the minimum value it can take is \\(y_0\\) when \\(x=x_0\\).\nNow let us turn into what happens with the function when \\(x\\) becomes very large (\\(x\\longrightarrow \\pm\\infty\\)). Another glance at the procedure tell us that the quantity \\((x-x_0)^2\\) becomes very large either way, multiplying it now by \\(1/4c\\) we must be clear about the sign of \\(c\\). If it is positive the function will grow without end in either case, if \\(c\\) is negative, it becomes very small.",
    "crumbs": [
      "Brief Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Focus-diretrix formulation"
    ]
  },
  {
    "objectID": "examples_of_pol_part_3.html",
    "href": "examples_of_pol_part_3.html",
    "title": "Line’s equation from two given points",
    "section": "",
    "text": "As we have seen a polynomial of degree one can either be specified by providing the parameters \\(a_0\\) and \\(a_1\\) or alternatively (as the author prefers by the way) by providing a point \\((x_0,y_0)\\) on the line and a slope value \\(m\\). This is what we know so far. In this section we see a third way of specifying one of these polynomials, namely, to provide the coordinates of any two distinct point on the line \\((x_0,y_0)\\) and \\((x_1, y_1)\\). We can already sense it, somehow the two point must determine \\(a_0\\) and \\(a_1\\) or \\((x_0,y_0)\\) and \\(m\\).\nHere is how to do it:\n\nHave the two point \\((-2,3)\\) and \\((3,-7)\\). These are a given. No action needed.\nAssume they both belong to the same line, i.e.,\n\\[\n\\begin{align}\n(-2,3)\\in \\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = a_1x+a_0 \\}\\\\\n(3,-7)\\in \\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = a_1x+a_0 \\}\n\\end{align}\n\\tag{1}\\]\nfor some \\(a_1\\) and \\(a_0\\) still unknown but we wish to determine. These two statements characterize the parameters \\(a_1\\) and \\(a_0\\).\nWe want to compute the values of \\(a_0\\) and \\(a_1\\) that make true the statements Equation 1. The statements Equation 1 are equivalent to the following system of equations:\n\\[\n\\begin{cases}\n3=a_1\\times(-2)+a_0\\\\\n-7=a_1\\times 3 +a_0\n\\end{cases}\n\\]\nJust plug in the point coordinates into the element-hood test of the sets \\(y=a_1x+a_0\\).\nSolve the system of equations using substitution, the solution is:\n\\[\na_0=-1 \\qquad a_1=-2\n\\]\nThe polynomial’s formula that passes through both points is:\n\\[\ny=-2 x-1\n\\tag{2}\\]\n\nAn alternative method is to find \\(m\\) from the coordinates of the two point using the formula ?@eq-slope_formula and then set \\((x_0,y_0)\\) in \\(y=m(x-x_0)+y_0\\) as either one of the two point.\nHere are the steps:\n\nCompute \\(m = (-7-3)/(3-(-2)) = -2\\). Warning! When using ?@eq-slope_formula, make sure you order the two point by the \\(x\\)-coordinate. In this example I assigned \\((x_0,y_0)=(-2,3)\\) and \\((x_1,y_1)=(3,-7)\\) because \\(-2&lt;3\\). This guarantees the denominator of ?@eq-slope_formula is positive.\nChoose the \\(x_0\\) and \\(y_0\\) in \\(y=m(x-x_0)+y_0\\) as either \\((-2,3)\\) or \\((3,-7)\\). I choose\n\n\\[\n(x_0,y_0) = (3,-7)\n\\]\n\nWrite the final result:\n\\[\ny=-2(x-3)-7\n\\]\nwhich is equivalent to Equation 2.",
    "crumbs": [
      "Brief Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Line's equation from two points"
    ]
  },
  {
    "objectID": "examples_of_pol_part_2.5.html",
    "href": "examples_of_pol_part_2.5.html",
    "title": "asymptotic to MARS",
    "section": "",
    "text": "Polynomials of degree one are lists of ordered pairs of the form:\n\\[\n\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y=a_1 x+a_0\\}\n\\tag{1}\\]\nThe constants \\(a_1\\) and \\(a_0\\) are either given to us or we must choose them - we say they are parameter of the function - they are the weights of \\(x^1\\) and \\(x^0\\) in the linear combination \\(a_1x+a_0\\). Here are some examples of choices:\n\\[\n\\begin{align}\n&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = -2x \\}\\\\\n&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = 2x \\}\\\\\n&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = x+1 \\}\\\\\n&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = -2x-1 \\}\n\\end{align}\n\\]\nGraphically these corresponds to lines in a plane:\n\n\n\n\n\n\n\n\n\nIn maths, sometimes, changing just the aesthetic of the problem can bring great insight into what it means. Here is some changes into the aesthetic of the equation \\(y=a_1x+a_0\\):\n\\[\ny=a_1x+a_0 = a_1 x + a_0 + a_1 x_0 -a_1 x_0 = a_1(x-x_0)+(a_0+a_1x_0)\n\\tag{2}\\]\nNow define the following quantities:\n\\[\nm:=a_1 \\qquad y_0:=a_0+a_1x_0\n\\tag{3}\\]\nEven if you have no clue how this can be a good idea, just wait and appreciate the consequences - the equation \\(y=a_1x+a_0\\) shows up dressed differently:\n\\[\ny=m(x-x_0)+y_0\n\\tag{4}\\]\nthe new clothing is introduced in Equation 3 .\nIt look different, a new aesthetic, but it is still the same equation as \\(y=a_1x+a_0\\) because the parameters \\(a_1\\), \\(a_0\\), \\(m\\), \\(x_0\\) and \\(y_0\\) are related through the equations Equation 3. If values of some of these parameters are given, these equations can be used to compute the other.\nAs example: You give \\(a_0\\) and \\(a_1\\) and I can compute \\(x_0\\) and \\(y_0\\). How? If you give me \\(a_0=3\\) and \\(a_1=2\\), I pick an arbitrary \\(x_0\\), for example \\(x_0=4\\) and compute\n\\[\ny_0=3+2\\times 4=11\n\\]\nIn conclusion we have two equations aesthetically different but exactly the same:\n\\[\ny=2x+3 \\iff y=2(x-4)+11\n\\tag{5}\\]\nThe right hand side aesthetic is much better because it allow us to see easily how fast these polynomials of degree one increase.\n\n\n\nThe constant \\(m\\) (the \\(2\\) in Equation 5) is called the slope of the line, what does it mean the value of the slope? The meaning of this constant - in fact any constant in mathematics - can be deduced by analyzing the equation in which it appears. In this case, we understand \\(m\\) by understanding \\(y=m(x-x_0)+y_0\\), rearranging it we find:\n\\[\nm=\\frac{y-y_0}{x-x_0}\n\\tag{6}\\]\nwhich tells us that \\(m\\) is a ratio of two distances, the height of the triangle \\(y-y_0\\) and the length of the base of the triangle \\(x-x_0\\).\n\n\n\n\n\n\nFigure 1: A line and two point.\n\n\n\nSince \\(m\\) is the ratio of these two distances, its sign determines weather the function is increasing or not. Observing ?@fig-window we see \\(x-x_0&gt;0\\) and \\(y-y_0&gt;0\\) therefore for this particular choice of two points we find \\(m&gt;0\\), the same conclusion would hold for any other. When \\(m\\) is positive, then the \\(y\\)’s in \\((x,y)\\) increase when the \\(x\\)’s increase as well.\nBy the same line of reasoning the \\(m&lt;0\\) would correspond to decreasing function, one where as \\(x\\) increases then the \\(y\\)’s decrease. And by the way, \\(m=0\\) correspond to horizontal lines but these are polynomials of degree zero and not of degree one. The larger \\(m\\) becomes the closer the line is to a vertical line, but it never truly becomes a vertical line.\nWe can actually arrive at these conclusions just looking at \\(y=m(x-x_0)+y_0\\), no rearranging required: when \\(m\\) is positive, then the greater \\(x-x_0\\) is, the more we add to the default value \\(y_0\\), thus the \\(y\\) must increase; if on the other hand \\(m\\) is negative, the term \\(m(x-x_0)\\) can only become more negative as \\(x-x_0\\) increases, thus lowering the value of \\(m(x-x_0)+y_0\\), that is, the value of \\(y\\), the function must be decreasing when \\(x-x_0\\) increases.\n\n\n\nWhat are the zeros in \\(P_1\\)? Look at Figure 1, there are (in this case) two types of zeros: \\(x\\)-zeros and \\(y\\)-zeros. The \\(x\\)-zeros occur when the \\(x\\) coordinate is zero and the \\(y\\)-zeros when \\(y\\) is zero. In other words, the zeros of a polynomials of degree one are the ordered pairs (in \\(P_1\\)) that satisfy either one of these equations:\n\\[\n\\begin{equation}\\begin{cases}y=y_0+m(x-x_0)\\\\y=0\\end{cases}\\qquad\\qquad\\begin{cases}y=y_0+m(x-x_0)\\\\x=0\\end{cases}\\end{equation}\n\\]\n\\[\n\\begin{equation}\\begin{cases}y_0=a_1x_0+a_0\\\\y=0\\end{cases}\\qquad\\qquad\\begin{cases}y=a_1x+a_0\\\\x=0\\end{cases}\\end{equation}\n\\]\n\\[\n(x_0,0)\\in \\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = a_1x+a_0 \\}\n\\]\n\\[\n(0,y_0)\\in \\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = a_1x+a_0 \\}\n\\]\nSolving the first systems of equations we get \\(x=(0-y_0)/m+x_0\\) and solving the second \\(y=y_0+m(0-x_0)\\). Hence we know zeros of \\(P_1\\):\n\\[\n\\begin{align}\n&\\textit{x-zero:} \\qquad (0,y_0-mx_0)\\\\\n&\\textit{y-zero:} \\qquad (x_0-y_0/m,0)\n\\end{align}\n\\]\n\n\n\nWe can get to know qualitatively what happens with points coordinates of \\(P_1\\) at its extreme ends of the set by supposing \\(x\\) is a very, very large number; there is however an important key point: we are not particularly worried about a specific large value of \\(x\\), like \\(x=10^{100}\\) , and its corresponding \\(y\\); what we really want to know is - as \\(x\\) is getting larger and larger, how does \\(y\\) behave then? It is a dynamical process, not a specific evaluation of the function.\n\nParaphrased differently, we want to know what happens to \\((x,y)\\) during the process of making \\(x\\) bigger and bigger? To state that \\(x\\) is getting bigger and bigger, we write \\(x\\longrightarrow \\infty\\). In such a dynamic case, what happens with the value \\(y\\)? Meaning, what is the dynamic solution \\(y\\) of the system:\n\\[\n\\begin{equation}\\begin{cases}y=y_0+m(x-x_0)\\\\x\\longrightarrow +\\infty\\end{cases}\\end{equation}\n\\tag{7}\\]\nLets make an intuitive guess about the solution of this problem (later, when we study limits we’ll see a more systematic way to solve these problems), the Figure 2 helps a lot. To understand how the values of \\(y\\) behave when \\(x\\) increases, we have to understand how the calculation \\(y_0+m(x-x_0)\\) behaves, when we keep increasing \\(x\\). A key aspect of it, is that the term \\(m x\\) gets larger and larger while \\(y_0-mx_0\\) remains a constant, eventually, \\(mx\\) is so much larger than this constant (we are increasing \\(x\\) after all) that our original equation \\(y=y_0+m(x-x_0)\\) becomes approximately given by \\(y\\sim mx\\). The functions \\(y_0+m(x-x_0)\\) and \\(mx\\) are different! But when \\(x\\) is large they are very similar because the constant \\(y_0-mx_0\\) becomes irrelevant. Now we shift our focus to the approximate equation and try to solve it:\n\\[\n\\begin{equation}\\begin{cases}y\\sim mx\\\\x\\longrightarrow +\\infty\\end{cases}\\end{equation}\n\\tag{8}\\]\nThis is easy, because its just an atomic polynomial. The dynamical solution to this problem is [ \\(y\\) will increase when \\(x\\) increases], we write the answer as \\(y\\longrightarrow +\\infty\\) when \\(m\\) is positive. As a consequence the answer to Equation 7 is also \\(y\\longrightarrow +\\infty\\).\nObserve that the function \\(y\\sim mx\\) is not equal to \\(y=y_0+m(x-x_0)\\), and thus their graphs are not equal, but when \\(x\\) is very large, they are similar, see picture below. Solving Equation 8 we automatically get the answer to the problem Equation 7 . Both answers are \\(y\\longrightarrow+\\infty\\).\n\n\n\n\n\n\nFigure 2: Does y grow when x grows?\n\n\n\nIf on the other hand \\(m\\) is negative then, by the same reasoning, \\(y\\) also becomes very negative.\nAwful Exercise: What happens to \\(y\\) when \\(x\\longrightarrow-\\infty\\)?",
    "crumbs": [
      "Brief Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Polynomials of degree one"
    ]
  },
  {
    "objectID": "examples_of_pol_part_2.5.html#polynomials-of-degree-one-are-lines",
    "href": "examples_of_pol_part_2.5.html#polynomials-of-degree-one-are-lines",
    "title": "asymptotic to MARS",
    "section": "",
    "text": "Polynomials of degree one are lists of ordered pairs of the form:\n\\[\n\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y=a_1 x+a_0\\}\n\\tag{1}\\]\nThe constants \\(a_1\\) and \\(a_0\\) are either given to us or we must choose them - we say they are parameter of the function - they are the weights of \\(x^1\\) and \\(x^0\\) in the linear combination \\(a_1x+a_0\\). Here are some examples of choices:\n\\[\n\\begin{align}\n&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = -2x \\}\\\\\n&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = 2x \\}\\\\\n&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = x+1 \\}\\\\\n&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = -2x-1 \\}\n\\end{align}\n\\]\nGraphically these corresponds to lines in a plane:\n\n\n\n\n\n\n\n\n\nIn maths, sometimes, changing just the aesthetic of the problem can bring great insight into what it means. Here is some changes into the aesthetic of the equation \\(y=a_1x+a_0\\):\n\\[\ny=a_1x+a_0 = a_1 x + a_0 + a_1 x_0 -a_1 x_0 = a_1(x-x_0)+(a_0+a_1x_0)\n\\tag{2}\\]\nNow define the following quantities:\n\\[\nm:=a_1 \\qquad y_0:=a_0+a_1x_0\n\\tag{3}\\]\nEven if you have no clue how this can be a good idea, just wait and appreciate the consequences - the equation \\(y=a_1x+a_0\\) shows up dressed differently:\n\\[\ny=m(x-x_0)+y_0\n\\tag{4}\\]\nthe new clothing is introduced in Equation 3 .\nIt look different, a new aesthetic, but it is still the same equation as \\(y=a_1x+a_0\\) because the parameters \\(a_1\\), \\(a_0\\), \\(m\\), \\(x_0\\) and \\(y_0\\) are related through the equations Equation 3. If values of some of these parameters are given, these equations can be used to compute the other.\nAs example: You give \\(a_0\\) and \\(a_1\\) and I can compute \\(x_0\\) and \\(y_0\\). How? If you give me \\(a_0=3\\) and \\(a_1=2\\), I pick an arbitrary \\(x_0\\), for example \\(x_0=4\\) and compute\n\\[\ny_0=3+2\\times 4=11\n\\]\nIn conclusion we have two equations aesthetically different but exactly the same:\n\\[\ny=2x+3 \\iff y=2(x-4)+11\n\\tag{5}\\]\nThe right hand side aesthetic is much better because it allow us to see easily how fast these polynomials of degree one increase.\n\n\n\nThe constant \\(m\\) (the \\(2\\) in Equation 5) is called the slope of the line, what does it mean the value of the slope? The meaning of this constant - in fact any constant in mathematics - can be deduced by analyzing the equation in which it appears. In this case, we understand \\(m\\) by understanding \\(y=m(x-x_0)+y_0\\), rearranging it we find:\n\\[\nm=\\frac{y-y_0}{x-x_0}\n\\tag{6}\\]\nwhich tells us that \\(m\\) is a ratio of two distances, the height of the triangle \\(y-y_0\\) and the length of the base of the triangle \\(x-x_0\\).\n\n\n\n\n\n\nFigure 1: A line and two point.\n\n\n\nSince \\(m\\) is the ratio of these two distances, its sign determines weather the function is increasing or not. Observing ?@fig-window we see \\(x-x_0&gt;0\\) and \\(y-y_0&gt;0\\) therefore for this particular choice of two points we find \\(m&gt;0\\), the same conclusion would hold for any other. When \\(m\\) is positive, then the \\(y\\)’s in \\((x,y)\\) increase when the \\(x\\)’s increase as well.\nBy the same line of reasoning the \\(m&lt;0\\) would correspond to decreasing function, one where as \\(x\\) increases then the \\(y\\)’s decrease. And by the way, \\(m=0\\) correspond to horizontal lines but these are polynomials of degree zero and not of degree one. The larger \\(m\\) becomes the closer the line is to a vertical line, but it never truly becomes a vertical line.\nWe can actually arrive at these conclusions just looking at \\(y=m(x-x_0)+y_0\\), no rearranging required: when \\(m\\) is positive, then the greater \\(x-x_0\\) is, the more we add to the default value \\(y_0\\), thus the \\(y\\) must increase; if on the other hand \\(m\\) is negative, the term \\(m(x-x_0)\\) can only become more negative as \\(x-x_0\\) increases, thus lowering the value of \\(m(x-x_0)+y_0\\), that is, the value of \\(y\\), the function must be decreasing when \\(x-x_0\\) increases.\n\n\n\nWhat are the zeros in \\(P_1\\)? Look at Figure 1, there are (in this case) two types of zeros: \\(x\\)-zeros and \\(y\\)-zeros. The \\(x\\)-zeros occur when the \\(x\\) coordinate is zero and the \\(y\\)-zeros when \\(y\\) is zero. In other words, the zeros of a polynomials of degree one are the ordered pairs (in \\(P_1\\)) that satisfy either one of these equations:\n\\[\n\\begin{equation}\\begin{cases}y=y_0+m(x-x_0)\\\\y=0\\end{cases}\\qquad\\qquad\\begin{cases}y=y_0+m(x-x_0)\\\\x=0\\end{cases}\\end{equation}\n\\]\n\\[\n\\begin{equation}\\begin{cases}y_0=a_1x_0+a_0\\\\y=0\\end{cases}\\qquad\\qquad\\begin{cases}y=a_1x+a_0\\\\x=0\\end{cases}\\end{equation}\n\\]\n\\[\n(x_0,0)\\in \\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = a_1x+a_0 \\}\n\\]\n\\[\n(0,y_0)\\in \\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = a_1x+a_0 \\}\n\\]\nSolving the first systems of equations we get \\(x=(0-y_0)/m+x_0\\) and solving the second \\(y=y_0+m(0-x_0)\\). Hence we know zeros of \\(P_1\\):\n\\[\n\\begin{align}\n&\\textit{x-zero:} \\qquad (0,y_0-mx_0)\\\\\n&\\textit{y-zero:} \\qquad (x_0-y_0/m,0)\n\\end{align}\n\\]\n\n\n\nWe can get to know qualitatively what happens with points coordinates of \\(P_1\\) at its extreme ends of the set by supposing \\(x\\) is a very, very large number; there is however an important key point: we are not particularly worried about a specific large value of \\(x\\), like \\(x=10^{100}\\) , and its corresponding \\(y\\); what we really want to know is - as \\(x\\) is getting larger and larger, how does \\(y\\) behave then? It is a dynamical process, not a specific evaluation of the function.\n\nParaphrased differently, we want to know what happens to \\((x,y)\\) during the process of making \\(x\\) bigger and bigger? To state that \\(x\\) is getting bigger and bigger, we write \\(x\\longrightarrow \\infty\\). In such a dynamic case, what happens with the value \\(y\\)? Meaning, what is the dynamic solution \\(y\\) of the system:\n\\[\n\\begin{equation}\\begin{cases}y=y_0+m(x-x_0)\\\\x\\longrightarrow +\\infty\\end{cases}\\end{equation}\n\\tag{7}\\]\nLets make an intuitive guess about the solution of this problem (later, when we study limits we’ll see a more systematic way to solve these problems), the Figure 2 helps a lot. To understand how the values of \\(y\\) behave when \\(x\\) increases, we have to understand how the calculation \\(y_0+m(x-x_0)\\) behaves, when we keep increasing \\(x\\). A key aspect of it, is that the term \\(m x\\) gets larger and larger while \\(y_0-mx_0\\) remains a constant, eventually, \\(mx\\) is so much larger than this constant (we are increasing \\(x\\) after all) that our original equation \\(y=y_0+m(x-x_0)\\) becomes approximately given by \\(y\\sim mx\\). The functions \\(y_0+m(x-x_0)\\) and \\(mx\\) are different! But when \\(x\\) is large they are very similar because the constant \\(y_0-mx_0\\) becomes irrelevant. Now we shift our focus to the approximate equation and try to solve it:\n\\[\n\\begin{equation}\\begin{cases}y\\sim mx\\\\x\\longrightarrow +\\infty\\end{cases}\\end{equation}\n\\tag{8}\\]\nThis is easy, because its just an atomic polynomial. The dynamical solution to this problem is [ \\(y\\) will increase when \\(x\\) increases], we write the answer as \\(y\\longrightarrow +\\infty\\) when \\(m\\) is positive. As a consequence the answer to Equation 7 is also \\(y\\longrightarrow +\\infty\\).\nObserve that the function \\(y\\sim mx\\) is not equal to \\(y=y_0+m(x-x_0)\\), and thus their graphs are not equal, but when \\(x\\) is very large, they are similar, see picture below. Solving Equation 8 we automatically get the answer to the problem Equation 7 . Both answers are \\(y\\longrightarrow+\\infty\\).\n\n\n\n\n\n\nFigure 2: Does y grow when x grows?\n\n\n\nIf on the other hand \\(m\\) is negative then, by the same reasoning, \\(y\\) also becomes very negative.\nAwful Exercise: What happens to \\(y\\) when \\(x\\longrightarrow-\\infty\\)?",
    "crumbs": [
      "Brief Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Polynomials of degree one"
    ]
  },
  {
    "objectID": "examples_of_pol.html",
    "href": "examples_of_pol.html",
    "title": "Examples of Polynomials",
    "section": "",
    "text": "In this section we analyse polynomials of degree one and two. These notes include the mediatrix equation for degree one and the focus-diretrix formulation for degree two."
  },
  {
    "objectID": "examples_of_pol.html#properties-of-the-atomic-polynomials",
    "href": "examples_of_pol.html#properties-of-the-atomic-polynomials",
    "title": "Examples of Polynomials",
    "section": "Properties of the atomic polynomials",
    "text": "Properties of the atomic polynomials\nRather than showing a picture of a small slice of the function as de did in Figure 1 we can instead zoom into some aspects of it.\n\nProperties of the connectivity between domain and codomain:\n\nRange: From inspection of Equation 1 (or Equation 2) we know the domain and codomain of these four functions is \\(\\mathbb{R}\\). The range of these functions is the image of the domain under the action of the function, we can infer what this image is by inspecting Figure 1, for example, the image of \\(\\mathbb{R}\\) under the action of \\(e_2\\) is the set of all positive reals as we check by the red curve hence we write \\(e_2(\\mathbb{R})=\\mathbb{R}^+_0\\). Similarly we find:\n\n\\[\ne_0(\\mathbb{R})=\\{1\\} \\qquad e_1(\\mathbb{R})=e_3(\\mathbb{R})=\\mathbb{R}\\qquad e_2(\\mathbb{R})=\\mathbb{R}^+_0\n\\]\n\nOnto: We also observe that for even powers of \\(n\\) such as \\(e_0\\) and \\(e_2\\), not all elements in the codomain \\(\\mathbb{R}\\) are hit by outputs, i.e., these are not onto function; as an example, [is \\(0\\) hit by some \\(e_0(x)\\) ? Is \\(-1\\) hit by \\(e_2(x)\\), for some \\(x\\) in its domain?] We can answer these question by trying to solve the equations \\(e_0(x)=0\\) and \\(e_2(x)=-1\\). To no avail! Neither have solutions \\(x\\) in the domain \\(\\mathbb{R}\\).\n1-1: Additionally nor are these even powers 1-1. Why? Because we can clearly see from Figure 1 that \\(e_0\\) is in fact the worst possible “labeller”, where all “fruits” \\(x\\) have exactly the same label \\(1\\), no other “label” in its codomain \\(\\mathbb{R}\\) is put into good use by this \\(e_0\\); the function \\(e_2\\) is not as bad since for each label we find just two elements in \\(\\mathbb{R}\\). The functions \\(e_1\\) and \\(e_3\\) are 1-1 since each element \\(x\\) in their domain have a unique element \\(y\\) in the codomain, hence if we choose a label \\(y\\) we uniquely know the \\(x\\). Summarizing:\n\\[\n\\begin{cases}e_0 \\qquad &\\text{worst possible labeler (one label for all inputs)}\\\\e_2 \\qquad  &\\text{bad labeller (one label for each pair of inputs)}\\\\e_1\\,\\, \\text{and}\\,\\, e_3 \\qquad &\\text{perfect (one label, one input)}\\end{cases}\n\\]\n\n\n\nProperties at interesting regions of the function:\n\nWho is bigger or smaller? The Figure 1 reveals something funny:\n\nfor points on the horizontal axis close to the origin, that is \\(-1&lt;x&lt;1\\), the outputs of the four functions have the following rank \\(x^3 &lt; x^2 &lt; x^1 &lt; x^0\\);\nprecisely at \\(x=1\\) we find they are all the same \\(x^0=x^1=x^2=x^3=1\\);\nmeanwhile for the remaining points, those in the region \\(x&lt;-1 \\lor 1&lt;x\\), the absolute value of these functions follow the reversed rank! \\(x^0&lt;x^1&lt;x^2&lt;x^3\\).\nFor \\(x\\) very far from \\(1\\) we find \\(x^0\\ll x^1\\ll x^2\\ll x^3\\), which means, for example, that \\(|x^3|\\) is much, much, much, much larger then \\(|x^2|\\), and so on.\n\nIn summary:\n\\[\n\\begin{cases}\nx^3 &lt; x^2 &lt; x^1 &lt; x^0 \\qquad &\\text{for $x$ close to 1}\\\\x^0=x^1=x^2=x^3 \\qquad &\\text{for $x=1$}\\\\x^0\\ll x^1\\ll x^2\\ll x^3 \\qquad &\\text{for $x$ far from $1$}\n\\end{cases}\n\\tag{3}\\]\nThis summary will be important when we study the limiting behavior of molecular polynomials.\nZeros: The zeros of a function are the coordinates of the points which intercept the axes. Observing Figure 1 we see polynomial \\(1\\) intercepts the y-axis at \\((0,1)\\) while all other polynomials intercept the x-axis and y-axis exactly at the origin \\((0,0)\\).\nMaxima and minima: By inspection of Figure 1, we observe the polynomials \\(1\\) has no maxima or minima, it is always flat; \\(x\\) and \\(x^3\\) on the other hand is a ramp, a never ending one, and thus also does not have a maximum or minimum. The only function exhibiting a minimum is \\(x^2\\), coincidentally, at \\((0,0)\\). Only when we study derivatives we’ll have the computational means to find these extrema.\n\n\n\n\n\n\n\nCommentary\n\n\n\nThe previous list of properties was already embedded in Figure 1 (or if you think about it, already at Equation 2), only by close observation we made it explicit. Of course, the list is not exhaustive, many other properties remain hidden in that picture, waiting to be dug out. This is why I said this list are a zoom into the picture, when we get closer we see more detail.\nAs additional examples we could also ask how fast a polynomial of degree one increases, i.e., what is its slope; for polynomials of degree two it would make sense to ask if it its graph is a cup or a cap, it all depends on \\(a_2\\) parameter.\nWe usually do not list the properties just as we did above, we just acknowledge in our mind that they exist and compute them as needed, since many times some of them are so bluntly obvious just by looking at the graph or the function formula, that no need is required for a calculation."
  },
  {
    "objectID": "examples_of_pol.html#polynomials-of-degree-one-are-lines",
    "href": "examples_of_pol.html#polynomials-of-degree-one-are-lines",
    "title": "Examples of Polynomials",
    "section": "Polynomials of degree one are lines",
    "text": "Polynomials of degree one are lines\nPolynomials of degree one are lists of ordered pairs of the form:\n\\[\n\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y=a_1 x+a_0\\}\n\\tag{4}\\]\nThe constants \\(a_1\\) and \\(a_0\\) are either given to us or we must choose them - we say they are parameter of the function - they are the weights of \\(x^1\\) and \\(x^0\\) in the linear combination \\(a_1x+a_0\\). Here are some examples of choices:\n\\[\n\\begin{align}\n&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = -2x \\}\\\\\n&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = 2x \\}\\\\\n&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = x+1 \\}\\\\\n&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = -2x-1 \\}\n\\end{align}\n\\]\nGraphically these corresponds to lines in a plane:\n\n\n\n\n\n\n\n\n\nIn maths, sometimes, changing just the aesthetic of the problem can bring great insight into what it means. Here is some changes into the aesthetic of the equation \\(y=a_1x+a_0\\):\n\\[\ny=a_1x+a_0 = a_1 x + a_0 + a_1 x_0 -a_1 x_0 = a_1(x-x_0)+(a_0+a_1x_0)\n\\tag{5}\\]\nNow define the following quantities:\n\\[\nm:=a_1 \\qquad y_0:=a_0+a_1x_0\n\\tag{6}\\]\nEven if you have no clue how this can be a good idea, just wait and appreciate the consequences - the equation \\(y=a_1x+a_0\\) shows up dressed differently:\n\\[\ny=m(x-x_0)+y_0\n\\tag{7}\\]\nthe new clothing is introduced in Equation 6 .\nIt look different, a new aesthetic, but it is still the same equation as \\(y=a_1x+a_0\\) because the parameters \\(a_1\\), \\(a_0\\), \\(m\\), \\(x_0\\) and \\(y_0\\) are related through the equations Equation 6. If values of some of these parameters are given, these equations can be used to compute the other.\nAs example: You give \\(a_0\\) and \\(a_1\\) and I can compute \\(x_0\\) and \\(y_0\\). How? If you give me \\(a_0=3\\) and \\(a_1=2\\), I pick an arbitrary \\(x_0\\), for example \\(x_0=4\\) and compute\n\\[\ny_0=3+2\\times 4=11\n\\]\nIn conclusion we have two equations aesthetically different but exactly the same:\n\\[\ny=2x+3 \\iff y=2(x-4)+11\n\\tag{8}\\]\nThe right hand side aesthetic is much better because it allow us to see easily how fast these polynomials of degree one increase.\n\n\nHow fast does the function increase?\nThe constant \\(m\\) (the \\(2\\) in Equation 8) is called the slope of the line, what does it mean the value of the slope? The meaning of this constant - in fact any constant in mathematics - can be deduced by analyzing the equation in which it appears. In this case, we understand \\(m\\) by understanding \\(y=m(x-x_0)+y_0\\), rearranging it we find:\n\\[\nm=\\frac{y-y_0}{x-x_0}\n\\tag{9}\\]\nwhich tells us that \\(m\\) is a ratio of two distances, the height of the triangle \\(y-y_0\\) and the length of the base of the triangle \\(x-x_0\\).\n\n\n\n\n\n\nFigure 2: A line and two point.\n\n\n\nSince \\(m\\) is the ratio of these two distances, its sign determines weather the function is increasing or not. Observing Figure 1 we see \\(x-x_0&gt;0\\) and \\(y-y_0&gt;0\\) therefore for this particular choice of two points we find \\(m&gt;0\\), the same conclusion would hold for any other. When \\(m\\) is positive, then the \\(y\\)’s in \\((x,y)\\) increase when the \\(x\\)’s increase as well.\nBy the same line of reasoning the \\(m&lt;0\\) would correspond to decreasing function, one where as \\(x\\) increases then the \\(y\\)’s decrease. And by the way, \\(m=0\\) correspond to horizontal lines but these are polynomials of degree zero and not of degree one. The larger \\(m\\) becomes the closer the line is to a vertical line, but it never truly becomes a vertical line.\nWe can actually arrive at these conclusions just looking at \\(y=m(x-x_0)+y_0\\), no rearranging required: when \\(m\\) is positive, then the greater \\(x-x_0\\) is, the more we add to the default value \\(y_0\\), thus the \\(y\\) must increase; if on the other hand \\(m\\) is negative, the term \\(m(x-x_0)\\) can only become more negative as \\(x-x_0\\) increases, thus lowering the value of \\(m(x-x_0)+y_0\\), that is, the value of \\(y\\), the function must be decreasing when \\(x-x_0\\) increases.\n\n\nZeros\nWhat are the zeros in \\(P_1\\)? Look at Figure 2, there are (in this case) two types of zeros: \\(x\\)-zeros and \\(y\\)-zeros. The \\(x\\)-zeros occur when the \\(x\\) coordinate is zero and the \\(y\\)-zeros when \\(y\\) is zero. In other words, the zeros of a polynomials of degree one are the ordered pairs (in \\(P_1\\)) that satisfy either one of these equations:\n\\[\n\\begin{equation}\\begin{cases}y=y_0+m(x-x_0)\\\\y=0\\end{cases}\\qquad\\qquad\\begin{cases}y=y_0+m(x-x_0)\\\\x=0\\end{cases}\\end{equation}\n\\]\n\\[\n\\begin{equation}\\begin{cases}y_0=a_1x_0+a_0\\\\y=0\\end{cases}\\qquad\\qquad\\begin{cases}y=a_1x+a_0\\\\x=0\\end{cases}\\end{equation}\n\\]\n\\[\n(x_0,0)\\in \\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = a_1x+a_0 \\}\n\\]\n\\[\n(0,y_0)\\in \\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = a_1x+a_0 \\}\n\\]\nSolving the first systems of equations we get \\(x=(0-y_0)/m+x_0\\) and solving the second \\(y=y_0+m(0-x_0)\\). Hence we know zeros of \\(P_1\\):\n\\[\n\\begin{align}\n&\\textit{x-zero:} \\qquad (0,y_0-mx_0)\\\\\n&\\textit{y-zero:} \\qquad (x_0-y_0/m,0)\n\\end{align}\n\\]\n\n\nWhen \\(x\\) is large, then what?\nWe can get to know qualitatively what happens with points coordinates of \\(P_1\\) at its extreme ends of the set by supposing \\(x\\) is a very, very large number; there is however an important key point: we are not particularly worried about a specific large value of \\(x\\), like \\(x=10^{100}\\) , and its corresponding \\(y\\); what we really want to know is - as \\(x\\) is getting larger and larger, how does \\(y\\) behave then? It is a dynamical process, not a specific evaluation of the function.\n\nParaphrased differently, we want to know what happens to \\((x,y)\\) during the process of making \\(x\\) bigger and bigger? To state that \\(x\\) is getting bigger and bigger, we write \\(x\\longrightarrow \\infty\\). In such a dynamic case, what happens with the value \\(y\\)? Meaning, what is the dynamic solution \\(y\\) of the system:\n\\[\n\\begin{equation}\\begin{cases}y=y_0+m(x-x_0)\\\\x\\longrightarrow +\\infty\\end{cases}\\end{equation}\n\\tag{10}\\]\nLets make an intuitive guess about the solution of this problem (later, when we study limits we’ll see a more systematic way to solve these problems), the Figure 3 helps a lot. To understand how the values of \\(y\\) behave when \\(x\\) increases, we have to understand how the calculation \\(y_0+m(x-x_0)\\) behaves, when we keep increasing \\(x\\). A key aspect of it, is that the term \\(m x\\) gets larger and larger while \\(y_0-mx_0\\) remains a constant, eventually, \\(mx\\) is so much larger than this constant (we are increasing \\(x\\) after all) that our original equation \\(y=y_0+m(x-x_0)\\) becomes approximately given by \\(y\\sim mx\\). The functions \\(y_0+m(x-x_0)\\) and \\(mx\\) are different! But when \\(x\\) is large they are very similar because the constant \\(y_0-mx_0\\) becomes irrelevant. Now we shift our focus to the approximate equation and try to solve it:\n\\[\n\\begin{equation}\\begin{cases}y\\sim mx\\\\x\\longrightarrow +\\infty\\end{cases}\\end{equation}\n\\tag{11}\\]\nThis is easy, because its just an atomic polynomial. The dynamical solution to this problem is [ \\(y\\) will increase when \\(x\\) increases], we write the answer as \\(y\\longrightarrow +\\infty\\) when \\(m\\) is positive. As a consequence the answer to Equation 10 is also \\(y\\longrightarrow +\\infty\\).\nObserve that the function \\(y\\sim mx\\) is not equal to \\(y=y_0+m(x-x_0)\\), and thus their graphs are not equal, but when \\(x\\) is very large, they are similar, see picture below. Solving Equation 11 we automatically get the answer to the problem Equation 10 . Both answers are \\(y\\longrightarrow+\\infty\\).\n\n\n\n\n\n\nFigure 3: Does y grow when x grows?\n\n\n\nIf on the other hand \\(m\\) is negative then, by the same reasoning, \\(y\\) also becomes very negative.\nAwful Exercise: What happens to \\(y\\) when \\(x\\longrightarrow-\\infty\\)?"
  },
  {
    "objectID": "examples_of_pol.html#mediatrix-equation",
    "href": "examples_of_pol.html#mediatrix-equation",
    "title": "Examples of Polynomials",
    "section": "Mediatrix Equation",
    "text": "Mediatrix Equation\nWith similar flavor as a line that passes through two point, our goal now is to find the equation of the line that is perpendicular to the line that passes through the point and is equidistant from the two points.\nThe word equidistant my cause some head scratching, here’s a picture\n\n\n\nA blue line that intercepts the two red points and the mediatrix in pink.\n\n\nFor each point \\((x,y)\\) on the pink line compute the distance from the red points, these two distances must be the same. This is what describes the mediatrix line (in pink).\n\n\n\n\n\n\nRecall\n\n\n\nThe distance, \\(d\\), between the point \\((x_0,y_0)\\) and the point \\((x,y)\\)\n\nis given by the Pythagoras theorem:\n\\[\nd^2 = (x-x_0)^2+(y-y_0)^2\n\\]\n\n\nFormally, lets say the red points have coordinates \\((x_0,y_0)\\) and \\((x_1,y_1)\\), our function is composed by the points which is equidistant from both, in mathematical notation, the mediatrix line is given by:\n\\[\n\\{(x,y)\\in\\mathbb{R}^2\\,\\,|\\,\\, (x-x_0)^2+(y-y_0)^2 = (x-x_1)^2+(y-y_1)^2\\}\n\\tag{14}\\]\nLets simplify the element-hood test:\n\\[\n\\begin{align}\n&(x-x_0)^2+(y-y_0)^2 = (x-x_1)^2+(y-y_1)^2\\\\\n\\iff &x^2-2x_0x+x_0^2 + y^2-2y_0y+y_0^2 = x^2-2x_1x+x_1^2 + y^2-2y_1y+y_1^2\\\\\n\\iff & 0= 2(y_0-y_1)y + 2(x_0-x_1)x+y_1^2-y_0^2+x_1^2-x_0^2\\\\\n\\iff & y = -\\frac{x_0-x_1}{y_0-y_1}x +\\frac{1}{2}(y_1^2+x_1^2-y_0^2-x_0^2)\\\\\n\\iff & y = -\\frac{1}{m} x + \\frac{1}{2}\\Delta^2\n\\end{align}\n\\]\nwhere in the last line I lumped terms into the parameters \\(m\\) and \\(\\Delta\\).\n\\[\nm:=\\frac{y_1-y_0}{x_1-x_0}\\qquad \\Delta^2:=y_1^2+x_1^2-y_0^2-x_0^2\n\\]\n\\(m\\) is the slope of the line that contains the red points, while \\(\\Delta^2\\) is the difference between the squared-distances of the red points with respect to the origin.\nAs we can see the second powers in the element-hood test cancel and what remain is the equation \\(y = -\\frac{1}{m} x + \\frac{1}{2}\\Delta^2\\), which is just a linear combination of \\(x\\) and \\(x^0\\) with weights \\(-1/m\\) and \\(\\Delta^2/2\\), the hallmark of a line equation of the line. We conclude, the set Equation 14 is equal to\n\\[\n\\{(x,y)\\in\\mathbb{R}^2\\,\\,|\\,\\, y = -\\frac{1}{m} x + \\frac{1}{2}\\Delta^2\\}\n\\]\nComparing with Equation 4 we identify \\(a_1 =-1/m\\) and \\(a_0=\\Delta^2/2\\)."
  },
  {
    "objectID": "examples_of_pol.html#zeros-of-these-polynomials",
    "href": "examples_of_pol.html#zeros-of-these-polynomials",
    "title": "Examples of Polynomials",
    "section": "Zeros of these polynomials:",
    "text": "Zeros of these polynomials:\nWe wish now to zoom into polynomials given as Equation 18 or Equation 19 and see the zeros of these function are expressed in term of the diretrix and focus properties.\nTo find vertical axis interception is to find the point \\((x,y)\\) in \\(P_2\\) whose \\(x=0\\):\n\\[\n\\begin{equation}\\begin{cases}y-y_0=\\frac{1}{4c}(x-x_0)^2\\\\x=0\\end{cases}\\implies y-y_0=\\frac{1}{4c}(0-x_0)^2\\implies y=\\frac{1}{4c}x_0^2+y_0\\end{equation}\n\\]\nwhich tells us the interception point coordinates \\((0,\\frac{1}{4c}x_0^2+y_0)\\) belonging to \\(P_2\\).\nWhen seeking for the points with \\(y=0\\) we must be more careful since they may not exist (the parabola might be completely above the x-axis). By substituting again \\(y=0\\) in the element-hood test we get:\n\\[\n\\begin{equation}\\begin{cases}y-y_0=\\frac{1}{4c}(x-x_0)^2\\\\y=0\\end{cases}\\implies(x-x_0)^2=-4cy_0\\end{equation}\n\\]\nWe must be careful when solving for \\(x\\) the later equation since we must have a positive right hand side, this is only possible if the parameter \\(y_0\\leq 0\\). This is the condition for the existence of zeros, i.e., it is the condition that ensures us the parabola is not completely above the x-axis.\n\n\n\nA parabola and the x-axis (diretrix and focus NOT shown).\n\n\nSaying \\(y_0\\leq 0\\) either means, \\(y_0=0\\), in which case we immediately know \\(x=x_0\\) and thus \\((x_0,0)\\) is the ONLY horizontal axis interception (the vertex sits exactly at the x-axis); or it means \\(y_0&lt;0\\), in this case we solve the equation for \\(x\\) as follows\n\\[\n\\begin{equation}\\begin{split}(x-x_0)^2&=4c|y_0|\\\\&\\implies x-x_0=\\pm\\sqrt{4c|y_0|}\\\\&\\implies x=x_0\\pm \\sqrt{4c|y_0|}\\end{split}\\end{equation}\n\\]\nThere are two interception, at \\((x_0+ \\sqrt{4c|y_0|},0)\\) and another at \\((x_0-\\sqrt{4c|y_0|},0)\\), which is expected when part of the parabola is below the x-axis."
  },
  {
    "objectID": "examples_of_pol.html#when-x-is-large",
    "href": "examples_of_pol.html#when-x-is-large",
    "title": "Examples of Polynomials",
    "section": "When \\(x\\) is large:",
    "text": "When \\(x\\) is large:\nTo understand the behavior when \\(x\\) is very large, the extreme ends of \\(P_2\\), we have to note the following:\n\nObserve the term \\((x-x_0)^2\\), it is a squared of a number \\(x-x_0\\), hence always positive. This implies the smallest value it takes is zero.\nWhen \\(c\\) is positive, then \\((x-x_0)^2/4c\\) is positive for any \\(x\\), the minimum is zero when we set \\(x-x_0=0\\).\n\nThe procedure \\((x-x_0)^2/4c+y_0\\) results from the addition of a strictly positive number \\((x-x_0)^2/4c\\) to \\(y_0\\), hence the minimum value it can take is \\(y_0\\) when \\(x=x_0\\).\nNow let us turn into what happens with the function when \\(x\\) becomes very large (\\(x\\longrightarrow \\pm\\infty\\)). Another glance at the procedure tell us that the quantity \\((x-x_0)^2\\) becomes very large either way, multiplying it now by \\(1/4c\\) we must be clear about the sign of \\(c\\). If it is positive the function will grow without end in either case, if \\(c\\) is negative, it becomes very small."
  },
  {
    "objectID": "example 2.html",
    "href": "example 2.html",
    "title": "Example 2",
    "section": "",
    "text": "Example 2:\nConsider a spin-1/2 particle whose energy eigenstates and eigenvalues are given by \\(|n\\rangle\\) and \\(E_n=n^2 E_1\\) for \\(n=1,2,..\\)\nAssume we have a system with two of these particles, which do not interact.\n\nWrite the ground state and first excited states when the total spin is zero \\(S=0\\). What are the corresponding eigenvalues?\nWhat is the ground state when the total spin is \\(1\\)?\n\nAnswer:\nThe single particle states are of the form \\(|\\text{spacial}\\rangle \\otimes|\\text{spin}\\rangle\\) where\n\\[\n\\begin{align}\n&|\\text{spacial}\\rangle \\in \\text{span}\\{\\text{Eigenstates of $h$ }\\}\\\\\n&|\\text{spin}\\rangle \\in \\text{span}\\{\\text{Eigenstates of $S^2$ and $S_z$}\\}\n\\end{align}\n\\]\nMore concretely we have \\(|n\\rangle\\otimes |m\\rangle\\) with \\(n=1,2,..\\) and \\(m=\\pm1/2\\); usually written as \\(|n,m\\rangle\\).\nWhy are the single particle eigenstates of this form? Answer: The observables \\(\\{h,S^2,S_z\\}\\) commutes (common eigenvectors exists) and the operators are independent (\\(h\\) is not a function of either \\(S^2\\) or \\(S_z\\)) and thus it suffices to specify the quantum numbers of each operator (the quantum number \\(s=1/2\\) is suppressed in the notation)\nThe possible quantum states of the total system are the eigenstates of the observables \\(\\{H,S_1^2,S_{1z}, S_2^2,S_{2z} \\}\\) with the total Hamiltonian as \\(H = h_1 +h_2+V_{12}\\) where \\(V_{12}\\) is zero since we are assuming non-interaction particles. We should look at these operators as extensions, for example the Hamiltonian is:\n\\[\nh_1 \\otimes 1_2 \\otimes 1_{s1} \\otimes 1_{s2}+ 1_1 \\otimes h_2 \\otimes 1_{s1} \\otimes 1_{s2}\n\\]\nwith\n\\[ h_i|i:n_i\\rangle =n_i^2E_1|i:n_i\\rangle \\]\nThe eigenstates of the system are of the form:\n\\[\n|1:n_1,m_1\\rangle \\otimes |2:n_2,m_2\\rangle\n\\]\nwhich can be written more compactly as:\n\\[\n|n_1,n_2\\rangle \\otimes |m_1,m_2\\rangle\n\\]\nHence for \\(i=1,2\\):\n\\[\n\\begin{align}\n&H|n_1,n_2\\rangle \\otimes |m_1,m_2\\rangle = \\frac{n_1^2+n_2^2}{2I}E_1|n_1,n_2\\rangle \\otimes |m_1,m_2\\rangle\\\\\n&S^2_i|n_1,n_2\\rangle \\otimes |m_1,m_2\\rangle = \\frac{3\\hbar^2}{4}|n_1,n_2\\rangle \\otimes |m_1,m_2\\rangle\\\\\n&S_{iz}|n_1,n_2\\rangle \\otimes |m_1,m_2\\rangle = m_i\\hbar |n_1,n_2\\rangle \\otimes |m_1,m_2\\rangle\n\\end{align}\n\\]\nThe two question posed refer to the total spin of the system, so it is a good idea to change the spin basis and write the spin part involving the total spin. We know from the previous theory that:\n\\[\n\\text{span}\\{|n_1,n_2\\rangle \\otimes |m_1,m_2\\rangle\\} = \\text{span}\\{|n_1,n_2\\rangle \\otimes |S,M\\rangle\\}\n\\]\nWhere again we suppress from the spin part this: \\(|\\overbrace{1/2,1/2},S,M\\rangle\\)\n\\[\n\\begin{align}\n&H|n_1,n_2\\rangle \\otimes |S,M\\rangle = \\frac{n_1^2+n_2^2}{2I}E_1|n_1,n_2\\rangle \\otimes |S,M\\rangle\\\\\n&S^2_i|n_1,n_2\\rangle \\otimes |S,M\\rangle = \\frac{3\\hbar^2}{4}|n_1,n_2\\rangle \\otimes |S,M\\rangle\\\\\n&S^2|n_1,n_2\\rangle \\otimes |S,M\\rangle = S(S+1)\\hbar^2|n_1,n_2\\rangle \\otimes |S,M\\rangle\\\\\n&S_{z}|n_1,n_2\\rangle \\otimes |S,M\\rangle = M\\hbar |n_1,n_2\\rangle \\otimes |S,M\\rangle\n\\end{align}\n\\]\n\nWe want the states of the form \\(|n_1,n_2\\rangle \\otimes |S,M\\rangle\\) such that the following statements are true:\n\\[\n\\begin{cases}\nP_{21}|n_1,n_2\\rangle \\otimes |S,M\\rangle =-1\\times|n_1,n_2\\rangle \\otimes |S,M\\rangle\\\\\n|S,M\\rangle = |S=0,M=0\\rangle\\\\\n\\frac{n_1^2+n_2^2}{2I}E_1\\quad \\text{is minimum}\n\\end{cases}\n\\] Since the spin part is antissimetric:\n\\[\n\\begin{align}\nP_{21} |0,0\\rangle &= P_{21} \\frac{1}{\\sqrt{2}}(|1:\\uparrow,2:\\downarrow\\rangle - |1:\\downarrow,2:\\uparrow\\rangle ) \\\\\n&= -1\\times \\frac{1}{\\sqrt{2}}(|1:\\uparrow,2:\\downarrow\\rangle - |1:\\downarrow,2:\\uparrow\\rangle )\\\\\n&-1\\times |0,0\\rangle\n\\end{align}\n\\] we seek a spacial ket that is symmetric and whose energy is minimum.\nThe spacial state \\(n_1=n_2=1\\) statisfies the criteria. Therefore the final answer is:\n\\[\n|1,1\\rangle\\otimes|0,0\\rangle\n\\]\nor\n\\[\n|1,1\\rangle \\otimes \\frac{1}{\\sqrt{2}}(|1:\\uparrow,2:\\downarrow\\rangle - |1:\\downarrow,2:\\uparrow\\rangle )\n\\]\nwhose eigenvalue is \\((1^2 + 1^2) E_1\\).\nFollowing a similar line of reasoning the first excited state with \\(S=0\\) must be:\n\\[\n\\frac{1}{\\sqrt{2}}(|2,1\\rangle + |1,2\\rangle) \\otimes |0,0\\rangle\n\\]\nObserve that the spacial spate is symmetric when we use the \\(+\\) sign!\nWhen \\(S=1\\) the spin state is one of the symetric triplet states:\n$$\n\\[\\begin{align}\n&\\ket{1,1}=\\ket{1/2,1/2}\\\\\n&\\ket{1,0}=\\frac{1}{\\sqrt{2}}\\left(\\ket{-\\frac{1}{2},\\frac{1}{2}}+\\ket{\\frac{1}{2},-\\frac{1}{2}}\\right)\\\\\n&\\ket{1,-1}=\\ket{-1/2,-1/2}\n\\end{align}\\] $$\nAs a result the spacial part must be antissymetric for the whole quantum state of the two particles to be antissymetric as PEP requires. Thus:\n\\[\n\\begin{align}\n&\\frac{1}{\\sqrt{2}}(|2,1\\rangle - |1,2\\rangle)\\otimes |1,1\\rangle\\\\\n&\\frac{1}{\\sqrt{2}}(|2,1\\rangle - |1,2\\rangle)\\otimes |1,0\\rangle\\\\\n&\\frac{1}{\\sqrt{2}}(|2,1\\rangle - |1,2\\rangle)\\otimes |1,-1\\rangle\\\\\n\\end{align}\n\\]\nNotice the minimum energy AND antissymetry of the spacial part only occurs at the FIRST excited state. The spacial ground state is the symmetric \\(|1,1\\rangle\\)."
  },
  {
    "objectID": "determinants.html",
    "href": "determinants.html",
    "title": "Determinants",
    "section": "",
    "text": "Determinant of a square \\(n\\times n\\) matrix is a special number we assign to the matrix and on which many other properties of it lie. It is a useful definition.\nRather than giving a formula for the calculation, lets list its properties that this formula would have:"
  },
  {
    "objectID": "determinants.html#determinant-of-a-2times-2-matrix",
    "href": "determinants.html#determinant-of-a-2times-2-matrix",
    "title": "Determinants",
    "section": "Determinant of a \\(2\\times 2\\) matrix:",
    "text": "Determinant of a \\(2\\times 2\\) matrix:\n\\[\n\\begin{align}\n\\left|\\begin{matrix} a &  b\\\\ c & d \\end{matrix}\\right| &=\n\\left|\\begin{matrix} a +0  &  b+0\\\\ c & d \\end{matrix}\\right|=\n\\left|\\begin{matrix}a & 0\\\\ c & d \\end{matrix}\\right|+\\left|\\begin{matrix}0 & b\\\\ c & d \\end{matrix}\\right|\\\\\n&=\\left|\\begin{matrix}a & 0\\\\ c+0 & d+0 \\end{matrix}\\right|+\\left|\\begin{matrix}0 & b\\\\ c+0 & d+0 \\end{matrix}\\right|\\\\\n&=\\left|\\begin{matrix}a & 0\\\\ 0 & d \\end{matrix}\\right|+\\left|\\begin{matrix}a & 0\\\\ c & 0 \\end{matrix}\\right|+\\left|\\begin{matrix}0 & b\\\\ 0 & d \\end{matrix}\\right|+\\left|\\begin{matrix}0 & b\\\\ c & 0\\end{matrix}\\right|\\\\\n&=\\left|\\begin{matrix}a & 0\\\\ 0 & d \\end{matrix}\\right|+\\left|\\begin{matrix}0 & b\\\\ c & 0\\end{matrix}\\right|\\\\\n&=ad-bc\n\\end{align}\n\\]\nImportant obs: We can use the properties of determinant to make every entry of the first row zero except one\n\\[\n\\left|\\begin{matrix}a & 0\\\\ c & d \\end{matrix}\\right|+\\left|\\begin{matrix}0 & b\\\\ c & d \\end{matrix}\\right|\n\\]\nand then do the same in the second row. However, in doing that remember that columns of zeros yield zero determinants, thus the nonzero entry on the second row must be put away of the column on which the nonzero is on the first row. Another way to put this: once we choose a nonzero entry in the first row, we make the remainder of the row entries and column entries all zero."
  },
  {
    "objectID": "determinants.html#determinant-of-a-3times-3-matrix",
    "href": "determinants.html#determinant-of-a-3times-3-matrix",
    "title": "Determinants",
    "section": "Determinant of a \\(3\\times 3\\) matrix",
    "text": "Determinant of a \\(3\\times 3\\) matrix\nInspired by the observation above we put zeros on the row of column of the nonzero entry of the first row:\n\\[\n\\begin{align}\\left|\\begin{matrix} A_{11} &  A_{12} & A_{13}\\\\ A_{21} & A_{22} & A_{23}\\\\A_{31} & A_{32} & A_{33} \\end{matrix}\\right| &= \\left|\\begin{matrix} A_{11} &  0 & 0\\\\ 0 & A_{22} & A_{23}\\\\0 & A_{32} & A_{33} \\end{matrix}\\right|+ \\left|\\begin{matrix} 0 &  A_{12} & 0\\\\ A_{21} & 0 & A_{23}\\\\A_{31} & 0 & A_{33} \\end{matrix}\\right|+ \\left|\\begin{matrix} 0 &  0 & A_{13}\\\\ A_{21} & A_{22} & 0\\\\A_{31} & A_{32} &0 \\end{matrix}\\right|\n\\end{align}\n\\]\nThen do the same in the second row:\n\\[\n\\left|\\begin{matrix} A_{11} &  0 & 0\\\\ 0 & A_{22} & A_{23}\\\\0 & A_{32} & A_{33} \\end{matrix}\\right| = \\left|\\begin{matrix} A_{11} &  0 & 0\\\\ 0 & A_{22} & 0\\\\0 & 0 & A_{33} \\end{matrix}\\right| + \\left|\\begin{matrix} A_{11} &  0 & 0\\\\ 0 & 0 & A_{23}\\\\0 & A_{32} & 0 \\end{matrix}\\right|=A_{11}(A_{22}A_{33}-A_{23}A_{32}) =A_{11}\n\\left|\\begin{matrix}A_{22} & A_{23}\\\\ A_{32} & A_{33} \\end{matrix}\\right|\n\\]\n\\[\n\\left|\\begin{matrix} 0 &  A_{12} & 0\\\\ A_{21} & 0 & A_{23}\\\\A_{31} & 0 & A_{33} \\end{matrix}\\right|=\\left|\\begin{matrix} 0 &  A_{12} & 0\\\\ A_{21} & 0 & 0\\\\0 & 0 & A_{33} \\end{matrix}\\right|+\\left|\\begin{matrix} 0 &  A_{12} & 0\\\\ 0 & 0 & A_{23}\\\\A_{31} & 0 & 0 \\end{matrix}\\right|=-A_{12}(A_{21}A_{33}-A_{23}A_{31})=-A_{12}\\left|\\begin{matrix}A_{21} & A_{23}\\\\ A_{31} & A_{33} \\end{matrix}\\right|\n\\]\n\\[\n\\left|\\begin{matrix} 0 &  0 & A_{13}\\\\ A_{21} & A_{22} & 0\\\\A_{31} & A_{32} &0 \\end{matrix}\\right|= \\left|\\begin{matrix} 0 &  0 & A_{13}\\\\ 0 & A_{22} & 0\\\\A_{31} & 0 &0 \\end{matrix}\\right|+\\left|\\begin{matrix} 0 &  0 & A_{13}\\\\ A_{21} & 0 & 0\\\\0 & A_{32} &0 \\end{matrix}\\right|=A_{13}(A_{21}A_{32}-A_{22}A_{31}) =A_{13}\\left|\\begin{matrix}A_{21} & A_{22}\\\\ A_{31} & A_{32} \\end{matrix}\\right|\n\\]\nWe conclude that:\n\\[\n\\det A = A_{11}\\left|\\begin{matrix}A_{22} & A_{23}\\\\ A_{32} & A_{33} \\end{matrix}\\right|-A_{12}\\left|\\begin{matrix}A_{21} & A_{23}\\\\ A_{31} & A_{33} \\end{matrix}\\right|+A_{13}\\left|\\begin{matrix}A_{21} & A_{22}\\\\ A_{31} & A_{32} \\end{matrix}\\right|\n\\]"
  },
  {
    "objectID": "determinants.html#general-formula-to-compute-determinants",
    "href": "determinants.html#general-formula-to-compute-determinants",
    "title": "Determinants",
    "section": "General formula to compute determinants",
    "text": "General formula to compute determinants\nFrom the rhs of these calculation we name \\(A_{11}\\), \\(A_{12}\\) and \\(A_{13}\\) the factors and the corresponding subdeterminants the cofactors\n\\[\n[\\text{cofactor of}\\,A_{ij}]=C_{ij} = (-1)^{i+j}\\det(\\text{matrix with row}\\,i\\,\\text{and column}\\,j\\,\\text{supressed})\n\\]\nThe funny term \\((-1)^{i+j}\\) just tracks the sign of the permutation we did above, note the \\(-\\left|\\begin{matrix}A_{21} & A_{23}\\\\ A_{31} & A_{33} \\end{matrix}\\right|\\) above.\nIt is easy to identify these signs if we remember \\((-1)^{i+j}\\) are the form of entries of the following matrix:\n\\[\n\\left(\\begin{matrix} + &  - & +\\\\ - & + & -\\\\+ & - & + \\end{matrix}\\right)\n\\]\nThus the general formula to compute determinants of square \\(n\\times n\\) matrices is:\n\\[\n\\det A = A_{11} C_{11} + A_{12} C_{12}+\\dots+A_{1n}C_{1n}\n\\]\nWith it we can write an important equality that will be useful when we introduce inverse matrices:\n\\[\n\\left(\\begin{matrix} A_{11} &  A_{12} & A_{13}\\\\ A_{21} & A_{22} & A_{23}\\\\ A_{31} & A_{32} & A_{33} \\end{matrix}\\right) \\left(\\begin{matrix} C_{11} &  C_{21} & C_{31}\\\\ C_{12} & C_{22} & C_{32}\\\\ C_{13} & C_{23} & C_{33} \\end{matrix}\\right)=\\left(\\begin{matrix} \\det A &  0 & 0\\\\ 0 & \\det A & 0\\\\ 0 & 0 & \\det A \\end{matrix}\\right)\n\\]\nIn compact form:\n\\[\nAC^\\intercal =I\\det A\n\\]"
  },
  {
    "objectID": "change_of_variables.html",
    "href": "change_of_variables.html",
    "title": "Change of variables in integrals",
    "section": "",
    "text": "It is important when dealing with change of variables to always remember what an integration of a function means:\n\\[\n\\int_{x_0}^{x_1} f(x)\\,\\,dx\n\\]\nRecall we are summing rectangles of base \\(dx\\) and height \\(f(x)\\) with \\(x\\) ranging between \\(x_0\\) and \\(x_1\\).\nIn other words, the integral means, for each \\(x\\) in this range compute the area \\(f(x)dx\\) and add the final result.\nNote that for each \\(x\\) there is a corresponding \\(f(x)dx\\); changing the variable \\(x\\) to some new variable \\(y\\) must not change this assignment.\nAssume \\(x\\) is related to \\(y\\) by some function \\(g\\), then \\(x=g(y)\\). The new variable \\(y\\) must now have a new range and this range must be in such a way that the corresponding \\(g(y)\\) ranges between \\(x_0\\) and \\(x_1\\) without repeating values. That is to say the function \\(g\\), must be bijective!\nIf this is the case, then, the variable \\(y\\) now ranges between \\([g^{-1}(x_0) , g^{-1}(x_1)]\\) and each \\(dx\\) (we have one \\(dx\\) for each \\(y\\)) is obtained from the linear approximation\n\\[\ndx=g'(y)dy\n\\]\nIn conclusion:\n\\[\n\\int_{x_0}^{x_1} f(x)\\,\\,dx = \\int_{g^{-1}(x_0)}^{g^{-1}(x_1)} f(g(y))g'(y)\\,\\,dy\n\\]\nHopefully you choice of \\(g(y)\\) make the integral on the rhs easier to evaluate.",
    "crumbs": [
      "Brief Notes",
      "Calculus",
      "Change of variables"
    ]
  },
  {
    "objectID": "angular_momentum_qm_short_summary.html",
    "href": "angular_momentum_qm_short_summary.html",
    "title": "Ideas to get started with Angular Momentum in Quantum Mechanics",
    "section": "",
    "text": "In QM there are essentially two types of angular momentum: Orbital Angular Momentum and Spin Angular momentum.\n\nOrbital angular momentum is defined as an analog of the classical one, as\n\\[\n\\mathbf{L}=\\mathbf{R}\\times \\mathbf{P}\n\\tag{1}\\] \\(\\mathbf{R}=(X,Y,Z)\\) and \\(\\mathbf{P}=(P_x,P_y,P_z)\\) are vectors whose entries are operators.\nThis formula is a compact way of writing:\n\\[\n\\begin{cases}\nL_x = YP_z-ZP_y\\\\\nL_y = Z P_x -XP_z\\\\\nL_z = XP_y-YP_x\n\\end{cases}\n\\tag{2}\\]\nFrom the three components of \\(\\mathbf{L}\\) and the canonical commutation relations:\n\\[\n[X,P_x]=i\\hbar \\qquad [Y,P_y]=i\\hbar \\qquad [Z,P_z]=i\\hbar\n\\]\nwe can derive the following ones:\n\\[\n[L_x,L_y]=i\\hbar L_z\\qquad [L_y,L_z]=i\\hbar L_x\\qquad [L_z,L_x]=i\\hbar L_y\\qquad [L_i,L_j]=0\\quad \\text{for}\\ i\\not= j\n\\tag{3}\\]\nFrom one perspective, these are the commutation relations for the components of the quantum angular momentum that stem from Equation 1 and Equation 3. From a different perspective we can say that the angular momentum Equation 1 is a particular example of \\(\\mathbf{L}\\) that satisfy these three commutation “equations” in Equation 3. In other words, \\(\\mathbf{R}\\times \\mathbf{P}\\) is a solution of Equation 3.\nIn QM one wants to talk about more general angular momentum, for example spin angular momentum, it has no classical analog and hence is not defined explicitly as \\(\\mathbf{R}\\times \\mathbf{P}\\). However one can still define it implicitly, by saying that whatever spin angular momentum operator is, it must satisfy the previous commutations relation.\n\nThese two observations motivates us to introduce a generic angular momentum, with letter \\(\\mathbf{J}\\), for which the only thing we know is that it obeys the rules:\n\\[\n[J_x,J_y]=i\\hbar J_z\\qquad [J_y,J_z]=i\\hbar J_x\\qquad [J_z,J_x]=i\\hbar J_y\\qquad [J_i,J_j]=0\\quad \\text{for}\\ i\\not= j\n\\tag{4}\\]\n\\(\\mathbf{J}\\) can be either the orbital angular momentum of a particle or the spin angular momentum or a combination of these, since all quantum angular momenta has commutation relations of form Equation 4 .\n\n\n\nGiven \\(\\mathbf{J}\\) we can introduce now its norm squared \\(J^2\\), defined as:\n\\[\nJ^2=J_x^2+J_y^2+J_z^2\n\\tag{5}\\]\nWe can easily show (we’ll skip the boring details) from Equation 4 that Equation 5 commutes with the three components of \\(\\mathbf{J}\\), that is:\n\\[\n[J^2,J_x]=0\\qquad[J^2,J_y]=0\\qquad[J^2,J_z]=0\n\\tag{6}\\]\nFor example, using Equation 4 we find:\n\\[\n\\begin{align}\n[J^2,J_z]&=[J_x^2+J_y^2+J_z^2,J_z]\\\\\n&=[J_x^2,J_z]+[J_y^2,J_z]+[J_z^2,J_z]\\\\\n&=0+0+0\\\\\n&=0\n\\end{align}\n\\]\nThat fact that \\(J^2\\) commutes with \\(J_z\\) will be of great importance, because it guarantees one can find (and thus define) a set of basis kets in the Hilbert space that are simultaneously eigenstates of both, \\(J^2\\) and \\(J_z\\). Moreover, since the commuting operators are hermitian (we say in QM they are observables) the eigenstates of the operators can be made orthogonal and the eigenvalues are real.\n\n\n\nOur goal is to define (to be mare specific, to define is to give meaningful names to something) a basis of eigenstates common to \\(J^2\\) and \\(J_z\\) for our Hilbert Space. Why? One of the most important goals in QM is to find the eigenstates of the Hamiltonian of the system, paraphrased differently, we say, we want to find the basis of eigenstates of \\(H\\). Imagine we have an Hamiltonian which commutes with \\(J^2\\) and \\(J_z\\), then we know we can find a basis of eigenstates common to the three operators. Knowing already what is the eigenbasis common to \\(\\{J^2,J_z\\}\\) greatly facilitates finding the eigenstates of \\(H\\). The three operators (observables) constitute a complete set of commuting observables (CSCO) \\[ \\{H,J^2,J_z\\} \\] and we want to define its eigenbasis.\n\n\n\nIn summary, skipping the proofs, what the commutation relations introduced in Equation 4 allow us to conclude is:\n\nThe eigenvalues of \\(J^2\\) are of the form \\(j(j+1)\\hbar^2\\) and those of \\(J_z\\) are \\(m\\hbar\\) where \\(j\\) is a positive integer or semi-integer and \\(-j\\leq m\\leq j\\) in steps of \\(1\\).\nThe corresponding eigenvectors can be labelled by the numbers \\(j\\) and \\(m\\).\n\nIn other words:\n\\[ \\begin{align}&J^2\\ket{j,m}=j(j+1)\\hbar^2\\ket{j,m},\\\\&J_z\\ket{j,m}=m\\hbar\\ket{j,m}.\\end{align}  \\tag{7}\\]\nThe only values that \\(j\\) can take are \\(0\\), \\(1/2\\), \\(3/2\\), \\(2\\), etc. For example, if \\(j=3/2\\) then \\(m=-3/2,-1/2,1/2,3/2\\).\n\n\n\nBy definition we have: \\[ J_+=J_x+iJ_y\\qquad J_-=J_x-iJ_y\\qquad.  \\tag{8}\\] There are now plenty commutations relations we can derive that relate these two operators and either \\(J^2\\) or \\(J\\), but the ultimate consequence of these relations is that they tell us how to organize the eigenbasis of \\(\\{J^2,J_z\\}\\), we’ll see that in 6).\nAdditionally, these definitions allow us to write:\n\\[ J_x = \\frac{1}{2}(J_++J_-) \\qquad J_y = \\frac{1}{2i}(J_+-J_-), \\]\nwhich will be useful later.\nHow do Equation 8 operators act on the eigenstates of \\(\\{J^2,J_z\\}\\)?\nAnswer:\n\\[ J_\\pm \\ket{n,j,m}=\\hbar \\sqrt{j(j+1)-m(m\\pm 1)}\\ket{n,j,m\\pm 1},  \\tag{9}\\]\nA formula which is, no surprise, derived from Equation 4 (through a rather complicated path). The names raising and lowering comes form the fact that \\(m\\) is either raised by \\(+1\\) or lowered by \\(-1\\) each time \\(J_\\pm\\) acts.\n\n\n\nFrom Equation 7 and Equation 9 emerges the following picture\n\n\n\n\n\n\nFigure 1: Eigenspaces for various values of j.\n\n\n\nwhich organizes the eigenstates for values of \\(j\\) from \\(0\\) up to \\(2\\), each blue dot in the picture corresponds to basis vector \\(\\ket{j,m}\\). These kets constitute an orthonormal basis for the Hilbert space, and in the picture we also see as horizontal boxes the subspaces \\(\\mathcal{E}(j=1/2)\\) up to \\(\\mathcal{E}(j=2)\\) spanned by the basis kets (blue dots).\n\n\n\n\n\n\nCommentary\n\n\n\n\nFrom the first equation in Equation 7 we notice that for a given value of \\(j\\), for example \\(j=1\\), then \\(\\{|1,1\\rangle,|1,0\\rangle,|1,-1\\rangle\\}\\) are eigenvectors of \\(J^2\\) with the same eigenvalue \\(1(1+1)\\hbar^2\\) , we therefore say the eigenvalue is 3-degenerate, or that the space \\(\\mathcal{E}(j=1)\\) spanned by this basis is 3-degenerate. We distinguish the 3 eigenvectors by the \\(J_z\\) eigenvalue \\(m\\hbar\\), or more simply by just the integer \\(m\\).\nSubspaces of the Hilbert space can now be specified by fixing \\(j\\) for example, or by fixing \\(m\\) for various values. The eigenbasis within each \\(\\mathcal{E}(j)\\) is connected by the raising and lowering operators as we see by the red and green arrows in the figure above.\n\n\n\n\n\n\nTo obtain the matrix representation of an operator on a given vector space we need to know how this operator acts on that space; we know how it acts on a given vector space by knowing how it acts on the chosen basis. Note: The matrix representation of the operator is then specific for the basis.\nFor example, let us choose the space \\(\\mathcal{E}(j=1)\\) of eigenvectors of \\(\\{J^2,J_z\\}\\), the basis is:\n\\[\n\\mathcal{B}=\\{|1,1\\rangle,|1,0\\rangle,|1,-1\\rangle\\}\n\\tag{10}\\]\nsee Figure 1 .\nTo know how \\(J_x\\) acts in \\(\\mathcal{E}(j=1)\\) we compute how it acts on the basis Equation 10:\n\\[\n\\begin{align}\nJ_x|1,1\\rangle &= \\frac{1}{2}(J_+ +J_-)|1,1\\rangle\\\\\n&=\\frac{1}{2}J_+|1,1\\rangle+\\frac{1}{2}J_-|1,1\\rangle\\\\\n&=\\frac{\\hbar}{2}\\sqrt{1(1+1)-1(1+1)}|1,2\\rangle+\\frac{\\hbar}{2}\\sqrt{1(1+1)-1(1-1)}|1,0\\rangle\\\\\n&=\\frac{\\hbar}{\\sqrt{2}}|1,0\\rangle\n\\end{align}\n\\tag{11}\\]\nWe conclude that when \\(J_x\\) acts on the basis element \\(|1,1\\rangle\\) it outputs \\(|1,0\\rangle\\) times a constant, notice the input and output both live in \\(\\mathcal{E}(j=1)\\).\nLet us see now what happens when \\(J_x\\) acts on the second basis ket \\(|1,0\\rangle\\):\n\\[\n\\begin{align}\nJ_x|1,0\\rangle &= \\frac{1}{2}(J_++J_-)|1,0\\rangle\\\\\n&=\\frac{\\hbar}{2}\\sqrt{1(1+1)-0(0+1)}|1,1\\rangle+\\frac{\\hbar}{2}\\sqrt{1(1+1)-0(0-1)}|1,1\\rangle|1,-1\\rangle\\\\\n&=\\frac{\\hbar}{\\sqrt{2}}(|1,1\\rangle+|1,-1\\rangle)\n\\end{align}\n\\tag{12}\\]\nAs \\(J_x\\) acts on \\(|1,0\\rangle\\) it gives a linear combination \\(|1,1\\rangle+|1,-1\\rangle\\) of basis vectors, which in turn also lives in \\(\\mathcal{E}(j=1)\\).\nAnd finally, a similar calculation would shows the action on the third ket is:\n\\[\nJ_x|1,-1\\rangle =\\frac{\\hbar}{\\sqrt{2}}|1,0\\rangle\n\\tag{13}\\]\nAgain, the input \\(|1,-1\\rangle\\) is in \\(\\mathcal {E}(j=1)\\) and so is the output \\(|1,0\\rangle\\).\n\n\n\n\n\n\nCommentary\n\n\n\nThe calculation above shows clearly that the the operator \\(J_x\\) maps the basis of \\(\\mathcal{E}(j=1)\\) into vectors in \\(\\mathcal{E}(j=1)\\). Therefore, in general it will map any ket of \\(\\mathcal{E}(j=1)\\) into another in \\(\\mathcal{E}(j=1)\\); the jargon for this observation is to say that “\\(\\mathcal{E}(j=1)\\) is globally invariant under the action of \\(J_x\\)”; more colloquially we just say “the action of \\(J_x\\) is stuck in the subspace”. This observation is a consequence of the fact that the operator in question \\(J_x\\) commutes with the operator \\(J^2\\) that characterizes the subspace\n\\[\n[J^2,J_x]=0 \\implies [\\text{the action of $J_x$ is stuck in the subspaces of $J^2$}]\n\\]\n\n\nNow lets get the matrix representation of \\(J_x\\) in \\(\\mathcal{E}(j=1)\\). The results in Equation 11, Equation 12 and Equation 13 are summarizes in step 0:\n\nStep zero:\n\\[\n\\begin{cases}\nJ_x|1,1\\rangle=\\frac{\\hbar}{\\sqrt{2}}|1,0\\rangle\\\\\nJ_x|1,0\\rangle =\\frac{\\hbar}{\\sqrt{2}}(|1,1\\rangle+|1,-1\\rangle)\\\\\nJ_x|1,-1\\rangle =\\frac{\\hbar}{\\sqrt{2}}|1,0\\rangle\n\\end{cases}\n\\tag{14}\\] note, these formulas are important: they tell us how \\(J_x\\) acts on the chosen basis of chosen subspace \\(\\mathcal{E}(j=1)\\). From then we can now write the corresponding matrix representation, but:\nFirst step: write the identity of \\(\\mathcal{E}(j=1)\\):\n\\[\n1 = \\sum_{m=-1,0,1} |1,m\\rangle\\langle1,m|\n\\]\nSecond step: write \\(J_x\\) between the identities:\n\n\\[\nJ_x = \\sum_{m,m'=1,0,-1}|1,m\\rangle\\langle1,m|J_x|1,m'\\rangle\\langle1,m'|\n\\tag{15}\\]\n\nThird step: substitute Equation 14 in Equation 15:\n\\[\n\\begin{align}\nJ_x &= \\frac{\\hbar}{\\sqrt{2}}\\left(|1,0\\rangle\\langle 1,1|+|1,1\\rangle\\langle 1,0|+|1,-1\\rangle\\langle 1,0|+|1,0\\rangle\\langle 1,-1|\\right)\n\\end{align}\n\\tag{16}\\]\nFourth, write Equation 16 using using matrix notation. If we agree to order the basis as \\(\\{|1,1\\rangle, |1,0\\rangle,|1,-1\\rangle\\}\\) we find:\n\n\nWe can name this matrix as \\([J_x]_\\mathcal{B}\\), thenotation remind us: this is a matrix representing the action of \\(J_x\\) in the \\(\\mathcal{B}\\)! The squiggly arrow just mean: “it is represented by”.\nRepeating these steps for \\(J_y\\) and \\(J_z\\) yields the results:\n\nThe matrix \\([J_z]_\\mathcal{B}\\) is diagonal since the basis elements we choose for \\(\\mathcal{E}(j=1)\\) are its eigenvectors, the diagonal elements are the the three \\(m\\hbar\\) eigenvalues \\(m=1,0,-1\\). Since they are also eigenvectors of \\(J^2\\) we find a diagonal with the \\(1(1+1)\\hbar^2=2\\hbar^2\\) eigenvalues repeated three times:\n\n\n\n\n\n\n\nCommentary\n\n\n\nThe equal sign in Equation 15 and Equation 16 is not accurate, because the right hand side is the representation only on the subspace \\(\\mathcal{E}(l=1)\\), while the left hand side is the “whole operator”. What would be accurate is to use the entire basis and write:\n\\[\nJ_x =\\sum_{l=0}^\\infty\\sum_{-l\\leq m,m'\\leq l}|l,m\\rangle\\langle l,m|J_x|l,m'\\rangle\\langle l,m'|\n\\]\nrather than Equation 15.\nWe could emphasize the distinction by defining a new operator \\(J'_x\\) which is restricted to act only in this subspace:\n\\[\nJ'_x = \\frac{\\hbar}{\\sqrt{2}}\\left(|1,0\\rangle\\langle 1,1|+|1,1\\rangle\\langle 1,0|+|1,-1\\rangle\\langle 1,0|+|1,0\\rangle\\langle 1,-1|\\right)\n\\tag{17}\\]\n\n\n\n\n\nThe intrinsic spin angular momentum of an electron is described by states living in the \\(\\mathcal{E}(j=1/2)\\) subspace (see Figure 1), the basis for this subspace is (as usual) furnished by the simultaneous eigenvectors of the \\(J^2\\) and \\(J_z\\):\n\\[\n\\begin{align}&J^2\\ket{1/2,m}=\\frac{3}{4}\\hbar^2\\ket{1/2,m}\\\\&J_z\\ket{1/2,m}=m\\hbar\\ket{1/2,m}\\end{align}\n\\tag{18}\\]\nwhere \\(m=\\pm 1/2\\). Therefore we say\n\\[\n\\mathcal{E}(j=1/2)=\\text{span}\n\\{|1/2,1/2\\rangle,|1/2,-1/2\\rangle\\}\n\\]\n\n\n\n\n\n\nNotation\n\n\n\n\ngiven \\(j=1/2\\) is fixed, it is usual to drop it from the ket notation and simply write \\(|m\\rangle\\) where \\(m=\\pm1/2\\).\nwe can rename \\(m\\) as \\(m_s\\) and then reexpress it as \\(m_s=\\sigma/2\\) with \\(\\sigma =\\pm\\) or \\(\\sigma=\\uparrow,\\downarrow\\), but for the present notes we’ll keep the \\(m_s\\) notation.\nThe \\(j=1/2\\) states are very important (they are the states that describe electrons spins for example), so much that we rename the operators \\(\\{J^2,J_z\\}\\) into \\(\\{S^2,S_z\\}\\) . Instead of Equation 18 we should write:\n\\[\n\\begin{align}&S^2\\ket{m_s}=\\frac{3}{4}\\hbar^2\\ket{m_s}\\\\&S_z\\ket{m_s}=m_s\\hbar\\ket{m_s}\\end{align}\n\\]\nto emphasize the act, these operators only act on \\(\\mathcal{E}(j=1/2)\\). Recall we already did something similar when we defined \\(J'_x\\) in Equation 17.\n\n\n\nThe basis states \\(\\{|1/2\\rangle,|-1/2\\rangle\\}\\) that span the subspace \\(\\mathcal{E}(1/2)\\) are connect by the raising \\(S_+\\) and lowering operator \\(S_-\\) introduced in Equation 9:\n\\[\nS_+\\ket{-1/2}=\\hbar\\sqrt{1/2(1/2+1)-1/2(1/2+1)}\\ket{1/2}=\\hbar\\sqrt{2}/2\\ket{1/2}\n\\tag{19}\\]\nThe raising and lowering operators are important for another reason, they tell us how \\(S_x\\) and \\(S_y\\) act on the eigenbasis of \\(\\{S^2,S_z\\}\\), from their definition we have:\n\\[\nS_x=\\frac{1}{2}(J_++J_-) \\qquad S_y=\\frac{1}{2i}(J_+-J_-)\n\\tag{20}\\]\n\n\n\n\n\n\nCommentary\n\n\n\nIn order to know how \\(S_x\\) and \\(S_y\\) act on this basis requires first to reexpress these operators in terms of operator we know how they act in this basis, the formulas Equation 20 together with Equation 19 do precisely that.\n\n\nBy knowing how an operator acts on a basis we know how obtain its matrix representation, for example, let us take a look at \\(S_z\\) and then \\(S_x\\) and \\(S_y\\). We can write the action of \\(S_z\\) on the basis \\(\\{\\ket{1/2},\\ket{-1/2}\\}\\) as:\n\\[\n\\begin{align}&S_z\\ket{1/2}=\\frac{\\hbar}{2}\\ket{1/2}+0\\ket{-1/2}\\\\&S_z\\ket{-1/2}=0\\ket{1/2}+\\frac{\\hbar}{2}\\ket{-1/2}\\end{align}\n\\]\nthis is step zero of the steps found in 7). Following them again, we compute the numbers \\(\\braket{m|S_z|m'}\\), which we organize into the matrix:\n\\[\n[S_z]_{\\mathcal{E}(j=1/2)}=\\frac{\\hbar}{2}\\begin{pmatrix}1 & 0\\\\0 & -1\\end{pmatrix}\n\\]\nTo get the matrices for \\(S_x\\) and \\(S_y\\) in the basis \\(\\{\\ket{-1/2},\\ket{1/2}\\}\\), act with them on the basis. Following the steps zero thought step four, the numbers \\(\\braket{m|S_x|m'}\\) and \\(\\braket{m|S_y|m'}\\) can be arranged into the matrices:\n\\[\n[S_x]_{\\mathcal{E}(j=1/2)}=\\frac{\\hbar}{2}\\begin{pmatrix}0 & 1\\\\1 & 0\\end{pmatrix}\\qquad[S_y]_{\\mathcal{E}(j=1/2)}=\\frac{\\hbar}{2}\\begin{pmatrix}0 & -i\\\\i & 0\\end{pmatrix}\n\\]\n\n\n\nThis section uses the example of the helium atom to explain why we focus in the next sections just on the spin part of the states of two electrons. See the Conclusion at the end.\nThe basis states of a system of two electrons such as the helium atom (2 electrons orbiting a nucleus) are of the form:\n\\[\n|n, l, m, m_s; n', l', m', m_s'\\rangle = \\frac{1}{\\sqrt{2}}(1-P_{21})(|1:n, l, m, m_s; 2: n', l',m',m_s'\\rangle)\n\\tag{21}\\]\nwhere \\(P_{21}\\) permutes the \\(1\\), \\(2\\) indices. The \\(n\\) is the principal quantum number, \\(l\\) together with \\(m\\) describes the orbit angular momentum of the first electron, finally \\(m_s\\) together with the suppressed \\(s=1/2\\) determines its spin angular momentum. The primed indices describe the second electron.\nSince both electron have semi-integer spin \\(s=s'=1/2\\) the quantum states \\(|n, l, m, m_s; n', l', m', m_s'\\rangle\\) must be antisymmetric for the exchange of particles.\nThe operator \\(1-P_{21}\\) acting on the tensor product ket \\(|1:n, l, m, m_s; 2: n', l',m',m_s'\\rangle\\) makes Equation 21 antisymmetric.\n\n\n\n\n\n\nRecall\n\n\n\nBy definition \\(|1:\\phi;2:\\psi\\rangle := |1:\\phi\\rangle \\otimes |2:\\psi\\rangle\\). (example: \\(\\phi=n,l,m,m_s\\) and \\(\\psi=n',l',m',m_s'\\))\nThe state \\((1+P_{21})|1:\\phi;2:\\psi\\rangle\\) is equal to \\(|1:\\phi;2:\\psi\\rangle + |1:\\psi;2:\\phi\\rangle\\), which is symmetric because exchange of \\(1\\rightarrow 2\\) leaves the ket unchanged:\n\\[\nP_{21} (|1:\\phi;2:\\psi\\rangle + |1:\\psi;2:\\phi\\rangle) = |1:\\phi;2:\\psi\\rangle + |1:\\psi;2:\\phi\\rangle\n\\]\nOn the other hand \\((1-P_{21})|1:\\phi;2:\\psi\\rangle\\) is equal to \\(|1:\\phi;2:\\psi\\rangle - |1:\\psi;2:\\phi\\rangle\\) which is antissymetric because a minus sign emerges from the permutation of indices:\n\\[\nP_{21} (|1:\\phi;2:\\psi\\rangle - |1:\\psi;2:\\phi\\rangle) = -(|1:\\phi;2:\\psi\\rangle - |1:\\psi;2:\\phi\\rangle)\n\\]\nIf the two particles where in the same state \\(\\phi=\\psi\\) then:\n\\[\n\\begin{align}\n&(1+P_{21})|1:\\phi;2:\\phi\\rangle = 2 |1:\\phi;2:\\phi\\rangle\\\\\n&(1-P_{21})|1:\\phi;2:\\phi\\rangle = 0\n\\end{align}\n\\]\nHence, the operator \\(1-P_{21}\\) prevents the two states to be equal, by reducing them to zero.\nThe operator \\(1\\pm P_{21}\\) is the mathematical agent that ensures the Pauli Exclusion Principle is obeyed. The \\(+\\) is applyed for boson i.e. for integer \\(j\\) particles; the \\(-\\) is applied for fermions, i.e., for particles with semi-integer \\(j\\).\n\n\nSince the spins are semi-integers \\(s=s'=1/2\\), by Pauli Exclusion Principle states that: two electrons in the same quantum, i.e., the primed and unprimed indices are equal \\(n=n'\\), \\(l=l'\\), \\(m=m'\\) and \\(m_s=m_s'\\), these states are not physical, meaning their wave function is \\(0\\). By writing Equation 21 where we find the operator \\(1-P_{21}\\), we render this ket to \\(0\\) if those indices are equal; and non-zero otherwise.\nA non-zero state is for example the case \\(n=n'=1\\), \\(l=l'=m=m'=0\\) and \\(m_s=\\uparrow\\) and \\(m_s'=\\downarrow\\):\n\\[\n\\begin{align}\n|1,0,0,\\uparrow;1,0,0,\\downarrow\\rangle &=\\frac{1}{\\sqrt{2}}(1-P_{21})(|1:1, 0, 0, \\uparrow; 2: 1, 0,0,\\downarrow\\rangle)\\\\\n&= \\frac{1}{\\sqrt{2}}(|1:1, 0, 0, \\uparrow; 2: 1, 0,0,\\downarrow\\rangle-|1:1, 0, 0, \\downarrow; 2: 1, 0,0,\\uparrow\\rangle)\\\\\n&=\\frac{1}{\\sqrt{2}}(\\underbrace{|1:1, 0, 0; 2: 1,0,0\\rangle}_\\text{spacial part}\\otimes(\\underbrace{|1:\\uparrow;2:\\downarrow\\rangle-|1:\\downarrow;2:\\uparrow\\rangle}_\\text{spin part})\n\\end{align}\n\\tag{22}\\]\nObserving the right hand side of Equation 22 we notice it has two parts:\n\nthe spacial part is symmetric for the exchange of indices, just apply \\(P_{21}\\) and we get a plus sign on the right hand side:\n\n\\[\nP_{21}|1:1, 0, 0; 2: 1,0,0\\rangle = |1:1, 0, 0; 2: 1,0,0\\rangle\n\\]\n\nthe spin part is antisymmetric, notice the minus sign:\n\n\\[\nP_{21} (|1:\\uparrow;2:\\downarrow\\rangle-|1:\\downarrow;2:\\uparrow\\rangle) = - (|1:\\uparrow;2:\\downarrow\\rangle-|1:\\downarrow;2:\\uparrow\\rangle)\n\\]\nAs a result the whole state Equation 22 acquires a minus sign when we switch \\(1\\) with \\(2\\) and it is considered antisymmetric:\n\\[\nP_{21} |1,0,0,1/2;1,0,0,-1/2\\rangle = - |1,0,0,1/2;1,0,0,-1/2\\rangle\n\\]\nConclusion: This simple example of the helium atom showed that the basis of the helium electrons are antissymetric kets of the form \\(\\text{space}\\otimes\\text{spin}\\). This example also showed us how PEP for fermions is incorporated mathematically into the ket states through \\(1-P_{12}\\), which makes the whole ket antissymetric. The whole ket includes the spacial and spin parts.\nI emphasize that neither the spacial or spin part have to necessarily be antissymetric, what is needed is that the whole state be antissymetric, and it must be so, because we are dealing with two semi-integer spin particles and PEP says so.\nFinally, since the basis of the helium atom we can split the spatial and spin parts, we will now devote the next sections focusing just on the spin part.\n\n\n\nThe basis states of the electron one is furnished by \\(\\{S_1^2,S_{1z}\\}\\) and for the electron two by \\(\\{S_2^2,S_{2z}\\}\\) in the usual manner:\n\\[\n\\begin{align}&S_1^2\\ket{m_1}=\\frac{3}{4}\\hbar^2\\ket{m_1}\\\\&S_{1z}\\ket{m_1}=m_1\\hbar\\ket{m_1}\\\\&S_2^2\\ket{m_2}=\\frac{3}{4}\\hbar^2\\ket{m_2}\\\\&S_{2z}\\ket{m_2}=m_2\\hbar\\ket{m_2}\\end{align}\n\\]\nThe spins magnitude of the two electrons is described by the quantum numbers\n\\[\ns_1=\\frac{1}{2}\\qquad s_2=\\frac{1}{2}\n\\]\nThe basis states of two electrons is formed from the tensor product \\(\\ket{1:m_1}\\otimes \\ket{2:m_2}\\), there are four basis elements:\n\\[\n\\{\\ket{1:\\uparrow}\\otimes \\ket{2:\\uparrow},\\ket{1:\\uparrow}\\otimes \\ket{2:\\downarrow},\\ket{1:\\downarrow}\\otimes \\ket{2:\\uparrow},\\ket{1:\\downarrow}\\otimes \\ket{2:\\downarrow}\\}\n\\tag{23}\\]\nIt is important to understand that:\n\nThe observables \\(S^2_1\\) ,\\(S_{1z}\\) act on the space spanned by \\(\\{\\ket{1:-1/2},\\ket{1:1/2}\\}\\) while the observables \\(S^2_2\\), \\(S_{2z}\\) act on \\(\\{\\ket{2:-1/2},\\ket{2:1/2}\\}\\) and do not act on tensor product states.\nWe define however the extensions of \\(S^2_1\\), \\(S_{1z}\\), which act on the tensor product space as\n\n\\[\n\\tilde{S}_1^2=S_1^2\\otimes 1_2\\qquad \\tilde{S}_{1z}=S_{1z}\\otimes 1_2\n\\]\nThe extensions of \\(S^2_2\\) ,\\(S_{2z}\\) are defined analogously:\n\\[\n\\tilde{S}_2^2=1_1 \\otimes S_2^2\\qquad \\tilde{S}_{2z}=1_1\\otimes S_{2z}\n\\]\nTherefore we have for example:\n\\[\n\\begin{align}\\tilde{S}_1^2(\\ket{m_1}\\otimes \\ket{m_2}) &= (S_1^2\\ket{m_1})\\otimes \\ket{m_2}\\\\&=(\\frac{3\\hbar}{4}\\ket{m_1})\\otimes \\ket{m_2}\\\\&=\\frac{3\\hbar}{4}\\ket{m_1}\\otimes \\ket{m_2}\\end{align}\n\\tag{24}\\]\n\n\n\n\n\n\nNotation\n\n\n\nNote how in the ket Equation 24 we suppressed the \\(j_1=j_2=1/2\\) , the \\(1:\\) and \\(2:\\) and start using \\(m\\) in place of the old \\(m_s\\). Moreover we should go one step further and define\n\\[\n|m_1,m_2\\rangle :=|1:m_1\\rangle \\otimes |2:m_2\\rangle\n\\]\nAnd by the way, drop the ~ in \\(\\tilde{S}_{1,2}^2\\) and \\(\\tilde{S}_{z1,2}\\).\nThis simplified notation comes at the price of leaving to the reader the work of understanding its meaning from the context.\n\n\nAlso, the set of operators\n\\[\n\\{\\tilde{S}_1^2,\\tilde{S}_{1z},\\tilde{S}_2^2,\\tilde{S}_{2z}\\}\n\\tag{25}\\]\ncommute and as a result we know it exists a basis of common eigenstates exists, name this basis as Equation 23. It follows:\n\\[\n\\begin{align}&\\tilde{S}_1^2\\ket{m_1,m_2}=\\tilde{S}_2^2\\ket{m_1,m_2}=\\frac{3}{4}\\hbar\\ket{m_1,m_2}\\\\&\\tilde{S}_{1z}\\ket{m_1,m_2}=m_1 \\hbar\\ket{m_1,m_2}\\\\&\\tilde{S}_{2z}\\ket{m_1,m_2}=m_2 \\hbar\\ket{m_1,m_2}\\\\\\end{align}\n\\tag{26}\\]\nIn conclusion, the spin properties of a system with two particles (electrons) whose spin state is described by kets in \\(\\mathcal{E}_1(j_1=1/2)\\) and \\(\\mathcal{E}_2(j_2=1/2)\\) is described by a state of the form\n\\[\n\\ket{m_1,m_2}:=\\ket{m_1}\\otimes\\ket{m_2}\\qquad m_{1,2}=\\pm1/2\n\\tag{27}\\]\nwhich constitute a basis for the tensor product state space\n\\[\n\\mathcal{E}_{12}=\\mathcal{E}_1\\otimes\\mathcal{E}_2\n\\]\nA pictorial organization of our basis states if given as follows:\n\n\n\nThe blue dots represent a basis ket, the pink boxes we’ll be explained in next section.\n\n\n\n\n\nThe old basis Equation 27 help to define the states space \\(\\mathcal{E}_{12}(j_1=1/2,j_2=1/2)\\) , we want now to introduce a new basis, in particular we want one whose kets are simultaneous eigenvalues of the following set of observables: \\[\n\\{S_1^2,S_2^2,S^2,S_z\\}\n\\tag{28}\\] where:\n\n\\(\\mathbf{S}=\\mathbf{S}_1+\\mathbf{S}_2=(S_{1x}+S_{2x},S_{1y}+S_{2y},S_{1z}+S_{2z})\\) is the total spin operator.\n\\(S^2=S_x^2+S_y^2+S_z^2\\) is the magnitude squared operator.\n\\(S_z=S_{1z}+S_{2z}\\) is the \\(z\\)-component of \\(\\mathbf{S}\\).\n\nWhy this set of operators? The Hamiltonian we got in our hands (not shown in this notes) might commute with Equation 28 observables rather Equation 25 . Our goal is still to find the eigenstates of \\(H\\) and that is facilitated when we know operators that commute with it.\nSince \\(\\{S_1^2,S_2^2,S^2,S_z\\}\\) is a set of commuting observables, we can define a basis of kets which are eigenvector of all these observables; the basis is labelled by the eigenvalues (or better yet, by the indices that label the eigenvalues): \\[\n\\begin{align}\n&S_1^2\\ket{j_1,j_2;S,M}=S^2_2\\ket{j_1,j_2;S,M}=\\frac{3}{4} \\hbar^2\\ket{S,M}\\\\\n&S^2\\ket{j_1,j_2;S,M}=S(S+1)\\hbar^2\\ket{j_1,j_2;S,M}\\\\\n&S_z\\ket{j_1,j_2;S,M}=M\\hbar\\ket{j_1,j_2;S,M}\n\\end{align}\n\\]where \\(j_1=j_2=1/2\\) (we can drop them from the kets, since they are fixed and just use \\(|S,M\\rangle\\)). Notice the operators \\(S^2\\) and \\(S_z\\) are in fact angular momentum operators because they result from adding angular momentum, thus they are just another particular case of \\(J^2\\) and \\(J_z\\) that satisfy the commutation relation Equation 4, as a result their eigenvalues are of the form \\(S(S+1)\\hbar^2\\) and \\(M\\hbar\\) with \\(-S\\leq M\\leq S\\) as we have seen in Equation 7.\n\n\n\nGiven \\(j_1=j_2=1/2\\), we still do not know what values \\(S\\) and \\(M\\) take. But two things we know:\n\nFrom the theory of general angular momentum given in 4), we find \\(-S\\leq M\\leq S\\).\nKnowing that \\(S_z=S_{1z} + S_{2z}\\) tell us how to act with \\(S_z\\) on the old basis \\(\\{\\ket{1/2,1/2},\\ket{-1/2,1/2},\\ket{1/2,-1/2},\\ket{-1/2,-1/2}\\}\\), that suffices to show what values \\(M\\) take. For that we act with \\(S_z\\) on each ket of this basis. \\[\nS_z\\ket{m_1,m_2}=(S_{1z}+S_{2z})\\ket{m_1,m_2}=(m_1+m_2)\\hbar \\ket{m_1,m_2}\n\\tag{29}\\]\n\nFrom this we conclude that \\[\nM=m_1+m_2\n\\] Hence, the values \\(M\\) can take are: \\[\nM\\in\\{-1,0,+1\\}\n\\tag{30}\\]\nLets use this information: what values \\(S\\) have to take that guarantee that \\(M\\) takes the values Equation 30 ? Notice the question: If I know \\(S\\), we can easily get the range of \\(M\\) because we know that \\(-S\\leq M\\leq S\\). The present question goes the other way, if I know the \\(M\\)’s (and we know see Equation 30), what \\(S\\)’s are possible? We can answer this question by constructing a diagram consistent with the range of \\(M\\)’s, for example for the range Equation 30 the corresponding \\(S\\)’s are:\n\nWe can clearly see that \\(S=0,1\\) are all possible values consistent with Equation 30.\nRather than constructing this picture we can use the rule: \\[\n|j_1-j_2|\\leq S\\leq j_1+j_2\n\\]However the diagram not only showed us what are the values of \\(S\\), it also organizes pictorially the eigenbasis \\(\\{\\ket{S,M}\\}\\) generated by Equation 28, which in turn reveals two subspaces (pink boxes) where the raising and lowering operators \\(S_\\pm\\) are stuck.\n\n\n\nWe want to express the kets \\(\\ket{S,M}\\) of the new basis in terms of \\(\\ket{m_1,m_2}\\) that constitute the old basis, to do that we argue pictorially:\n\nOn the left side of the picture we see a graphical organization of the old basis and on the right side the new basis. We identify in each basis the elements that span the subspaces \\(\\mathcal{E}(M)\\) for each \\(M\\); this is easily done on the right picture because the horizontal axis already tells us the \\(M\\)’s, on the left picture these subspaces are diagonal because each basis ket is an eigenvector of \\(S_z\\) with eigenvalue \\(M\\hbar=(m_1+m_2)\\hbar\\).\nFrom the pictures we know: \\[\n\\begin{align}\n&\\ket{S=1,M=1}\\in \\mathcal{E}(M=1)\\\\\n&\\ket{m_1=1/2,m_2=1/2}\\in \\mathcal{E}(M=1)\n\\end{align}\n\\] and since this subspace is one-dimensional, these two basis kets must coincide. We write: \\[\n\\ket{1,1}=\\ket{1/2,1/2}\n\\tag{31}\\]\nThis is a key step, since we just succeeded in establishing a first connection between both basis. The remaining connections follow from this one, as falling dominos, using the lower operator \\(S_-:=S_{1-}+S_{2-}\\) .\n\nActing with the lower operator on both sides of Equation 31 we find the next connection between these basis kets:\n\\[\nS_-\\ket{1,1}=(S_{1-}+S_{2-})\\ket{1/2,1/2}\n\\]\nThe left hand side gives us: \\[\n\\hbar\\sqrt{1(1+1)-0(0-1)}\\ket{1,0}\n\\] The right hand side is: \\[\n\\hbar\\sqrt{\\frac{1}{2}\\left(\\frac{1}{2}+1\\right)-\\frac{1}{2}\\left(\\frac{1}{2}-1\\right)}\\left(\\ket{-\\frac{1}{2},\\frac{1}{2}}+\\ket{\\frac{1}{2},-\\frac{1}{2}}\\right)\n\\] Thus: \\[\n\\hbar\\sqrt{2}\\ket{1,0}=\\hbar\\left(\\ket{-\\frac{1}{2},\\frac{1}{2}}+\\ket{\\frac{1}{2},-\\frac{1}{2}}\\right)\n\\] Which yields the second connection between basis kets: \\[\n\\ket{1,0}=\\frac{1}{\\sqrt{2}}\\left(\\ket{-\\frac{1}{2},\\frac{1}{2}}+\\ket{\\frac{1}{2},-\\frac{1}{2}}\\right)\n\\] The third connection relates \\(|0,0\\rangle\\) with the old basis, to establish it, we use the properties of vector spaces:\n\nWe know \\(|0,0\\rangle\\) is a linear combination of \\(\\ket{-1/2,1/2}\\) and \\(\\ket{1/2,-1/2}\\) thus we write: \\[\n\\ket{0,0}=c_1\\ket{-1/2,1/2}+c_2\\ket{1/2,-1/2}\n\\] for some complex numbers \\(c_{1,2}\\).\nWe know \\(|0,0\\rangle\\) is orthogonal to \\(\\ket{1,0}\\), thus we write: \\[\n0=\\braket{1,0|0,0}=\\frac{1}{\\sqrt{2}}(c_1+c_2)\n\\]\nWe also know it is normalized, i.e., \\[\n1=\\braket{0,0|0,0}=|c_1|^2+|c_2|^2\n\\]\n\nSolving these three equations we conclude one possible solution is: \\[\nc_1=-c_2=\\frac{1}{\\sqrt{2}}\n\\] Hence, the second basis vector in \\(\\mathcal{E}(M=0)\\) is: \\[\n\\ket{0,0}=\\frac{1}{\\sqrt{2}}\\left(\\ket{-\\frac{1}{2},\\frac{1}{2}}-\\ket{\\frac{1}{2},-\\frac{1}{2}}\\right)\n\\] Finally, to get the last connection between the new and old basis, we act with either \\(S_-\\) on \\(\\ket{1,0}\\):\n\nOr we simply notice that \\(\\mathcal{E}(M=-1)\\) is also one dimensional and therefore: \\[\n\\ket{1,-1}=\\ket{-1/2,-1/2}\n\\]\n\n\n\nThe connection between the basis \\(\\{\\ket{S,M}\\}\\) of eigenstates of the set \\(\\{S_1^2,S_2^2;S^2,S_z\\}\\) and the basis \\(\\{\\ket{m_1,m_2}\\}\\) of eigenstates of the set \\(\\{S_1^2,S_{1z},S^2_2,S_{2z}\\}\\) given \\(S_1=S_2=1/2\\) is: \\[\n\\ket{0,0}=\\frac{1}{\\sqrt{2}}\\left(\\ket{-\\frac{1}{2},\\frac{1}{2}}-\\ket{\\frac{1}{2},-\\frac{1}{2}}\\right)\n\\] which spans the one dimension subspace \\(\\mathcal{E}(S=0)\\) and is thus called a singlet state; the three states of \\(\\mathcal{E}(S=1)\\) are called a triplet \\[\n\\begin{align}\n&\\ket{1,1}=\\ket{1/2,1/2}\\\\\n&\\ket{1,0}=\\frac{1}{\\sqrt{2}}\\left(\\ket{-\\frac{1}{2},\\frac{1}{2}}+\\ket{\\frac{1}{2},-\\frac{1}{2}}\\right)\\\\\n&\\ket{1,-1}=\\ket{-1/2,-1/2}\n\\end{align}\n\\]\n\n\n\n\nIt is traditional to use the symbols \\(L_x\\), \\(L_y\\) ,\\(L_z\\) and \\(L^2\\) in place of \\(J_x\\), \\(J_y\\) ,\\(J_z\\) and \\(J^2\\) when speaking about the orbital angular momentum. Orbital angular momentum states live in \\(\\mathcal{E}(j)\\) subspaces with integer \\(j\\) in contrast with spin angular momentum which live in subspaces with semi-integers. See Figure 1.\nFrom Equation 7 we have:\n\\[\n\\begin{align}&L^2\\ket{l,m_l}=l(l+1)\\hbar^2\\ket{l,m},\\\\&L_z\\ket{l,m}=m\\hbar\\ket{l,m}.\\end{align}\n\\tag{32}\\]\nwith \\(l=0,1,2,...\\) and \\(-l\\leq m \\leq l\\).\nThe formulas Equation 32 together with the choice of the position basis \\(\\{|x,y,z\\rangle\\}\\) allow us to give their position representation:\n\\[ \\begin{align} &\\langle x,y,z|L^2\\ket{l,m}=l(l+1)\\hbar^2\\langle x,y,z|l,m\\rangle\\\\ &\\langle x,y,z|L_z\\ket{l,m}=m\\hbar\\langle x,y,z|l,m\\rangle \\end{align} \\]\nSubstituting Equation 2 we find:\n\\[ \\begin{align}&\\langle x,y,z|(L_x^2+L_y^2+L_z^2)\\ket{l,m}=l(l+1)\\hbar^2\\langle x,y,z|l,m\\rangle\\\\ &\\langle x,y,z|(XP_y-YP_x)\\ket{l,m}=m\\hbar\\langle x,y,z|l,m\\rangle\\end{align}  \\tag{33}\\]\n\n\n\n\n\n\nRecall\n\n\n\nThe \\(X\\) and \\(P_x\\) operators have the following representation in the \\(\\{|x\\rangle\\}\\) basis:\n\\[ \\langle x|X|\\psi\\rangle = x\\langle x|\\psi\\rangle \\qquad \\langle x|P_x|\\psi\\rangle = \\frac{\\hbar}{i}\\frac{\\partial}{\\partial x}\\langle x|\\psi\\rangle \\]\n\n\nFrom the second equation in Equation 33 we find:\n\\[ \\begin{align} \\langle x,y,z|L_z|\\psi\\rangle &= \\langle x,y,z|(XP_y-YP_x)|\\psi\\rangle\\\\ &= \\langle x,y,z|(xP_y-yP_x)|\\psi\\rangle\\\\ &= \\langle x,y,z|(x\\frac{\\hbar}{i}\\frac{\\partial}{\\partial y}-y\\frac{\\hbar}{i}\\frac{\\partial}{\\partial x})|\\psi\\rangle\\\\ &= \\frac{\\hbar}{i}\\left(x\\frac{\\partial}{\\partial y}-y\\frac{\\partial}{\\partial x}\\right)\\langle x,y,z|\\psi\\rangle\\\\ &= \\frac{\\hbar}{i}\\left(x\\frac{\\partial}{\\partial y}-y\\frac{\\partial}{\\partial x}\\right)\\psi(x,y,z) \\end{align} \\] Therefore, in the position basis, the \\(L_z\\) operator acts on functions \\(\\psi(x,y,z)\\) as the operation:\n\\[ L_z \\rightsquigarrow \\frac{\\hbar}{i}\\left(x\\frac{\\partial}{\\partial y}-y\\frac{\\partial}{\\partial x}\\right)  \\tag{34}\\]\nSimilarly we find:\n\\[ L_x \\rightsquigarrow \\frac{\\hbar}{i}\\left(y\\frac{\\partial}{\\partial z}-z\\frac{\\partial}{\\partial y}\\right) \\qquad L_y \\rightsquigarrow \\frac{\\hbar}{i}\\left(z\\frac{\\partial}{\\partial x}-x\\frac{\\partial}{\\partial z}\\right)  \\tag{35}\\]\nIt will be useful to change from cartesian coordinates \\(x,y,z\\) into polar coordinates \\(r,\\theta,\\phi\\). The map between these coordinates is established by the equations:\n\\[ \\begin{align} & x = r \\sin \\theta \\cos \\phi\\\\ & y = r \\sin \\theta \\sin \\phi\\\\ & z = r \\cos \\theta \\end{align} \\]\nprovided \\(r\\geq 0\\), \\(0\\leq\\theta\\leq\\pi\\) and \\(0\\leq\\phi&lt; 2\\pi\\).\nTo convert Equation 34 and Equation 35 into polar coordinates we proceed as follows:\n\nDraw a diagram:\n\n\n\nRelate the derivatives in cartesian and polar coordinates encoded in the diagram:\n\n\\[ \\begin{pmatrix} \\partial_r\\psi\\\\ \\partial_\\theta\\psi\\\\ \\partial_\\phi\\psi \\end{pmatrix} = \\begin{pmatrix} \\partial_r x & \\partial_r y & \\partial_r z\\\\ \\partial_\\theta x & \\partial_\\theta y & \\partial_\\theta z\\\\ \\partial_\\phi x & \\partial_\\phi y & \\partial_\\phi z \\end{pmatrix} \\begin{pmatrix} \\partial_x\\psi\\\\ \\partial_y \\psi\\\\ \\partial_z \\psi \\end{pmatrix} \\]\n\nCompute the matrix (Easy)\n\n\\[ \\begin{pmatrix} \\sin\\theta \\cos\\phi & \\sin\\theta \\sin \\phi & \\cos\\theta\\\\ r \\cos\\theta \\cos\\phi & r \\cos\\theta \\sin \\phi & -r \\sin \\theta\\\\ -r\\sin\\theta \\sin \\phi & r \\sin\\theta \\cos\\phi & 0 \\end{pmatrix}  \\tag{36}\\]\n\nInvert the system Equation 36 (Hard, use Mathematica)\n\n\\[ \\begin{pmatrix} \\partial_x\\psi\\\\ \\partial_y\\psi\\\\ \\partial_z\\psi \\end{pmatrix} = \\begin{pmatrix} \\sin\\theta \\cos \\phi   & r^{-1}\\cos\\theta \\cos\\phi & -(r\\sin\\theta)^{-1} \\sin\\phi\\\\ \\sin\\theta \\sin \\phi   & r^{-1}\\cos\\theta \\sin\\phi & (r\\sin\\theta)^{-1} \\cos\\phi\\\\ \\cos\\theta & -r^{-1}\\sin\\theta & 0 \\end{pmatrix} \\begin{pmatrix} \\partial_r\\psi\\\\ \\partial_\\theta \\psi\\\\ \\partial_\\phi \\psi \\end{pmatrix} \\]\n\nSubstitute the results into Equation 34 and Equation 35 and simplify:\n\\[ \\begin{align} &L_x \\rightsquigarrow i\\hbar\\left(\\sin\\phi \\frac{\\partial}{\\partial \\theta}+\\frac{\\cos\\phi}{\\tan\\theta} \\frac{\\partial}{\\partial \\phi}\\right)\\\\ &L_y \\rightsquigarrow i\\hbar\\left(-\\cos\\phi \\frac{\\partial}{\\partial \\theta}+\\frac{\\sin\\phi}{\\tan\\theta} \\frac{\\partial}{\\partial \\phi}\\right)\\\\ &Lz \\rightsquigarrow \\frac{\\hbar}{i} \\frac{\\partial}{\\partial \\phi} \\end{align} \\]\nFrom these results we can now derive the action of \\(L^2\\) in polar coordinates:\n\\[ \\begin{align} L^2\\psi &= L_x^2\\psi + L_y^2\\psi+L_z^2\\psi\\\\ &=(i\\hbar)^2\\left(\\sin\\phi \\frac{\\partial}{\\partial \\theta}+\\frac{\\cos\\phi}{\\tan\\theta} \\frac{\\partial}{\\partial \\phi}\\right)^2\\psi +(i\\hbar)^2\\left(-\\cos\\phi \\frac{\\partial}{\\partial \\theta}+\\frac{\\sin\\phi}{\\tan\\theta} \\frac{\\partial}{\\partial \\phi}\\right)^2\\psi+ \\left(\\frac{\\hbar}{i}\\right)^2 \\frac{\\partial^2}{\\partial \\phi^2}\\psi \\end{align}  \\tag{37}\\] Now we need to expand each square, the first becomes:\n\n\\[ (i\\hbar)^2\\left(\\sin^2\\phi \\frac{\\partial^2\\psi}{\\partial \\theta^2}+ \\frac{\\cos^2\\phi}{\\tan^2\\theta}\\frac{\\partial^2\\psi}{\\partial \\phi^2}-\\frac{\\sin\\phi\\cos\\phi}{\\sin^2\\theta}\\frac{\\partial \\psi}{\\partial \\phi}+\\frac{\\sin\\phi\\cos\\phi}{\\tan\\theta}\\frac{\\partial^2 \\psi}{\\partial\\theta\\partial \\phi}\\right) \\]\nthe second term becomes:\n\\[ (i\\hbar)^2\\left(\\cos^2\\phi \\frac{\\partial^2\\psi}{\\partial \\theta^2}+ \\frac{\\sin^2\\phi}{\\tan^2\\theta}\\frac{\\partial^2\\psi}{\\partial \\phi^2}+\\frac{\\cos\\phi\\sin\\phi}{\\sin^2\\theta}\\frac{\\partial \\psi}{\\partial \\phi}-\\frac{\\cos\\phi\\sin\\phi}{\\tan\\theta}\\frac{\\partial^2 \\psi}{\\partial\\theta\\partial \\phi}\\right) \\]\nSubstituting into Equation 37 and canceling terms yields the final result:\n\\[ L^2 \\rightsquigarrow -\\hbar^2\\left(\\frac{\\partial^2}{\\partial \\theta^2} + \\frac{1}{\\sin^2\\theta} \\frac{\\partial^2}{\\partial \\phi^2} + \\frac{1}{\\tan\\theta}\\frac{\\partial}{\\partial \\theta}\\right) \\]\nAs a consequence, the system of equations in Equation 33 becomes a system of partial differential equations:\n\\[ \\begin{align} &-\\hbar^2 \\left(\\frac{\\partial}{\\partial \\theta^2}+\\frac{1}{\\tan \\theta} \\frac{\\partial}{\\partial \\theta}+\\frac{1}{\\sin^2\\theta}\\frac{\\partial}{\\partial^2 \\phi^2}\\right)\\psi_{l,m}(r, \\theta, \\phi)=l(l+1)\\hbar^2\\psi_{l,m}(r, \\theta, \\phi)\\\\ & \\frac{\\hbar}{i} \\frac{\\partial}{\\partial \\phi}\\psi_{l,m}(r, \\theta, \\phi)=m\\hbar \\psi_{l,m}(r, \\theta, \\phi) \\end{align} \\]\nThe solutions \\(\\psi_{l,m}(r,\\theta,\\phi)\\) are the eigenfunctions common to the \\(L^2\\) and \\(L_z\\) operators associated with the eigenvalues \\(l(l+1)\\hbar^2\\) and \\(m\\hbar\\). These solution exist because these two operators commute. Our ultimate goal is to solve them, but in these notes we’ll content ourselves by just identifying some of its properties.\n\n\n\nSuppose our system of two electrons is described by states in \\(\\mathcal{E}(S=1)\\) (see triplet states in 13)) and we want to add to it another electron, which obviously is described by states in \\(\\mathcal{E}(j_3=1/2)\\). How are we going to describe the quantum states of the three particles?\nIn 10)-14) we had one electron living in \\(\\mathcal{E}(j_1=1/2)\\) and added to it another electron, which was, as usual, in some state in \\(\\mathcal{E}(j_2=1/2)\\). Back then:\n\nwe constructed a tensor product basis for \\(\\mathcal{E}(j_1=1/2)\\otimes \\mathcal{E}(j_2=1/2)\\) to describe the states of the two electrons\nintroduced a new and more convenient basis\nwe connected both basis\n\nOur goal in this section is analogous, we will start to construct a basis for the tensor product space \\(\\mathcal{E}(S=1)\\otimes \\mathcal{E}(j_3=1/2)\\), then introduce a more convenient basis and finally connect the new with the old.\n\n\nWe note that the CSCO that is about to generate a basis for \\(\\mathcal{E}(S_{12}=1)\\otimes \\mathcal{E}(j_3=1/2)\\) is given by \\[\n\\{\\overbrace{S_1^2,S_2^2}^\\text{background};\\overbrace{S_{12}^2,S_{12}^z}^\\text{1st spin},\\overbrace{S_3^2,S_3^z}^\\text{2ndspin}\\}\n\\] We can split this set into three parts:\n\n1st spin angular momentum: the main operators that we need to specify the state of the two first electrons in the system are \\(S_{12}^2\\) and \\(S_{12}^z\\).\nbackground: how do we know we added two electron? To answer that we need two extra operators, \\(S_1^2\\) and \\(S_2^2\\), to provide background information on what spins where added. By including these in the CSCO, the eigenkets will include \\(s_1\\) and \\(s_2\\) which when set as equal to \\(1/2\\) automatically says, these are two electron spin we did add.\n2nd spin angular momentum: The main operators for the third electron are \\(S_3^2\\) and \\(S_3^z\\).\n\nThese six operators commute among each other and as a result we can define a basis of eigenvectors common to all the observables, which are indexed by the corresponding quantum numbers:\n\\[\n\\ket{s_1,s_2;S_{12},M_{12},s_3,m_3}\n\\] What do we know about these states?\n\n\\(s_1=s_2=1/2\\) because the 1st spin result from adding two electron spins.\n\\(0=|s_1-s_2|\\leq S_{12}\\leq s_1+s_2=1\\) in steps of \\(1\\)\n\\(-1\\leq M_{12}\\leq 1\\)\n\\(s_3=1/2\\)\n\\(-1/2\\leq m_3\\leq 1/2\\)\n\\(\\ket{s_1,s_2;S_{12},M_{12},s_3,m_3}:=\\ket{s_1,s_2;S_{12},M_{12}}\\otimes\\ket{s_3,m_3}\\)\nMoreover we know the ket \\(\\ket{s_1,s_2;S_{12},M_{12}}\\) can be written in terms of the individual states of electron \\(1\\) and \\(2\\) either as a singlet or triplet state, see 14). Since we opted for \\(S_{12}=1\\) this state be one of the triplet states.\n\nGiven \\(S_{12}=1\\) and \\(s_3=1/2\\) we can organize this basis (which we call the old basis) with the following picture:\n\n\n\nThe old basis.\n\n\nLets us now change the basis of our tensor product space, for that we choose the new CSCO: \\[\n\\{\\overbrace{S_1^2,S_2^2,S_3^2,S_{12}^2}^\\text{background};\\overbrace{S_{tot}^2,S_{tot}^z}^\\text{total spin}\\}\n\\] whose eigenbasis is defined as usual by the quantum numbers determining each eigenvalue of the observables in the set: \\[\n\\ket{s_1,s_2,s_3,S_{12};S_{tot},M_{tot}}\n\\]\nSpecifically we are interested in the basis states which have \\[\ns_1=s_2=s_3=1/2\\qquad S_{12}=1\n\\]\nbecause we did add three electron spins and the first two are in state belonging to \\(\\mathcal{E}(S_{12}=1)\\). Note this subspace is my choice, we could had chosen \\(\\mathcal{E}(S_{12}=0)\\), see 13). This is the new basis.\nThe operators \\(S_{tot}^2\\) and \\(S_{tot}^z\\) describe the magnitude and the \\(z-\\)component of the total angular momentum, and we know from the general rules of angular momentum in 4) that the following relation holds: \\[\n-S_{tot}\\leq M_{tot}\\leq S_{tot}\n\\] We do not know the values of \\(S_{tot}\\) are, yet, just like we did not know what were the values of \\(S_{12}\\) when we added the systems 1+2 in 12). But we can guess what they are from the possible values of \\(M_{tot}\\), just as we did to guess what \\(S_{12}\\) was from the known values of \\(M=m_1+m_2\\), see 12).\nSince the kets of the old basis are also eigenvectors of \\(S_{tot}^z\\), let us act with this operator on the old basis to get the corresponding eigenvalues \\(M_{tot}\\), the calculations are analogous to Equation 29; we summarize the results by the pink boxes in the following picture:\n\nKnowing that \\(M_{tot}=-3/2,-1/2,1/2,3/2\\) and the fact that these numbers must be between \\(-S_{tot}\\) and \\(S_{tot}\\) we attempt to reconstruct the diagram that organizes the basis:\n\nFrom the picture we conclude we must have either \\(S_{tot}=1/2,3/2\\) and we confirm again the general rule: \\[\n1/2=|S_{12}-s_3|\\leq S_{tot}\\leq S_{12}+s_3=3/2\\qquad \\text{in steps of }1\n\\] Let us now relate the old basis on the left with the new basis on the right, the idea is exactly the same as in 13):\n\nThe two subspaces \\(\\mathcal{E}(M_{tot}=3/2)\\) are one dimensional and thus: \\[\n\\ket{S_{tot}=3/2,M_{tot}=3/2}=\\ket{M_{12}=1}\\otimes\\ket{m_3=1/2}\n\\]The ket on the lhs which previously was just a ket with indices of the eigenvalues, now has meaning, it is equal to the tensor product on the rhs.\nActing now with the lowering operator \\(S_{tot}^-=S_{12}^-+S_1^-\\) on this equality we find: \\[\nS_{tot}^-\n\\ket{\\frac{3}{2},\\frac{3}{2}}=\\hbar\\sqrt{3}\\ket{\\frac{3}{2},\\frac{1}{2}}\n\\] The right side of the equality is: \\[\n\\begin{align}\nS_{tot}^-\n\\ket{\\frac{3}{2},\\frac{3}{2}}&=(S_{12}^-+S_1^-)(\\ket{1}\\otimes\\ket{\\frac{1}{2}})\\\\\n&=\\hbar\\sqrt{2}\\ket{0}\\otimes\\ket{\\frac{1}{2}}+\\hbar\\ket{1}\\otimes\\ket{-\\frac{1}{2}}\n\\end{align}\n\\] Therefore we conclude: \\[\n\\ket{\\frac{3}{2},\\frac{1}{2}}=\\sqrt{\\frac{2}{3}}\\ket{0}\\otimes\\ket{\\frac{1}{2}}+\\frac{1}{\\sqrt{3}}\\ket{1}\\otimes\\ket{-\\frac{1}{2}}\n\\] We now seek to relate the eigenvector \\(\\ket{S_{tot}=1/2,M_{tot}=1/2}\\) with the old basis, we do that by writing its properties:\n\nit must be a linear combination of the form: \\[\n    \\ket{S_{tot}=1/2,M_{tot}=1/2}=\\alpha \\ket{0}\\otimes\\ket{\\frac{1}{2}} + \\beta \\ket{1}\\otimes\\ket{-\\frac{1}{2}}\n    \\]\nit must normalized: \\[\n|\\alpha|^2+|\\beta|^2=1\n\\]\nit must be perpendicular to \\(\\ket{\\frac{3}{2},\\frac{1}{2}}\\) .\n\nA pair of \\(\\alpha\\) and \\(\\beta\\) that satisfy all the above constraints yields: \\[\n\\ket{\\frac{1}{2},\\frac{1}{2}}=\\frac{1}{\\sqrt{3}} \\ket{0}\\otimes\\ket{\\frac{1}{2}} - \\sqrt{\\frac{2}{3}} \\ket{1}\\otimes\\ket{-\\frac{1}{2}}\n\\] We can continue now and act with \\(S_{tot}^-\\) on \\(\\ket{\\frac{3}{2},\\frac{1}{2}}\\) to get us \\(\\ket{\\frac{3}{2},-\\frac{1}{2}}\\); we find: \\[\n\\ket{\\frac{3}{2},-\\frac{1}{2}}=\\frac{1}{\\sqrt{3}}\\ket{-1}\\otimes\\ket{\\frac{1}{2}}+\\sqrt{\\frac{2}{3}}\\ket{0}\\otimes\\ket{-\\frac{1}{2}}\n\\] The ket \\(\\ket{\\frac{1}{2},-\\frac{1}{2}}\\) is found from its properties just as we did for \\(\\ket{\\frac{1}{2},\\frac{1}{2}}\\): \\[\n\\ket{\\frac{1}{2},-\\frac{1}{2}}=\\sqrt{\\frac{2}{3}}\\ket{-1}\\otimes\\ket{\\frac{1}{2}}-\\frac{1}{\\sqrt{3}}\\ket{0}\\otimes\\ket{-\\frac{1}{2}}\n\\] Finally we are left with: \\[\n\\ket{\\frac{3}{2},-\\frac{3}{2}}=\\ket{-1}\\otimes\\ket{-\\frac{1}{2}}\n\\] since the subspace \\(\\mathcal{E}(M_{tot}=-3/2)\\) is one dimensional.\n\n\n\n\nThe eigenbasis of the CSCO \\(\\{S_1^2,S_2^2,S_3^2,S_{12}^2;S_{tot}^2,S_{tot}^z\\}\\) is related with the eigenbasis of the CSCO \\(\\{S_1^2,S_2^2;S_{12}^2,S_{12}^z,S_3^2,S_3^z\\}\\) as:\n\n\\(\\mathcal{E}(M_{12}=3/2)\\) \\[\n\\ket{S_{tot}=3/2,M_{tot}=3/2}=\\ket{M_{12}=1}\\otimes\\ket{m_3=1/2}\n\\]\n\\(\\mathcal{E}(M_{12}=1/2)\\) \\[\n\\begin{align}\n&\\ket{S_{tot}=\\frac{3}{2},M_{tot}=\\frac{1}{2}}=\\sqrt{\\frac{2}{3}}\\ket{M_{12}=0}\\otimes\\ket{m_3=\\frac{1}{2}}+\\\\\n&\\frac{1}{\\sqrt{3}}\\ket{M_{12}=1}\\otimes\\ket{m_3=-\\frac{1}{2}}\\\\\n&\\ket{S_{tot}=\\frac{1}{2},M_{tot}=\\frac{1}{2}}=\\frac{1}{\\sqrt{3}} \\ket{M_{12}=0}\\otimes\\ket{m_3=\\frac{1}{2}} -\\\\\n&\\sqrt{\\frac{2}{3}} \\ket{M_{12}=1}\\otimes\\ket{m_3=-\\frac{1}{2}}\n\\end{align}\n\\]\n\\(\\mathcal{E}(M_{12}=-1/2)\\) \\[\n\\begin{align}\n&\\ket{S_{tot}=\\frac{3}{2},M_{tot}=-\\frac{1}{2}}=\\frac{1}{\\sqrt{3}}\\ket{M_{12}=-1}\\otimes\\ket{m_3=\\frac{1}{2}}+\\\\\n&\\sqrt{\\frac{2}{3}}\\ket{M_{12}=0}\\otimes\\ket{m_3=-\\frac{1}{2}}\\\\\n&\\ket{S_{tot}=\\frac{1}{2},M_{tot}-\\frac{1}{2}}=\\sqrt{\\frac{2}{3}}\\ket{M_{12}=-1}\\otimes\\ket{m_3=\\frac{1}{2}}-\\\\\n&\\frac{1}{\\sqrt{3}}\\ket{M_{12}=0}\\otimes\\ket{m_3=-\\frac{1}{2}}\n\\end{align}\n\\]\n\\(\\mathcal{E}(M_{12}=-3/2)\\) \\[\n\\ket{S_{tot}=\\frac{3}{2},M_{tot}=-\\frac{3}{2}}=\\ket{M_{12}=-1}\\otimes\\ket{m_3=-\\frac{1}{2}}\n\\] In turn we must now replace the basis kets \\(\\ket{S_{12}=1,M_{12}}\\) by the elements of the triplet from 14) if you want to relate the new basis with the basis generated by the CSCO\n\\[\n\\{S_1^2,S_1^z;S_2^2,S_2^z;S_3^2,S_3^z\\}\n\\]",
    "crumbs": [
      "Brief Notes",
      "Quantum Theory",
      "Angular Momentum"
    ]
  },
  {
    "objectID": "angular_momentum_qm_short_summary.html#commutation-relations-are-fundamental",
    "href": "angular_momentum_qm_short_summary.html#commutation-relations-are-fundamental",
    "title": "Ideas to get started with Angular Momentum in Quantum Mechanics",
    "section": "",
    "text": "In QM there are essentially two types of angular momentum: Orbital Angular Momentum and Spin Angular momentum.\n\nOrbital angular momentum is defined as an analog of the classical one, as\n\\[\n\\mathbf{L}=\\mathbf{R}\\times \\mathbf{P}\n\\tag{1}\\] \\(\\mathbf{R}=(X,Y,Z)\\) and \\(\\mathbf{P}=(P_x,P_y,P_z)\\) are vectors whose entries are operators.\nThis formula is a compact way of writing:\n\\[\n\\begin{cases}\nL_x = YP_z-ZP_y\\\\\nL_y = Z P_x -XP_z\\\\\nL_z = XP_y-YP_x\n\\end{cases}\n\\tag{2}\\]\nFrom the three components of \\(\\mathbf{L}\\) and the canonical commutation relations:\n\\[\n[X,P_x]=i\\hbar \\qquad [Y,P_y]=i\\hbar \\qquad [Z,P_z]=i\\hbar\n\\]\nwe can derive the following ones:\n\\[\n[L_x,L_y]=i\\hbar L_z\\qquad [L_y,L_z]=i\\hbar L_x\\qquad [L_z,L_x]=i\\hbar L_y\\qquad [L_i,L_j]=0\\quad \\text{for}\\ i\\not= j\n\\tag{3}\\]\nFrom one perspective, these are the commutation relations for the components of the quantum angular momentum that stem from Equation 1 and Equation 3. From a different perspective we can say that the angular momentum Equation 1 is a particular example of \\(\\mathbf{L}\\) that satisfy these three commutation “equations” in Equation 3. In other words, \\(\\mathbf{R}\\times \\mathbf{P}\\) is a solution of Equation 3.\nIn QM one wants to talk about more general angular momentum, for example spin angular momentum, it has no classical analog and hence is not defined explicitly as \\(\\mathbf{R}\\times \\mathbf{P}\\). However one can still define it implicitly, by saying that whatever spin angular momentum operator is, it must satisfy the previous commutations relation.\n\nThese two observations motivates us to introduce a generic angular momentum, with letter \\(\\mathbf{J}\\), for which the only thing we know is that it obeys the rules:\n\\[\n[J_x,J_y]=i\\hbar J_z\\qquad [J_y,J_z]=i\\hbar J_x\\qquad [J_z,J_x]=i\\hbar J_y\\qquad [J_i,J_j]=0\\quad \\text{for}\\ i\\not= j\n\\tag{4}\\]\n\\(\\mathbf{J}\\) can be either the orbital angular momentum of a particle or the spin angular momentum or a combination of these, since all quantum angular momenta has commutation relations of form Equation 4 .",
    "crumbs": [
      "Brief Notes",
      "Quantum Theory",
      "Angular Momentum"
    ]
  },
  {
    "objectID": "angular_momentum_qm_short_summary.html#the-norm-squared-commutes-with-everyone---what-are-the-consequences",
    "href": "angular_momentum_qm_short_summary.html#the-norm-squared-commutes-with-everyone---what-are-the-consequences",
    "title": "Ideas to get started with Angular Momentum in Quantum Mechanics",
    "section": "",
    "text": "Given \\(\\mathbf{J}\\) we can introduce now its norm squared \\(J^2\\), defined as:\n\\[\nJ^2=J_x^2+J_y^2+J_z^2\n\\tag{5}\\]\nWe can easily show (we’ll skip the boring details) from Equation 4 that Equation 5 commutes with the three components of \\(\\mathbf{J}\\), that is:\n\\[\n[J^2,J_x]=0\\qquad[J^2,J_y]=0\\qquad[J^2,J_z]=0\n\\tag{6}\\]\nFor example, using Equation 4 we find:\n\\[\n\\begin{align}\n[J^2,J_z]&=[J_x^2+J_y^2+J_z^2,J_z]\\\\\n&=[J_x^2,J_z]+[J_y^2,J_z]+[J_z^2,J_z]\\\\\n&=0+0+0\\\\\n&=0\n\\end{align}\n\\]\nThat fact that \\(J^2\\) commutes with \\(J_z\\) will be of great importance, because it guarantees one can find (and thus define) a set of basis kets in the Hilbert space that are simultaneously eigenstates of both, \\(J^2\\) and \\(J_z\\). Moreover, since the commuting operators are hermitian (we say in QM they are observables) the eigenstates of the operators can be made orthogonal and the eigenvalues are real.",
    "crumbs": [
      "Brief Notes",
      "Quantum Theory",
      "Angular Momentum"
    ]
  },
  {
    "objectID": "angular_momentum_qm_short_summary.html#the-idea",
    "href": "angular_momentum_qm_short_summary.html#the-idea",
    "title": "Ideas to get started with Angular Momentum in Quantum Mechanics",
    "section": "",
    "text": "Our goal is to define (to be mare specific, to define is to give meaningful names to something) a basis of eigenstates common to \\(J^2\\) and \\(J_z\\) for our Hilbert Space. Why? One of the most important goals in QM is to find the eigenstates of the Hamiltonian of the system, paraphrased differently, we say, we want to find the basis of eigenstates of \\(H\\). Imagine we have an Hamiltonian which commutes with \\(J^2\\) and \\(J_z\\), then we know we can find a basis of eigenstates common to the three operators. Knowing already what is the eigenbasis common to \\(\\{J^2,J_z\\}\\) greatly facilitates finding the eigenstates of \\(H\\). The three operators (observables) constitute a complete set of commuting observables (CSCO) \\[ \\{H,J^2,J_z\\} \\] and we want to define its eigenbasis.",
    "crumbs": [
      "Brief Notes",
      "Quantum Theory",
      "Angular Momentum"
    ]
  },
  {
    "objectID": "angular_momentum_qm_short_summary.html#eigenvalues-and-eigenvectors-of-angular-momentum-operators",
    "href": "angular_momentum_qm_short_summary.html#eigenvalues-and-eigenvectors-of-angular-momentum-operators",
    "title": "Ideas to get started with Angular Momentum in Quantum Mechanics",
    "section": "",
    "text": "In summary, skipping the proofs, what the commutation relations introduced in Equation 4 allow us to conclude is:\n\nThe eigenvalues of \\(J^2\\) are of the form \\(j(j+1)\\hbar^2\\) and those of \\(J_z\\) are \\(m\\hbar\\) where \\(j\\) is a positive integer or semi-integer and \\(-j\\leq m\\leq j\\) in steps of \\(1\\).\nThe corresponding eigenvectors can be labelled by the numbers \\(j\\) and \\(m\\).\n\nIn other words:\n\\[ \\begin{align}&J^2\\ket{j,m}=j(j+1)\\hbar^2\\ket{j,m},\\\\&J_z\\ket{j,m}=m\\hbar\\ket{j,m}.\\end{align}  \\tag{7}\\]\nThe only values that \\(j\\) can take are \\(0\\), \\(1/2\\), \\(3/2\\), \\(2\\), etc. For example, if \\(j=3/2\\) then \\(m=-3/2,-1/2,1/2,3/2\\).",
    "crumbs": [
      "Brief Notes",
      "Quantum Theory",
      "Angular Momentum"
    ]
  },
  {
    "objectID": "angular_momentum_qm_short_summary.html#raising-and-lowering-operators",
    "href": "angular_momentum_qm_short_summary.html#raising-and-lowering-operators",
    "title": "Ideas to get started with Angular Momentum in Quantum Mechanics",
    "section": "",
    "text": "By definition we have: \\[ J_+=J_x+iJ_y\\qquad J_-=J_x-iJ_y\\qquad.  \\tag{8}\\] There are now plenty commutations relations we can derive that relate these two operators and either \\(J^2\\) or \\(J\\), but the ultimate consequence of these relations is that they tell us how to organize the eigenbasis of \\(\\{J^2,J_z\\}\\), we’ll see that in 6).\nAdditionally, these definitions allow us to write:\n\\[ J_x = \\frac{1}{2}(J_++J_-) \\qquad J_y = \\frac{1}{2i}(J_+-J_-), \\]\nwhich will be useful later.\nHow do Equation 8 operators act on the eigenstates of \\(\\{J^2,J_z\\}\\)?\nAnswer:\n\\[ J_\\pm \\ket{n,j,m}=\\hbar \\sqrt{j(j+1)-m(m\\pm 1)}\\ket{n,j,m\\pm 1},  \\tag{9}\\]\nA formula which is, no surprise, derived from Equation 4 (through a rather complicated path). The names raising and lowering comes form the fact that \\(m\\) is either raised by \\(+1\\) or lowered by \\(-1\\) each time \\(J_\\pm\\) acts.",
    "crumbs": [
      "Brief Notes",
      "Quantum Theory",
      "Angular Momentum"
    ]
  },
  {
    "objectID": "angular_momentum_qm_short_summary.html#the-structure-of-the-eigenbasis",
    "href": "angular_momentum_qm_short_summary.html#the-structure-of-the-eigenbasis",
    "title": "Ideas to get started with Angular Momentum in Quantum Mechanics",
    "section": "",
    "text": "From Equation 7 and Equation 9 emerges the following picture\n\n\n\n\n\n\nFigure 1: Eigenspaces for various values of j.\n\n\n\nwhich organizes the eigenstates for values of \\(j\\) from \\(0\\) up to \\(2\\), each blue dot in the picture corresponds to basis vector \\(\\ket{j,m}\\). These kets constitute an orthonormal basis for the Hilbert space, and in the picture we also see as horizontal boxes the subspaces \\(\\mathcal{E}(j=1/2)\\) up to \\(\\mathcal{E}(j=2)\\) spanned by the basis kets (blue dots).\n\n\n\n\n\n\nCommentary\n\n\n\n\nFrom the first equation in Equation 7 we notice that for a given value of \\(j\\), for example \\(j=1\\), then \\(\\{|1,1\\rangle,|1,0\\rangle,|1,-1\\rangle\\}\\) are eigenvectors of \\(J^2\\) with the same eigenvalue \\(1(1+1)\\hbar^2\\) , we therefore say the eigenvalue is 3-degenerate, or that the space \\(\\mathcal{E}(j=1)\\) spanned by this basis is 3-degenerate. We distinguish the 3 eigenvectors by the \\(J_z\\) eigenvalue \\(m\\hbar\\), or more simply by just the integer \\(m\\).\nSubspaces of the Hilbert space can now be specified by fixing \\(j\\) for example, or by fixing \\(m\\) for various values. The eigenbasis within each \\(\\mathcal{E}(j)\\) is connected by the raising and lowering operators as we see by the red and green arrows in the figure above.",
    "crumbs": [
      "Brief Notes",
      "Quantum Theory",
      "Angular Momentum"
    ]
  },
  {
    "objectID": "angular_momentum_qm_short_summary.html#matrix-representation-of-j2-j_x-j_y-and-j_z-in-mathcalej1",
    "href": "angular_momentum_qm_short_summary.html#matrix-representation-of-j2-j_x-j_y-and-j_z-in-mathcalej1",
    "title": "Ideas to get started with Angular Momentum in Quantum Mechanics",
    "section": "",
    "text": "To obtain the matrix representation of an operator on a given vector space we need to know how this operator acts on that space; we know how it acts on a given vector space by knowing how it acts on the chosen basis. Note: The matrix representation of the operator is then specific for the basis.\nFor example, let us choose the space \\(\\mathcal{E}(j=1)\\) of eigenvectors of \\(\\{J^2,J_z\\}\\), the basis is:\n\\[\n\\mathcal{B}=\\{|1,1\\rangle,|1,0\\rangle,|1,-1\\rangle\\}\n\\tag{10}\\]\nsee Figure 1 .\nTo know how \\(J_x\\) acts in \\(\\mathcal{E}(j=1)\\) we compute how it acts on the basis Equation 10:\n\\[\n\\begin{align}\nJ_x|1,1\\rangle &= \\frac{1}{2}(J_+ +J_-)|1,1\\rangle\\\\\n&=\\frac{1}{2}J_+|1,1\\rangle+\\frac{1}{2}J_-|1,1\\rangle\\\\\n&=\\frac{\\hbar}{2}\\sqrt{1(1+1)-1(1+1)}|1,2\\rangle+\\frac{\\hbar}{2}\\sqrt{1(1+1)-1(1-1)}|1,0\\rangle\\\\\n&=\\frac{\\hbar}{\\sqrt{2}}|1,0\\rangle\n\\end{align}\n\\tag{11}\\]\nWe conclude that when \\(J_x\\) acts on the basis element \\(|1,1\\rangle\\) it outputs \\(|1,0\\rangle\\) times a constant, notice the input and output both live in \\(\\mathcal{E}(j=1)\\).\nLet us see now what happens when \\(J_x\\) acts on the second basis ket \\(|1,0\\rangle\\):\n\\[\n\\begin{align}\nJ_x|1,0\\rangle &= \\frac{1}{2}(J_++J_-)|1,0\\rangle\\\\\n&=\\frac{\\hbar}{2}\\sqrt{1(1+1)-0(0+1)}|1,1\\rangle+\\frac{\\hbar}{2}\\sqrt{1(1+1)-0(0-1)}|1,1\\rangle|1,-1\\rangle\\\\\n&=\\frac{\\hbar}{\\sqrt{2}}(|1,1\\rangle+|1,-1\\rangle)\n\\end{align}\n\\tag{12}\\]\nAs \\(J_x\\) acts on \\(|1,0\\rangle\\) it gives a linear combination \\(|1,1\\rangle+|1,-1\\rangle\\) of basis vectors, which in turn also lives in \\(\\mathcal{E}(j=1)\\).\nAnd finally, a similar calculation would shows the action on the third ket is:\n\\[\nJ_x|1,-1\\rangle =\\frac{\\hbar}{\\sqrt{2}}|1,0\\rangle\n\\tag{13}\\]\nAgain, the input \\(|1,-1\\rangle\\) is in \\(\\mathcal {E}(j=1)\\) and so is the output \\(|1,0\\rangle\\).\n\n\n\n\n\n\nCommentary\n\n\n\nThe calculation above shows clearly that the the operator \\(J_x\\) maps the basis of \\(\\mathcal{E}(j=1)\\) into vectors in \\(\\mathcal{E}(j=1)\\). Therefore, in general it will map any ket of \\(\\mathcal{E}(j=1)\\) into another in \\(\\mathcal{E}(j=1)\\); the jargon for this observation is to say that “\\(\\mathcal{E}(j=1)\\) is globally invariant under the action of \\(J_x\\)”; more colloquially we just say “the action of \\(J_x\\) is stuck in the subspace”. This observation is a consequence of the fact that the operator in question \\(J_x\\) commutes with the operator \\(J^2\\) that characterizes the subspace\n\\[\n[J^2,J_x]=0 \\implies [\\text{the action of $J_x$ is stuck in the subspaces of $J^2$}]\n\\]\n\n\nNow lets get the matrix representation of \\(J_x\\) in \\(\\mathcal{E}(j=1)\\). The results in Equation 11, Equation 12 and Equation 13 are summarizes in step 0:\n\nStep zero:\n\\[\n\\begin{cases}\nJ_x|1,1\\rangle=\\frac{\\hbar}{\\sqrt{2}}|1,0\\rangle\\\\\nJ_x|1,0\\rangle =\\frac{\\hbar}{\\sqrt{2}}(|1,1\\rangle+|1,-1\\rangle)\\\\\nJ_x|1,-1\\rangle =\\frac{\\hbar}{\\sqrt{2}}|1,0\\rangle\n\\end{cases}\n\\tag{14}\\] note, these formulas are important: they tell us how \\(J_x\\) acts on the chosen basis of chosen subspace \\(\\mathcal{E}(j=1)\\). From then we can now write the corresponding matrix representation, but:\nFirst step: write the identity of \\(\\mathcal{E}(j=1)\\):\n\\[\n1 = \\sum_{m=-1,0,1} |1,m\\rangle\\langle1,m|\n\\]\nSecond step: write \\(J_x\\) between the identities:\n\n\\[\nJ_x = \\sum_{m,m'=1,0,-1}|1,m\\rangle\\langle1,m|J_x|1,m'\\rangle\\langle1,m'|\n\\tag{15}\\]\n\nThird step: substitute Equation 14 in Equation 15:\n\\[\n\\begin{align}\nJ_x &= \\frac{\\hbar}{\\sqrt{2}}\\left(|1,0\\rangle\\langle 1,1|+|1,1\\rangle\\langle 1,0|+|1,-1\\rangle\\langle 1,0|+|1,0\\rangle\\langle 1,-1|\\right)\n\\end{align}\n\\tag{16}\\]\nFourth, write Equation 16 using using matrix notation. If we agree to order the basis as \\(\\{|1,1\\rangle, |1,0\\rangle,|1,-1\\rangle\\}\\) we find:\n\n\nWe can name this matrix as \\([J_x]_\\mathcal{B}\\), thenotation remind us: this is a matrix representing the action of \\(J_x\\) in the \\(\\mathcal{B}\\)! The squiggly arrow just mean: “it is represented by”.\nRepeating these steps for \\(J_y\\) and \\(J_z\\) yields the results:\n\nThe matrix \\([J_z]_\\mathcal{B}\\) is diagonal since the basis elements we choose for \\(\\mathcal{E}(j=1)\\) are its eigenvectors, the diagonal elements are the the three \\(m\\hbar\\) eigenvalues \\(m=1,0,-1\\). Since they are also eigenvectors of \\(J^2\\) we find a diagonal with the \\(1(1+1)\\hbar^2=2\\hbar^2\\) eigenvalues repeated three times:\n\n\n\n\n\n\n\nCommentary\n\n\n\nThe equal sign in Equation 15 and Equation 16 is not accurate, because the right hand side is the representation only on the subspace \\(\\mathcal{E}(l=1)\\), while the left hand side is the “whole operator”. What would be accurate is to use the entire basis and write:\n\\[\nJ_x =\\sum_{l=0}^\\infty\\sum_{-l\\leq m,m'\\leq l}|l,m\\rangle\\langle l,m|J_x|l,m'\\rangle\\langle l,m'|\n\\]\nrather than Equation 15.\nWe could emphasize the distinction by defining a new operator \\(J'_x\\) which is restricted to act only in this subspace:\n\\[\nJ'_x = \\frac{\\hbar}{\\sqrt{2}}\\left(|1,0\\rangle\\langle 1,1|+|1,1\\rangle\\langle 1,0|+|1,-1\\rangle\\langle 1,0|+|1,0\\rangle\\langle 1,-1|\\right)\n\\tag{17}\\]",
    "crumbs": [
      "Brief Notes",
      "Quantum Theory",
      "Angular Momentum"
    ]
  },
  {
    "objectID": "angular_momentum_qm_short_summary.html#the-matrix-representation-of-j2-j_x-j_y-and-j_z-in-mathcalej12-spin-angular-momentum",
    "href": "angular_momentum_qm_short_summary.html#the-matrix-representation-of-j2-j_x-j_y-and-j_z-in-mathcalej12-spin-angular-momentum",
    "title": "Ideas to get started with Angular Momentum in Quantum Mechanics",
    "section": "",
    "text": "The intrinsic spin angular momentum of an electron is described by states living in the \\(\\mathcal{E}(j=1/2)\\) subspace (see Figure 1), the basis for this subspace is (as usual) furnished by the simultaneous eigenvectors of the \\(J^2\\) and \\(J_z\\):\n\\[\n\\begin{align}&J^2\\ket{1/2,m}=\\frac{3}{4}\\hbar^2\\ket{1/2,m}\\\\&J_z\\ket{1/2,m}=m\\hbar\\ket{1/2,m}\\end{align}\n\\tag{18}\\]\nwhere \\(m=\\pm 1/2\\). Therefore we say\n\\[\n\\mathcal{E}(j=1/2)=\\text{span}\n\\{|1/2,1/2\\rangle,|1/2,-1/2\\rangle\\}\n\\]\n\n\n\n\n\n\nNotation\n\n\n\n\ngiven \\(j=1/2\\) is fixed, it is usual to drop it from the ket notation and simply write \\(|m\\rangle\\) where \\(m=\\pm1/2\\).\nwe can rename \\(m\\) as \\(m_s\\) and then reexpress it as \\(m_s=\\sigma/2\\) with \\(\\sigma =\\pm\\) or \\(\\sigma=\\uparrow,\\downarrow\\), but for the present notes we’ll keep the \\(m_s\\) notation.\nThe \\(j=1/2\\) states are very important (they are the states that describe electrons spins for example), so much that we rename the operators \\(\\{J^2,J_z\\}\\) into \\(\\{S^2,S_z\\}\\) . Instead of Equation 18 we should write:\n\\[\n\\begin{align}&S^2\\ket{m_s}=\\frac{3}{4}\\hbar^2\\ket{m_s}\\\\&S_z\\ket{m_s}=m_s\\hbar\\ket{m_s}\\end{align}\n\\]\nto emphasize the act, these operators only act on \\(\\mathcal{E}(j=1/2)\\). Recall we already did something similar when we defined \\(J'_x\\) in Equation 17.\n\n\n\nThe basis states \\(\\{|1/2\\rangle,|-1/2\\rangle\\}\\) that span the subspace \\(\\mathcal{E}(1/2)\\) are connect by the raising \\(S_+\\) and lowering operator \\(S_-\\) introduced in Equation 9:\n\\[\nS_+\\ket{-1/2}=\\hbar\\sqrt{1/2(1/2+1)-1/2(1/2+1)}\\ket{1/2}=\\hbar\\sqrt{2}/2\\ket{1/2}\n\\tag{19}\\]\nThe raising and lowering operators are important for another reason, they tell us how \\(S_x\\) and \\(S_y\\) act on the eigenbasis of \\(\\{S^2,S_z\\}\\), from their definition we have:\n\\[\nS_x=\\frac{1}{2}(J_++J_-) \\qquad S_y=\\frac{1}{2i}(J_+-J_-)\n\\tag{20}\\]\n\n\n\n\n\n\nCommentary\n\n\n\nIn order to know how \\(S_x\\) and \\(S_y\\) act on this basis requires first to reexpress these operators in terms of operator we know how they act in this basis, the formulas Equation 20 together with Equation 19 do precisely that.\n\n\nBy knowing how an operator acts on a basis we know how obtain its matrix representation, for example, let us take a look at \\(S_z\\) and then \\(S_x\\) and \\(S_y\\). We can write the action of \\(S_z\\) on the basis \\(\\{\\ket{1/2},\\ket{-1/2}\\}\\) as:\n\\[\n\\begin{align}&S_z\\ket{1/2}=\\frac{\\hbar}{2}\\ket{1/2}+0\\ket{-1/2}\\\\&S_z\\ket{-1/2}=0\\ket{1/2}+\\frac{\\hbar}{2}\\ket{-1/2}\\end{align}\n\\]\nthis is step zero of the steps found in 7). Following them again, we compute the numbers \\(\\braket{m|S_z|m'}\\), which we organize into the matrix:\n\\[\n[S_z]_{\\mathcal{E}(j=1/2)}=\\frac{\\hbar}{2}\\begin{pmatrix}1 & 0\\\\0 & -1\\end{pmatrix}\n\\]\nTo get the matrices for \\(S_x\\) and \\(S_y\\) in the basis \\(\\{\\ket{-1/2},\\ket{1/2}\\}\\), act with them on the basis. Following the steps zero thought step four, the numbers \\(\\braket{m|S_x|m'}\\) and \\(\\braket{m|S_y|m'}\\) can be arranged into the matrices:\n\\[\n[S_x]_{\\mathcal{E}(j=1/2)}=\\frac{\\hbar}{2}\\begin{pmatrix}0 & 1\\\\1 & 0\\end{pmatrix}\\qquad[S_y]_{\\mathcal{E}(j=1/2)}=\\frac{\\hbar}{2}\\begin{pmatrix}0 & -i\\\\i & 0\\end{pmatrix}\n\\]",
    "crumbs": [
      "Brief Notes",
      "Quantum Theory",
      "Angular Momentum"
    ]
  },
  {
    "objectID": "angular_momentum_qm_short_summary.html#quantum-states-of-two-electrons",
    "href": "angular_momentum_qm_short_summary.html#quantum-states-of-two-electrons",
    "title": "Ideas to get started with Angular Momentum in Quantum Mechanics",
    "section": "",
    "text": "This section uses the example of the helium atom to explain why we focus in the next sections just on the spin part of the states of two electrons. See the Conclusion at the end.\nThe basis states of a system of two electrons such as the helium atom (2 electrons orbiting a nucleus) are of the form:\n\\[\n|n, l, m, m_s; n', l', m', m_s'\\rangle = \\frac{1}{\\sqrt{2}}(1-P_{21})(|1:n, l, m, m_s; 2: n', l',m',m_s'\\rangle)\n\\tag{21}\\]\nwhere \\(P_{21}\\) permutes the \\(1\\), \\(2\\) indices. The \\(n\\) is the principal quantum number, \\(l\\) together with \\(m\\) describes the orbit angular momentum of the first electron, finally \\(m_s\\) together with the suppressed \\(s=1/2\\) determines its spin angular momentum. The primed indices describe the second electron.\nSince both electron have semi-integer spin \\(s=s'=1/2\\) the quantum states \\(|n, l, m, m_s; n', l', m', m_s'\\rangle\\) must be antisymmetric for the exchange of particles.\nThe operator \\(1-P_{21}\\) acting on the tensor product ket \\(|1:n, l, m, m_s; 2: n', l',m',m_s'\\rangle\\) makes Equation 21 antisymmetric.\n\n\n\n\n\n\nRecall\n\n\n\nBy definition \\(|1:\\phi;2:\\psi\\rangle := |1:\\phi\\rangle \\otimes |2:\\psi\\rangle\\). (example: \\(\\phi=n,l,m,m_s\\) and \\(\\psi=n',l',m',m_s'\\))\nThe state \\((1+P_{21})|1:\\phi;2:\\psi\\rangle\\) is equal to \\(|1:\\phi;2:\\psi\\rangle + |1:\\psi;2:\\phi\\rangle\\), which is symmetric because exchange of \\(1\\rightarrow 2\\) leaves the ket unchanged:\n\\[\nP_{21} (|1:\\phi;2:\\psi\\rangle + |1:\\psi;2:\\phi\\rangle) = |1:\\phi;2:\\psi\\rangle + |1:\\psi;2:\\phi\\rangle\n\\]\nOn the other hand \\((1-P_{21})|1:\\phi;2:\\psi\\rangle\\) is equal to \\(|1:\\phi;2:\\psi\\rangle - |1:\\psi;2:\\phi\\rangle\\) which is antissymetric because a minus sign emerges from the permutation of indices:\n\\[\nP_{21} (|1:\\phi;2:\\psi\\rangle - |1:\\psi;2:\\phi\\rangle) = -(|1:\\phi;2:\\psi\\rangle - |1:\\psi;2:\\phi\\rangle)\n\\]\nIf the two particles where in the same state \\(\\phi=\\psi\\) then:\n\\[\n\\begin{align}\n&(1+P_{21})|1:\\phi;2:\\phi\\rangle = 2 |1:\\phi;2:\\phi\\rangle\\\\\n&(1-P_{21})|1:\\phi;2:\\phi\\rangle = 0\n\\end{align}\n\\]\nHence, the operator \\(1-P_{21}\\) prevents the two states to be equal, by reducing them to zero.\nThe operator \\(1\\pm P_{21}\\) is the mathematical agent that ensures the Pauli Exclusion Principle is obeyed. The \\(+\\) is applyed for boson i.e. for integer \\(j\\) particles; the \\(-\\) is applied for fermions, i.e., for particles with semi-integer \\(j\\).\n\n\nSince the spins are semi-integers \\(s=s'=1/2\\), by Pauli Exclusion Principle states that: two electrons in the same quantum, i.e., the primed and unprimed indices are equal \\(n=n'\\), \\(l=l'\\), \\(m=m'\\) and \\(m_s=m_s'\\), these states are not physical, meaning their wave function is \\(0\\). By writing Equation 21 where we find the operator \\(1-P_{21}\\), we render this ket to \\(0\\) if those indices are equal; and non-zero otherwise.\nA non-zero state is for example the case \\(n=n'=1\\), \\(l=l'=m=m'=0\\) and \\(m_s=\\uparrow\\) and \\(m_s'=\\downarrow\\):\n\\[\n\\begin{align}\n|1,0,0,\\uparrow;1,0,0,\\downarrow\\rangle &=\\frac{1}{\\sqrt{2}}(1-P_{21})(|1:1, 0, 0, \\uparrow; 2: 1, 0,0,\\downarrow\\rangle)\\\\\n&= \\frac{1}{\\sqrt{2}}(|1:1, 0, 0, \\uparrow; 2: 1, 0,0,\\downarrow\\rangle-|1:1, 0, 0, \\downarrow; 2: 1, 0,0,\\uparrow\\rangle)\\\\\n&=\\frac{1}{\\sqrt{2}}(\\underbrace{|1:1, 0, 0; 2: 1,0,0\\rangle}_\\text{spacial part}\\otimes(\\underbrace{|1:\\uparrow;2:\\downarrow\\rangle-|1:\\downarrow;2:\\uparrow\\rangle}_\\text{spin part})\n\\end{align}\n\\tag{22}\\]\nObserving the right hand side of Equation 22 we notice it has two parts:\n\nthe spacial part is symmetric for the exchange of indices, just apply \\(P_{21}\\) and we get a plus sign on the right hand side:\n\n\\[\nP_{21}|1:1, 0, 0; 2: 1,0,0\\rangle = |1:1, 0, 0; 2: 1,0,0\\rangle\n\\]\n\nthe spin part is antisymmetric, notice the minus sign:\n\n\\[\nP_{21} (|1:\\uparrow;2:\\downarrow\\rangle-|1:\\downarrow;2:\\uparrow\\rangle) = - (|1:\\uparrow;2:\\downarrow\\rangle-|1:\\downarrow;2:\\uparrow\\rangle)\n\\]\nAs a result the whole state Equation 22 acquires a minus sign when we switch \\(1\\) with \\(2\\) and it is considered antisymmetric:\n\\[\nP_{21} |1,0,0,1/2;1,0,0,-1/2\\rangle = - |1,0,0,1/2;1,0,0,-1/2\\rangle\n\\]\nConclusion: This simple example of the helium atom showed that the basis of the helium electrons are antissymetric kets of the form \\(\\text{space}\\otimes\\text{spin}\\). This example also showed us how PEP for fermions is incorporated mathematically into the ket states through \\(1-P_{12}\\), which makes the whole ket antissymetric. The whole ket includes the spacial and spin parts.\nI emphasize that neither the spacial or spin part have to necessarily be antissymetric, what is needed is that the whole state be antissymetric, and it must be so, because we are dealing with two semi-integer spin particles and PEP says so.\nFinally, since the basis of the helium atom we can split the spatial and spin parts, we will now devote the next sections focusing just on the spin part.",
    "crumbs": [
      "Brief Notes",
      "Quantum Theory",
      "Angular Momentum"
    ]
  },
  {
    "objectID": "angular_momentum_qm_short_summary.html#spin-quantum-states-of-two-electrons",
    "href": "angular_momentum_qm_short_summary.html#spin-quantum-states-of-two-electrons",
    "title": "Ideas to get started with Angular Momentum in Quantum Mechanics",
    "section": "",
    "text": "The basis states of the electron one is furnished by \\(\\{S_1^2,S_{1z}\\}\\) and for the electron two by \\(\\{S_2^2,S_{2z}\\}\\) in the usual manner:\n\\[\n\\begin{align}&S_1^2\\ket{m_1}=\\frac{3}{4}\\hbar^2\\ket{m_1}\\\\&S_{1z}\\ket{m_1}=m_1\\hbar\\ket{m_1}\\\\&S_2^2\\ket{m_2}=\\frac{3}{4}\\hbar^2\\ket{m_2}\\\\&S_{2z}\\ket{m_2}=m_2\\hbar\\ket{m_2}\\end{align}\n\\]\nThe spins magnitude of the two electrons is described by the quantum numbers\n\\[\ns_1=\\frac{1}{2}\\qquad s_2=\\frac{1}{2}\n\\]\nThe basis states of two electrons is formed from the tensor product \\(\\ket{1:m_1}\\otimes \\ket{2:m_2}\\), there are four basis elements:\n\\[\n\\{\\ket{1:\\uparrow}\\otimes \\ket{2:\\uparrow},\\ket{1:\\uparrow}\\otimes \\ket{2:\\downarrow},\\ket{1:\\downarrow}\\otimes \\ket{2:\\uparrow},\\ket{1:\\downarrow}\\otimes \\ket{2:\\downarrow}\\}\n\\tag{23}\\]\nIt is important to understand that:\n\nThe observables \\(S^2_1\\) ,\\(S_{1z}\\) act on the space spanned by \\(\\{\\ket{1:-1/2},\\ket{1:1/2}\\}\\) while the observables \\(S^2_2\\), \\(S_{2z}\\) act on \\(\\{\\ket{2:-1/2},\\ket{2:1/2}\\}\\) and do not act on tensor product states.\nWe define however the extensions of \\(S^2_1\\), \\(S_{1z}\\), which act on the tensor product space as\n\n\\[\n\\tilde{S}_1^2=S_1^2\\otimes 1_2\\qquad \\tilde{S}_{1z}=S_{1z}\\otimes 1_2\n\\]\nThe extensions of \\(S^2_2\\) ,\\(S_{2z}\\) are defined analogously:\n\\[\n\\tilde{S}_2^2=1_1 \\otimes S_2^2\\qquad \\tilde{S}_{2z}=1_1\\otimes S_{2z}\n\\]\nTherefore we have for example:\n\\[\n\\begin{align}\\tilde{S}_1^2(\\ket{m_1}\\otimes \\ket{m_2}) &= (S_1^2\\ket{m_1})\\otimes \\ket{m_2}\\\\&=(\\frac{3\\hbar}{4}\\ket{m_1})\\otimes \\ket{m_2}\\\\&=\\frac{3\\hbar}{4}\\ket{m_1}\\otimes \\ket{m_2}\\end{align}\n\\tag{24}\\]\n\n\n\n\n\n\nNotation\n\n\n\nNote how in the ket Equation 24 we suppressed the \\(j_1=j_2=1/2\\) , the \\(1:\\) and \\(2:\\) and start using \\(m\\) in place of the old \\(m_s\\). Moreover we should go one step further and define\n\\[\n|m_1,m_2\\rangle :=|1:m_1\\rangle \\otimes |2:m_2\\rangle\n\\]\nAnd by the way, drop the ~ in \\(\\tilde{S}_{1,2}^2\\) and \\(\\tilde{S}_{z1,2}\\).\nThis simplified notation comes at the price of leaving to the reader the work of understanding its meaning from the context.\n\n\nAlso, the set of operators\n\\[\n\\{\\tilde{S}_1^2,\\tilde{S}_{1z},\\tilde{S}_2^2,\\tilde{S}_{2z}\\}\n\\tag{25}\\]\ncommute and as a result we know it exists a basis of common eigenstates exists, name this basis as Equation 23. It follows:\n\\[\n\\begin{align}&\\tilde{S}_1^2\\ket{m_1,m_2}=\\tilde{S}_2^2\\ket{m_1,m_2}=\\frac{3}{4}\\hbar\\ket{m_1,m_2}\\\\&\\tilde{S}_{1z}\\ket{m_1,m_2}=m_1 \\hbar\\ket{m_1,m_2}\\\\&\\tilde{S}_{2z}\\ket{m_1,m_2}=m_2 \\hbar\\ket{m_1,m_2}\\\\\\end{align}\n\\tag{26}\\]\nIn conclusion, the spin properties of a system with two particles (electrons) whose spin state is described by kets in \\(\\mathcal{E}_1(j_1=1/2)\\) and \\(\\mathcal{E}_2(j_2=1/2)\\) is described by a state of the form\n\\[\n\\ket{m_1,m_2}:=\\ket{m_1}\\otimes\\ket{m_2}\\qquad m_{1,2}=\\pm1/2\n\\tag{27}\\]\nwhich constitute a basis for the tensor product state space\n\\[\n\\mathcal{E}_{12}=\\mathcal{E}_1\\otimes\\mathcal{E}_2\n\\]\nA pictorial organization of our basis states if given as follows:\n\n\n\nThe blue dots represent a basis ket, the pink boxes we’ll be explained in next section.",
    "crumbs": [
      "Brief Notes",
      "Quantum Theory",
      "Angular Momentum"
    ]
  },
  {
    "objectID": "angular_momentum_qm_short_summary.html#a-new-basis-for-the-tensor-state-space",
    "href": "angular_momentum_qm_short_summary.html#a-new-basis-for-the-tensor-state-space",
    "title": "Ideas to get started with Angular Momentum in Quantum Mechanics",
    "section": "",
    "text": "The old basis Equation 27 help to define the states space \\(\\mathcal{E}_{12}(j_1=1/2,j_2=1/2)\\) , we want now to introduce a new basis, in particular we want one whose kets are simultaneous eigenvalues of the following set of observables: \\[\n\\{S_1^2,S_2^2,S^2,S_z\\}\n\\tag{28}\\] where:\n\n\\(\\mathbf{S}=\\mathbf{S}_1+\\mathbf{S}_2=(S_{1x}+S_{2x},S_{1y}+S_{2y},S_{1z}+S_{2z})\\) is the total spin operator.\n\\(S^2=S_x^2+S_y^2+S_z^2\\) is the magnitude squared operator.\n\\(S_z=S_{1z}+S_{2z}\\) is the \\(z\\)-component of \\(\\mathbf{S}\\).\n\nWhy this set of operators? The Hamiltonian we got in our hands (not shown in this notes) might commute with Equation 28 observables rather Equation 25 . Our goal is still to find the eigenstates of \\(H\\) and that is facilitated when we know operators that commute with it.\nSince \\(\\{S_1^2,S_2^2,S^2,S_z\\}\\) is a set of commuting observables, we can define a basis of kets which are eigenvector of all these observables; the basis is labelled by the eigenvalues (or better yet, by the indices that label the eigenvalues): \\[\n\\begin{align}\n&S_1^2\\ket{j_1,j_2;S,M}=S^2_2\\ket{j_1,j_2;S,M}=\\frac{3}{4} \\hbar^2\\ket{S,M}\\\\\n&S^2\\ket{j_1,j_2;S,M}=S(S+1)\\hbar^2\\ket{j_1,j_2;S,M}\\\\\n&S_z\\ket{j_1,j_2;S,M}=M\\hbar\\ket{j_1,j_2;S,M}\n\\end{align}\n\\]where \\(j_1=j_2=1/2\\) (we can drop them from the kets, since they are fixed and just use \\(|S,M\\rangle\\)). Notice the operators \\(S^2\\) and \\(S_z\\) are in fact angular momentum operators because they result from adding angular momentum, thus they are just another particular case of \\(J^2\\) and \\(J_z\\) that satisfy the commutation relation Equation 4, as a result their eigenvalues are of the form \\(S(S+1)\\hbar^2\\) and \\(M\\hbar\\) with \\(-S\\leq M\\leq S\\) as we have seen in Equation 7.",
    "crumbs": [
      "Brief Notes",
      "Quantum Theory",
      "Angular Momentum"
    ]
  },
  {
    "objectID": "angular_momentum_qm_short_summary.html#what-are-the-values-of-quantum-numbers-s-and-m",
    "href": "angular_momentum_qm_short_summary.html#what-are-the-values-of-quantum-numbers-s-and-m",
    "title": "Ideas to get started with Angular Momentum in Quantum Mechanics",
    "section": "",
    "text": "Given \\(j_1=j_2=1/2\\), we still do not know what values \\(S\\) and \\(M\\) take. But two things we know:\n\nFrom the theory of general angular momentum given in 4), we find \\(-S\\leq M\\leq S\\).\nKnowing that \\(S_z=S_{1z} + S_{2z}\\) tell us how to act with \\(S_z\\) on the old basis \\(\\{\\ket{1/2,1/2},\\ket{-1/2,1/2},\\ket{1/2,-1/2},\\ket{-1/2,-1/2}\\}\\), that suffices to show what values \\(M\\) take. For that we act with \\(S_z\\) on each ket of this basis. \\[\nS_z\\ket{m_1,m_2}=(S_{1z}+S_{2z})\\ket{m_1,m_2}=(m_1+m_2)\\hbar \\ket{m_1,m_2}\n\\tag{29}\\]\n\nFrom this we conclude that \\[\nM=m_1+m_2\n\\] Hence, the values \\(M\\) can take are: \\[\nM\\in\\{-1,0,+1\\}\n\\tag{30}\\]\nLets use this information: what values \\(S\\) have to take that guarantee that \\(M\\) takes the values Equation 30 ? Notice the question: If I know \\(S\\), we can easily get the range of \\(M\\) because we know that \\(-S\\leq M\\leq S\\). The present question goes the other way, if I know the \\(M\\)’s (and we know see Equation 30), what \\(S\\)’s are possible? We can answer this question by constructing a diagram consistent with the range of \\(M\\)’s, for example for the range Equation 30 the corresponding \\(S\\)’s are:\n\nWe can clearly see that \\(S=0,1\\) are all possible values consistent with Equation 30.\nRather than constructing this picture we can use the rule: \\[\n|j_1-j_2|\\leq S\\leq j_1+j_2\n\\]However the diagram not only showed us what are the values of \\(S\\), it also organizes pictorially the eigenbasis \\(\\{\\ket{S,M}\\}\\) generated by Equation 28, which in turn reveals two subspaces (pink boxes) where the raising and lowering operators \\(S_\\pm\\) are stuck.",
    "crumbs": [
      "Brief Notes",
      "Quantum Theory",
      "Angular Momentum"
    ]
  },
  {
    "objectID": "angular_momentum_qm_short_summary.html#relating-the-new-basis-with-the-old-basis",
    "href": "angular_momentum_qm_short_summary.html#relating-the-new-basis-with-the-old-basis",
    "title": "Ideas to get started with Angular Momentum in Quantum Mechanics",
    "section": "",
    "text": "We want to express the kets \\(\\ket{S,M}\\) of the new basis in terms of \\(\\ket{m_1,m_2}\\) that constitute the old basis, to do that we argue pictorially:\n\nOn the left side of the picture we see a graphical organization of the old basis and on the right side the new basis. We identify in each basis the elements that span the subspaces \\(\\mathcal{E}(M)\\) for each \\(M\\); this is easily done on the right picture because the horizontal axis already tells us the \\(M\\)’s, on the left picture these subspaces are diagonal because each basis ket is an eigenvector of \\(S_z\\) with eigenvalue \\(M\\hbar=(m_1+m_2)\\hbar\\).\nFrom the pictures we know: \\[\n\\begin{align}\n&\\ket{S=1,M=1}\\in \\mathcal{E}(M=1)\\\\\n&\\ket{m_1=1/2,m_2=1/2}\\in \\mathcal{E}(M=1)\n\\end{align}\n\\] and since this subspace is one-dimensional, these two basis kets must coincide. We write: \\[\n\\ket{1,1}=\\ket{1/2,1/2}\n\\tag{31}\\]\nThis is a key step, since we just succeeded in establishing a first connection between both basis. The remaining connections follow from this one, as falling dominos, using the lower operator \\(S_-:=S_{1-}+S_{2-}\\) .\n\nActing with the lower operator on both sides of Equation 31 we find the next connection between these basis kets:\n\\[\nS_-\\ket{1,1}=(S_{1-}+S_{2-})\\ket{1/2,1/2}\n\\]\nThe left hand side gives us: \\[\n\\hbar\\sqrt{1(1+1)-0(0-1)}\\ket{1,0}\n\\] The right hand side is: \\[\n\\hbar\\sqrt{\\frac{1}{2}\\left(\\frac{1}{2}+1\\right)-\\frac{1}{2}\\left(\\frac{1}{2}-1\\right)}\\left(\\ket{-\\frac{1}{2},\\frac{1}{2}}+\\ket{\\frac{1}{2},-\\frac{1}{2}}\\right)\n\\] Thus: \\[\n\\hbar\\sqrt{2}\\ket{1,0}=\\hbar\\left(\\ket{-\\frac{1}{2},\\frac{1}{2}}+\\ket{\\frac{1}{2},-\\frac{1}{2}}\\right)\n\\] Which yields the second connection between basis kets: \\[\n\\ket{1,0}=\\frac{1}{\\sqrt{2}}\\left(\\ket{-\\frac{1}{2},\\frac{1}{2}}+\\ket{\\frac{1}{2},-\\frac{1}{2}}\\right)\n\\] The third connection relates \\(|0,0\\rangle\\) with the old basis, to establish it, we use the properties of vector spaces:\n\nWe know \\(|0,0\\rangle\\) is a linear combination of \\(\\ket{-1/2,1/2}\\) and \\(\\ket{1/2,-1/2}\\) thus we write: \\[\n\\ket{0,0}=c_1\\ket{-1/2,1/2}+c_2\\ket{1/2,-1/2}\n\\] for some complex numbers \\(c_{1,2}\\).\nWe know \\(|0,0\\rangle\\) is orthogonal to \\(\\ket{1,0}\\), thus we write: \\[\n0=\\braket{1,0|0,0}=\\frac{1}{\\sqrt{2}}(c_1+c_2)\n\\]\nWe also know it is normalized, i.e., \\[\n1=\\braket{0,0|0,0}=|c_1|^2+|c_2|^2\n\\]\n\nSolving these three equations we conclude one possible solution is: \\[\nc_1=-c_2=\\frac{1}{\\sqrt{2}}\n\\] Hence, the second basis vector in \\(\\mathcal{E}(M=0)\\) is: \\[\n\\ket{0,0}=\\frac{1}{\\sqrt{2}}\\left(\\ket{-\\frac{1}{2},\\frac{1}{2}}-\\ket{\\frac{1}{2},-\\frac{1}{2}}\\right)\n\\] Finally, to get the last connection between the new and old basis, we act with either \\(S_-\\) on \\(\\ket{1,0}\\):\n\nOr we simply notice that \\(\\mathcal{E}(M=-1)\\) is also one dimensional and therefore: \\[\n\\ket{1,-1}=\\ket{-1/2,-1/2}\n\\]",
    "crumbs": [
      "Brief Notes",
      "Quantum Theory",
      "Angular Momentum"
    ]
  },
  {
    "objectID": "angular_momentum_qm_short_summary.html#summary-of-the-two-electron-case",
    "href": "angular_momentum_qm_short_summary.html#summary-of-the-two-electron-case",
    "title": "Ideas to get started with Angular Momentum in Quantum Mechanics",
    "section": "",
    "text": "The connection between the basis \\(\\{\\ket{S,M}\\}\\) of eigenstates of the set \\(\\{S_1^2,S_2^2;S^2,S_z\\}\\) and the basis \\(\\{\\ket{m_1,m_2}\\}\\) of eigenstates of the set \\(\\{S_1^2,S_{1z},S^2_2,S_{2z}\\}\\) given \\(S_1=S_2=1/2\\) is: \\[\n\\ket{0,0}=\\frac{1}{\\sqrt{2}}\\left(\\ket{-\\frac{1}{2},\\frac{1}{2}}-\\ket{\\frac{1}{2},-\\frac{1}{2}}\\right)\n\\] which spans the one dimension subspace \\(\\mathcal{E}(S=0)\\) and is thus called a singlet state; the three states of \\(\\mathcal{E}(S=1)\\) are called a triplet \\[\n\\begin{align}\n&\\ket{1,1}=\\ket{1/2,1/2}\\\\\n&\\ket{1,0}=\\frac{1}{\\sqrt{2}}\\left(\\ket{-\\frac{1}{2},\\frac{1}{2}}+\\ket{\\frac{1}{2},-\\frac{1}{2}}\\right)\\\\\n&\\ket{1,-1}=\\ket{-1/2,-1/2}\n\\end{align}\n\\]",
    "crumbs": [
      "Brief Notes",
      "Quantum Theory",
      "Angular Momentum"
    ]
  },
  {
    "objectID": "angular_momentum_qm_short_summary.html#the-matrix-representation-of-j2-j_x-j_y-and-j_z-in-mathcalej-orbital-angular-momentum",
    "href": "angular_momentum_qm_short_summary.html#the-matrix-representation-of-j2-j_x-j_y-and-j_z-in-mathcalej-orbital-angular-momentum",
    "title": "Ideas to get started with Angular Momentum in Quantum Mechanics",
    "section": "",
    "text": "It is traditional to use the symbols \\(L_x\\), \\(L_y\\) ,\\(L_z\\) and \\(L^2\\) in place of \\(J_x\\), \\(J_y\\) ,\\(J_z\\) and \\(J^2\\) when speaking about the orbital angular momentum. Orbital angular momentum states live in \\(\\mathcal{E}(j)\\) subspaces with integer \\(j\\) in contrast with spin angular momentum which live in subspaces with semi-integers. See Figure 1.\nFrom Equation 7 we have:\n\\[\n\\begin{align}&L^2\\ket{l,m_l}=l(l+1)\\hbar^2\\ket{l,m},\\\\&L_z\\ket{l,m}=m\\hbar\\ket{l,m}.\\end{align}\n\\tag{32}\\]\nwith \\(l=0,1,2,...\\) and \\(-l\\leq m \\leq l\\).\nThe formulas Equation 32 together with the choice of the position basis \\(\\{|x,y,z\\rangle\\}\\) allow us to give their position representation:\n\\[ \\begin{align} &\\langle x,y,z|L^2\\ket{l,m}=l(l+1)\\hbar^2\\langle x,y,z|l,m\\rangle\\\\ &\\langle x,y,z|L_z\\ket{l,m}=m\\hbar\\langle x,y,z|l,m\\rangle \\end{align} \\]\nSubstituting Equation 2 we find:\n\\[ \\begin{align}&\\langle x,y,z|(L_x^2+L_y^2+L_z^2)\\ket{l,m}=l(l+1)\\hbar^2\\langle x,y,z|l,m\\rangle\\\\ &\\langle x,y,z|(XP_y-YP_x)\\ket{l,m}=m\\hbar\\langle x,y,z|l,m\\rangle\\end{align}  \\tag{33}\\]\n\n\n\n\n\n\nRecall\n\n\n\nThe \\(X\\) and \\(P_x\\) operators have the following representation in the \\(\\{|x\\rangle\\}\\) basis:\n\\[ \\langle x|X|\\psi\\rangle = x\\langle x|\\psi\\rangle \\qquad \\langle x|P_x|\\psi\\rangle = \\frac{\\hbar}{i}\\frac{\\partial}{\\partial x}\\langle x|\\psi\\rangle \\]\n\n\nFrom the second equation in Equation 33 we find:\n\\[ \\begin{align} \\langle x,y,z|L_z|\\psi\\rangle &= \\langle x,y,z|(XP_y-YP_x)|\\psi\\rangle\\\\ &= \\langle x,y,z|(xP_y-yP_x)|\\psi\\rangle\\\\ &= \\langle x,y,z|(x\\frac{\\hbar}{i}\\frac{\\partial}{\\partial y}-y\\frac{\\hbar}{i}\\frac{\\partial}{\\partial x})|\\psi\\rangle\\\\ &= \\frac{\\hbar}{i}\\left(x\\frac{\\partial}{\\partial y}-y\\frac{\\partial}{\\partial x}\\right)\\langle x,y,z|\\psi\\rangle\\\\ &= \\frac{\\hbar}{i}\\left(x\\frac{\\partial}{\\partial y}-y\\frac{\\partial}{\\partial x}\\right)\\psi(x,y,z) \\end{align} \\] Therefore, in the position basis, the \\(L_z\\) operator acts on functions \\(\\psi(x,y,z)\\) as the operation:\n\\[ L_z \\rightsquigarrow \\frac{\\hbar}{i}\\left(x\\frac{\\partial}{\\partial y}-y\\frac{\\partial}{\\partial x}\\right)  \\tag{34}\\]\nSimilarly we find:\n\\[ L_x \\rightsquigarrow \\frac{\\hbar}{i}\\left(y\\frac{\\partial}{\\partial z}-z\\frac{\\partial}{\\partial y}\\right) \\qquad L_y \\rightsquigarrow \\frac{\\hbar}{i}\\left(z\\frac{\\partial}{\\partial x}-x\\frac{\\partial}{\\partial z}\\right)  \\tag{35}\\]\nIt will be useful to change from cartesian coordinates \\(x,y,z\\) into polar coordinates \\(r,\\theta,\\phi\\). The map between these coordinates is established by the equations:\n\\[ \\begin{align} & x = r \\sin \\theta \\cos \\phi\\\\ & y = r \\sin \\theta \\sin \\phi\\\\ & z = r \\cos \\theta \\end{align} \\]\nprovided \\(r\\geq 0\\), \\(0\\leq\\theta\\leq\\pi\\) and \\(0\\leq\\phi&lt; 2\\pi\\).\nTo convert Equation 34 and Equation 35 into polar coordinates we proceed as follows:\n\nDraw a diagram:\n\n\n\nRelate the derivatives in cartesian and polar coordinates encoded in the diagram:\n\n\\[ \\begin{pmatrix} \\partial_r\\psi\\\\ \\partial_\\theta\\psi\\\\ \\partial_\\phi\\psi \\end{pmatrix} = \\begin{pmatrix} \\partial_r x & \\partial_r y & \\partial_r z\\\\ \\partial_\\theta x & \\partial_\\theta y & \\partial_\\theta z\\\\ \\partial_\\phi x & \\partial_\\phi y & \\partial_\\phi z \\end{pmatrix} \\begin{pmatrix} \\partial_x\\psi\\\\ \\partial_y \\psi\\\\ \\partial_z \\psi \\end{pmatrix} \\]\n\nCompute the matrix (Easy)\n\n\\[ \\begin{pmatrix} \\sin\\theta \\cos\\phi & \\sin\\theta \\sin \\phi & \\cos\\theta\\\\ r \\cos\\theta \\cos\\phi & r \\cos\\theta \\sin \\phi & -r \\sin \\theta\\\\ -r\\sin\\theta \\sin \\phi & r \\sin\\theta \\cos\\phi & 0 \\end{pmatrix}  \\tag{36}\\]\n\nInvert the system Equation 36 (Hard, use Mathematica)\n\n\\[ \\begin{pmatrix} \\partial_x\\psi\\\\ \\partial_y\\psi\\\\ \\partial_z\\psi \\end{pmatrix} = \\begin{pmatrix} \\sin\\theta \\cos \\phi   & r^{-1}\\cos\\theta \\cos\\phi & -(r\\sin\\theta)^{-1} \\sin\\phi\\\\ \\sin\\theta \\sin \\phi   & r^{-1}\\cos\\theta \\sin\\phi & (r\\sin\\theta)^{-1} \\cos\\phi\\\\ \\cos\\theta & -r^{-1}\\sin\\theta & 0 \\end{pmatrix} \\begin{pmatrix} \\partial_r\\psi\\\\ \\partial_\\theta \\psi\\\\ \\partial_\\phi \\psi \\end{pmatrix} \\]\n\nSubstitute the results into Equation 34 and Equation 35 and simplify:\n\\[ \\begin{align} &L_x \\rightsquigarrow i\\hbar\\left(\\sin\\phi \\frac{\\partial}{\\partial \\theta}+\\frac{\\cos\\phi}{\\tan\\theta} \\frac{\\partial}{\\partial \\phi}\\right)\\\\ &L_y \\rightsquigarrow i\\hbar\\left(-\\cos\\phi \\frac{\\partial}{\\partial \\theta}+\\frac{\\sin\\phi}{\\tan\\theta} \\frac{\\partial}{\\partial \\phi}\\right)\\\\ &Lz \\rightsquigarrow \\frac{\\hbar}{i} \\frac{\\partial}{\\partial \\phi} \\end{align} \\]\nFrom these results we can now derive the action of \\(L^2\\) in polar coordinates:\n\\[ \\begin{align} L^2\\psi &= L_x^2\\psi + L_y^2\\psi+L_z^2\\psi\\\\ &=(i\\hbar)^2\\left(\\sin\\phi \\frac{\\partial}{\\partial \\theta}+\\frac{\\cos\\phi}{\\tan\\theta} \\frac{\\partial}{\\partial \\phi}\\right)^2\\psi +(i\\hbar)^2\\left(-\\cos\\phi \\frac{\\partial}{\\partial \\theta}+\\frac{\\sin\\phi}{\\tan\\theta} \\frac{\\partial}{\\partial \\phi}\\right)^2\\psi+ \\left(\\frac{\\hbar}{i}\\right)^2 \\frac{\\partial^2}{\\partial \\phi^2}\\psi \\end{align}  \\tag{37}\\] Now we need to expand each square, the first becomes:\n\n\\[ (i\\hbar)^2\\left(\\sin^2\\phi \\frac{\\partial^2\\psi}{\\partial \\theta^2}+ \\frac{\\cos^2\\phi}{\\tan^2\\theta}\\frac{\\partial^2\\psi}{\\partial \\phi^2}-\\frac{\\sin\\phi\\cos\\phi}{\\sin^2\\theta}\\frac{\\partial \\psi}{\\partial \\phi}+\\frac{\\sin\\phi\\cos\\phi}{\\tan\\theta}\\frac{\\partial^2 \\psi}{\\partial\\theta\\partial \\phi}\\right) \\]\nthe second term becomes:\n\\[ (i\\hbar)^2\\left(\\cos^2\\phi \\frac{\\partial^2\\psi}{\\partial \\theta^2}+ \\frac{\\sin^2\\phi}{\\tan^2\\theta}\\frac{\\partial^2\\psi}{\\partial \\phi^2}+\\frac{\\cos\\phi\\sin\\phi}{\\sin^2\\theta}\\frac{\\partial \\psi}{\\partial \\phi}-\\frac{\\cos\\phi\\sin\\phi}{\\tan\\theta}\\frac{\\partial^2 \\psi}{\\partial\\theta\\partial \\phi}\\right) \\]\nSubstituting into Equation 37 and canceling terms yields the final result:\n\\[ L^2 \\rightsquigarrow -\\hbar^2\\left(\\frac{\\partial^2}{\\partial \\theta^2} + \\frac{1}{\\sin^2\\theta} \\frac{\\partial^2}{\\partial \\phi^2} + \\frac{1}{\\tan\\theta}\\frac{\\partial}{\\partial \\theta}\\right) \\]\nAs a consequence, the system of equations in Equation 33 becomes a system of partial differential equations:\n\\[ \\begin{align} &-\\hbar^2 \\left(\\frac{\\partial}{\\partial \\theta^2}+\\frac{1}{\\tan \\theta} \\frac{\\partial}{\\partial \\theta}+\\frac{1}{\\sin^2\\theta}\\frac{\\partial}{\\partial^2 \\phi^2}\\right)\\psi_{l,m}(r, \\theta, \\phi)=l(l+1)\\hbar^2\\psi_{l,m}(r, \\theta, \\phi)\\\\ & \\frac{\\hbar}{i} \\frac{\\partial}{\\partial \\phi}\\psi_{l,m}(r, \\theta, \\phi)=m\\hbar \\psi_{l,m}(r, \\theta, \\phi) \\end{align} \\]\nThe solutions \\(\\psi_{l,m}(r,\\theta,\\phi)\\) are the eigenfunctions common to the \\(L^2\\) and \\(L_z\\) operators associated with the eigenvalues \\(l(l+1)\\hbar^2\\) and \\(m\\hbar\\). These solution exist because these two operators commute. Our ultimate goal is to solve them, but in these notes we’ll content ourselves by just identifying some of its properties.",
    "crumbs": [
      "Brief Notes",
      "Quantum Theory",
      "Angular Momentum"
    ]
  },
  {
    "objectID": "angular_momentum_qm_short_summary.html#adding-3-angular-momenta",
    "href": "angular_momentum_qm_short_summary.html#adding-3-angular-momenta",
    "title": "Ideas to get started with Angular Momentum in Quantum Mechanics",
    "section": "",
    "text": "Suppose our system of two electrons is described by states in \\(\\mathcal{E}(S=1)\\) (see triplet states in 13)) and we want to add to it another electron, which obviously is described by states in \\(\\mathcal{E}(j_3=1/2)\\). How are we going to describe the quantum states of the three particles?\nIn 10)-14) we had one electron living in \\(\\mathcal{E}(j_1=1/2)\\) and added to it another electron, which was, as usual, in some state in \\(\\mathcal{E}(j_2=1/2)\\). Back then:\n\nwe constructed a tensor product basis for \\(\\mathcal{E}(j_1=1/2)\\otimes \\mathcal{E}(j_2=1/2)\\) to describe the states of the two electrons\nintroduced a new and more convenient basis\nwe connected both basis\n\nOur goal in this section is analogous, we will start to construct a basis for the tensor product space \\(\\mathcal{E}(S=1)\\otimes \\mathcal{E}(j_3=1/2)\\), then introduce a more convenient basis and finally connect the new with the old.\n\n\nWe note that the CSCO that is about to generate a basis for \\(\\mathcal{E}(S_{12}=1)\\otimes \\mathcal{E}(j_3=1/2)\\) is given by \\[\n\\{\\overbrace{S_1^2,S_2^2}^\\text{background};\\overbrace{S_{12}^2,S_{12}^z}^\\text{1st spin},\\overbrace{S_3^2,S_3^z}^\\text{2ndspin}\\}\n\\] We can split this set into three parts:\n\n1st spin angular momentum: the main operators that we need to specify the state of the two first electrons in the system are \\(S_{12}^2\\) and \\(S_{12}^z\\).\nbackground: how do we know we added two electron? To answer that we need two extra operators, \\(S_1^2\\) and \\(S_2^2\\), to provide background information on what spins where added. By including these in the CSCO, the eigenkets will include \\(s_1\\) and \\(s_2\\) which when set as equal to \\(1/2\\) automatically says, these are two electron spin we did add.\n2nd spin angular momentum: The main operators for the third electron are \\(S_3^2\\) and \\(S_3^z\\).\n\nThese six operators commute among each other and as a result we can define a basis of eigenvectors common to all the observables, which are indexed by the corresponding quantum numbers:\n\\[\n\\ket{s_1,s_2;S_{12},M_{12},s_3,m_3}\n\\] What do we know about these states?\n\n\\(s_1=s_2=1/2\\) because the 1st spin result from adding two electron spins.\n\\(0=|s_1-s_2|\\leq S_{12}\\leq s_1+s_2=1\\) in steps of \\(1\\)\n\\(-1\\leq M_{12}\\leq 1\\)\n\\(s_3=1/2\\)\n\\(-1/2\\leq m_3\\leq 1/2\\)\n\\(\\ket{s_1,s_2;S_{12},M_{12},s_3,m_3}:=\\ket{s_1,s_2;S_{12},M_{12}}\\otimes\\ket{s_3,m_3}\\)\nMoreover we know the ket \\(\\ket{s_1,s_2;S_{12},M_{12}}\\) can be written in terms of the individual states of electron \\(1\\) and \\(2\\) either as a singlet or triplet state, see 14). Since we opted for \\(S_{12}=1\\) this state be one of the triplet states.\n\nGiven \\(S_{12}=1\\) and \\(s_3=1/2\\) we can organize this basis (which we call the old basis) with the following picture:\n\n\n\nThe old basis.\n\n\nLets us now change the basis of our tensor product space, for that we choose the new CSCO: \\[\n\\{\\overbrace{S_1^2,S_2^2,S_3^2,S_{12}^2}^\\text{background};\\overbrace{S_{tot}^2,S_{tot}^z}^\\text{total spin}\\}\n\\] whose eigenbasis is defined as usual by the quantum numbers determining each eigenvalue of the observables in the set: \\[\n\\ket{s_1,s_2,s_3,S_{12};S_{tot},M_{tot}}\n\\]\nSpecifically we are interested in the basis states which have \\[\ns_1=s_2=s_3=1/2\\qquad S_{12}=1\n\\]\nbecause we did add three electron spins and the first two are in state belonging to \\(\\mathcal{E}(S_{12}=1)\\). Note this subspace is my choice, we could had chosen \\(\\mathcal{E}(S_{12}=0)\\), see 13). This is the new basis.\nThe operators \\(S_{tot}^2\\) and \\(S_{tot}^z\\) describe the magnitude and the \\(z-\\)component of the total angular momentum, and we know from the general rules of angular momentum in 4) that the following relation holds: \\[\n-S_{tot}\\leq M_{tot}\\leq S_{tot}\n\\] We do not know the values of \\(S_{tot}\\) are, yet, just like we did not know what were the values of \\(S_{12}\\) when we added the systems 1+2 in 12). But we can guess what they are from the possible values of \\(M_{tot}\\), just as we did to guess what \\(S_{12}\\) was from the known values of \\(M=m_1+m_2\\), see 12).\nSince the kets of the old basis are also eigenvectors of \\(S_{tot}^z\\), let us act with this operator on the old basis to get the corresponding eigenvalues \\(M_{tot}\\), the calculations are analogous to Equation 29; we summarize the results by the pink boxes in the following picture:\n\nKnowing that \\(M_{tot}=-3/2,-1/2,1/2,3/2\\) and the fact that these numbers must be between \\(-S_{tot}\\) and \\(S_{tot}\\) we attempt to reconstruct the diagram that organizes the basis:\n\nFrom the picture we conclude we must have either \\(S_{tot}=1/2,3/2\\) and we confirm again the general rule: \\[\n1/2=|S_{12}-s_3|\\leq S_{tot}\\leq S_{12}+s_3=3/2\\qquad \\text{in steps of }1\n\\] Let us now relate the old basis on the left with the new basis on the right, the idea is exactly the same as in 13):\n\nThe two subspaces \\(\\mathcal{E}(M_{tot}=3/2)\\) are one dimensional and thus: \\[\n\\ket{S_{tot}=3/2,M_{tot}=3/2}=\\ket{M_{12}=1}\\otimes\\ket{m_3=1/2}\n\\]The ket on the lhs which previously was just a ket with indices of the eigenvalues, now has meaning, it is equal to the tensor product on the rhs.\nActing now with the lowering operator \\(S_{tot}^-=S_{12}^-+S_1^-\\) on this equality we find: \\[\nS_{tot}^-\n\\ket{\\frac{3}{2},\\frac{3}{2}}=\\hbar\\sqrt{3}\\ket{\\frac{3}{2},\\frac{1}{2}}\n\\] The right side of the equality is: \\[\n\\begin{align}\nS_{tot}^-\n\\ket{\\frac{3}{2},\\frac{3}{2}}&=(S_{12}^-+S_1^-)(\\ket{1}\\otimes\\ket{\\frac{1}{2}})\\\\\n&=\\hbar\\sqrt{2}\\ket{0}\\otimes\\ket{\\frac{1}{2}}+\\hbar\\ket{1}\\otimes\\ket{-\\frac{1}{2}}\n\\end{align}\n\\] Therefore we conclude: \\[\n\\ket{\\frac{3}{2},\\frac{1}{2}}=\\sqrt{\\frac{2}{3}}\\ket{0}\\otimes\\ket{\\frac{1}{2}}+\\frac{1}{\\sqrt{3}}\\ket{1}\\otimes\\ket{-\\frac{1}{2}}\n\\] We now seek to relate the eigenvector \\(\\ket{S_{tot}=1/2,M_{tot}=1/2}\\) with the old basis, we do that by writing its properties:\n\nit must be a linear combination of the form: \\[\n    \\ket{S_{tot}=1/2,M_{tot}=1/2}=\\alpha \\ket{0}\\otimes\\ket{\\frac{1}{2}} + \\beta \\ket{1}\\otimes\\ket{-\\frac{1}{2}}\n    \\]\nit must normalized: \\[\n|\\alpha|^2+|\\beta|^2=1\n\\]\nit must be perpendicular to \\(\\ket{\\frac{3}{2},\\frac{1}{2}}\\) .\n\nA pair of \\(\\alpha\\) and \\(\\beta\\) that satisfy all the above constraints yields: \\[\n\\ket{\\frac{1}{2},\\frac{1}{2}}=\\frac{1}{\\sqrt{3}} \\ket{0}\\otimes\\ket{\\frac{1}{2}} - \\sqrt{\\frac{2}{3}} \\ket{1}\\otimes\\ket{-\\frac{1}{2}}\n\\] We can continue now and act with \\(S_{tot}^-\\) on \\(\\ket{\\frac{3}{2},\\frac{1}{2}}\\) to get us \\(\\ket{\\frac{3}{2},-\\frac{1}{2}}\\); we find: \\[\n\\ket{\\frac{3}{2},-\\frac{1}{2}}=\\frac{1}{\\sqrt{3}}\\ket{-1}\\otimes\\ket{\\frac{1}{2}}+\\sqrt{\\frac{2}{3}}\\ket{0}\\otimes\\ket{-\\frac{1}{2}}\n\\] The ket \\(\\ket{\\frac{1}{2},-\\frac{1}{2}}\\) is found from its properties just as we did for \\(\\ket{\\frac{1}{2},\\frac{1}{2}}\\): \\[\n\\ket{\\frac{1}{2},-\\frac{1}{2}}=\\sqrt{\\frac{2}{3}}\\ket{-1}\\otimes\\ket{\\frac{1}{2}}-\\frac{1}{\\sqrt{3}}\\ket{0}\\otimes\\ket{-\\frac{1}{2}}\n\\] Finally we are left with: \\[\n\\ket{\\frac{3}{2},-\\frac{3}{2}}=\\ket{-1}\\otimes\\ket{-\\frac{1}{2}}\n\\] since the subspace \\(\\mathcal{E}(M_{tot}=-3/2)\\) is one dimensional.",
    "crumbs": [
      "Brief Notes",
      "Quantum Theory",
      "Angular Momentum"
    ]
  },
  {
    "objectID": "angular_momentum_qm_short_summary.html#summary-of-the-3-electron-case",
    "href": "angular_momentum_qm_short_summary.html#summary-of-the-3-electron-case",
    "title": "Ideas to get started with Angular Momentum in Quantum Mechanics",
    "section": "",
    "text": "The eigenbasis of the CSCO \\(\\{S_1^2,S_2^2,S_3^2,S_{12}^2;S_{tot}^2,S_{tot}^z\\}\\) is related with the eigenbasis of the CSCO \\(\\{S_1^2,S_2^2;S_{12}^2,S_{12}^z,S_3^2,S_3^z\\}\\) as:\n\n\\(\\mathcal{E}(M_{12}=3/2)\\) \\[\n\\ket{S_{tot}=3/2,M_{tot}=3/2}=\\ket{M_{12}=1}\\otimes\\ket{m_3=1/2}\n\\]\n\\(\\mathcal{E}(M_{12}=1/2)\\) \\[\n\\begin{align}\n&\\ket{S_{tot}=\\frac{3}{2},M_{tot}=\\frac{1}{2}}=\\sqrt{\\frac{2}{3}}\\ket{M_{12}=0}\\otimes\\ket{m_3=\\frac{1}{2}}+\\\\\n&\\frac{1}{\\sqrt{3}}\\ket{M_{12}=1}\\otimes\\ket{m_3=-\\frac{1}{2}}\\\\\n&\\ket{S_{tot}=\\frac{1}{2},M_{tot}=\\frac{1}{2}}=\\frac{1}{\\sqrt{3}} \\ket{M_{12}=0}\\otimes\\ket{m_3=\\frac{1}{2}} -\\\\\n&\\sqrt{\\frac{2}{3}} \\ket{M_{12}=1}\\otimes\\ket{m_3=-\\frac{1}{2}}\n\\end{align}\n\\]\n\\(\\mathcal{E}(M_{12}=-1/2)\\) \\[\n\\begin{align}\n&\\ket{S_{tot}=\\frac{3}{2},M_{tot}=-\\frac{1}{2}}=\\frac{1}{\\sqrt{3}}\\ket{M_{12}=-1}\\otimes\\ket{m_3=\\frac{1}{2}}+\\\\\n&\\sqrt{\\frac{2}{3}}\\ket{M_{12}=0}\\otimes\\ket{m_3=-\\frac{1}{2}}\\\\\n&\\ket{S_{tot}=\\frac{1}{2},M_{tot}-\\frac{1}{2}}=\\sqrt{\\frac{2}{3}}\\ket{M_{12}=-1}\\otimes\\ket{m_3=\\frac{1}{2}}-\\\\\n&\\frac{1}{\\sqrt{3}}\\ket{M_{12}=0}\\otimes\\ket{m_3=-\\frac{1}{2}}\n\\end{align}\n\\]\n\\(\\mathcal{E}(M_{12}=-3/2)\\) \\[\n\\ket{S_{tot}=\\frac{3}{2},M_{tot}=-\\frac{3}{2}}=\\ket{M_{12}=-1}\\otimes\\ket{m_3=-\\frac{1}{2}}\n\\] In turn we must now replace the basis kets \\(\\ket{S_{12}=1,M_{12}}\\) by the elements of the triplet from 14) if you want to relate the new basis with the basis generated by the CSCO\n\\[\n\\{S_1^2,S_1^z;S_2^2,S_2^z;S_3^2,S_3^z\\}\n\\]",
    "crumbs": [
      "Brief Notes",
      "Quantum Theory",
      "Angular Momentum"
    ]
  },
  {
    "objectID": "analysis_of_system.html",
    "href": "analysis_of_system.html",
    "title": "Analysis of a system of equations",
    "section": "",
    "text": "How many solutions are there?\nAs a result of the using the elimination algorithm one of three scenarios will unfold:\n\nThere is just one solution\nThere is no solution\nThere is more than one solution. In which case, how many?\n\nImagine we have a system of equations before us - equivalently a matrix \\(A\\) and a \\(\\mathbf{b}\\) vector or equivalently an extended matrix. Whether we have one, none or many solution has to do with the coefficients of the system - equivalently it has to do with the choice of \\(A\\) and \\(\\mathbf{b}\\) or equivalently, it has to do with the entries of the extended matrix.\nOne way to answer the question is to solve the system and count how many solutions we find, elimination is a tool to do that.\nThe matrix-vector and then the extended matrix notation as just a new aesthetic for an actual system of equations. An aesthetic that made easier the use of the elimination algorithm, because it just focus on the coefficients; when the system is simple enough we went back to the system notation, to easily solve it.\nWhat we are going to do now is to give a deeper meaning to this notation, which in turn will make you see why some \\(A\\)’s together with \\(b\\)’s give one, none or many solutions. The matrix-vector notation or equivalently the extended matrix notation are like looking at an object, in this case a system of equation, from a different angle.\nBefore elaborating on what this deeper view is, we need some preparation, specifically we need to introduce the concepts:\n\nMatrix Multiplication and linear combination of rows\nInverse Matrix\nPivots and rank of a matrix\nTwo special Subspaces: Column space and Nullspace\n\nWe’ll focus on Example 4."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "angular_momentum_qm.html",
    "href": "angular_momentum_qm.html",
    "title": "Angular Momentum in QM",
    "section": "",
    "text": "Falar da estrutura math: Hamiltoniano-&gt;(x,p)-&gt;L-&gt;Leis conservação\nTwo examples:\n\nAngular momentum of a planet orbiting the sun\nAngular momentum of a planet orbiting on its axis"
  },
  {
    "objectID": "angular_momentum_qm.html#the-importance-of-angular-momentum-in-cm",
    "href": "angular_momentum_qm.html#the-importance-of-angular-momentum-in-cm",
    "title": "Angular Momentum in QM",
    "section": "",
    "text": "Falar da estrutura math: Hamiltoniano-&gt;(x,p)-&gt;L-&gt;Leis conservação\nTwo examples:\n\nAngular momentum of a planet orbiting the sun\nAngular momentum of a planet orbiting on its axis"
  },
  {
    "objectID": "angular_momentum_qm.html#what-is-angular-momentum-in-qm",
    "href": "angular_momentum_qm.html#what-is-angular-momentum-in-qm",
    "title": "Angular Momentum in QM",
    "section": "What is angular momentum in QM?",
    "text": "What is angular momentum in QM?\nFalar/referenciar a estrutura math: Hamiltoniano-&gt;Algebra-&gt;pdf-&gt;momentos (Intro cohen + algebra)\nTwo examples:\n\nAngular momentum of electron around a nucleus\nSpin angular momentum of an electron"
  },
  {
    "objectID": "angular_momentum_qm.html#angular-momentum-in-qm",
    "href": "angular_momentum_qm.html#angular-momentum-in-qm",
    "title": "Angular Momentum in QM",
    "section": "Angular momentum in QM",
    "text": "Angular momentum in QM\nClassical angular momentum is defined as \\[\n\\mathbf{L}=\\mathbf{r}\\times \\mathbf{p}\n\\]\nwhere for example, the last component of \\(\\mathbf{L}\\) is \\[\nL_z=xp_y-p_yx.\n\\]To obtain the QM angular momentum we replace the positions coordinates \\(x,y,z\\) and momentum \\(p_x,p_y,p_z\\) by the operators \\(X,Y,Z\\) and \\(P_x,P_y,P_z\\). This gives us:\n\\[\n\\mathbf{L}=\\mathbf{R}\\times \\mathbf{P}\n\\]where \\(\\mathbf{R}=(X,Y,Z)\\) and \\(\\mathbf{P}=(P_x,P_y,P_z)\\) are vectors whose entries are operators. From the three components of \\(\\mathbf{L}\\) we can derive the following commutations relations:\n\\[\n[L_x,L_y]=i\\hbar L_z\\qquad [L_y,L_z]=i\\hbar L_x\\qquad [L_z,L_x]=i\\hbar L_y\\qquad [L_i,L_j]=0\\quad \\text{for}\\ i\\not= j\n\\]\nfrom the canonical ones:\n\\[\n[X,P_x]=i\\hbar \\qquad [Y,P_y]=i\\hbar \\qquad [Z,P_z]=i\\hbar\n\\]\nFrom a different perspective we can say that the angular momentum operators satisfy these three commutation “equations”. In QM one wants to talk about more general angular momentum, for example spin angular momentum, it has no classical analog and hence is not defined explicitly as \\(\\mathbf{r}\\times \\mathbf{p}\\). However one can still define it implicitly, by saying that whatever spin angular momentum operator is, it must satisfy the previous three commutations relation. Hence we introduce a general angular momentum \\(J\\) which obeys the rules\n\\[\n[J_x,J_y]=i\\hbar J_z\\qquad [J_y,J_z]=i\\hbar J_x\\qquad [J_z,J_x]=i\\hbar J_y\\qquad [J_i,J_j]=0\\quad \\text{for}\\ i\\not= j\n\\tag{1}\\]\n\\(\\mathbf{J}\\) can be either the orbital angular momentum of a particle or the spin angular momentum or a combination of these, since all quantum angular momenta has commutation relations of that form. Given \\(\\mathbf{J}\\) we can introduce now its norm squared \\(J^2\\), defined as:\n\\[\nJ^2=J_x^2+J_y^2+J_z^2\n\\]\nMoreover, we can easily show from Equation 1 that this operator commute with the three components of \\(\\mathbf{J}\\):\n\\[\n[J^2,J_x]=0\\qquad[J^2,J_y]=0\\qquad[J^2,J_z]=0\n\\tag{2}\\]\nfor example, using Equation 1 we find:\n\\[\n\\begin{align}\n[J^2,J_z]&=[J_x^2+J_y^2+J_z^2,J_z]\\\\\n&=[J_x^2,J_z]+[J_y^2,J_z]+[J_z^2,J_z]\\\\\n&=0+0+0\\\\\n&=0\n\\end{align}\n\\]\nThat fact that \\(J^2\\) commutes with \\(J_z\\) will be of great importance, because it guarantees one can find (and thus define) a set of basis kets in the Hilbert space that are simultaneously eigenstates of both. Moreover, since the commuting operators are hermitian (we say in QM they are observables) the eigenstates of the operators can be made orthogonal and the eigenvalues are real."
  },
  {
    "objectID": "angular_momentum_qm.html#goal",
    "href": "angular_momentum_qm.html#goal",
    "title": "Angular Momentum in QM",
    "section": "Goal",
    "text": "Goal\nDefine a basis of eigenstates common to \\(J^2\\) and \\(J_z\\) for our Hilbert Space."
  },
  {
    "objectID": "angular_momentum_qm.html#why",
    "href": "angular_momentum_qm.html#why",
    "title": "Angular Momentum in QM",
    "section": "Why?",
    "text": "Why?\nOne of the most important goals in QM is to find the eigenstates of the Hamiltonian of the system, paraphrased differently, we say, we want to find the basis of eigenstates of \\(H\\). Imagine we have an Hamiltonian which commutes with \\(J^2\\) and \\(J_z\\), then we know we can find a basis of eigenstates common to the three operators. Knowing already what is the eigenbasis common to \\(\\{J^2,J_z\\}\\) greatly facilitates finding the eigenstates of \\(H\\). The three operators (observables) constitute a complete set of commuting observables (CSCO) \\[\n\\{H,J^2,J_z\\}\n\\] and we want to find its eigenbasis."
  },
  {
    "objectID": "angular_momentum_qm.html#raising-and-lowering-operators",
    "href": "angular_momentum_qm.html#raising-and-lowering-operators",
    "title": "Angular Momentum in QM",
    "section": "Raising and lowering operators",
    "text": "Raising and lowering operators\nBy definition we have: \\[\nJ_+=J_x+iJ_y\\qquad J_-=J_x-iJ_y\\qquad.\n\\] There are now plenty commutations relations we can derive from these two operators in conjunction with the \\(J^2\\) and \\(J\\), but the ultimate consequences of these relations is that they tell us how to organize the eigenbasis of \\(\\{J^2,J_z\\}\\), see next section and Figure 1 .\nThese definitions allow us to write:\n\\[\nJ_x = \\frac{1}{2}(J_++J_-) \\qquad J_y = \\frac{1}{2i}(J_+-J_-),\n\\tag{3}\\]\nwhich will be useful later."
  },
  {
    "objectID": "angular_momentum_qm.html#eigenvalues-and-eigenvectors",
    "href": "angular_momentum_qm.html#eigenvalues-and-eigenvectors",
    "title": "Angular Momentum in QM",
    "section": "Eigenvalues and eigenvectors",
    "text": "Eigenvalues and eigenvectors\nFor any operator we can define its eigenvalues \\(\\lambda\\) and eigenvectors \\(|\\psi\\rangle\\) in a generic manner as we did in basic linear algebra courses:\n\\[\nJ^2|\\lambda\\rangle = \\lambda |\\lambda\\rangle\\\\\nJ_z|\\lambda_z\\rangle = \\lambda_z |\\lambda_z\\rangle\n\\tag{4}\\]\nIn summary, skipping the proofs, what the commutation relations introduced above and the definition of raising and lowering operators allow us to do leads us to conclude that:\n\nThe eigenvalues of \\(J^2\\) are of the form \\(j(j+1)\\hbar^2\\) and those of \\(J_z\\) are \\(m\\hbar\\) where \\(j\\) is a positive integer or semi-integer and \\(-j\\leq m\\leq j\\) in steps of \\(1\\).\nThe corresponding eigenvectors can be labelled by the numbers \\(j\\) , \\(m\\) and \\(n\\). Instead of @eq_general_eval_evect we write:\n\n\\[\n\\begin{align}&J^2\\ket{n,j,m}=j(j+1)\\hbar^2\\ket{n,j,m},\\\\&J_z\\ket{n,j,m}=m\\hbar\\ket{n,j,m}.\\end{align}\n\\tag{5}\\]\nThe only values that \\(j\\) can take are \\(0\\), \\(1/2\\), \\(3/2\\), \\(2\\), etc; while \\(n\\) can either be an integer or a continuous number that we use to label and thus distinguish the kets in the basis.\n\nFor each \\(n\\), within \\(\\mathcal{E}(n,j)\\) the operators \\(J_\\pm\\) links each vector of the basis, specifically we have:\n\n\\[ J_\\pm \\ket{n,j,m}=\\hbar \\sqrt{j(j+1)-m(m\\pm 1)}\\ket{n,j,m\\pm 1},  \\tag{6}\\]\na formula which is derived from commutation relations (through a rather complicated path). The names raising and lowering operators comes form the fact that \\(m\\) is either raised by \\(+1\\) or lowered by \\(-1\\) each time \\(J_\\pm\\) acts.\n\n\n\n\n\n\nFigure 1: Eigenspaces for various values of j and a specific value of n.\n\n\n\nEach blue dot in the picture corresponds to basis vector \\(\\ket{n,j,m}\\), the horizontal boxes show subspaces \\(\\mathcal{E}(n,j)\\) of the Hilbert space. From a different perspective we have:\n\n\n\nEigenspace for j=1 and the slice for n=1. (n=0,1,2,…)\n\n\n\n\n\n\n\n\nObserve\n\n\n\n\nFrom the first equation in Equation 5 we notice that for a given value of \\(n\\) and \\(j\\), for example \\(n=1\\) and \\(j=1\\), then \\(\\{|1,1,1\\rangle,|1,1,0\\rangle,|1,1,-1\\rangle\\}\\) are eigenvectors of \\(J^2\\) with the same eigenvalue \\(1(1+1)\\hbar^2\\) , we therefore say the eigenvalue is 3-degenate, or that the space \\(\\mathcal{E}(n=1,j=1)\\) spanned by this basis is 3-degenerate. We distinguish the 3 eigenvectors by the \\(J_z\\) eigenvalue \\(m\\hbar\\), or more simply by just the integer \\(m\\).\nSubspaces of the Hilbert space can now be specified by fixing \\(j\\) and \\(n\\) for example, or by fixing \\(m\\) and \\(n\\) for various values. We know how its basis is then labelled and also how it is connected by the raising and lowering operators. With this information, we can compute how operators act in these subspaces, as an example see the next section."
  },
  {
    "objectID": "angular_momentum_qm.html#the-action-of-j2-and-j_x-in-mathcalenj1",
    "href": "angular_momentum_qm.html#the-action-of-j2-and-j_x-in-mathcalenj1",
    "title": "Angular Momentum in QM",
    "section": "The action of \\(J^2\\) and \\(J_x\\) in \\(\\mathcal{E}(n,j=1)\\)",
    "text": "The action of \\(J^2\\) and \\(J_x\\) in \\(\\mathcal{E}(n,j=1)\\)\nTo compute the action we need first to specify a space (or subspace) and for that we have to specify its basis. For example, let us choose the subspace of eigenvectors of \\(\\{J^2,J_z\\}\\) with eigenvalue for a given \\(n\\) and \\(l=1\\):\n\\[\n\\mathcal{E}(n,j=1)=\\text{span}\\{|n,1,1\\rangle,|n,1,0\\rangle,|n,1,-1\\rangle\\}\n\\]\nsee Figure 1 .\n\n\n\n\n\n\nChange of notation\n\n\n\nWe drop the \\(n\\) index for easier read of the formulas. Later we will recover it.\n\n\nWe act with \\(J_x\\) on the basis of the chosen subspace. What is the output? Answer:\n\\[\n\\begin{align}\nJ_x|1,1\\rangle &= \\frac{1}{2}(J_+ +J_-)|1,1\\rangle\\\\\n&=\\frac{1}{2}J_+|1,1\\rangle+\\frac{1}{2}J_-|1,1\\rangle\\\\\n&=\\frac{\\hbar}{2}\\sqrt{1(1+1)-1(1+1)}|1,2\\rangle+\\frac{\\hbar}{2}\\sqrt{1(1+1)-1(1-1)}|1,0\\rangle\\\\\n&=\\frac{\\hbar}{\\sqrt{2}}|1,0\\rangle\n\\end{align}\n\\tag{7}\\]\nWe conclude that when \\(J_x\\) acts on the basis element \\(|1,1\\rangle\\) it outputs \\(|1,0\\rangle\\) times a constant, notice the input and output both live in \\(\\mathcal{E}(j=1)\\). Let us see now what happens when \\(J_x\\) acts on the second basis ket \\(|1,0\\rangle\\):\n\\[\n\\begin{align}\nJ_x|1,0\\rangle &= \\frac{1}{2}(J_++J_-)|1,0\\rangle\\\\\n&=\\frac{\\hbar}{2}\\sqrt{1(1+1)-0(0+1)}|1,1\\rangle+\\frac{\\hbar}{2}\\sqrt{1(1+1)-0(0-1)}|1,1\\rangle|1,-1\\rangle\\\\\n&=\\frac{\\hbar}{\\sqrt{2}}(|1,1\\rangle+|1,-1\\rangle)\n\\end{align}\n\\tag{8}\\]\nAs \\(J_x\\) acts on \\(|1,0\\rangle\\) it gives a linear combination \\(|1,1\\rangle+|1,-1\\rangle\\) of basis vectors, which in turn also lives in \\(\\mathcal{E}(j=1)\\).\nAnd finally,a similar calculation would shows the action on the third ket is:\n\\[\nJ_x|1,-1\\rangle =\\frac{\\hbar}{\\sqrt{2}}|1,0\\rangle\n\\tag{9}\\]\nAgain, the input \\(|1,-1\\rangle\\) is in \\(\\mathcal {E}(j=1)\\) and so is the output \\(|1,0\\rangle\\).\n\n\n\n\n\n\nObservation\n\n\n\nThe calculation above show clearly that the the operator \\(J_x\\) maps the basis of \\(\\mathcal{E}(j=1)\\) into vectors in that subspace. Therefore, in general it will map any ket of \\(\\mathcal{E}(j=1)\\) into another in \\(\\mathcal{E}(j=1)\\); the jargon for this observation is to say that “\\(\\mathcal{E}(j=1)\\) is globally invariant under the action of \\(J_x\\)”; more colloquially we just say “the action of \\(J_x\\) is stuck in the subspace”. These calculation confirm the prediction already made from the fact that \\([J^2,J_x]=0\\).\n\n\nThe results in Equation 7, Equation 8 and Equation 9 allow us to write:\n\\[\nJ_x = \\sum_{m,m'=1,0,-1}|1,m\\rangle\\langle1,m|J_x|1,m'\\rangle\\langle1,m'|\n\\]\nexplicitly as (LINK PARA REPRE GEN Ops)\n\\[\n\\begin{align}\nJ_x &= \\frac{\\hbar}{\\sqrt{2}}\\left(|1,0\\rangle\\langle 1,1|+|1,1\\rangle\\langle 1,0|+|1,-1\\rangle\\langle 1,0|+|1,0\\rangle\\langle 1,-1|\\right)\n\\end{align}\n\\]\nwhich in turn can be rewritten using using matrix notation. If we agree to order for the basis kets as \\(\\{|1,1\\rangle, |10\\rangle,|1,-1\\rangle\\}\\) we find:"
  },
  {
    "objectID": "angular_momentum_qm.html#the-action-of-j_y-and-j_z-in-mathcalenj1",
    "href": "angular_momentum_qm.html#the-action-of-j_y-and-j_z-in-mathcalenj1",
    "title": "Angular Momentum in QM",
    "section": "The action of \\(J_y\\) and \\(J_z\\) in \\(\\mathcal{E}(n,j=1)\\)",
    "text": "The action of \\(J_y\\) and \\(J_z\\) in \\(\\mathcal{E}(n,j=1)\\)\nRepeating these calculation for \\(J_y\\) and \\(J_z\\) yields the results:\n\nThe matrix for \\(J_z\\) is diagonal since the basis elements we choose fo \\(\\mathcal{E}(j=1)\\) are its eigenvectors, the diagonal elements are the the three \\(m\\hbar\\) eigenvalues for \\(m=1,0,-1\\); and since they are also eigenvectors of \\(J^2\\) we find a diagonal with the \\(1(1+1)\\hbar^2=2\\hbar^2\\) eigenvalues repeated three times:\n\nAll of above assume a given index \\(n\\) was chosen."
  },
  {
    "objectID": "angular_momentum_qm.html#the-action-of-j2-and-j_x-j_y-and-j_z-in-mathcalenj12-spin-angular-momentum",
    "href": "angular_momentum_qm.html#the-action-of-j2-and-j_x-j_y-and-j_z-in-mathcalenj12-spin-angular-momentum",
    "title": "Angular Momentum in QM",
    "section": "The action of \\(J^2\\) and \\(J_x\\), \\(J_y\\) and \\(J_z\\) in \\(\\mathcal{E}(n,j=1/2)\\) (Spin angular momentum)",
    "text": "The action of \\(J^2\\) and \\(J_x\\), \\(J_y\\) and \\(J_z\\) in \\(\\mathcal{E}(n,j=1/2)\\) (Spin angular momentum)\nThe intrinsic spin angular momentum of an electron is described by states living in the \\(\\mathcal{E}(j=1/2)\\) subspace (see Figure 1), the basis for this subspace is (as usual) furnished by the simultaneous eigenvectors of the \\(J^2\\) and \\(J_z\\):\n\\[\n\\begin{align}&J^2\\ket{1/2,m}=\\frac{3}{4}\\hbar^2\\ket{1/2,m}\\\\&J_z\\ket{1/2,m}=m\\hbar\\ket{1/2,m}\\end{align}\n\\tag{10}\\]\nwhere \\(m=\\pm 1/2\\). Therefore we say\n\\[\n\\mathcal{E}(j=1/2)=\\text{span}\n\\{|1/2,1/2\\rangle,|1/2,-1/2\\rangle\\}\n\\]\n\n\n\n\n\n\nNotation\n\n\n\n\nBesides dropping the index \\(n\\) introduced in Equation 5,\ngiven \\(j=1/2\\) is fixed it is usual to drop it as well and simply write \\(|m\\rangle\\) where \\(m=\\pm1/2\\).\nwe can rename \\(m\\) as \\(m_s\\) and reexpress is as \\(m_s=\\sigma/2\\) with \\(\\sigma =\\pm\\) or \\(\\sigma=\\uparrow,\\downarrow\\), but for the present notes we’ll keep the \\(m_s\\) notation.\nBecause the \\(j=1/2\\) angular momentum states are very important (they are the states that describe electrons for example) we rename them into spin angular momentum and to emphasize that we use the notation \\(\\{S^2,S_z\\}\\) for the \\(\\{J^2,J_z\\}\\) that act strikli in \\(\\mathcal{E}(j=1/2)\\). Instead of Equation 10 we should write:\n\\[\n\\begin{align}&S^2\\ket{m_s}=\\frac{3}{4}\\hbar^2\\ket{m_s}\\\\&S_z\\ket{m_s}=m_s\\hbar\\ket{m_s}\\end{align}\n\\]\n\n\n\nThe basis states \\(|m_s\\rangle\\) are connect by the raising \\(S_+\\) and lowereing operator \\(S_-\\) we seen in Equation 6, adapting to the present electron basis states we have:\n\\[\nS_+\\ket{-1/2}=\\hbar\\sqrt{1/2(1/2+1)-1/2(1/2+1)}\\ket{1/2}=\\hbar\\sqrt{2}/2\\ket{1/2}\n\\tag{11}\\]\nWe can see that the basis states in \\(\\mathcal{E}(1/2)\\) can be mapped into each other by acting with \\(S_\\pm\\).\nThe raising and lowering operators are important for another reason, they tell us how \\(S_x\\) and \\(S_y\\) act on the eigenbasis of \\(\\{S^2,S_z\\}\\), from their definition we have:\n\\[\nS_x=\\frac{1}{2}(J_++J_-) \\qquad S_y=\\frac{1}{2i}(J_+-J_-)\n\\tag{12}\\]\n\n\n\n\n\n\nComment\n\n\n\nIn order to know how \\(S_x\\) and \\(S_y\\) act on this basis requires first to reexpress these operators in terms of operator we know how they act in this basis, the formulas Equation 12 together with Equation 11 do precisely that.\n\n\nA similar calculation to Equation 7 can now be performed which would show that their action is stuck in the \\(\\mathcal{E}(j=1/2)\\), after all they both commute with \\(S^2\\) operator and thus \\(j=1/2\\) must be preserved.\n\nMatrix Representation\nBy knowing how an operator acts on a basis we know how obtain its matrix representation, for example, lets take a look at \\(S_z\\) and then \\(S_x\\) and \\(S_y\\). We can write the action of \\(S_z\\) on the basis \\(\\{\\ket{1/2},\\ket{-1/2}\\}\\) as:\n\\[\n\\begin{align}&S_z\\ket{1/2}=\\frac{\\hbar}{2}\\ket{1/2}+0\\ket{-1/2}\\\\&S_z\\ket{-1/2}=0\\ket{1/2}+\\frac{\\hbar}{2}\\ket{-1/2}\\end{align}\n\\]\nFrom this result we can get the numbers \\(\\braket{m|S_z|m'}\\). We can exhibit all these numbers as a matrix:\n\\[\n[S_z]=\\frac{\\hbar}{2}\\begin{pmatrix}1 & 0\\\\0 & -1\\end{pmatrix}\n\\]\nGetting the matrices for \\(S_x\\) and \\(S_y\\) in the eigenbasis \\(\\{\\ket{-1/2},\\ket{1/2}\\}\\), we write them in terms of the raising and lowering operators. After calculations analogous to Equation 7, the numbers \\(\\braket{m|S_x|m'}\\) and \\(\\braket{m|S_y|m'}\\) can be arranged into the matrices:\n\\[\n[S_x]=\\frac{\\hbar}{2}\\begin{pmatrix}0 & 1\\\\1 & 0\\end{pmatrix}\\qquad[S_y]=\\frac{\\hbar}{2}\\begin{pmatrix}0 & -i\\\\i & 0\\end{pmatrix}\n\\]"
  },
  {
    "objectID": "angular_momentum_qm.html#spin-quantum-states-of-a-system-with-two-electrons",
    "href": "angular_momentum_qm.html#spin-quantum-states-of-a-system-with-two-electrons",
    "title": "Angular Momentum in QM",
    "section": "Spin quantum states of a system with two electrons",
    "text": "Spin quantum states of a system with two electrons\nConsider two electrons each in its own isolated system and their spin properties described by the eigenstates of \\(\\{S_1^2,S_{1z}\\}\\) and \\(\\{S_2^2,S_{2z}\\}\\):\n\\[\n\\begin{align}&S_1^2\\ket{m_1}=\\frac{3}{4}\\hbar^2\\ket{m_1}\\\\&S_{1z}\\ket{m_1}=m_1\\hbar\\ket{m_1}\\\\&S_2^2\\ket{m_2}=\\frac{3}{4}\\hbar^2\\ket{m_2}\\\\&S_{2z}\\ket{m_2}=m_2\\hbar\\ket{m_2}\\end{align}\n\\]\nThe spins magnitude of the two electrons is described by the quantum numbers\n\\[\ns_1=\\frac{1}{2}\\qquad s_2=\\frac{1}{2}\n\\]\nThe quantum states of the two isolated systems are the states living in a state space whose basis is formed from the tensor product \\(\\ket{m_1}\\otimes \\ket{m_2}\\) of the individual eigenstates:\n\\[\n\\text{span}\\{\\ket{m_1}\\otimes \\ket{m_2}\\,\\,|\\,\\,m,m_2=\\pm1/2\\}\n\\]\n\n\n\n\n\n\nComment\n\n\n\nTwo isolated systems each with one electron is equivalent to a single system with two non-interacting electrons.\n\n\nIt is important to understand that:\n\nThe observables \\(S^2_1\\) ,\\(S_{1z}\\) act on the space spanned by \\(\\{\\ket{-1/2_1},\\ket{1/2_1}\\}\\) while the observables \\(S^2_2\\), \\(S_{2z}\\) act on \\(\\{\\ket{-1/2_2},\\ket{1/2_2}\\}\\) and do not act on tensor product states.\nWe define however the extensions of \\(S^2_1\\), \\(S_{1z}\\), which act on the tensor product space as\n\n\\[\n\\tilde{S}_1^2=S_1^2\\otimes 1_2\\qquad \\tilde{S}_{1z}=S_{1z}\\otimes 1_2\n\\]\nThe extensions of \\(S^2_2\\) ,\\(S_{2z}\\) are defined analogously:\n\\[\n\\tilde{S}_2^2=1_1 \\otimes S_2^2\\qquad \\tilde{S}_{2z}=1_1\\otimes S_{2z}\n\\]\nTherefore we have:\n\\[\n\\begin{align}\\tilde{S}_1^2(\\ket{m_1}\\otimes \\ket{m_2}) &= (S_1^2\\ket{m_1})\\otimes \\ket{m_2}\\\\&=(\\frac{3\\hbar}{4}\\ket{m_1})\\otimes \\ket{m_2}\\\\&=\\frac{3\\hbar}{4}\\ket{m_1}\\otimes \\ket{m_2}\\end{align}\n\\tag{13}\\]\nSimilarly for the remainder operators.\n(ESTA COMPARAÇÃO TEM DE SER APROFUNDADA)\nAlso, the set of operators \\(\\{\\tilde{S}_1^2,\\tilde{S}_{1z},\\tilde{S}_2^2,\\tilde{S}_{2z}\\}\\) commute and as a result we know it exists a basis of common eigenstates which we label by the eigenvalues, we define them as \\(\\ket{m_1,m_2}\\). In mathematical terms we have:\n\\[\n\\begin{align}&\\tilde{S}_1^2\\ket{m_1,m_2}=\\tilde{S}_2^2\\ket{m_1,m_2}=\\frac{3}{4}\\hbar\\ket{m_1,m_2}\\\\&\\tilde{S}_{1z}\\ket{m_1,m_2}=m_1 \\hbar\\ket{m_1,m_2}\\\\&\\tilde{S}_{2z}\\ket{m_1,m_2}=m_2 \\hbar\\ket{m_1,m_2}\\\\\\end{align}\n\\tag{14}\\]\nComparing Equation 13 and the first equation in Equation 14 we are forced to conclude that \\(\\ket{m_1,m_2}\\) are the tensor product of the eigenstates of the individual electron spins:\n\\[\n\\ket{m_1,m_2}:=\\ket{m_1}\\otimes\\ket{m_2}\n\\]\nIf we were unaware of the kets \\(|m_1\\rangle \\otimes |m_2\\rangle\\) and ?@eq-setill, we would only know the eigenbasis is labelled by the eigenvalues and, just as stated in Equation 14.\n\n\n\n\n\n\nNotation\n\n\n\nNote how we suppressed the \\(j_1=j_2=1/2\\) that label the \\(\\tilde{S}_{1,2}^2\\) eigenvalues for simplicity. As a further simplification of notation we’ll from now on omit the ~ in\\(\\tilde{S}_{1,2}^2\\) and \\(\\tilde{S}_{z1,2}\\), it is left to the reader that these operators are extensions into tensor product spaces.\n\n\nIn conclusion, the spin properties of a system with two particles (electrons) whose spin state is described by kets in \\(\\mathcal{E}_1(j_1=1/2)\\) and \\(\\mathcal{E}_2(j_2=1/2)\\) is described by a state of the form\n\\[\n\\ket{m_1,m_2}:=\\ket{m_1}\\otimes\\ket{m_2}\\qquad m_{1,2}=\\pm1/2\n\\]\nwhich constitute a basis for the tensor product state space\n\\[\n\\mathcal{E}_{12}=\\mathcal{E}_1\\otimes\\mathcal{E}_2\n\\]\nA pictorial organization of our basis states if given as follows:\n\n\n\nThe blue dots represent a basis ket, the pink boxes we’ll be explained in next section."
  },
  {
    "objectID": "angular_momentum_qm.html#a-new-basis-for-the-tensor-state-space",
    "href": "angular_momentum_qm.html#a-new-basis-for-the-tensor-state-space",
    "title": "Angular Momentum in QM",
    "section": "A new basis for the tensor state space",
    "text": "A new basis for the tensor state space\nWe want for the space \\(\\mathcal{E}_{12}(j_1=1/2,j_2=1/2)\\) a basis of kets which are simultaneous eigenvalues of the following set of observables: \\[\n\\{S_1^2,S_2^2,S^2,S_z\\}\n\\] where \\(\\mathbf{S}=\\mathbf{S}_1+\\mathbf{S}_2=(S_{1x}+S_{2x},S_{1y}+S_{2y},S_{1z}+S_{2z})\\), the magnitude squared operator is \\(S^2=S_x^2+S_y^2+S_z^2\\) and z-component \\(S_z=S_{1z}+S_{2z}\\)."
  },
  {
    "objectID": "angular_momentum_qm.html#why-1",
    "href": "angular_momentum_qm.html#why-1",
    "title": "Angular Momentum in QM",
    "section": "Why?",
    "text": "Why?\nThe Hamiltonian we got in our hands might commute with these observables rather than the old ones. Our goal is still to diagonalize \\(H\\). (terminar estar notas realmente diagonalizando Hamiltnonianos)"
  },
  {
    "objectID": "angular_momentum_qm.html#the-new-basis",
    "href": "angular_momentum_qm.html#the-new-basis",
    "title": "Angular Momentum in QM",
    "section": "The new basis",
    "text": "The new basis\nSince \\(\\{S_1^2,S_2^2,S^2,S_z\\}\\) is a CSCO, we can define a basis of kets which are eigenvector of all these observables; the basis is labelled by the eigenvalues WHY???!!! (or better yet, by the indices that label the eigenvalues): \\[\n\\begin{align}\n&S_1^2\\ket{j_1,j_2;S,M}=S^2_2\\ket{j_1,j_2;S,M}=\\frac{3}{4} \\hbar^2\\ket{S,M}\\\\\n&S^2\\ket{j_1,j_2;S,M}=S(S+1)\\hbar^2\\ket{j_1,j_2;S,M}\\\\\n&S_z\\ket{j_1,j_2;S,M}=M\\hbar\\ket{j_1,j_2;S,M}\n\\end{align}\n\\]where \\(j_1=j_2=1/2\\). Notice that \\(S^2\\) and \\(S_z\\) are angular momentum operators because they result of adding angular momentum, thus they are just another particular case of \\(J^2\\) and \\(J_z\\) we defined previously, as a result their eigenvalues are of the form \\(j(j+1)\\hbar^2\\) and \\(m\\hbar\\) with \\(-j\\leq m\\leq j\\), which for the present case we write under new symbols as \\(S(S+1)\\hbar^2\\) and \\(M\\hbar\\) with \\(-S\\leq M\\leq S\\)."
  },
  {
    "objectID": "angular_momentum_qm.html#what-are-the-values-of-s-and-m",
    "href": "angular_momentum_qm.html#what-are-the-values-of-s-and-m",
    "title": "Angular Momentum in QM",
    "section": "What are the values of \\(S\\) and \\(M\\)?",
    "text": "What are the values of \\(S\\) and \\(M\\)?\nWe still do not know what values \\(S\\) and \\(M\\) take. But two thing we know:\n\nFrom the theory of general angular momentum introduced above, we find \\(-S\\leq M\\leq S\\).\nKnowing that \\(S_z=S_{1z} + S_{2z}\\) tell us how to act with \\(S_z\\) on the old basis \\(\\{\\ket{1/2,1/2},\\ket{-1/2,1/2},\\ket{1/2,-1/2},\\ket{-1/2,-1/2}\\}\\), that suffices to show what values \\(M\\) take. Let us act with \\(S_z\\) on each ket of this basis. \\[\nS_z\\ket{m_1,m_2}=(S_{1z}+S_{2z})\\ket{m_1,m_2}=(m_1+m_2)\\hbar \\ket{m_1,m_2}\n\\]\n\nFrom this we conclude that \\[\nM=m_1+m_2\n\\] The values \\(M\\) can take are: \\[\nM\\in\\{-1,0,+1\\}\n\\] Now, what values \\(S\\) need to take that guarantee that \\(M\\) takes these values? Notice the question: If I know \\(S\\), I can easily get the range of \\(M\\) because I know that \\(-S\\leq M\\leq M\\). The present question goes the other way, if I know the \\(M\\)’s, what \\(S\\)’s are possible. We can answer this question by constructing a diagram similar to the one done before:\n\nWe can clearly see that \\(S=0,1\\). A closer inspection reveals that the following condition holds in general: \\[\n|j_1-j_2|\\leq S\\leq j_1+j_2\n\\] This diagram not only showed us what is \\(S\\), it also organizes pictorially the basis \\(\\{\\ket{1/2,1/2;S,M}\\}\\) generated by the CSCO. In it is also shown two subspaces where the raising and lowering operators \\(S_\\pm\\) act. We will drop the \\(j_1=j_2=1/2\\) from the ket notation since it is clear from context and write just \\(\\ket{S,M}\\)."
  },
  {
    "objectID": "angular_momentum_qm.html#relating-the-two-basis",
    "href": "angular_momentum_qm.html#relating-the-two-basis",
    "title": "Angular Momentum in QM",
    "section": "Relating the two basis",
    "text": "Relating the two basis\nWe want to express the kets \\(\\ket{S,M}\\) of the new basis in terms of \\(\\ket{m_1,m_2}\\) that constitute the old basis, to do that we argue pictorially:\n\nOn the left side of the picture we see a graphical organization of the old basis and on the right side the new basis. We identify in each basis, the elements that span the subspaces \\(\\mathcal{E}(M)\\) for each \\(M\\); this is easily done on the right picture because the horizontal axis already tells us the \\(M\\)’s, on the left picture we need to recall that each basis ket is an eigenvector of \\(S_z\\) with eigenvalue \\(M\\hbar=(m_1+m_2)\\hbar\\). Careful inspection of these subspaces shows us that: \\[\n\\begin{align}\n&\\ket{S=1,M=1}\\in \\mathcal{E}(M=1)\\\\\n&\\ket{m_1=1/2,m_2=1/2}\\in \\mathcal{E}(M=1)\n\\end{align}\n\\] and since the subspace is one-dimensional, these two basis kets must coincide. We write: \\[\n\\ket{1,1}=\\ket{1/2,1/2}\n\\] We just succeeded in establishing the first connection of both basis, more, the remaining connections follow from this one, as falling dominos. Using the lower operator defined as \\(S_-:=S_{1-}+S_{2-}\\) we find \\(\\ket{1,0}\\):\n\n\\[\nS_-\\ket{1,1}=(S_{1-}+S_{2-})\\ket{1/2,1/2}\n\\]\nThe left hand side gives us: \\[\n\\hbar\\sqrt{1(1+1)-0(0-1)}\\ket{1,0}\n\\] The right hand side is: \\[\n\\hbar\\sqrt{\\frac{1}{2}\\left(\\frac{1}{2}+1\\right)-\\frac{1}{2}\\left(\\frac{1}{2}-1\\right)}\\left(\\ket{-\\frac{1}{2},\\frac{1}{2}}+\\ket{\\frac{1}{2},-\\frac{1}{2}}\\right)\n\\] Thus: \\[\n\\hbar\\sqrt{2}\\ket{1,0}=\\hbar\\left(\\ket{-\\frac{1}{2},\\frac{1}{2}}+\\ket{\\frac{1}{2},-\\frac{1}{2}}\\right)\n\\] Which yields: \\[\n\\ket{1,0}=\\frac{1}{\\sqrt{2}}\\left(\\ket{-\\frac{1}{2},\\frac{1}{2}}+\\ket{\\frac{1}{2},-\\frac{1}{2}}\\right)\n\\] In \\(\\mathcal{E}(M=1)\\) there is still \\(\\ket{0,0}\\), we use its properties to derive the connection:\n\nWe know it is a linear combination of \\(\\ket{-1/2,1/2}\\) and \\(\\ket{1/2,-1/2}\\) thus we write: \\[\n\\ket{0,0}=c_1\\ket{-1/2,1/2}+c_2\\ket{1/2,-1/2}\n\\] for some complex numbers \\(c_{1,2}\\).\nWe know it is orthogonal to \\(\\ket{1,0}\\), thus we write: \\[\n0=\\braket{1,0|0,0}=\\frac{1}{\\sqrt{2}}(c_1+c_2)\n\\]\nWe also know it is normalized, i.e., \\[\n1=\\braket{0,0|0,0}=|c_1|^2+|c_2|^2\n\\]\n\nSolving these equations we conclude one possible solution is: \\[\nc_1=-c_2=\\frac{1}{\\sqrt{2}}\n\\] Hence, the second basis vector in \\(\\mathcal{E}(M=0)\\) is: \\[\n\\ket{0,0}=\\frac{1}{\\sqrt{2}}\\left(\\ket{-\\frac{1}{2},\\frac{1}{2}}-\\ket{\\frac{1}{2},-\\frac{1}{2}}\\right)\n\\] Finally, to get the last connection between the new and old basis, we act with either \\(S_-\\) on \\(\\ket{1,0}\\):\n\nOr we simply notice that \\(\\mathcal{E}(M=-1)\\) is also one dimensional and therefore: \\[\n\\ket{1,-1}=\\ket{-1/2,-1/2}\n\\]"
  },
  {
    "objectID": "angular_momentum_qm.html#summary",
    "href": "angular_momentum_qm.html#summary",
    "title": "Angular Momentum in QM",
    "section": "Summary",
    "text": "Summary\nThe connection between the basis \\(\\{\\ket{S,M}\\}\\) of eigenstates of the CSCO \\(\\{S_1^2,S_2^2;S^2,S_z\\}\\) and the basis \\(\\{\\ket{m_1,m_2}\\}\\) of eigenstates of the CSCO \\(\\{S_1^2,S_{1z},S^2_2,S_{2z}\\}\\) given \\(S_1=S_2=1/2\\) is: \\[\n\\ket{0,0}=\\frac{1}{\\sqrt{2}}\\left(\\ket{-\\frac{1}{2},\\frac{1}{2}}-\\ket{\\frac{1}{2},-\\frac{1}{2}}\\right)\n\\] which spans the one dimension subspace \\(\\mathcal{E}(S=0)\\) and is thus called a singlet state; the three states of \\(\\mathcal{E}(S=1)\\) are called a triplet \\[\n\\begin{align}\n&\\ket{1,1}=\\ket{1/2,1/2}\\\\\n&\\ket{1,0}=\\frac{1}{\\sqrt{2}}\\left(\\ket{-\\frac{1}{2},\\frac{1}{2}}+\\ket{\\frac{1}{2},-\\frac{1}{2}}\\right)\\\\\n&\\ket{1,-1}=\\ket{-1/2,-1/2}\n\\end{align}\n\\]"
  },
  {
    "objectID": "angular_momentum_qm.html#the-orbital-angular-momentum",
    "href": "angular_momentum_qm.html#the-orbital-angular-momentum",
    "title": "Angular Momentum in QM",
    "section": "The Orbital Angular Momentum",
    "text": "The Orbital Angular Momentum\n(DESCRIBES THE INTEGER \\(j\\)’s)\nSo far we described the framework of a generic angular momentum, which followed solely from its commutation relations; we want now to see a particular example of the \\(J^2\\) and \\(J_z\\) operators, namely, when they are the magnitude-squared of the orbital angular momentum of a particle and its z-component, to be explicit, let:\n\\[\nS_-\\ket{1/2}=\\hbar\\sqrt{1/2(1/2+1)-1/2(1/2-1)}\\ket{-1/2}=\\hbar\\ket{-1/2}\n\\]\nwhich can then be normalized by dividing it \\(\\hbar\\). On the other hand we have:\n\\[\n\\begin{align}\n&J_x := YP_z-ZP_y\\\\\n&J_y:=ZP_x-XP_z\\\\\n&J_z:= XP_y-YP_x \\\\\n&J^2:=J_x^2+J_y^2+J_z^2\n\\end{align}\n\\tag{15}\\]\nThe commutation relations Equation 1 and Equation 2 that we assumed in previous sections for the symbols on the lhs of Equation 15 also hold for the symbols on the rhs\nIn other words, the explicit form of \\(J_x\\), \\(J_y\\) and \\(J_z\\) that we defined in Equation 15, by itself satisfies Equation 1 and Equation 2 provided:\n\\[\n[X,P_x] = [Y,P_y] = [Z,P_z]=i\\hbar\n\\]\nAs an example:\n\\[\n\\begin{align}\n[J_x,J_y]&=[YP_z-ZP_y,ZP_x-XP_z]\\\\\n&=[YP_z,ZP_x]-[YP_z,XP_z]-[ZP_y,ZP_x]+[ZP_y,XP_z]\\\\\n&=Y[P_z,Z]P_x -0-0+X[Z,P_z]P_y\\\\\n&=-i\\hbar YP_x+i\\hbar XP_y\\\\\n&=i\\hbar (XP_y-YP_x)\\\\\n&=i\\hbar J_z\n\\end{align}\n\\tag{16}\\]\njust as we assumed in Equation 1 . Analogous calculation show our definitions satisfies the remaining commutation relations\n\n\n\n\n\n\nChange of Notation\n\n\n\nIt is traditional to use the symbols \\(L_x\\), \\(L_y\\) ,\\(L_z\\) and \\(L^2\\) in place of \\(J_x\\), \\(J_y\\) ,\\(J_z\\) and \\(J^2\\) when speaking about the orbital angular momentum. Recall the former notation is used for a generic angular momentum, not necessarily the orbital one.\n\n\nThe rhs of Equation 15 together with the choice of the position basis \\(\\{|x,y,z\\rangle\\}\\) allow us to give the position representation of the eigenvalue-eigenvector equations we saw in Equation 5:\n\\[\n\\begin{align}\n&\\langle x,y,z|L^2\\ket{l,m}=l(l+1)\\hbar^2\\langle x,y,z|\\ket{l,m}\\\\\n&\\langle x,y,z|L_z\\ket{l,m}=m\\hbar\\langle x,y,z|\\ket{l,m}\n\\end{align}\n\\tag{17}\\]\nSubstituting Equation 15 we find:\n\\[\n\\begin{align}&\\langle x,y,z|(L_x^2+L_y^2+L_z^2)\\ket{l,m}=l(l+1)\\hbar^2\\langle x,y,z|\\ket{l,m}\\\\\n&\\langle x,y,z|(XP_y-YP_x)\\ket{l,m}=m\\hbar\\langle x,y,z|\\ket{l,m}\\end{align}\n\\tag{18}\\]\n\n\n\n\n\n\nRecall\n\n\n\nThe \\(X\\) and \\(P_x\\) operators have the following representation in the \\(\\{|x\\rangle\\}\\) basis:\n\\[\n\\langle x|X|\\psi\\rangle = x\\langle x|\\psi\\rangle \\qquad \\langle x|P_x|\\psi\\rangle = \\frac{\\hbar}{i}\\frac{\\partial}{\\partial x}\\langle x|\\psi\\rangle\n\\]\n\n\nThe second equation in Equation 18 we find:\n\\[\n\\begin{align}\n\\langle x,y,z|L_z|\\psi\\rangle &= \\langle x,y,z|(XP_y-YP_x)|\\psi\\rangle\\\\\n&= \\langle x,y,z|(xP_y-yP_x)|\\psi\\rangle\\\\\n&= \\langle x,y,z|(x\\frac{\\hbar}{i}\\frac{\\partial}{\\partial y}-y\\frac{\\hbar}{i}\\frac{\\partial}{\\partial x})|\\psi\\rangle\\\\\n&= \\frac{\\hbar}{i}\\left(x\\frac{\\partial}{\\partial y}-y\\frac{\\partial}{\\partial x}\\right)\\langle x,y,z|\\psi\\rangle\\\\\n&= \\frac{\\hbar}{i}\\left(x\\frac{\\partial}{\\partial y}-y\\frac{\\partial}{\\partial x}\\right)\\psi(x,y,z)\n\\end{align}\n\\] Therefore, in the position basis, the \\(L_z\\) operator acts on functions \\(\\psi(x,y,z)\\) as the operation:\n\\[\nL_z \\rightsquigarrow \\frac{\\hbar}{i}\\left(x\\frac{\\partial}{\\partial y}-y\\frac{\\partial}{\\partial x}\\right)\n\\tag{19}\\]\nSimilarly we find:\n\\[\nL_x \\rightsquigarrow \\frac{\\hbar}{i}\\left(y\\frac{\\partial}{\\partial z}-z\\frac{\\partial}{\\partial y}\\right) \\qquad L_y \\rightsquigarrow \\frac{\\hbar}{i}\\left(z\\frac{\\partial}{\\partial x}-x\\frac{\\partial}{\\partial z}\\right)\n\\tag{20}\\]\nWe could substitute the rhs of Equation 19 and Equation 20 into the commutation relations and run an analogous calculation to Equation 16, the would conclude that they indeed satisfy those equations as well.\nIt will be useful to change from cartesian coordinates \\(x,y,z\\) into polar coordinates \\(r,\\theta,\\phi\\). The map between these coordinates is established by the equations:\n\\[\n\\begin{align}\n& x = r \\sin \\theta \\cos \\phi\\\\\n& y = r \\sin \\theta \\sin \\phi\\\\\n& z = r \\cos \\theta\n\\end{align}\n\\]\nprovided \\(r\\geq 0\\), \\(0\\leq\\theta\\leq\\pi\\) and \\(0\\leq\\phi&lt; 2\\pi\\)\nTo convert Equation 19 and Equation 20 into polar coordinates we proceed as follows:\n\nDraw a diagram:\n\n\n\nRelate the derivatives in cartesian and polar coordinates encoded in the diagram:\n\n\\[\n\\begin{pmatrix}\n\\partial_r\\psi\\\\\n\\partial_\\theta\\psi\\\\\n\\partial_\\phi\\psi\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\partial_r x & \\partial_r y & \\partial_r z\\\\\n\\partial_\\theta x & \\partial_\\theta y & \\partial_\\theta z\\\\\n\\partial_\\phi x & \\partial_\\phi y & \\partial_\\phi z\n\\end{pmatrix}\n\\begin{pmatrix}\n\\partial_x\\psi\\\\\n\\partial_y \\psi\\\\\n\\partial_z \\psi\n\\end{pmatrix}\n\\tag{21}\\]\n\nCompute the matrix (Easy)\n\n\\[\n\\begin{pmatrix}\n\\sin\\theta \\cos\\phi & \\sin\\theta \\sin \\phi & \\cos\\theta\\\\\nr \\cos\\theta \\cos\\phi & r \\cos\\theta \\sin \\phi & -r \\sin \\theta\\\\\n-r\\sin\\theta \\sin \\phi & r \\sin\\theta \\cos\\phi & 0\n\\end{pmatrix}\n\\]\n\nInvert the system Equation 21 (Hard, use Mathematica)\n\n\\[\n\\begin{pmatrix}\n\\partial_x\\psi\\\\\n\\partial_y\\psi\\\\\n\\partial_z\\psi\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\sin\\theta \\cos \\phi   & r^{-1}\\cos\\theta \\cos\\phi & -(r\\sin\\theta)^{-1} \\sin\\phi\\\\\n\\sin\\theta \\sin \\phi   & r^{-1}\\cos\\theta \\sin\\phi & (r\\sin\\theta)^{-1} \\cos\\phi\\\\\n\\cos\\theta & -r^{-1}\\sin\\theta & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n\\partial_r\\psi\\\\\n\\partial_\\theta \\psi\\\\\n\\partial_\\phi \\psi\n\\end{pmatrix}\n\\]\n\nSubstitute the results into Equation 19 and Equation 20 and simplify gives:\n\\[\n\\begin{align}\n&L_x \\rightsquigarrow i\\hbar\\left(\\sin\\phi \\frac{\\partial}{\\partial \\theta}+\\frac{\\cos\\phi}{\\tan\\theta} \\frac{\\partial}{\\partial \\phi}\\right)\\\\\n&L_y \\rightsquigarrow i\\hbar\\left(-\\cos\\phi \\frac{\\partial}{\\partial \\theta}+\\frac{\\sin\\phi}{\\tan\\theta} \\frac{\\partial}{\\partial \\phi}\\right)\\\\\n&Lz \\rightsquigarrow \\frac{\\hbar}{i} \\frac{\\partial}{\\partial \\phi}\n\\end{align}\n\\]\nFrom these results we can now derive the action of \\(L^2\\) in polar coordinates:\n\\[\n\\begin{align}\nL^2\\psi &= L_x^2\\psi + L_y^2\\psi+L_z^2\\psi\\\\\n&=(i\\hbar)^2\\left(\\sin\\phi \\frac{\\partial}{\\partial \\theta}+\\frac{\\cos\\phi}{\\tan\\theta} \\frac{\\partial}{\\partial \\phi}\\right)^2\\psi\n+(i\\hbar)^2\\left(-\\cos\\phi \\frac{\\partial}{\\partial \\theta}+\\frac{\\sin\\phi}{\\tan\\theta} \\frac{\\partial}{\\partial \\phi}\\right)^2\\psi+\n\\left(\\frac{\\hbar}{i}\\right)^2 \\frac{\\partial^2}{\\partial \\phi^2}\\psi\n\\end{align}\n\\tag{22}\\] Now we need to expand each square, the first becomes:\n\n\\[\n(i\\hbar)^2\\left(\\sin^2\\phi \\frac{\\partial^2\\psi}{\\partial \\theta^2}+ \\frac{\\cos^2\\phi}{\\tan^2\\theta}\\frac{\\partial^2\\psi}{\\partial \\phi^2}-\\frac{\\sin\\phi\\cos\\phi}{\\sin^2\\theta}\\frac{\\partial \\psi}{\\partial \\phi}+\\frac{\\sin\\phi\\cos\\phi}{\\tan\\theta}\\frac{\\partial^2 \\psi}{\\partial\\theta\\partial \\phi}\\right)\n\\]\nthe second term becomes:\n\\[\n(i\\hbar)^2\\left(\\cos^2\\phi \\frac{\\partial^2\\psi}{\\partial \\theta^2}+ \\frac{\\sin^2\\phi}{\\tan^2\\theta}\\frac{\\partial^2\\psi}{\\partial \\phi^2}+\\frac{\\cos\\phi\\sin\\phi}{\\sin^2\\theta}\\frac{\\partial \\psi}{\\partial \\phi}-\\frac{\\cos\\phi\\sin\\phi}{\\tan\\theta}\\frac{\\partial^2 \\psi}{\\partial\\theta\\partial \\phi}\\right)\n\\]\nSubstituting into Equation 22 and canceling terms yields the final result:\n\\[\nL^2 \\rightsquigarrow -\\hbar^2\\left(\\frac{\\partial^2}{\\partial \\theta^2} + \\frac{1}{\\sin^2\\theta} \\frac{\\partial^2}{\\partial \\phi^2} + \\frac{1}{\\tan\\theta}\\frac{\\partial}{\\partial \\theta}\\right)\n\\tag{23}\\]\nAs a consequence, the system of equations in Equation 18 becomes a system of partial differential equations:\n\\[\n\\begin{align}\n&-\\hbar^2 \\left(\\frac{\\partial}{\\partial \\theta^2}+\\frac{1}{\\tan \\theta} \\frac{\\partial}{\\partial \\theta}+\\frac{1}{\\sin^2\\theta}\\frac{\\partial}{\\partial^2 \\phi^2}\\right)\\psi_{l,m}(r, \\theta, \\phi)=l(l+1)\\hbar^2\\psi_{l,m}(r, \\theta, \\phi)\\\\\n& \\frac{\\hbar}{i} \\frac{\\partial}{\\partial \\phi}\\psi_{l,m}(r, \\theta, \\phi)=m\\hbar \\psi_{l,m}(r, \\theta, \\phi)\n\\end{align}\n\\tag{24}\\]\nThe solutions \\(\\psi_{l,m}(r,\\theta,\\phi)\\) are the eigenfunctions common to the \\(L^2\\) and \\(L_z\\) operators associated with the eigenvalues \\(l(l+1)\\hbar^2\\) and \\(m\\hbar\\). These solution exist because these two operators commute. Our ultimate goal is to solve them, but in these notes we’ll content ourselves by just identifying some of its properties."
  },
  {
    "objectID": "basis_of_vector_spaces.html",
    "href": "basis_of_vector_spaces.html",
    "title": "Definition of basis and its consequences",
    "section": "",
    "text": "Video\n\n\n\ndefinition of basis\nThe concept of basis is a natural one after the discussion on vector spaces so far.\nFor example take the vector space \\(\\mathbb{R}^2\\), we can promote the vectors \\(\\begin{pmatrix}1 \\\\0\\end{pmatrix}=:\\mathbf{e}_1\\) and \\(\\begin{pmatrix}0\\\\1\\end{pmatrix}=:\\mathbf{e}_2\\) to the status of basis vectors. Pictorially they look like the pink vectors:\nand by visual inspection alone it is clear they are independent, using analytics we confirm their independence as well by solving:\n\\[\n\\alpha\\mathbf{e}_1 +\\beta\\mathbf{e}_2 = \\mathbf{0} \\implies \\alpha\\begin{pmatrix}1 \\\\0\\end{pmatrix} +\\beta \\begin{pmatrix}0 \\\\1\\end{pmatrix} = \\begin{pmatrix}0 \\\\0\\end{pmatrix}\\implies \\alpha=\\beta=0\n\\]\nThe only solution are zeros, thus \\(\\mathbf{e}_1\\) and \\(\\mathbf{e}_2\\) are independent.\nWith \\(\\{\\mathbf{e}_1,\\mathbf{e}_2\\}\\) we can write any vector of \\(\\mathbb{R}^2\\) as some appropriate l.c. (choice of \\(c_1\\) and \\(c_2\\)):\n\\[\n\\mathbf{v} = c_1\\mathbf{e}_1+c_2\\mathbf{e}_2\\\n\\]\nReading this formula from left to right we can see that \\(\\mathbf{v}\\) is decomposed into two pieces, reading from right to left we see we are constructing the vector \\(\\mathbf{v}\\) . Given a basis, each vector in our vector space can be decomposed in a unique manner!\nThe numbers \\(c_1\\) and \\(c_2\\) are called the components of the vector \\(\\mathbf{v}\\) wrt to the chosen basis. How do we compute them is the subject of a later section.\nNotice that \\(\\{\\mathbf{e}_1,\\mathbf{e}_2\\}\\) is not the only basis we can choose for \\(\\mathbb{R}^2\\), in fact any two (and not three or four or…) vectors with distinct directions can be used, for example:\n\\[\n\\mathbb{R}^2 = span\\{\\mathbf{e}_1,\\mathbf{e}_2\\}=span\\{\\mathbf{e}'_1,\\mathbf{e}'_2\\}=span\\{\\mathbf{e}''_1,\\mathbf{e}''_2\\}=\\dots\n\\]\nWhich implies \\(\\mathbf{v}\\) can be written as:\n\\[\n\\mathbf{v} = c_1 \\mathbf{e}_1+c_2\\mathbf{e}_2=c_1' \\mathbf{e}_1'+c_2'\\mathbf{e}_2'=c_1'' \\mathbf{e}_1''+c_2''\\mathbf{e}_2''=\\dots\n\\]\nThe same \\(\\mathbf{v}\\) and multiple points of view suggests the components of different basis must be related, the subject of the Section on Change of Basis.\nObserve that:",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Basis of vector spaces"
    ]
  },
  {
    "objectID": "basis_of_vector_spaces.html#is-it-a-basis-or-not-of-which-vector-space",
    "href": "basis_of_vector_spaces.html#is-it-a-basis-or-not-of-which-vector-space",
    "title": "Definition of basis and its consequences",
    "section": "Is it a basis or not? Of which vector space?",
    "text": "Is it a basis or not? Of which vector space?\nLooking at our definition above we can see that any set of independent vectors constitute a basis of some vector space. Essentially to determine whether a set is a basis or not is the same as asking whether the vectors are or not independent, and this question was already addressed before.\nFor the moment lets see two examples.\nExample 1: Imagine the following spans:\n\\[\nA=\\{\\begin{pmatrix} 1\\\\2\\end{pmatrix},\\begin{pmatrix} 3\\\\0\\end{pmatrix},\\begin{pmatrix} 0\\\\3\\end{pmatrix}\\}\\qquad B=\\{\\begin{pmatrix} 1\\\\2\\end{pmatrix},\\begin{pmatrix} 3\\\\0\\end{pmatrix}\\}\n\\]\nOnly the \\(B\\) has the status of a basis. Why? See the definition above and checking the two criteria, \\(B\\) fulfills both but \\(A\\) does not fulfill the independence criteria.\nWe can check this by solving:\n\\[\n\\alpha \\begin{pmatrix} 1\\\\2\\end{pmatrix} +\\beta \\begin{pmatrix} 1\\\\0\\end{pmatrix}+\\gamma\\begin{pmatrix} 0\\\\1\\end{pmatrix}=\\begin{pmatrix} 0\\\\0\\end{pmatrix}\n\\]\nwhose solution is: \\(\\gamma=2\\), \\(\\alpha=1\\) and \\(\\beta= -1\\). The \\(B\\) basis, expands a space with dimension \\(2\\).\nExample 2: Consider now the st:\n\\[ C=\\{( 1,0,1),(0,0,1)\\}\\qquad D=\\{( 1,0,1),( 0,0,1),( 1,0,3)\\} \\]\nThe set \\(C\\) is a basis that expands a two dimensional space, because we have two vectors. We can actually say more, because these vectors live in \\(\\mathbb{R}^3\\) and hence this space is in fact a subspace of it. The set \\(D\\) is not a basis because the vectors are not independent; the solution is \\(\\alpha=1\\), \\(\\beta=2\\) and \\(\\gamma=-1\\). Note, by the criteria 1. of a basis, \\(C\\) is not a basis of \\(\\mathbb{R}^3\\).\n\n\n\n\n\n\nCommentary\n\n\n\n\nTo test the (in)dependence of a candidate set to basis vectors will be equivalent to solving a problem like \\(A\\mathbf{x}=\\mathbf{0}\\).\nIf we have a set of independent vectors, then they generate a vector space. We describe the space as: \\(A = span\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\}\\) and \\(\\dim A =n\\). Later we’ll see another way to describe the set: also through the equation of the form \\(\\tilde{A}\\mathbf{x}=\\mathbf{0}\\), whose solutions are the \\(\\mathbf{v}_1,\\dots\\mathbf{v}_n\\).\n\nSo, the same kind of problem \\(A\\mathbf{x}=\\mathbf{0}\\) will, as we shall see, occur in two situations: test (in)dependence of sets of vectors, generate a basis for a subspace.\n\n\nExercises: 1.5.1,2",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Basis of vector spaces"
    ]
  },
  {
    "objectID": "basis_of_vector_spaces.html#creating-the-appropriate-basis-for-a-vector-space",
    "href": "basis_of_vector_spaces.html#creating-the-appropriate-basis-for-a-vector-space",
    "title": "Definition of basis and its consequences",
    "section": "Creating the appropriate basis for a vector space?",
    "text": "Creating the appropriate basis for a vector space?\n\n\n\n\n\n\nVideo\n\n\n\nexample below\n\n\nExample 1: Imagine we have the following set \\(A=\\{(1,1,0),(1,0,1)\\}\\) how do we create from it a basis for \\(\\mathbb{R}^3\\)? Well, we have to append to this set one new vectors that point in different direction. That way we would have three independent vectors living in \\(\\mathbb{R}^3\\) which would allow us to cover the entire space by l.c. The new vector cannot be a l.c. of the vectors already present in \\(A\\).\nTo find the new vector in a systematic manner, we notice, a key aspect of it, since the missing vector belongs to \\(\\mathbb{R}^3\\), then it has the form \\((a,b,c)\\) for \\(a,b,c\\in \\mathbb{R}\\). Our problem, is thus to make the third column of the following matrix\n\\[\n\\begin{pmatrix}1 & 1 & a\\\\1 & 0 & b\\\\0 & 1 & c\\end{pmatrix}\n\\]\nindependent of the first two, to achieve that we simplify it through elimination:\n\\[\n\\begin{pmatrix}\n1 & 1 & a\\\\\n1 & 0 & b\\\\\n0 & 1 & c\n\\end{pmatrix}\n\\overset{l_2'=l_2-l_1}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 1 & a\\\\\n0 & -1 & b-a\\\\\n0 & 1 & c\\end{pmatrix}\n\\overset{l_2\\leftrightarrow l_3}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 1 & a\\\\\n0 & 1 & c\\\\\n0 & -1 & b-a\n\\end{pmatrix}\n\\overset{l_3'=l_3+l_2}{\\longrightarrow}\n\\begin{pmatrix}\n\\mathbf{1} & 1 & a\\\\\n0 & \\mathbf{1} & c\\\\\n0 & 0 & b-a+c\n\\end{pmatrix}\n\\]\nWe identify pivots in the first two columns, meaning they are independent, we want now to choose \\(a,b,c\\in \\mathbb{R}\\) so such that we gain a third pivot at position \\(A_{33}\\). For example: \\(b=1\\) and \\(a=c =0\\) guarantees that \\(b-a+c\\not =0\\), a new pivot emerged!\nWe conclude the set \\(A'=\\{(1,1,0),(1,0,1),(0,1,0\\}\\) has only independent vectors and is a basis of \\(\\mathbf{R}^3\\).\nExample 2: Assume not the set \\(A=\\{(0,0,1)\\}\\) and we want to complete \\(A\\) so as to turn it into a basis of the subspace \\(\\{(x,y,z)\\in\\mathbb{R}^3\\,\\,|\\,\\, y=2x\\}\\); notice this is our [pink-blue plane]. Since the missing vector belongs to this plane, then it must have the form \\((a,2a,b)\\). [Note: \\((0,0,1)\\) fits this form.]\nTo answer, we have to simplify:\n\\[\n\\begin{pmatrix}\n0 & a\\\\\n0 & 2a\\\\\n1 & b\n\\end{pmatrix}\n\\overset{l_1\\leftrightarrow l_3}{\\longrightarrow}\n\\begin{pmatrix}\n\\mathbf{1} & b\\\\\n0 & 2a\\\\\n0 & a\n\\end{pmatrix}\n\\]\nWe have one pivot in column 1 and want a second pivot in column 2. A possible solution to the candidate \\(2a\\) into a pivot is choose \\(b=0\\) and \\(a=1\\). Thus \\(A'=\\{(0,0,1),(1,2,0)\\}\\) is a basis for the [pink-blue subspace].\nExample 3: Suppose we have the following vector space \\(\\mathbb{V}=\\{(x,y)\\,\\,|\\,\\, x+2y-3z=0\\}\\). Note it is a two dimensional space because the three entries of the vector are constrained by the equation; another way to arrive at this conclusion is to recognize that the equation is the equation of plane perpendicular to \\((1,2,-3)\\) that passes (as any vector space should) through the origin. Choosing values for two of the variables and solving for the third yields the desired vectors, for example, let \\(z=0\\) and \\(y=-1/2\\), then \\(x=1\\); choosing \\(y=0\\) and \\(z=1\\) gives us \\(x=3\\). These calculation gave us two basis vectors:\n\\[\nC=\\{(1,-1/2,0),(3,0,1)\\}\n\\]\nwhich are clearly independent.\nExercises: 1.5.3,6,7",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Basis of vector spaces"
    ]
  },
  {
    "objectID": "collision theory intro.html",
    "href": "collision theory intro.html",
    "title": "collision theory intro",
    "section": "",
    "text": "Two particles which the only relevant interaction is local when compared with interaction at long range, long range interaction is weak enough that the particles move with constant velocity. When these particles get close enough their straight line trajectories are changed, to this process we call a collision process. (See Sec. 10.6 Greg) In general collisions can give rise to chemical reactions and thus new particles described in general by: \\[A_1+A_2\\longrightarrow B_1 +B_2+B_3\\]We will not deal with these types of reactions but of reactions where the identity of each particle is preserved, we call these scattering reactions. Schematically we have: \\[A_1+A_2\\longrightarrow A_1+A_2\\]An interactions is elastic if none of internal degrees of freedom of the participants is changed during the process. (Cohen Chap VIII A-1) The analysis of the collision process between two particles can be done exactly, by computing the trajectory of both particles from the initial conditions and using Newtons Laws. This can be done in any reference frame in particular either the Lab frame or the CM frame. We’ll discuss both. We can also describe the collision process with statistics, where we attach a probability law to the final velocity vectors given initial ones. We want to focus in these notes on the connection between the rigorous approach and the statistics one.\n\\[\\text{CM}\\longrightarrow \\text{Col. Cross section}\\longrightarrow \\text{Prob Law}\\longrightarrow \\text{averages}\\] The collision cross section is an intermediary quantity that occurs in the probability law (function composition)"
  },
  {
    "objectID": "collision theory intro.html#what-is-a-collision",
    "href": "collision theory intro.html#what-is-a-collision",
    "title": "collision theory intro",
    "section": "",
    "text": "Two particles which the only relevant interaction is local when compared with interaction at long range, long range interaction is weak enough that the particles move with constant velocity. When these particles get close enough their straight line trajectories are changed, to this process we call a collision process. (See Sec. 10.6 Greg) In general collisions can give rise to chemical reactions and thus new particles described in general by: \\[A_1+A_2\\longrightarrow B_1 +B_2+B_3\\]We will not deal with these types of reactions but of reactions where the identity of each particle is preserved, we call these scattering reactions. Schematically we have: \\[A_1+A_2\\longrightarrow A_1+A_2\\]An interactions is elastic if none of internal degrees of freedom of the participants is changed during the process. (Cohen Chap VIII A-1) The analysis of the collision process between two particles can be done exactly, by computing the trajectory of both particles from the initial conditions and using Newtons Laws. This can be done in any reference frame in particular either the Lab frame or the CM frame. We’ll discuss both. We can also describe the collision process with statistics, where we attach a probability law to the final velocity vectors given initial ones. We want to focus in these notes on the connection between the rigorous approach and the statistics one.\n\\[\\text{CM}\\longrightarrow \\text{Col. Cross section}\\longrightarrow \\text{Prob Law}\\longrightarrow \\text{averages}\\] The collision cross section is an intermediary quantity that occurs in the probability law (function composition)"
  },
  {
    "objectID": "collision theory intro.html#center-of-mass-position-and-relative-position",
    "href": "collision theory intro.html#center-of-mass-position-and-relative-position",
    "title": "collision theory intro",
    "section": "Center of Mass Position and Relative Position",
    "text": "Center of Mass Position and Relative Position\nThe center of mass is given by the following formula:\n\\[\n\\mathbf{R} = \\frac{m_1 }{M}\\mathbf{r}_1 +\\frac{m_2 }{M}\\mathbf{r}_2\\qquad M=m_1+m_2\n\\tag{1}\\]\na linear combination of the current positions weighted by the relative mass.\nWhile the relative position, as the name suggests is just:\n\\[\n\\mathbf{r} = \\mathbf{r}_1-\\mathbf{r}_2\n\\]\nBoth measured with respect to \\(\\mathcal{O}\\).\nThese two formulas allow us to compute the new generalized coordinates \\(\\mathbf{R}\\) and \\(\\mathbf{r}\\) from \\(\\mathbf{r}_1\\) and \\(\\mathbf{r}_2\\). Inverting them we get:\n\\[\n\\begin{cases}\n\\mathbf{r}_1 = \\mathbf{R} +\\frac{m_2}{M}\\mathbf{r}\\\\\n\\mathbf{r}_2 = \\mathbf{R} -\\frac{m_1}{M}\\mathbf{r}\n\\end{cases}\n\\tag{2}\\] we choose the reference frame attach located at \\(\\mathbf{R}\\), i.e., we choose \\(\\mathcal{O} = \\mathcal{O}_{cm}\\), then Equation 2 becomes:\n\\[\n\\begin{cases}\n\\mathbf{r}_{1,cm} = \\frac{m_2}{M}\\mathbf{r}\\\\\n\\mathbf{r}_{2,cm} =  -\\frac{m_1}{M}\\mathbf{r}\n\\end{cases}\n\\tag{3}\\]\nwhere we introduce the subscript \\(cm\\) to reminds us that the components of these vectors are with respect to the new frame; absence of subscript, like in \\(\\mathbf{r}\\) mean the components are with respect to the old reference frame."
  },
  {
    "objectID": "collision theory intro.html#total-momentum",
    "href": "collision theory intro.html#total-momentum",
    "title": "collision theory intro",
    "section": "Total Momentum",
    "text": "Total Momentum\nThe total linear momentum of the particles is the sum of the momentum of particle \\(1\\) and \\(2\\). This quantity is related with \\(\\mathbf{R}\\) as we see:\n\\[\n\\mathbf{P} = \\mathbf{p}_1+\\mathbf{p}_2=m_1\\dot{\\mathbf{r}}_1+m_2\\dot{\\mathbf{r}}_2=M\\dot{\\mathbf{R}}\n\\tag{4}\\]\nDifferentiating we find:\n\\[\n\\dot{\\mathbf{P}} = \\mathbf{F}_{1,tot}+\\mathbf{F}_{2,tot}=M\\ddot{\\mathbf{R}}\n\\]\nThe total force on \\(1\\) can be split into a sum of the external forces acting on \\(1\\) plus the force \\(2\\) does on \\(1\\), similar split is done for the particle \\(2\\). Since internal forces cancel, we know \\(\\mathbf{F}_{1,tot}+\\mathbf{F}_{2,tot}=\\mathbf{F}_{ext,tot}\\). We conclude:\n\\[\n\\mathbf{F}_{ext,tot}=M\\ddot{\\mathbf{R}}\n\\] This formula has a very important consequence:\n\nIf the external forces are zero then the acceleration of the center of mass is zero.\nIf the acceleration of the center of mass is zero, then its velocity is zero or constant.\nIf the center of mass velocity is zero or constant, then a reference frame attach to it \\(\\mathcal{O}_{cm}\\) is an inertial reference frame.\n\nThe frame \\(\\mathcal{O}_{cm}\\) will be a useful definition when studying collisions. Because in this frame the components of velocity of the center of mass are zero \\(\\dot{\\mathbf{R}}_{cm}=\\mathbf{0}\\) and as a result by Equation 4 we have:\n\\[\n\\begin{align}\n&\\mathbf{p}_1+\\mathbf{p}_2=\\mathbf{0}\\\\\n\\implies & \\mathbf{p}_1=-\\mathbf{p}_2\n\\end{align}\n\\tag{5}\\]\nMoreover, in this new frame the measurements of \\(\\dot{\\mathbf{r}}_1\\) and \\(\\dot{\\mathbf{r}}_2\\) become using Equation 3 :\n\\[ \\begin{cases} \\dot{\\mathbf{r}}_{1,cm} = \\frac{m_2}{M}\\dot{\\mathbf{r}}\\\\ \\dot{\\mathbf{r}}_{2,cm} =  -\\frac{m_1}{M}\\dot{\\mathbf{r}} \\end{cases} \\]\nwhich leads us to explicit formulas for the linear momentum:\n\\[\n\\begin{cases}\n\\mathbf{p}_{1,cm} = m_1 \\dot{\\mathbf{r}}_{1,cm} = \\mu\\dot{\\mathbf{r}}\\\\\n\\mathbf{p}_{2,cm} = m_2 \\dot{\\mathbf{r}}_{2,cm} = -\\mu\\dot{\\mathbf{r}}\n\\end{cases}\n\\tag{6}\\]\nAs expected they satisfy the equation Equation 5 ."
  },
  {
    "objectID": "collision theory intro.html#kinetic-energy",
    "href": "collision theory intro.html#kinetic-energy",
    "title": "collision theory intro",
    "section": "Kinetic Energy",
    "text": "Kinetic Energy\nThe total kinetic energy of the particles is the sum of the kinetic energies, using the usual formulas we have wrt to any \\(\\mathcal{O}\\):\n\\[\nT = \\frac{1}{2} m_1 \\dot{\\mathbf{r}}_1+\\frac{1}{2} m_2 \\dot{\\mathbf{r}}_2\n\\]\nAs we have seen in Equation 2, from \\(\\mathbf{r}_1\\) and \\(\\mathbf{r}_2\\) we can compute the generalized coordinates \\(\\mathbf{R}\\) and \\(\\mathbf{r}\\). Differentiating, relates the velocities:\n\\[\n\\begin{cases}\n\\dot{\\mathbf{r}}_1 = \\dot{\\mathbf{R}} +\\frac{m_2}{M}\\dot{\\mathbf{r}}\\\\\n\\dot{\\mathbf{r}}_2 = \\dot{\\mathbf{R}} -\\frac{m_1}{M}\\dot{\\mathbf{r}}\n\\end{cases}\n\\]\nThese equations are valid for velocities measured wrt to any \\(\\mathcal{O}\\) of our choice, they are general.\nSubstituting in \\(T\\) we find:\n\\[\nT = \\frac{1}{2} M\\dot{\\mathbf{R}}^2+\\frac{1}{2} \\mu\\dot{\\mathbf{r}}^2\\qquad \\mu = \\frac{m_1m_2}{M}\n\\tag{7}\\]\nThe total kinetic energy of \\(1\\) plus \\(2\\) at time \\(t\\) measured wrt to \\(\\mathcal{O}\\) is equal to the sum of the kinetic energy of a massive particle \\(M\\) moving with velocity \\(\\dot{\\mathbf{R}}\\) plus a smaller particle \\(\\mu\\) moving with velocity \\(\\dot{\\mathbf{r}}\\), both again, measured wrt to \\(\\mathcal{O}\\).\nThe energy in Equation 7 is NOT the kinetic energy wrt to \\(\\mathcal{O}_{cm}\\), it is just another way of computing the kinetic energy wrt to \\(\\mathcal{O}\\). Note that both \\(\\dot{\\mathbf{R}}\\) and \\(\\dot{\\mathbf{r}}\\) have components with respect with the frame of reference.\nIf we choose the \\(\\mathcal{O}_{cm}\\) frame of reference then \\(\\dot{\\mathbf{r}}_1\\) and \\(\\dot{\\mathbf{r}}_2\\) in Equation 3 become:\n\\[\n\\begin{cases}\n\\dot{\\mathbf{r}}_{1,cm} = \\frac{m_2}{M}\\dot{\\mathbf{r}}\\\\\n\\dot{\\mathbf{r}}_{2,cm} =  -\\frac{m_1}{M}\\dot{\\mathbf{r}}\n\\end{cases}\n\\]\nThese formulas are specific for velocities measured wrt center of mass reference frame.\nThe total kinetic energy as measured in \\(\\mathcal{O}_{cm}\\) becomes after simplifying:\n\\[\nT_{cm} = \\frac{1}{2} m_1 \\dot{\\mathbf{r}}_{1,cm}+\\frac{1}{2} m_2 \\dot{\\mathbf{r}}_{2,cm} =  \\frac{1}{2}\\mu \\dot{\\mathbf{r}}^2\n\\]"
  },
  {
    "objectID": "collision theory intro.html#elastic-collisions",
    "href": "collision theory intro.html#elastic-collisions",
    "title": "collision theory intro",
    "section": "Elastic Collisions",
    "text": "Elastic Collisions\nOur goal is to describe the momenta and kinetic energy in a two particle elastic collision. If we assume there is no external forces acting on the particles the center of mass position moves at constant velocity and therefore constitute a inertial reference frame \\(\\mathcal{O}_{cm}\\). This is the first point of view we’ll adopt. The second point of view is a special case of a lab reference frame - the conventional lab frame for collisions - it is set in such a way that particle \\(2\\), the target is initially at rest. I would like to emphasize the key word initially at rest, that means that in the reference frame, after the collision, the particle \\(2\\) is no longer at rest.\nSee Fig. 14.14 Taylor\n\nIn \\(\\mathcal{O}_{cm}\\) the initial and final momenta of both particles is given by Equation 6\n\\[\n\\begin{align}\n\\mathbf{p}_{1,cm} = -\\mathbf{p}_{2,cm} \\equiv \\mathbf{p}_{cm}= \\mu \\dot{\\mathbf{r}}\\\\\n\\mathbf{p}_{1,cm}' = -\\mathbf{p}_{2,cm}' \\equiv \\mathbf{p}_{cm}'=\\mu \\dot{\\mathbf{r}}'\n\\end{align}\n\\tag{8}\\]\nIn \\(\\mathcal{O}_{lab}\\) we find\n\\[\n\\begin{align}\n&\\mathbf{p}_{1,lab} =\\lambda \\mathbf{p}+\\mathbf{p}\\\\\n&\\mathbf{p}_{1,lab}' = \\lambda \\mathbf{p}+\\mathbf{p}'\\qquad \\lambda =\\frac{m_1}{m_2}\\\\\n&\\mathbf{p}_{2,lab}=\\mathbf{p}'_{2,lab}=\\mathbf{0}\n\\end{align}\n\\]\nproof: In this reference frame, \\(\\mathbf{p}_{1,lab} =m_1 \\dot{\\mathbf{r}}_1\\) which when written in generalized coordinates of center of mass plus relative distance become:\n\\[\n\\begin{align}\n\\mathbf{p}_{1,lab} &=m_1 \\dot{\\mathbf{r}}_1\\\\\n&=m_1(\\dot{\\mathbf{R}}+\\frac{m_2}{M}\\dot{\\mathbf{r}})\\\\\n&=m_1\\dot{\\mathbf{R}}+\\mu\\dot{\\mathbf{r}}\n\\end{align}\n\\tag{9}\\]\n\nNow, for particle \\(2\\) we also write \\(\\mathbf{p}_{2,lab} =m_2 \\dot{\\mathbf{r}}_2\\), but since this particle is at rest in the lab frame we have \\(\\mathbf{p}_{2,lab}=\\mathbf{0}\\). Additionally, from the definition of center of mass Equation 1 we simplify Equation 9 using the observation that:\n\\[\n\\begin{cases}\n\\dot{\\mathbf{R}} = \\frac{m_1}{M}\\dot{\\mathbf{r}}_1=\\frac{m_1}{M}\\dot{\\mathbf{r}} =\\frac{\\mu}{m_2}\\dot{\\mathbf{r}}=\\frac{\\mathbf{p}}{m_2}\\\\\n\\mathbf{r}=\\mathbf{r}_1-\\mathbf{r}_2 = \\mathbf{r}_1\n\\end{cases}\n\\tag{10}\\]\nwhere at the last step we used Equation 8; the momentum \\(\\mathbf{p}\\) is of a fictitious particle with mass \\(\\mu\\). Substituting this result in Equation 9 we find:\n\\[\n\\mathbf{p}_{1,lab} = m_1\\dot{\\mathbf{R}}+\\mu\\dot{\\mathbf{r}}=\\lambda \\mathbf{p}+\\mathbf{p}\\qquad \\lambda =\\frac{m_1}{m_2}\n\\]\nThe momentum of \\(1\\) after collision has the same form as Equation 9 :\n\\[\n\\mathbf{p}'_{1,lab}=m_1\\dot{\\mathbf{R}}'+\\mu\\dot{\\mathbf{r}}'\n\\tag{11}\\]\nObserve that \\(\\mu \\dot{\\mathbf{r}}'=\\mathbf{p}'\\); since \\(\\mathbf{F}_{1,ext}=\\mathbf{0}\\) then \\(M\\dot{\\mathbf{R}} = M\\dot{\\mathbf{R}}'\\), which means the velocity of the center of mass is the same before and after the collision and as a result we can compute it using Equation 10:\n\\[\n\\dot{\\mathbf{R}}' = \\dot{\\mathbf{R}}=\\frac{\\mathbf{p}}{m_2}\n\\]\nSubstituting in Equation 11 yields:\n\\[\n\\mathbf{p}_{1,lab}' = \\lambda \\mathbf{p}+\\mathbf{p}'\\qquad \\lambda =\\frac{m_1}{m_2}\n\\]\nThe momentum of particle \\(2\\) after the collision is computed as:\n\\[\n\\begin{align}\n\\mathbf{p}_{2,lab}' &= \\mathbf{p}_{1,lab}+\\mathbf{p}_{2,lab}-\\mathbf{p}_{1,lab}'\\\\\n&=\\lambda \\mathbf{p}+\\mathbf{p}+\\mathbf{0}-\\lambda \\mathbf{p}-\\mathbf{p}'\\\\\n&=\\mathbf{p}-\\mathbf{p}'\n\\end{align}\n\\]\nWe further find that for this particular lab frame where \\(2\\) is at rest:\n\\[\nT_{lab}=\\frac{M}{m_2}T_{cm}\n\\]\nproof: The kinetic energy after the collision is the sum of kinetic energies of both particles, particle \\(2\\) this time has nonzero kinetic energy, we can prove that this sum yields the result above, although an easier way is to notice that kinetic energy is conserved since the collision is elastic and computing the kinetic energy before the collision (when particle \\(2\\) is still at rest) is simple:\n\\[\n\\begin{align}\nT_{lab}&=\\frac{\\mathbf{p}_{1,lab}^2}{2 m_1}+\\frac{\\mathbf{p}_{2,lab}^2}{2 m_2}\\\\\n&=\\frac{(\\lambda \\mathbf{p}+\\mathbf{p})^2}{2m_1}+\\mathbf{0}\\\\\n&=\\frac{(\\lambda +1)^2}{2m_1}\\mathbf{p}^2+\\mathbf{0}\\\\\n&=\\frac{((m_1+m_2)/m_2)^2}{2m_1}\\mathbf{p}^2\\\\\n&=\\frac{m_1}{2\\mu^2}\\mathbf{p}^2\\\\\n&=\\frac{M}{m_2}T_{cm}\n\\end{align}\n\\]\n\\[\n\\begin{align}\nT&=\\frac{\\mathbf{p}_{1,lab}'^2}{2 m_1}+\\frac{\\mathbf{p}_{2,lab}'^2}{2 m_2}\\\\\n&=\\frac{(\\lambda \\mathbf{p}+\\mathbf{p}')^2}{2m_1}+\\frac{(\\mathbf{p}-\\mathbf{p}')^2}{2m_2}\\\\\n&=\n\\end{align}\n\\]"
  },
  {
    "objectID": "collision theory intro.html#inelastic-collisions",
    "href": "collision theory intro.html#inelastic-collisions",
    "title": "collision theory intro",
    "section": "Inelastic Collisions",
    "text": "Inelastic Collisions\nIn special relativity we know that the relativistic energy of a moving particle is given by:\n\\[\nE = p_4c=\\gamma m c^2\\sim m c^2 +\\frac{1}{2}m v^2\\qquad v\\ll c\n\\]\nThe limit being called the non-relativistic regime.\nConsider a collision between two particles \\(1\\) and \\(2\\). If the system is isolated then there is conservation of four-momentum which implies conservation of relativistic energy, because this is the fourth component of four-momentum. In the non-relativistic regime the conservation of energy is:\n\\[\n\\begin{align}\n&m_1^ic^2+\\frac{1}{2}m_1^i(v_1^i)^2+m_2^ic^2+\\frac{1}{2}m_2^i(v_2^i)^2 \\\\\n&=m_1^fc^2+\\frac{1}{2}m_1^f(v_1^f)^2+m_2^fc^2+\\frac{1}{2}m_2^f(v_2^f)^2\n\\end{align}\n\\]\nwhich is equivalent to:\n\\[\n\\begin{align}\n&\\overbrace{(m_1^i+m_2^i)}^{M^i}c^2+\\overbrace{\\frac{1}{2}m_1^i(v_1^i)^2+\\frac{1}{2}m_2^i(v_2^i)^2}^{T^i} \\\\\n&=\\underbrace{(m_1^f+m_2^f)}_{M^f}c^2+\\underbrace{\\frac{1}{2}m_1^f(v_1^f)^2+\\frac{1}{2}m_2^f(v_2^f)^2}_{T^f}\n\\end{align}\n\\tag{12}\\]\nIf we assume, as we do in classical mechanics, that the mass of particles does not change then mass-energy terms cancel and the equality above reduces to:\n\\[\nT^i=T^f\n\\]\nThe kinetic energy is conserved and we call it an elastic collision.\nIn an inelastic collision such as a collision between two atoms their internal energy (mass-energy terms) can be disturbed and as a result their kinetic energies before and after cannot be the same \\(T^i\\not = T^f\\), as a result from Equation 12, the masses before and after cannot be the same \\(M^i \\not = M^f\\).\nIf relativistic energy is conserved, then the mass cannot be conserved. When an atom gains internal energy, it gains mass.\nThe following formula Equation 12 is equivalent to:\n\\[\nQ\\equiv \\Delta M c^2 =-\\Delta T\n\\]\nHere are some cases:\n\nIn an exothermic reaction \\(\\Delta T &gt;0\\), thus \\(\\Delta M&lt;0\\), the total rest mass of products of the reaction is smaller than the total mass of the reactants.\nIn an endothermic reaction the scenario signs are reversed.\nIn an elastic collision then \\(\\Delta T =0\\) and as we have discussed the rest mass is unchanged \\(\\Delta M =0\\).\nExcitation reaction:\n\\[\ne + Hg\\longrightarrow e+Hg^*\n\\]\nThe electron does not change its mass due the collision because it has no internal structure, the mercury atom increases its quantum energy by \\(4.9eV\\), it becomes an excited atom. The kinetic energy of the emerging particles is smaller by \\(4.9eV\\), we write \\(\\Delta T = -4.9 eV\\), thus:\n\\[\n\\begin{align}\n&[(m^f_e+m^f_{Hg})-(m^i_e+m^i_{Hg})]c^2 = -\\Delta T = 4.9 eV\\\\\n\\implies &[m^f_{Hg}-m^i_{Hg}]=\\frac{4.9 eV}{c^2}=8.7\\times 10^{-36}kg\n\\end{align}\n\\]"
  },
  {
    "objectID": "collision theory intro.html#using-binomial-to-describe-a-collision",
    "href": "collision theory intro.html#using-binomial-to-describe-a-collision",
    "title": "collision theory intro",
    "section": "Using Binomial to describe a collision",
    "text": "Using Binomial to describe a collision\nWhen a particle \\(A_1\\) moves toward \\(A_2\\) either it collides or not depending on the parameters \\(b\\) and \\(\\sigma\\). Since I do not know \\(b\\), neither I know the nature of the interaction process, let us fill the vacuum with assumptions on what is going on. We can either make assumptions on the nature of the one interaction between two particles and then derive the resulting trajectory. Or I could make assumption on what I think would happen if the scattering process is repeated many times or instead I could make assumption on the process on scattering of one \\(A_1\\) and many \\(A_2\\). For the latter assume \\(A_2\\) particles are uniformly distributed in space. If \\(N_1\\) particles where send one by one (or at once) toward \\(N_2\\) particles, the \\(A_1\\) particle would see a density \\(n_2\\).\n\nand for each \\(A_1\\) incoming we would have a Bernoulli trial with probability: \\[p(\\text{hit one $A_2$})=\\frac{N_2 \\sigma}{A}=n_2\\sigma\\]Note that the cross section \\(\\sigma\\) describes only whether a particle \\(A_2\\) is hit or not, and does not care about the direction of the scattered \\(A_1\\). Later we’ll define a new quantity the scattering cross section which also takes into account this direction.\nAs a result we have the Binomial probability law on the random variable \\(X=[x \\,\\,\\text{particles hit}]\\): \\[P(X=x)=\\binom{N_1}{x}p^x(1-p)^{N_1-x} \\] The expected value of this r.v. is: \\[E[X]=N_1 p = N_1 n_2 \\sigma\\] which we rename as \\(N_{sc}\\) just as Taylor does in formula (14.2) of Chap. 14, also we rename \\(N_{inc}:=N_1\\), \\(n_{tar} :=n_2\\):\n\\[\nN_{sc}= N_{inc}n_{tar} \\sigma\n\\tag{13}\\]\nA schematic situation is shown in the following picture\n\nThis diagram is a sequence of \\(N\\) Bernoulli trials which are in turn characterized by the parameter \\(p\\), to the attached the Binomial probability law. Mathematically the diagram shows a sequence of trials which can succeed or not. Physically each trial can be interpreted as we want, the probability law is the same formula. For example, each branch can be seen as a trial for hit or miss of \\(N=3\\) incoming particles toward a uniformly distributed set of target particles which yield a probability of hit given by \\(n_2\\sigma\\). Another example is consider just one incoming particle toward the targets and each trial describes an encounter which can result in a hit or miss. Either way the Bernoulli formula remains exactly the same.\n\n\n\n\n\n\nCommentary\n\n\n\nThe two examples focus on a vertical and horizontal slices of an ensemble. In the horizontal slice view, we have one incoming particle and \\(N_1\\) trials (for \\(t\\) seconds) with probability of hit given by \\(p\\). In this case \\(P(X=x)\\) should be interpreted as the probability of the same molecule collide \\(x\\) times in \\(N\\) trials. In particular \\(P(X=1)\\) is the probability that one particle collides just once in \\(N_1\\) trials. \\(P(X=1)\\) does NOT mean the probability that one particle survives \\(N_1-1\\) times and then collides at the \\(N_1\\) trial, this probability would be given by \\(p(1-p)^{N_1-1}\\) just as written in p. 466 of Reif."
  },
  {
    "objectID": "collision theory intro.html#different-processes-and-targets",
    "href": "collision theory intro.html#different-processes-and-targets",
    "title": "collision theory intro",
    "section": "Different processes and Targets",
    "text": "Different processes and Targets\nWhen \\(A_1\\) collides with \\(A_2\\) other processes can occur:\n\nIt is scattered into a new direction, the effective area of \\(A_2\\) that causes this effect is called \\(\\sigma_{sc}\\) (it is the \\(\\sigma\\) seen above. This process can occur elastically, in this case, the kinetic energy of \\(A_1\\)is unchanged and so it the internal energy (mass-energy); or can occur inelatically, in this case the kinetic energy changes. We can introduce this distinction as:\n\\[\n\\sigma_{sc}=\\sigma_{el}+\\sigma_{inel}\n\\]\nIt is absorbed, if we imagine \\(A_1\\) hitting the region of \\(A_2\\) that is like putty, then \\(A_1\\) becomes part of \\(A_1\\), it is absorbed. In this case we define the effective region responsible for this effect as \\(\\sigma_{cap}\\) and the relation between the incoming particles and those absorbed is\n\\[\nN_{cap} = N_{inc} n_{tar} \\sigma_{cap}\n\\]\nIt ionizes, in this case we define \\(\\sigma_{ion}\\).\n\nExample: When an electron hits a chlorine atom, it can either be scattered into a new direction, be absorbed (chlorine can absorb an extra electron) or be ionized (one electron from chlorine is removed). The overall relation is:\n\\[\nN_{tot}= N_{inc}n_{tar}\\sigma_{tot} \\qquad \\sigma_{tot} = \\overbrace{\\sigma_{el}+\\sigma_{inel}}^{\\sigma_{sc}}+\\sigma_{cap}+\\sigma_{ion}\n\\]\nEach one of these effective cross section depends on the kinetic energy of the inc electron, for example:\n\\[\n\\sigma_{ion}=\n\\begin{cases}\n\\text{positive }\\qquad &T_{inc}\\geq E_{ionization}\\\\\n0\\qquad &T_{inc}&lt;E_{ionization}\n\\end{cases}\n\\]\nThe energy of ionization is the energy (eigenvalue) is the energy of the outermost electron (wave function with largest radius)"
  },
  {
    "objectID": "collision theory intro.html#using-poisson-distribution-to-describe-collisions",
    "href": "collision theory intro.html#using-poisson-distribution-to-describe-collisions",
    "title": "collision theory intro",
    "section": "Using Poisson distribution to describe collisions",
    "text": "Using Poisson distribution to describe collisions\nIf we assume we send each one of \\(N_1\\) particles each \\(dt\\) seconds (i.e. this is just a way to guarantee each incoming particle does not interact with each other) and make \\(N_1\\) larger and larger while \\(p\\) is fixed, the Bernoulli distribution formula is the same as above, but the expected value gets also larger and larger; this is one way to compute the limit distribution, lets see another, more useful one. Take the limit of \\(N_1\\) while keeping the average value constant instead. Pictorially we have:\n\n\n\nA particle is sent each dt seconds, either it hits or misses the array of targets. In t seconds it is sent N=infinity particles.\n\n\nDefine now the r.v. \\(X=x \\,\\,\\text{particles hit in $t$ seconds}\\), then using the Binomial prob law we find: \\[P(X=x)=\\binom{N_1}{x}p^x\\left(1-p\\right)^{N_1-x}\\] The expected value is \\(E[X]= p N_1\\). We would like to take now \\(N_1\\rightarrow \\infty\\), but this entails the expected value to go to infinity if we assume \\(p\\) constant. For the present scenario a better model is to assume the expected value is constant, i.e., we expect in any \\(t\\) seconds to find on average the same number of hits. To fix the expected value lets rename it as the product \\(w t\\). Hence let us introduce the quantity \\(w\\) such that: \\[w t\\coloneqq N_1 p =N_1 dt\\, w\\] As a consequence of fixing the average value though the fix of \\(w\\) we find as a side effect: \\[p(\\text{hit between $t$ and $t+dt$})\\coloneqq w dt =\\frac{n_2\\sigma}{t}dt\\] We see now that \\(w\\) is the probability of hit per unit time. And \\(dt=t/N_1\\). As a result the probability law is given by \\[P(X=x)=\\binom{N_1}{x}\\left(\\frac{w t}{N_1}\\right)^x\\left(1-\\frac{w t}{N_1}\\right)^{N_1-x}\\] The expected value is fix at \\(E[X]= p N_1=w t\\) in this limit.\n\n\n\n\n\n\nCommentary\n\n\n\n_We need not define the expected value as \\(wt\\), we could have just defined instead \\(E[X]\\coloneqq \\lambda\\) and fix \\(\\lambda\\), this is done in Sec. 3.8 of Wackerly. Setting it to \\(wt\\) was just a psychologically convenient way to make the parameter \\(t\\) appear in the equation. Note the Poisson distribution depends on \\(wt\\) and not on \\(t\\) or \\(w\\) alone.\n\n\nHaving established all parameters and their dependencies and what is constant, we only have one independent variable in the probability law. Taking the limit as \\(N_1\\) goes to infinity we arrive at: \\[P(X=x)=\\frac{(w t)^x}{x!}e^{-w t}\\] Which is the Poisson distribution. But what does it mean? Going back to the diagram above, we have an infinite number of trials between \\(0\\) and \\(t\\) seconds with a small probability \\(p\\), we can interpret them by imagining a set of infinite particles incoming to an uniformly distributed set of target particles each branch the correspond to the trial of a given particle; note how this interpretation is not good, because we would guess that the probability for each hit or miss is finite and not the small number \\(n_2\\sigma dt/t\\). Let us consider a second interpretation, one particle incoming toward the targets and at each trial (which occurs in \\(dt\\) seconds) decides whether it hit or miss, with probability that must ensure the macroscopic observed value of \\(wt\\) hits in \\(t\\) seconds. Under this last interpretation, \\(P(X=0)\\) is \\(e^{-w t}\\) is the probability of that particle suffering no collisions in \\(t\\) seconds at the collision rate \\(w\\). \\(P(X=1)=wt e^{-w t}\\) is the probability that the one incoming particle to collide once in \\(t\\) seconds, but it takes into account, see \\(wt\\) (represented by the binomial coefficient in Bernoulli) the fact that the collision can occur at any time during those \\(t\\) seconds.\nNotice that \\(P(X=1)\\) does NOT mean it survives every trial between \\(0\\) and \\(t\\) and then collides. That quantity is computed in p. 466 and 467 of Reif. and here is the gist of it, fixing the particular branch described by \\[\\overbrace{tt\\dots tth}^N\\]where t=miss and h= hit. We know that it has probability \\((1-p)^{N-1}p\\) of occurring (it is the [[Bernoulli process#^b9ae87|Geometric distribution]]), we are not interested in any other sequence of hits and misses other than \\(tt \\dots h\\), hence we did not take into account the \\(\\binom{N}{1}\\) factor, substituting \\(p=w\\,dt\\): \\[P(\\text{survives for $N-1$ trials and then hit})=\\left(1-w\\,dt\\right)^{N-1}w\\, dt=\\left(1-\\frac{wt}{N}\\right)^{N-1}w \\frac{t}{N}\\] this probability law (for this fixed branch) only depends on \\(t\\) and \\(N\\), but taking the limit of \\(N\\longrightarrow \\infty\\) while \\(w\\) is constant we get \\[P(\\text{survives for $t$ seconds and then hit})=e^{-wt}w\\,dt\\] which depends only on \\(t\\); this is the [[Bernoulli Process vs Poisson Process|Exponential Distribution]] its the one derived by Reif. in Sec. 12.1. The expected value of r.v. \\(t\\) is: \\[E[t]=\\int_0^\\infty dt\\,e^{wt}w t=\\frac{1}{w}\\] which is also the characteristic decay of the probability law."
  },
  {
    "objectID": "collision theory intro.html#example-of-a-theoretical-calculation-of-the-differential-scattering-cross-section",
    "href": "collision theory intro.html#example-of-a-theoretical-calculation-of-the-differential-scattering-cross-section",
    "title": "collision theory intro",
    "section": "Example of a theoretical calculation of the differential scattering cross-section",
    "text": "Example of a theoretical calculation of the differential scattering cross-section\nCollision between two billiard balls. Assumptions:\n\nBoth are spherical, therefore the scattering cross-section is independent of \\(\\phi\\)\nUsing 1. We already used the Hamiltonian (EC+EP) to compute \\(b(\\theta)\\) (depending on the potential energy of interaction between the particle we have different functions \\(b\\).\n\nThe function \\(b(\\theta)\\) tell us what position \\(A_1\\) must have so that it land in the solid angle:\n\\[\nd\\Omega'=\\int_0^{2\\pi}(\\sin \\theta \\,d\\theta) \\,\\,d\\phi = 2\\pi \\sin\\theta\\, d\\theta\n\\]\nFixing \\(\\theta\\), i.e., fixing the ring, tell us the \\(b\\) and thus the area of \\(A_2\\) that \\(A_1\\) must hit into land in this ring. The area is:\n\\[\nd\\sigma_{sc} = 2\\pi b\\,\\,db\n\\]\nThe ratio of this area and the solid angle yields the scattering cross section:\n\\[\n\\frac{d\\sigma_{sc}}{d\\Omega'}=\\frac{b}{\\sin \\theta}\\left|\\frac{db}{d\\theta}\\right|\n\\]\nENDO VS EXOTERMICO e Energia relativista\nTubos, ver tb reif\nÉ preciso saber calcular diferential cross section a partir dos potenciais?\nMomento angular e energia do movimento circular; onde é que ele quer chegar com estas formulas? Ver slides 4 e 5."
  },
  {
    "objectID": "einstein_model_of_solid.html#classical-model",
    "href": "einstein_model_of_solid.html#classical-model",
    "title": "Einstein model for a solid",
    "section": "Classical Model",
    "text": "Classical Model\nWhat is the total energy of small oscillations?\nThe system is a box attached to a spring, the coordinate \\(x\\) measures the deviation from the equilibrium position and the coordinate \\(p\\) is the box’s linear momentum. These coordinates are real functions of time.\n\n\n\n\n\n\nFigure 1: One dimensional oscilator box always wants to return to the equilibrium position. For small amplitudes \\(A\\), the spring force on the box is given by Hook’s law.\n\n\n\nThe total energy of the system: We want to obtain a formula for the total energy in terms of the system coordinates using classical theory since its most familiar to us.\nRecall it is the sum of the kinetic energy of the box, which has the usual form \\(p^2/2m\\), and the potential energy \\(v(x)\\) that characterizes the spring force. What function \\(v(x)\\) can possibly describe well such a system?\nNote the assumptions! We want to describe small oscillations, i.e., the deviation \\(x\\) from equilibrium must be really small, much smaller than 1, mathematically \\(|x|\\ll 1\\) or if you prefer, the limit as \\(x \\longrightarrow 0\\).\nWith this information in our hands we can do something wonderful; we can break \\(v(x)\\) in two parts: the relevant part and negligible one. Using the power of Taylor series we find:\n\\[ v(x) = \\overbrace{v(0) + v'(0)x + \\frac{1}{2}v''(0)x^2}^{Relevant}+\\mathcal{O}(x^3).  \\tag{1}\\]\nTo simplify \\(v(x)\\): the constant term \\(v(0)\\) is set to zero to simplify notation and since \\(v(x)\\) has a minimum at \\(x=0\\), \\(v'(0)=0\\) aswell. Additionally, we identify \\(v''(0)\\) as Hook constant \\(k\\) which, by the way, can be rewritten as \\(m\\omega^2\\), the \\(\\omega\\) is the characteristic frequency \\(\\omega\\) of an oscillator for which this approximation is valid.\nThis makes this formula pleasantly simple\n\\[\nv(x) \\sim \\frac{1}{2}kx^2 =\\frac{1}{2}m \\omega^2x^2\n\\tag{2}\\]\nPictorially, the rhs describes the parabola (pink) that better fits the original potential energy (blue).\n\n\n\n\n\n\nFigure 2: The actual potential vs its approximation, valid when x is small.\n\n\n\nWhat have we achieved so far? We got the formula Equation 2 . Look at it as the functional form of any function that describes the potential energy as a function of \\(x\\) of an oscillator, oscillating :) with small amplitude. Small compared to what, small enough that the third order \\(\\mathcal{O}(x^3)\\) terms of the Taylor Equation 1 are negligible compared with \\(\\mathcal{O}(x^2)\\) term.\nThe total energy of the system is now one step away, its called the Hamiltonian \\(h\\) of the system as is (as usual) the sum of the kinetic energy plus potential energy which we express has:\n\\[\nh=\\frac{p^2}{2m}+\\frac{1}{2}m \\omega^2x^2.\n\\tag{3}\\]\nWith the classical mindset we could, if we wish, derive the laws of motion for this spring-box system using this Hamiltonian, these laws are:\n\\[\n\\begin{cases}\n\\dot{x} = \\frac{\\partial h}{\\partial p}\\\\\n\\dot{p} = - \\frac{\\partial h}{\\partial x}\n\\end{cases}\n\\]\nThis system of differential equations yields the functions \\(x(t)\\) and \\(p(t)\\). But to compute them is not our goal.\n\n\n\n\n\n\nComment\n\n\n\nIs it me, or did I pull the frequency \\(\\omega\\) out of my a#$. Right? Here is some insight, imagine a stiff spring, then we expect a high frequency of oscillation, this is what the formula\n\\[ \\omega =\\sqrt{\\frac{k}{m}},  \\]\ntells us, the higher the \\(k\\) for a fixed mass \\(m\\), the higher the frequency \\(\\omega\\). The formula is derived from classical theory, if you don’t remember how, don’t worry since we will not need it anyway, just take it as a fact of life that makes intuitive sense."
  },
  {
    "objectID": "examples_of_functions.html",
    "href": "examples_of_functions.html",
    "title": "Examples_of_functions",
    "section": "",
    "text": "Examples of functions\nSome examples illustrate again Definition 1 in practice:\n\nExample 1 Let \\(X=\\{a,b,c\\}\\) and \\(Y=\\{1,2\\}\\). All possible pairs are listed in the cartesian product \\(X \\times Y=\\{(a,1),(a,2),(b,1),(b,2),(c,1),(c,2)\\}\\). Relations and functions are subsets of the cartesian product, give some examples.\n\n\nSolution 1. A special subset of these pairs is the function \\(F\\coloneqq \\{(a,1),(b,1),(c,2)\\}\\). The domain of this function is \\(\\{a,b,c\\}\\) since these are all the \\(x\\)’s in \\(F\\); the codomain is \\(Y\\) and the range which contains all \\(y\\)’s of \\(F\\) is \\(R_F=\\{1,2\\}\\). Another subset of \\(X \\times Y\\) could be \\(G\\coloneqq \\{(a,1),(b,1),(c,2),(a,2)\\}\\), it is not special since the element \\(a\\) is associated with two distinct elements \\(1\\) and \\(2\\), thus appearing twice, rendering Equation 3 a false statement. This subset is not a function, but it is a relation whose domain is \\(\\{a,b,c\\}\\) and range \\(\\{1,2\\}\\).\nThe list \\(H\\coloneqq\\{(a,1),(b,1)\\}\\) is a function whose domain is \\(\\{a,b\\}\\) which is a subset of \\(X\\) and range is just \\(\\{1\\}\\) which is a subset of the codomain \\(Y\\). We see that not every element of \\(X\\) or \\(Y\\) have to be used in defining a function.\n\n\nExample 2 Now consider the set of natural numbers \\(\\mathbb{N}\\) and the set of all pairs \\(\\mathbb{N}\\times \\mathbb{N}\\). Illustrate examples of functions.\n\n\nSolution 2. The special subset \\(G\\coloneqq \\{(9,1),(4,4),(1,9)\\}\\) is a function where the domain and range are the same set \\(D_G=R_G=\\{1,4,9\\}\\). The sets \\(\\{(1,1),(2,1),(3,1)\\}\\) and \\(\\{(3,1),(5,1),(13,1)\\}\\) are also functions, but \\(\\{(1,1),(1,2),(3,1)\\}\\) is not.\n\n\nExample 3 Consider now the set \\(\\mathbb{R}\\times \\mathbb{R}\\). The truth set of the statement \\(y=x^2\\) is the subset of all pairs \\((x,y)\\) that make it true, formally we write \\(\\{(x,y)\\in \\mathbb{R}\\times \\mathbb{R}\\,|\\,\\,y=x^2\\}\\). This list of pairs is special since for every real \\(x\\) we choose, the calculation \\(x^2\\) yields a single number, which is always positive or zero. What is the domain, codomain and range?\n\n\nSolution 3. The pairs of this set come from the cartesian product \\(\\mathbb{R}\\times \\mathbb{R}\\), the proposition \\(y=x^2\\) does not put any restriction on the choice of \\(x\\), thus \\(x\\) can take any real value, meaning the domain is \\(\\mathbb{R}\\). On the other hand, this proposition demands that \\(y\\) be positive or zero, because \\(x^2\\) it is for any \\(x\\) in the domain, as a result the range is just the positive reals or zero \\(\\mathbb{R}^+_0\\).\n\n\nExercise 1 Consider the sets \\(A\\coloneqq \\{(x,y) \\in \\mathbb{R}\\times\\mathbb{R}\\,|\\,\\, y=1\\}\\) and \\(B\\coloneqq \\{(x,y) \\in \\mathbb{R}\\times \\mathbb{R}\\,|\\,\\, x=1\\}\\). Convince yourself whether it is or not a function using the Definition 1. For both specify the domain, range and codomain. What about \\(B'\\coloneqq \\{(x,y) \\in \\mathbb{R}\\times \\{2\\}\\,|\\,\\, x=1\\}\\)? Solution\n\n\nExercise 2 With \\(X\\coloneqq \\{\\alpha,\\beta,\\gamma,\\delta\\}\\) and \\(Y\\coloneqq \\{1,2,3,4\\}\\) create relations, two that are not functions and two that are. Make sure you create function, which use all \\(X\\) and others that do not. Solution"
  },
  {
    "objectID": "examples_of_pol_part_1.html",
    "href": "examples_of_pol_part_1.html",
    "title": "Atomic Polynomials",
    "section": "",
    "text": "The simplest polynomial involve procedures of the form \\(1\\), \\(x\\), \\(x^2\\), \\(x^3\\), etc. These functions are given by\n\\[\n\\begin{align}\ne_n:\\,\\, \\mathbb{R} &\\longrightarrow\\mathbb{R}\\\\x &\\longmapsto e_n(x):= x^n\n\\end{align}\n\\tag{1}\\]\nfor an integer \\(n\\) of our choice. The function Equation 1 can also be given under the relation notation by:\\[\nE_n:= \\{(x,y)\\in\\mathbb{R}^2\\,\\,|\\,\\, y=x^n\\}\n\\tag{2}\\]\nThe graphical version of these functions for \\(n=0,1,2,3\\) is:\nThe graph shows the slice of the functions \\(E_n\\) where \\(x\\) runs from \\(-1.5\\) up to \\(1.5\\) and not the whole function, which according to Equation 1 or Equation 2 has the domain ranging on all reals \\(\\mathbb{R}\\). The slice was chosen because it exhibits interesting part of the function: the region where it intercept the axes (the zeros of the function) and shows maxima/minima.",
    "crumbs": [
      "Brief Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Atomic Polynomials"
    ]
  },
  {
    "objectID": "examples_of_pol_part_1.html#properties-of-the-atomic-polynomials",
    "href": "examples_of_pol_part_1.html#properties-of-the-atomic-polynomials",
    "title": "Atomic Polynomials",
    "section": "Properties of the atomic polynomials",
    "text": "Properties of the atomic polynomials\nRather than showing a picture of a small slice of the function as de did in Figure 1 we can instead zoom into some aspects of it.\n\nProperties of the connectivity between domain and codomain:\n\nRange: From inspection of Equation 1 (or Equation 2) we know the domain and codomain of these four functions is \\(\\mathbb{R}\\). The range of these functions is the image of the domain under the action of the function, we can infer what this image is by inspecting Figure 1, for example, the image of \\(\\mathbb{R}\\) under the action of \\(e_2\\) is the set of all positive reals as we check by the red curve hence we write \\(e_2(\\mathbb{R})=\\mathbb{R}^+_0\\). Similarly we find:\n\n\\[\ne_0(\\mathbb{R})=\\{1\\} \\qquad e_1(\\mathbb{R})=e_3(\\mathbb{R})=\\mathbb{R}\\qquad e_2(\\mathbb{R})=\\mathbb{R}^+_0\n\\]\n\nOnto: We also observe that for even powers of \\(n\\) such as \\(e_0\\) and \\(e_2\\), not all elements in the codomain \\(\\mathbb{R}\\) are hit by outputs, i.e., these are not onto function; as an example, [is \\(0\\) hit by some \\(e_0(x)\\) ? Is \\(-1\\) hit by \\(e_2(x)\\), for some \\(x\\) in its domain?] We can answer these question by trying to solve the equations \\(e_0(x)=0\\) and \\(e_2(x)=-1\\). To no avail! Neither have solutions \\(x\\) in the domain \\(\\mathbb{R}\\).\n1-1: Additionally nor are these even powers 1-1. Why? Because we can clearly see from Figure 1 that \\(e_0\\) is in fact the worst possible “labeller”, where all “fruits” \\(x\\) have exactly the same label \\(1\\), no other “label” in its codomain \\(\\mathbb{R}\\) is put into good use by this \\(e_0\\); the function \\(e_2\\) is not as bad since for each label we find just two elements in \\(\\mathbb{R}\\). The functions \\(e_1\\) and \\(e_3\\) are 1-1 since each element \\(x\\) in their domain have a unique element \\(y\\) in the codomain, hence if we choose a label \\(y\\) we uniquely know the \\(x\\). Summarizing:\n\\[\n\\begin{cases}e_0 \\qquad &\\text{worst possible labeler (one label for all inputs)}\\\\e_2 \\qquad  &\\text{bad labeller (one label for each pair of inputs)}\\\\e_1\\,\\, \\text{and}\\,\\, e_3 \\qquad &\\text{perfect (one label, one input)}\\end{cases}\n\\]\n\n\n\nProperties at interesting regions of the function:\n\nWho is bigger or smaller? The Figure 1 reveals something funny:\n\nfor points on the horizontal axis close to the origin, that is \\(-1&lt;x&lt;1\\), the outputs of the four functions have the following rank \\(x^3 &lt; x^2 &lt; x^1 &lt; x^0\\);\nprecisely at \\(x=1\\) we find they are all the same \\(x^0=x^1=x^2=x^3=1\\);\nmeanwhile for the remaining points, those in the region \\(x&lt;-1 \\lor 1&lt;x\\), the absolute value of these functions follow the reversed rank! \\(x^0&lt;x^1&lt;x^2&lt;x^3\\).\nFor \\(x\\) very far from \\(1\\) we find \\(x^0\\ll x^1\\ll x^2\\ll x^3\\), which means, for example, that \\(|x^3|\\) is much, much, much, much larger then \\(|x^2|\\), and so on.\n\nIn summary:\n\\[\n\\begin{cases}\nx^3 &lt; x^2 &lt; x^1 &lt; x^0 \\qquad &\\text{for $x$ close to 1}\\\\x^0=x^1=x^2=x^3 \\qquad &\\text{for $x=1$}\\\\x^0\\ll x^1\\ll x^2\\ll x^3 \\qquad &\\text{for $x$ far from $1$}\n\\end{cases}\n\\tag{3}\\]\nThis summary will be important when we study the limiting behavior of molecular polynomials.\nZeros: The zeros of a function are the coordinates of the points which intercept the axes. Observing Figure 1 we see polynomial \\(1\\) intercepts the y-axis at \\((0,1)\\) while all other polynomials intercept the x-axis and y-axis exactly at the origin \\((0,0)\\).\nMaxima and minima: By inspection of Figure 1, we observe the polynomials \\(1\\) has no maxima or minima, it is always flat; \\(x\\) and \\(x^3\\) on the other hand is a ramp, a never ending one, and thus also does not have a maximum or minimum. The only function exhibiting a minimum is \\(x^2\\), coincidentally, at \\((0,0)\\). Only when we study derivatives we’ll have the computational means to find these extrema.\n\n\n\n\n\n\n\nCommentary\n\n\n\nThe previous list of properties was already embedded in Figure 1 (or if you think about it, already at Equation 2), only by close observation we made it explicit. Of course, the list is not exhaustive, many other properties remain hidden in that picture, waiting to be dug out. This is why I said this list are a zoom into the picture, when we get closer we see more detail.\nAs additional examples we could also ask how fast a polynomial of degree one increases, i.e., what is its slope; for polynomials of degree two it would make sense to ask if it its graph is a cup or a cap, it all depends on \\(a_2\\) parameter.\nWe usually do not list the properties just as we did above, we just acknowledge in our mind that they exist and compute them as needed, since many times some of them are so bluntly obvious just by looking at the graph or the function formula, that no need is required for a calculation.",
    "crumbs": [
      "Brief Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Atomic Polynomials"
    ]
  },
  {
    "objectID": "examples_of_pol_part_2.html",
    "href": "examples_of_pol_part_2.html",
    "title": "Molecular Polynomials",
    "section": "",
    "text": "Molecular Polynomials\nCombining atomic polynomials through linear combinations we obtain molecular polynomials.\n\n\n\n\n\n\nRecall\n\n\n\nA linear combination is an expression where things are multiplied by constants and then added. Here is an example\n\\[\n4x^3+5x+2\n\\]\nThe constants \\(4\\), \\(5\\) and \\(2\\) multiply \\(x^3\\), \\(x^1\\) and \\(x^0\\); which are then summed. Notice how \\(x^0\\) is hidden, to find it, remember that \\(2 =2\\times 1\\) and \\(1=x^0\\).",
    "crumbs": [
      "Brief Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Molecular Polynomials"
    ]
  },
  {
    "objectID": "examples_of_pol_part_4.html",
    "href": "examples_of_pol_part_4.html",
    "title": "Second order polynomials are parabolas",
    "section": "",
    "text": "The relation definitions of these types of polynomials is:\n\\[\n\\{(x,y)\\in\\mathbb{R}^2\\,\\,|\\,\\, y = a_2 x^2+a_1 x+a_0\\}\n\\tag{1}\\]\nfor some real parameters \\(a_0\\), \\(a_1\\) and \\(a_2\\), having \\(a_2 \\not = 0\\) guarantees this is a second order polynomial.\nHere are some choices of parameters and the corresponding graphs:\n\n\n\n\n\n\n\n\n\nThe graph above exhibits the windows on the set Equation 1 where interesting behavior of these functions occur, by interesting, I mean, we see see the x-axis and y-axis intersections as well as minima and maxima; outside of the picture, these function either increase or decrease monotonically toward infinity, that’s uninteresting since nothing else happens there.\nTo know these function even better it is useful to compute by hand (analytically) some important characteristics:\n\ny-axis intercept: we want to know the coordinates on Equation 1 of the form \\((0,y_0)\\).\n\\[\n(0,y_0)\\in \\{(x,y)\\in\\mathbb{R}^2\\,\\,|\\,\\, y = a_2 x^2+a_1 x+a_0\\} \\iff y = a_2 \\times 0 +a_1\\times 0+ a_0 \\iff y=a_0\n\\]\nThus the point \\((0,a_0)\\) is the interception of the polynomial with the vertical axis.\nx-axis intercept: seeking point of the form \\((x_0,0)\\) requires us to solve the equation\n\\[\n(x_0,0)\\in \\{(x,y)\\in\\mathbb{R}^2\\,\\,|\\,\\, y = a_2 x^2+a_1 x+a_0\\}\\iff 0 = a_2 x_0^2+a_1 x_0+a_0\n\\]\nTo solve the second order equation we may use the Resolvent Formula if a solution exists. More on this later.\nwhen \\(x\\) is large (far from \\(1\\)): the process of progressively increasing \\(x\\) and seeing what happens to the corresponding \\(y\\)’s.\nThe idea is to “solve” the dynamical problem:\n\\[\n\\begin{cases}y = a_2 x^2+a_1 x+a_0\\\\\nx\\longrightarrow \\pm\\infty \\end{cases}\n\\tag{2}\\]\nby computing how \\(y\\) behaves when \\(x\\) increases without bound.\nWe solve this problem using our knowledge of how the atomic polynomials behave individually, see the last ordering relation in ?@eq-relations_of_atoms. We know that when \\(x\\) is far from \\(1\\) we must have:\n\\[\na_0 \\ll a_1 x \\ll a_2x^2\n\\]\nSince \\(a_2x^2\\) term is much larger than \\(a_1 x\\) and \\(a_0\\), we can drop them from the equation \\(y=a_2x^2+a_1 x+a_0\\) , doing so giving us an approximate equation, \\(y \\sim a_2 x^2\\), which is simple and thus easy to graph. The graph of both functions is very similar.\n\n\n\nExact equation for all \\(x\\)\nApproximate equation when \\(x\\) is large\n\n\n\n\n\\(y = a_2 x^2+a_1 x+a_0\\)\n\\(y \\sim a_2 x^2\\)\n\n\n\nThat means, if you know how to solve\n\\[\n\\begin{cases}y \\sim a_2 x^2\\\\x\\longrightarrow \\pm\\infty \\end{cases}\n\\tag{3}\\]\nyou know how to solve Equation 2 . But Equation 3 is easy, since it is just an atomic polynomial: if \\(x\\) gets larger and larger then \\(y\\) gets larger and larger, the solution is \\(y\\longrightarrow +\\infty\\), and we conclude the same thing must happen in Equation 2. Problem solved, but solving a similar but simpler problem!",
    "crumbs": [
      "Brief Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Second order polynomials"
    ]
  },
  {
    "objectID": "Example_of_pol_division_4.html",
    "href": "Example_of_pol_division_4.html",
    "title": "Example_of_pol_division_4",
    "section": "",
    "text": "The elementary school algorithm we seen above is just one of many ways in which we can compute the division os \\(1001\\) by \\(12\\). Lets follow, this time, a slightly different path.\nRecall the main question: What is the solution \\(q\\) and \\(r\\) of the equation:\n\\[ 1001 = q \\times 12 + r \\qquad 0\\leq r&lt;12 \\]\nThere are many paths and depending on the quality of our guesses for \\(q\\) the number of steps is different.\nThe shortest path is to immediately guess the answer, just one step, though a difficult one. We would have to guess just glancing ?@eq-question that \\(q=83\\) and \\(r=5\\). That is hard or you are luck.\nLets decompose \\(1001\\) into powers of \\(10\\) to aid our guesses (other decomposition are possible, we seen another in the elementary school’s approach; the powers of \\(10\\) is the basic one because it is exactly what the symbol \\(1001\\) means)\n\\[ 1001 = 1\\cdot 10^3+0\\cdot 10^2 +0\\cdot 10^1+1\\cdot 10^0 = 1\\cdot 10^3 +1\\cdot 10^0 \\]\nNoting \\(12=10+2\\), lets guess the solution for:\n\\[ 1\\cdot 10^3 +1 = q\\cdot(10+2)+r \\]\nGuess: \\(q=8\\cdot 10\\) guarantees that \\(q\\cdot 10\\) is close to \\(1\\cdot 10^3\\) (notice \\(8\\) is almost \\(10\\))\nSubstitution gives:\n\\[ \\begin{align} &1\\cdot 10^3 +1 = 8\\cdot 10^2 +16 \\cdot 10 + r\\\\ \\implies&1\\cdot 10^3 +1 = 9\\cdot10^2+6\\cdot10+r\\\\ \\implies & r=1\\cdot 10^2-6\\cdot 10+1 \\\\ \\implies&r=4\\cdot10+1 \\end{align} \\]\nThe remainder is positive and larger than the divisor \\(12\\) so we keep going."
  },
  {
    "objectID": "Example_of_pol_division_4.html#example4",
    "href": "Example_of_pol_division_4.html#example4",
    "title": "Example_of_pol_division_4",
    "section": "",
    "text": "The elementary school algorithm we seen above is just one of many ways in which we can compute the division os \\(1001\\) by \\(12\\). Lets follow, this time, a slightly different path.\nRecall the main question: What is the solution \\(q\\) and \\(r\\) of the equation:\n\\[ 1001 = q \\times 12 + r \\qquad 0\\leq r&lt;12 \\]\nThere are many paths and depending on the quality of our guesses for \\(q\\) the number of steps is different.\nThe shortest path is to immediately guess the answer, just one step, though a difficult one. We would have to guess just glancing ?@eq-question that \\(q=83\\) and \\(r=5\\). That is hard or you are luck.\nLets decompose \\(1001\\) into powers of \\(10\\) to aid our guesses (other decomposition are possible, we seen another in the elementary school’s approach; the powers of \\(10\\) is the basic one because it is exactly what the symbol \\(1001\\) means)\n\\[ 1001 = 1\\cdot 10^3+0\\cdot 10^2 +0\\cdot 10^1+1\\cdot 10^0 = 1\\cdot 10^3 +1\\cdot 10^0 \\]\nNoting \\(12=10+2\\), lets guess the solution for:\n\\[ 1\\cdot 10^3 +1 = q\\cdot(10+2)+r \\]\nGuess: \\(q=8\\cdot 10\\) guarantees that \\(q\\cdot 10\\) is close to \\(1\\cdot 10^3\\) (notice \\(8\\) is almost \\(10\\))\nSubstitution gives:\n\\[ \\begin{align} &1\\cdot 10^3 +1 = 8\\cdot 10^2 +16 \\cdot 10 + r\\\\ \\implies&1\\cdot 10^3 +1 = 9\\cdot10^2+6\\cdot10+r\\\\ \\implies & r=1\\cdot 10^2-6\\cdot 10+1 \\\\ \\implies&r=4\\cdot10+1 \\end{align} \\]\nThe remainder is positive and larger than the divisor \\(12\\) so we keep going."
  },
  {
    "objectID": "Example_of_pol_division_4.html#commentary",
    "href": "Example_of_pol_division_4.html#commentary",
    "title": "Example_of_pol_division_4",
    "section": "Commentary",
    "text": "Commentary\nNote that a guesses \\(q=9\\cdot 10\\) or \\(q=10^2\\) would lead to a negative remainders. These two guesses would be fine since we only need the final remainder to be positive, not the intermediate ones. Our choice of \\(q=8\\) is motivated by convenience.\nWe have to solve:\n\\[ 4\\cdot 10+1 = q'\\cdot(10+2)+r' \\]\nfor \\(q'\\) and \\(r'\\), because \\(41\\) is larger than \\(12\\) and thus still divisible by \\(12\\), or said in another way, \\(41\\) is not the smallest remainder (which must be between \\(0\\) and \\(12\\)) we can get.\nGuess’: \\(q'=3\\)\nSubstitute:\n\\[ \\begin{align} &4\\cdot 10+1 = 3\\cdot 10 + 6 + r'\\\\ \\implies &r'=1\\cdot 10-6+1 = 4+1=5 \\end{align} \\]\nFrom ?@eq-step1II and ?@eq-step2II we conclude that:\n\\[ 1001=(8\\cdot 10+3)\\cdot 12+5 \\]\nwhich simplifies into:\n\\[ 1001=83\\cdot 12+5 \\]\nEXERCISE: Making your initial guess \\(q=10^3\\) and arrive at this result.\nSOLUTION:\nIn this solution we present the steps to solve the problem in two parallel ways: using guesswork to solve the equations and the table-like approach (see the pictures).\nThe basic problem at hand can be presented in the following fashion:\n\nGuess: \\(q=10^2\\) so that \\(q\\cdot (10+2)\\) has a \\(10^3\\) term to cancel the \\(1\\cdot 10^3\\) from \\(1001\\).\nThis basic step can be presented as:\n\nSubstituting we find:\n\\[ \\begin{align} &1\\cdot 10^3 + 1 = 10^2(10+2)+r\\\\ \\implies & r=-2\\cdot 10^2+1 \\end{align} \\]\nWhich is represented by:\n\nThe current remainder is negative but is still divisible by \\(12\\) (given it is larger than \\(12\\)):\n\\[ -2\\cdot 10^2+1 = q'\\cdot (10+2) +r' \\]\nGuess’: \\(q' = -2\\cdot 10\\) so that the \\(-2\\cdot 10^2\\) is canceled on the lhs.\nThis guess step can be represented as:\n\nSubstitute:\n\\[ \\begin{align} &-2\\cdot 10^2 +1 = -2\\cdot 10^2 -4\\cdot 10 +r'\\\\ \\implies & r' = 4\\cdot 10 +1 \\end{align} \\]\nPresented as:\n\nSince the remainder \\(4\\cdot 10+1\\) is divisible by \\(12\\) we write:\n\\[ 4\\cdot 10 +1 = q''\\cdot (10+2)+r'' \\]\nwhich leads to our next guess.\nGuess’’: \\(q''=4\\) to cancel the lhs \\(4\\cdot 10\\)\nSubstitution:\n\\[ \\begin{align} &4\\cdot 10 +1 =4\\cdot 10+8 + r''\\\\ \\implies& r'' = -7 \\end{align} \\]\nShow in:\n\nCollecting terms:\n\\[ 1001=(10^2-2\\cdot 10+4)\\cdot 12 -7 \\]\nOur remainder in negative, hence add \\(0=12-12\\) to the rhs:\n\\[ 1001=(10^2-2\\cdot 10+4-1)\\cdot 12+(12-7) \\]\nIn conclusion:\n\\[ 1001=83\\cdot 12 +5 \\]"
  },
  {
    "objectID": "exer_withperequisites.html",
    "href": "exer_withperequisites.html",
    "title": "Exercise",
    "section": "",
    "text": "In this exercise we’ll make sure you understand the statement \\(\\forall x : \\exists ! y :(x,y)\\in F\\). There are two key perquisites\n\nFirst, consider \\(x\\in\\{a,i,o\\}\\) and \\(A(x)\\) an \\(x\\)-dependent proposition. Then the generic statement \\(\\forall x : A(x)\\) is a compact version of \\(A(a)\\land A(b)\\land A(c)\\), we write:\n\\[ \\begin{equation}\\forall x : A(x) \\iff A(a)\\land A(i)\\land A(o)\\end{equation} \\]\nFor some propositions \\(A\\) this statement is true for other it will be false, for example: \\(A(x)\\coloneqq [\\text{$x$ is not $z$}]\\) makes \\(\\forall x : A(x)\\) true, while \\(A(x)\\coloneqq [\\text{$x$ is below $p$}]\\) makes it false. Give one example of an \\(A(x)\\) that makes (\\(\\ref{eq:thestat2}\\)) true and other that makes it false.\nSecond, we have the following equivalence:\n\\[ \\begin{equation}\\exists x : A(x) \\iff A(a) \\lor A(i) \\lor A(o)\\end{equation} \\]\nConsider \\(x\\in \\{a,b,c\\}\\). When \\(A(x)\\coloneqq [\\text{$x$ is $d$}]\\) the statement is false, but when \\(A(x)\\coloneqq [\\text{$x$ is a vowel}]\\) it is true; moreover since there are two vowels we have:\n\\[ \\begin{equation}A(a)\\lor A(i)\\lor A(o) \\longrightarrow T \\lor T \\lor F\\end{equation} \\]\nThere are two true proposition in that string of \\(or\\)’s. Give examples of \\(A(x)\\) that make it true and other false.\nThird, \\(\\exists ! x : A(x)\\) can only be true when either \\(A(a)\\) or \\(A(i)\\) or \\(A(o)\\) is true, exclusively:\n\\[ \\begin{equation}\\exists ! x : A(x) \\iff A(a)\\dot{\\lor} A(i)\\dot{\\lor} A(o)\\end{equation} \\]\nThat means that \\(A(x)\\coloneqq [\\text{$x$ is a vowel}]\\) no longer makes the statement true, but \\([\\text{$x$ is a consonat}]\\) will make it true. Again, give examples that make the statement true and false.\n\nWith these prerequisites at hand all we have to do is to see the statement \\(\\forall x : \\exists ! y :(x,y)\\in F\\) as \\(\\forall x : B(x)\\) where \\(B(x)\\coloneqq [\\exists ! y :(x,y)\\in F]\\). Let \\(x\\in \\{a,b,c\\}\\) and \\(y\\in\\{1,2\\}\\). Unpack the meaning of \\(B(x)\\) for each \\(x\\) using \\((c)\\) and the then the meaning of \\(\\forall x : B(x)\\) using prerequisite \\((a)\\), you should get a statement with only one unknown, which is \\(F\\).\nNow, consider these possible \\(F\\)’s:\n\n\\(\\{(a,1),(b,2)\\}\\)\n\\(\\{(a,1),(b,1),(c,1)\\}\\)\n\\(\\{(a,1),(b,1),(c,1),(a,2)\\}\\)\n\\(\\{(c,2)\\}\\)\n\nFor which ones the statement \\(\\forall x : \\exists ! y :(x,y)\\in F\\) is true and for which ones it is false, if any.\nSolution to exer:withperequisites"
  },
  {
    "objectID": "functions-introduction.html",
    "href": "functions-introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nIt is remarkable how the world we see around us can be understood with two simple concepts: the concept of set and the concept of function. These are two fundamental and natural ways of organizing the information we gather from the world.\nSets are essentially lists, with them we can conveniently gather things that share some common property: for example the set of your friends, the set of videos you liked in the last month, the set of fruits in your local supermarket, the set of letters we use to write this phrase. With our ability to group things together allow us to start thinking about the sets themselves rather than their elements, liberating us of mental “ram” memory to focus on other tasks.\nGrouping things is not enough to organize information. Because things in the world interact, we must have some way to describe that interaction and the dependencies that emerge. The concept of relation is essential in that description; by relation we mean a way to connect elements of distinct sets: The sets of fruits and the set of positive real numbers are related, either by the label they carry or their price; the set of fruits and the set of friends are related, because each one prefer some fruits over other fruits; the set of letter in the alphabet and the set of fruit is also related, just let each fruit be assigned a letter, say the first in their name.\nIn particular we will focus on a special kind of relations, namely functions, in Sec. \\(\\ref{sec:asarelation}\\) we introduce why are functions a particular case of relations. In Sec. \\(\\ref{sec:4ways}\\) we will show how a function can be manifested as a list of pairs, a diagram, a table or a graph; all equivalent points of view, with theirs advantages and disadvantages. Alternatively we can view functions as a procedure that transforms (map) elements of one set into the elements of the other, we introduce this notion in Sec. \\(\\ref{sec:asaprocedure}\\), which is also the most difficult. The relation and procedure points of view on functions are equivalent, we will stress that in Sec. \\(\\ref{sec:equiv}\\).\nThe great diversity in ways we can connect elements of two sets shows in diversity of functions we will encounter in this chapter and others, it will turn out to be important to classify theirs global behavior as , or , because some functions are better for describing certain aspects of the world than others. These notions are introduced in Sec. \\(\\ref{sec:sib}\\).\nFinally, if we can assign an element of a given set to another element of a different set, then we can assign many, this “super assignment” called the image of a set is introduced in Sec. \\(\\ref{sec:defs}\\) with some special examples which will be useful in later chapters; additionally we introduce in this section the reverse of this super assignment, the so called pre-image of a set.",
    "crumbs": [
      "Brief Notes",
      "Basic Maths",
      "Functions",
      "Introduction"
    ]
  },
  {
    "objectID": "how_many_solutions.html",
    "href": "how_many_solutions.html",
    "title": "One, none or many solutions?",
    "section": "",
    "text": "With the above discussion understood we have the following cases:"
  },
  {
    "objectID": "how_many_solutions.html#subspaces",
    "href": "how_many_solutions.html#subspaces",
    "title": "One, none or many solutions?",
    "section": "Subspaces",
    "text": "Subspaces"
  },
  {
    "objectID": "how_many_solutions.html#dot-product",
    "href": "how_many_solutions.html#dot-product",
    "title": "One, none or many solutions?",
    "section": "Dot product",
    "text": "Dot product\nFalar novamente dos subespaços do secundário, que já mensionei acima na sec dos subespaços, mas desta vez dar usar a equação com produto interno tal como fazes nos teus resumos de GA do 11o ano."
  },
  {
    "objectID": "how_many_solutions.html#transpose-of-a-matrix",
    "href": "how_many_solutions.html#transpose-of-a-matrix",
    "title": "One, none or many solutions?",
    "section": "Transpose of a matrix",
    "text": "Transpose of a matrix"
  },
  {
    "objectID": "how_many_solutions.html#rank-of-a-matrix",
    "href": "how_many_solutions.html#rank-of-a-matrix",
    "title": "One, none or many solutions?",
    "section": "Rank of a matrix",
    "text": "Rank of a matrix"
  },
  {
    "objectID": "how_many_solutions.html#basis",
    "href": "how_many_solutions.html#basis",
    "title": "One, none or many solutions?",
    "section": "Basis?",
    "text": "Basis?"
  },
  {
    "objectID": "how_many_solutions.html#matrices-as-linear-operators",
    "href": "how_many_solutions.html#matrices-as-linear-operators",
    "title": "One, none or many solutions?",
    "section": "Matrices as linear Operators?",
    "text": "Matrices as linear Operators?"
  },
  {
    "objectID": "how_many_solutions.html#four-subspaces-of-a-matrix",
    "href": "how_many_solutions.html#four-subspaces-of-a-matrix",
    "title": "One, none or many solutions?",
    "section": "Four subspaces of a matrix",
    "text": "Four subspaces of a matrix\n\\[\n\\begin{align}\n&\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| &b_1\\\\\n2 & 4 & 6 & 8 &\\bigm| &b_2\\\\\n3 & 6 & 8 & 10 &\\bigm| &b_3\n\\end{pmatrix}\n\\overset{l_2' = l_2-2l_1}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| &b_1\\\\\n0 & 0 & 2 & 4 &\\bigm| &b_2-2b_1\\\\\n3 & 6 & 8 & 10 &\\bigm| &b_3\n\\end{pmatrix}\\\\\n&\\overset{l_3' = l_3-3l_1}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| &b_1\\\\\n0 & 0 & 2 & 4 &\\bigm| &b_2-2b_1\\\\\n0 & 0 & 2 & 4 &\\bigm| &b_3-3b_1\n\\end{pmatrix}\n\\overset{l_3'=l_3-l_2}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| &b_1\\\\\n0 & 0 & 2 & 4 &\\bigm| &b_2-2b_1\\\\\n0 & 0 & 0 & 0 &\\bigm| &b_3-b_2-b_1\n\\end{pmatrix}\\\\\n&\\overset{l_2'=1/2l_2}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| &b_1\\\\\n0 & 0 & 1 & 2 &\\bigm| &b_2/2-b_1\\\\\n0 & 0 & 0 & 0 &\\bigm| &b_3-b_2-b_1\n\\end{pmatrix}\n\\overset{l_1'=l_1-2l_2}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 0 & -2 &\\bigm| &3b_1-b_2\\\\\n0 & 0 & 1 & 2 &\\bigm| &b_2/2-b_1\\\\\n0 & 0 & 0 & 0 &\\bigm| &b_3-b_2-b_1\n\\end{pmatrix}\n\\end{align}\n\\]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Brief Notes",
    "section": "",
    "text": "Getting the feet wet with Polynomials\nAngular momentum in quantum mechanics\nEinstein Model of Solid"
  },
  {
    "objectID": "index.html#notes",
    "href": "index.html#notes",
    "title": "Brief Notes",
    "section": "",
    "text": "Getting the feet wet with Polynomials\nAngular momentum in quantum mechanics\nEinstein Model of Solid"
  },
  {
    "objectID": "inverse_matrix.html",
    "href": "inverse_matrix.html",
    "title": "Inverse Matrix",
    "section": "",
    "text": "Only full rank square matrices can have inverses. The inverse of \\(A\\) is the unique solution \\(A^{-1}\\) of the equation:\n\\[\nAA^{-1} = I = A^{-1}A\n\\]\nHow do we solve this equation?"
  },
  {
    "objectID": "inverse_matrix.html#how-to-compute-an-inverse",
    "href": "inverse_matrix.html#how-to-compute-an-inverse",
    "title": "Inverse Matrix",
    "section": "How to compute an inverse?",
    "text": "How to compute an inverse?\nConsider the matrix\n\\[\nA=\\left(\\begin{matrix}1 & 2 \\\\3 & 5  \\end{matrix}\\right)\n\\]\nThe goal is to solve:\n\\[\n\\left(\\begin{matrix}1 & 2 \\\\3 & 5  \\end{matrix}\\right)\\left(\\begin{matrix}a & c \\\\b & d  \\end{matrix}\\right)=\\left(\\begin{matrix}1 & 0 \\\\0 & 1  \\end{matrix}\\right)\n\\]\nClose inspection of the product between the matrices shows us that we can break this problem into two independent problems:\n\\[\n\\begin{align}\n&\\left(\\begin{matrix}1 & 2 \\\\3 & 5  \\end{matrix}\\right)\\left(\\begin{matrix}a\\\\b  \\end{matrix}\\right) = \\left(\\begin{matrix}1\\\\0  \\end{matrix}\\right) \\\\\n&\\left(\\begin{matrix}1 & 2 \\\\3 & 5  \\end{matrix}\\right)\\left(\\begin{matrix}c\\\\d  \\end{matrix}\\right) = \\left(\\begin{matrix}0\\\\1  \\end{matrix}\\right)\n\\end{align}\n\\]\nWith elimination we find:\n\\[\n\\begin{align}\n&\\left(\\begin{matrix}1 & 2 \\\\3 & 5  \\end{matrix}\\;\\middle|\\;\\begin{matrix}1\\\\0 \\end{matrix}\\right)\n\\overset{l_2'=l_2-3l_1}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 2 \\\\0 & -1  \\end{matrix}\\;\\middle|\\;\\begin{matrix}1\\\\-3 \\end{matrix}\\right)\n\\overset{l_2'=-l_2}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 2 \\\\0 & 1  \\end{matrix}\\;\\middle|\\;\\begin{matrix}1\\\\3 \\end{matrix}\\right)\n\\overset{l_1'=l_1-2l_2}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 0 \\\\0 & 1  \\end{matrix}\\;\\middle|\\;\\begin{matrix}-5\\\\3 \\end{matrix}\\right)\\\\\n&\\left(\\begin{matrix}1 & 2 \\\\3 & 5  \\end{matrix}\\;\\middle|\\;\\begin{matrix}0\\\\1 \\end{matrix}\\right)\n\\overset{l_2'=l_2-3l_1}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 2 \\\\0 & -1  \\end{matrix}\\;\\middle|\\;\\begin{matrix}0\\\\1 \\end{matrix}\\right)\n\\overset{l_2'=-l_2}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 2 \\\\0 & 1  \\end{matrix}\\;\\middle|\\;\\begin{matrix}0\\\\-1 \\end{matrix}\\right)\n\\overset{l_1'=l_1-2l_2}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 0 \\\\0 & 1  \\end{matrix}\\;\\middle|\\;\\begin{matrix}2\\\\-1 \\end{matrix}\\right)\n\\end{align}\n\\tag{1}\\]\nFrom the right systems we conclude that the inverse matrix is:\n\\[\nA^{-1}=\\left(\\begin{matrix}-5 & 3 \\\\2 & -1  \\end{matrix}\\right)\n\\]\nChecking the result:\n\\[\nAA^{-1}=\\left(\\begin{matrix}1 & 2 \\\\3 & 5  \\end{matrix}\\right)\\left(\\begin{matrix}-5 & 2 \\\\3 & -1  \\end{matrix}\\right)=\\left(\\begin{matrix}1 & 0 \\\\0 & 1  \\end{matrix}\\right)\n\\]\nIt works!\nObservations:\n\nObserving the calculations Equation 1 we notice the arrow operators are the same for both systems, we solved both systems with the intent (as elimination method prescribes) of simplifying the matrix as much as possible, since in this case we are dealing with a full rank matrix, the end point simplification must be an identity matrix. What is different is the right vectors, which in the end tell us the columns of the inverse.\n\n\n\n\n\n\n\nRecall\n\n\n\nIn case it is not clear why the columns \\((-5,3)\\) and \\((2,-1)\\) tell us the columns of \\(A^{-1}\\), remember the meaning of the extended matrix notation in Equation 1:\n\\[\n\\begin{align}\n&\\left(\\begin{matrix}1 & 0 \\\\0 & 1  \\end{matrix}\\;\\middle|\\;\\begin{matrix}-5\\\\3 \\end{matrix}\\right)\\leftrightsquigarrow\\left(\\begin{matrix}1 & 0 \\\\0 & 1  \\end{matrix}\\right)\\left(\\begin{matrix}a\\\\b \\end{matrix}\\right)=\\left(\\begin{matrix}-5\\\\3\\end{matrix}\\right) \\implies \\left(\\begin{matrix}a\\\\b \\end{matrix}\\right)=\\left(\\begin{matrix}-5\\\\3\\end{matrix}\\right)\\\\\n&\\left(\\begin{matrix}1 & 0 \\\\0 & 1  \\end{matrix}\\;\\middle|\\;\\begin{matrix}2\\\\-1 \\end{matrix}\\right)\\leftrightsquigarrow\\left(\\begin{matrix}1 & 0 \\\\0 & 1  \\end{matrix}\\right)\\left(\\begin{matrix}c\\\\d \\end{matrix}\\right)=\\left(\\begin{matrix}2\\\\-1\\end{matrix}\\right) \\implies \\left(\\begin{matrix}c\\\\d \\end{matrix}\\right)=\\left(\\begin{matrix}2\\\\-1\\end{matrix}\\right)\n\\end{align}\n\\]\n\n\n\nSince the calculations operations in Equation 1 are the same, the main difference being the right hand side vector on which they are applied, rather than doing the same thing twice, we do it one shot adopting the following notation for Equation 1:\n\\[\n\\left(\\begin{matrix}1 & 2 \\\\3 & 5  \\end{matrix}\\;\\middle|\\;\\begin{matrix}1 & 0\\\\0 & 1\\end{matrix}\\right)\n\\overset{l_2'=l_2-3l_1}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 2 \\\\0 & -1  \\end{matrix}\\;\\middle|\\;\\begin{matrix}1 & 0\\\\-3 &1 \\end{matrix}\\right)\n\\overset{l_2'=-l_2}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 2 \\\\0 & 1  \\end{matrix}\\;\\middle|\\;\\begin{matrix}1 & 0\\\\3 & -1\\end{matrix}\\right)\n\\overset{l_1'=l_1-2l_2}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 0 \\\\0 & 1  \\end{matrix}\\;\\middle|\\;\\begin{matrix}-5 & 2\\\\3 & -1 \\end{matrix}\\right)\\\\\n\\]\nWe turn the matrix \\(A\\) into an \\(I\\) with elimination operators, those applied to the columns of \\(I\\) yield the \\(A^{-1}\\).\n\nWe distill the essential strategy to compute the inverse of \\(A\\) from the previous example:\n\nStep 1: Write the extended matrix \\([A|I]\\)\nStep 2: Apply elimination operation on all entries bellow and above the diagonal so that the block \\(A\\) ends becoming an \\(I\\), in the process apply the same operation on the extended \\(I\\) block.\nStep 3: The final result has the form \\([I| A^{-1}]\\), read the inverse matrix entries from this notation and check your results by computing \\(AA^{-1}=I\\).\nStep 4: Congratulations!"
  },
  {
    "objectID": "inverse_matrix.html#matrix-with-no-inverse",
    "href": "inverse_matrix.html#matrix-with-no-inverse",
    "title": "Inverse Matrix",
    "section": "Matrix with no inverse",
    "text": "Matrix with no inverse\nWhen a matrix does not have a full rank, it has no inverse [comment: in Linear function section we’ll explain why in terms of maps, for now the determinant =0 suffices, the determinant argument is a practical argument, the conceptual argument requires the idea of linear function, but that will only come later]\nThe reason being the determinant is zero and as a result: the calculation Equation 2 cannot be performed.\nNot every square matrix will have an inverse. For example \\(E_1\\) is square, thus it may or not have an inverse \\(E_1^{-1}\\), on the other hand the matrix \\(A\\) is rectangular, thus no inverse exists.\nHow do we compute an inverse of a matrix?\nThe \\(E\\)’s matrices have inverses, always and thus:\n\\[\nAx=b \\iff EAx=Eb \\iff A'x=b'\n\\]\nare equivalent and and as a result their truth sets are equal! Finding the truth set of the later is easier than the former."
  },
  {
    "objectID": "limits_of_sequences.html",
    "href": "limits_of_sequences.html",
    "title": "Limits of sequences",
    "section": "",
    "text": "Sequences are also functions, and as a result many of the concepts we introduced for functions apply to sequences as well. For instance, we may:\n\nmake a table out of them\nplot them in some relevant window\ndescribe in detail some slices of it; in particular the region when \\(n\\) is large.\n\nIn these notes we will focus on the later.\nConsider the sequence:\n\\[\nu_n=2+\\frac{1}{n}\n\\]whose graph for \\(n=1,.. ,20\\) is:\n\n\n\n\n\nA few things strikes us in the region when \\(n\\) is large:\n\nAs \\(n\\) gets larger and larger, then \\(2+1/n\\) gets closer and closer to \\(2\\).\nThe values \\(u_n\\) never actually reaches the value of \\(2\\) because \\(1/n\\) is never zero, no matter how large \\(n\\) is.\n\nIn other word: the sequence \\(u_n\\) eventually lies within any interval centered at \\(2\\).\nLets make this wording rigorous:\n\nTo say that an interval centered at \\(2\\) contains a sequence means: \\(|2-u_n|&lt;\\varepsilon\\). The radius of the interval is \\(\\varepsilon\\) and is positive, because we are speaking about an interval, not a point like thing!\nThe eventually words means that after some value \\(N\\), any \\(n\\), guarantees \\(|2-u_n|&lt;\\varepsilon\\)\nThe any word means, that the above is true for any positive \\(\\varepsilon\\)\n\nPutting it all together:\n\n(1) For any \\(\\varepsilon&gt;0\\), the following is true:\n\n(2) That there exists some \\(N\\) that guarantees that:\n\n(3) for any \\(n\\geq N\\) the following statement is true\n\n(4) the statement that \\(|2-u_n|&lt;\\varepsilon\\)\n\n\n\n\nWe write this description using complicated mathematical notation as:\n\\[ \\overbrace{\\forall \\varepsilon &gt;0 }^{(1)}:\\overbrace{\\exists N}^{(2)}:\\overbrace{\\forall  n\\geq N}^{(3)}: (\\overbrace{ |2-u_n|&lt;\\varepsilon)}^{(4)}  \\tag{1}\\]\nSince it is just complicated to write Equation 1, the following abbreviation is introduced:\n\\[\n\\lim_{n\\longrightarrow \\infty}u_n = 2\n\\]\nThe lhs should be view as a single symbol that says: “the limit value of \\(2+1/n\\) when \\(n\\) gets larger and larger (infinity is not a number)”. The rhs tell us that \\(2\\) is the limit value; or better said, the point around which the sequence eventually always lurks.\n\n\n\n\n\n\nCommentary\n\n\n\nSequences of the form \\(x_0\\pm1/n\\) are important since they describe a step by step approach to the number \\(x_0\\) from values above \\(+\\) or below \\(-\\); without never actually getting there. They will be relevant when computing.",
    "crumbs": [
      "Brief Notes",
      "Calculus",
      "Limits of sequences"
    ]
  },
  {
    "objectID": "limits_of_sequences.html#an-example",
    "href": "limits_of_sequences.html#an-example",
    "title": "Limits of sequences",
    "section": "An example",
    "text": "An example\nWe will prove that:\n\\[\n\\lim \\frac{4n+1}{2n}=2\n\\]\nMeaning we want to prove that:\n\\[\n\\forall \\varepsilon &gt;0:\\exists N:\\forall n\\geq N : |2-(4n+1)/2n|&lt;\\varepsilon\n\\]\nWe proceed by unpacking the meaning with indented assumptions:\n\nAssume we have some value \\(\\varepsilon\\), though it is arbitrary:\n\nWe seek to find an \\(N\\) that makes \\(\\forall n\\geq N : |2-(4n+1)/2n|&lt;\\varepsilon\\) true, i.e., we seek a lower bound on \\(n\\) that makes this true:\n\nTo do that, we assume that we have some value \\(n^*\\) which is larger than the \\(N\\) and which makes the statement \\(|2-(4n^*+1)/2n^*|&lt;\\varepsilon\\) true. This much we know about \\(n^*\\). No special conclusion was taken, yet.\nNow simplify this statement and see if we find something new about \\(n^*\\):\n\\[\n\\begin{align}\n& |2-\\frac{4n^*+1}{2n^*}|&lt;\\varepsilon\\\\\n&\\implies |\\frac{4n^*-4n^*+1}{2n^*}|&lt;\\varepsilon\\\\\n&\\implies |\\frac{1}{2n^*}|&lt;\\varepsilon\\\\\n&\\implies n^*&gt;\\frac{1}{2\\varepsilon}\n\\end{align}\n\\]\n\n\n\nNow, this is significant! Lets recollect what we found in the three indentations: Given a radius \\(\\varepsilon\\), a \\(n^*\\)bounded below by \\(N\\) and which obeys our statement is a \\(n^*\\) larger than \\(1/2\\varepsilon\\). The consequences of this are inescapable. If you choose smaller and smaller \\(\\varepsilon\\) the larger and larger \\(n^*\\) must be. \\(\\varepsilon\\) can be make as small as we wish and we can always find, through the inequality \\(n^*&gt;1/2\\varepsilon\\), a \\(n^*\\) that makes \\(|2-(4n^*+1)/2n^*|&lt;\\varepsilon\\) true. And since \\(N\\) is smaller than \\(n^*\\) we can, in conclusion, always say it exist a lower bound \\(N\\) for this \\(n^*\\).\nTo say \\(\\lim \\frac{4n+1}{2n}=2\\) is just an abbreviation for the whole reasoning made above!",
    "crumbs": [
      "Brief Notes",
      "Calculus",
      "Limits of sequences"
    ]
  },
  {
    "objectID": "limits_of_sequences.html#basic-sequences-limits",
    "href": "limits_of_sequences.html#basic-sequences-limits",
    "title": "Limits of sequences",
    "section": "Basic sequences limits",
    "text": "Basic sequences limits\nUsing arguments such as the one shown above we can establish a myriad of limits for basic sequences, we are going ino the details of the proofs and just list the results:\n\n\\(\\lim \\frac{a}{b}=\\frac{a}{b}\\) provided \\(b\\not = 0\\)\n\\(\\lim \\frac{an+b}{cn+d}=\\frac{a}{c}\\) provided \\(c\\not = 0\\)\n\\(\\lim \\frac{1}{n^p} =0\\), if \\(p&gt;0\\)\nso on …\n\nWe can also prove basic rules for limits, an important one is:\n\nIf \\(a_n\\) and \\(b_n\\) are convergent to limits \\(a\\) and \\(b\\), then:\n\nthe sequence \\(a_n+b_n\\) converges to \\(a+b\\).\nthe sequence \\(a_nb_n\\) converges to \\(ab\\)\nthe sequence \\(a_n/b_n\\) converges to \\(a/b\\) if \\(b\\not=0\\)\nso on…\n\n\nThe idea behind these pre-made limits and the rules is similar to the one from derivatives: If you want to compute a limit of complicated sequence, then your goal is to rearrange that sequence and the limit in such a way that the basic pre-made limits can be used. Giving us the limit value of the complicated sequence.",
    "crumbs": [
      "Brief Notes",
      "Calculus",
      "Limits of sequences"
    ]
  },
  {
    "objectID": "linear_combinations.html",
    "href": "linear_combinations.html",
    "title": "Linear combinations of vectors",
    "section": "",
    "text": "Video:\n\n\n\nLinear Combinations\n\n\nWe’ll consider a vector an array of numbers:\n\\[ \\begin{pmatrix}1\\\\5\\end{pmatrix}, \\begin{pmatrix}1\\\\-5\\\\0.1\\end{pmatrix},\\begin{pmatrix}x\\\\y\\\\z\\end{pmatrix} \\]\nWhat can you do with vectors?\nAnswer: linear combinations! Which can be viewed analytically or geometrically.\nAnalytical l.c. : A linear combination is a computation that looks like this:\n\\[ 2 \\mathbf{u}+4\\mathbf{v} = 2\\begin{pmatrix} 1\\\\3\\end{pmatrix}+4\\begin{pmatrix} 1\\\\-1\\end{pmatrix} \\]\nFirst multiply the vectors by the scalars, then add the vectors by adding the entries, the result is:\n\\[ \\begin{pmatrix} 2\\\\6\\end{pmatrix}+\\begin{pmatrix} 4\\\\-4\\end{pmatrix} = \\begin{pmatrix} 6\\\\2 \\end{pmatrix} \\]\nA generic linear combination looks like this:\n\\[ a \\mathbf{u}+b\\mathbf{v} = a\\begin{pmatrix} 1\\\\5\\end{pmatrix}+b\\begin{pmatrix} 2\\\\10\\end{pmatrix} = \\begin{pmatrix}a +2b\\\\5a +10b \\end{pmatrix} \\]\nwhere \\(a\\) and \\(b\\) are scalars.\nIf you know how to combine two vectors, you know how to combine three and so on.\n\nDefinition 1 The list of vectors \\(\\mathbf{u}_1,\\dots,\\mathbf{u}_k\\) from \\(\\mathbb{R}^n\\) (i.e. \\(\\mathbf{u}_i=(u_{i1},\\dots,u_{ik})\\)), can be linearly combined with numbers \\(c_1,\\dots,c_k\\) as:\n\\[\nc_1\\mathbf{u}_1+\\dots +c_k\\mathbf{u}_k =: \\sum_{j=1}^kc_j\\mathbf{u}_j=:\\mathbf{v}\n\\]\nThis l.c. is a new vector of \\(\\mathbb{R}^n\\), call it \\(\\mathbf{v}\\).\n\nGeometrical l.c. : A l.c. is performed geometrically using the parallelogram rule:\n\n\nExercise 1 Solve 1.1.2.c",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Linear Combinations"
    ]
  },
  {
    "objectID": "linear_functions.html",
    "href": "linear_functions.html",
    "title": "Linear functions",
    "section": "",
    "text": "Up to now, we learned a more practical part of the course:\nIt always how to compute. Now we enter a more conceptual part of the course and organize ideas."
  },
  {
    "objectID": "linear_functions.html#is-it-linear-or-not",
    "href": "linear_functions.html#is-it-linear-or-not",
    "title": "Linear functions",
    "section": "Is it linear or not?",
    "text": "Is it linear or not?\n\nExample 1:\nConsider the function:\n\\[\n\\begin{align}f:\\mathbb{R}&\\longrightarrow \\mathbb{R}\\\\ x&\\longmapsto f(x):=ax\\end{align}\n\\]\nwhere \\(a\\) is some real constant. This is a typical high-school function also known as proportionality function aka polynomial of degree one.\nThe domain and codomain are both \\(\\mathbb{R}\\).\nTo check its linear we choose two generic elements of the domain \\(x_1\\) and \\(x_2\\) and l.c. them using a generic coefficients \\(c_1\\) and \\(c_2\\) :\n\\[\nf(c_1 x_1+ c_2 x_2) = a ( c_1 x_1+ c_2 x_2) = ac_1 x_1+ ac_2 x_2 = c_1f(x_1)+c_2f(x_2)\n\\]\nIndeed it is.\n\n\nExample 2:\nNow a function from \\(\\mathbb{R}\\) into \\(\\mathbb{R}^2\\)\n\\[\n\\begin{align}g:\\mathbb{R}&\\longrightarrow \\mathbb{R}^2\\\\ x&\\longmapsto g(x):=(2x,x)\\end{align}\n\\]\nIs this function linear?\n\\[\ng(c_1 x_1+ c_2 x_2)=(2(c_1 x_1+ c_2 x_2),c_1 x_1+ c_2 x_2) = c_1(2x_1+,x_1)+c_2(2x_2,x_2)=c_1g(x_1)+c_2g(x_2)\n\\]\nBecause our choices where arbitrary, yes, this \\(g\\) is linear.\n\n\nExample 3:\nThe projection function, picks out the \\(x\\) component of the vector \\((x,y)\\):\n\\[\n\\begin{align}\\pi:\\mathbb{R}^2&\\longrightarrow \\mathbb{R}\\\\ (x,y)&\\longmapsto \\pi(x,y):=x\\end{align}\n\\]\nIs it linear?\n\\[\n\\pi(c_1(x_1,y_1)+c_2(x_2,y_2)) = \\pi (c_1x_1+c_2x_2,c_1y_1+c_2y_2)=c_1x_1+c_2x_2 =c_1 \\pi(x_1,y_1) + c_2\\pi(x_2,y_2)\n\\]\nA l.c. of inputs - \\((x_1,y_1)\\) and \\((x_2,y_2)\\) - yields the same l.c. of outputs - \\(\\pi(x_1,y_1)\\) and \\(\\pi(x_2,y_2)\\).\n\n\nExample 4:\nFrom \\(\\mathbb{R}^2\\) to \\(\\mathbb{R}^2\\):\n\\[\n\\begin{align}\\Phi:\\mathbb{R}^2&\\longrightarrow \\mathbb{R}^2\\\\ (x,y)&\\longmapsto \\Phi(x,y):=(2x-y,x+y)\\end{align}\n\\]\nIs it?\n\\[\n\\begin{align}\n\\Phi(c_1(x_1,y_1)+c_2(x_2,y_2)) &= \\Phi(c_1x_1+c_2x_2,c_1y_1+c_2y_2)\\\\\n&=(2(c_1x_1+c_2x_2)-(c_1y_1+c_2y_2),c_1x_1+c_2x_2+c_1y_1+c_2y_2)\\\\\n&=c_1(2x_1-y_1,x_1+y_1)+c_2(2x_2-y_2,x_2+y_2)\\\\\n&=c_1\\Phi(x_1,y_1)+c_2\\Phi(x_2,y_2)\n\\end{align}\n\\]\nYes.\n\n\nExample 5:\nFrom \\(\\mathbb{R}^2\\) to \\(\\mathbb{R}^2\\) once more:\n\\[ \\begin{align}\\Xi:\\mathbb{R}^2&\\longrightarrow \\mathbb{R}^2\\\\ (x,y)&\\longmapsto \\Xi(x,y):=(2x-2y,x-y)\\end{align} \\]\n?\n\\[\n\\begin{align}\n\\Xi(c_1(x_1,y_1)+c_2(x_2,y_2)) &= \\Xi(c_1x_1+c_2x_2,c_1y_1+c_2y_2)\\\\\n&=(2(c_1x_1+c_2x_2)-2(c_1y_1+c_2y_2),c_1x_1+c_2x_2-c_1y_1-c_2y_2)\\\\\n&=c_1(2x_1-2y_1,x_1-y_1)+c_2(2x_2-2y_2,x_2-y_2)\\\\\n&=c_1\\Xi(x_1,y_1)+c_2\\Xi(x_2,y_2)\n\\end{align}\n\\]\nIt is.\n\n\nExample 6: (NON-linear)\nAgain, from \\(\\mathbb{R}^2\\) to \\(\\mathbb{R}^2\\), but this time we have a square involved, hence the name square function:\n\\[\n\\begin{align}\\square:\\mathbb{R}^2&\\longrightarrow \\mathbb{R}^2\\\\ (x,y)&\\longmapsto \\square(x,y):=(2x^2,y)\\end{align}\n\\]\nChecking as usual:\n\\[\n\\begin{align}\n\\square(c_1(x_1,y_1)+c_2(x_2,y_2))&=\\square(c_1x_1+c_2x_2,c_1y_1+c_2y_2)\\\\\n&=(2(c_1x_1+c_2x_2)^2,c_1y_1+c_2y_2)\\\\\n&=(2((c_1x_1)^2+2c_1c_2x_1x_2+(c_2x_2)^2),c_1y_1+c_2y_2)\\\\\n&\\not=c_1\\square(x_1,y_1)+c_2\\square(x_2,y_2)\n\\end{align}\n\\]\nThe range is \\(\\square(\\mathbb{R}^2)=[0,\\infty)\\times\\mathbb{R}\\).\n\n\n\n\n\n\nCommentary\n\n\n\n\nIt is usual to give a more break down \\(f(\\nu \\bf{u}+\\lambda \\bf{v}) = \\nu f (\\bf{u}) + \\lambda f(\\bf{v})\\) into two rules:\n\n\\(f(\\bf{u}+\\bf{v})=f(\\bf{u})+f(\\bf{v})\\)\n\\(f(\\lambda \\bf{u})=\\lambda f(\\bf{u})\\)\n\nRather than checking one more complex rule we can check two simpler rules.\nNotice the summation and multiplication are defined in distinct vector spaces."
  },
  {
    "objectID": "linear_functions.html#example-7-matrices-are-procedures-that-define-linear-functions",
    "href": "linear_functions.html#example-7-matrices-are-procedures-that-define-linear-functions",
    "title": "Linear functions",
    "section": "Example 7: Matrices are procedures that define linear functions",
    "text": "Example 7: Matrices are procedures that define linear functions\nIdea: Consider two vector spaces \\(\\mathbb{R}^4\\) and \\(\\mathbb{R}^3\\). The vectors that live in \\(\\mathbb{R}^4\\) have the form \\(\\mathbf{x}=(x,y,z,w)\\) while those in \\(\\mathbb{R}^2\\) are \\(\\mathbf{u}=(u,v,t)\\).\nA function \\(f\\) that maps vectors from \\(\\mathbb{R}^4\\) and \\(\\mathbb{R}^3\\) can be constructed from the procedure \\(A\\mathbf{x}\\).\nWe define this function as:\n\\[ \\begin{align} f:\\mathbb{R}^4&\\longrightarrow \\mathbb{R}^3\\\\ \\mathbf{x}&\\longmapsto f(\\mathbf{x}):=A\\mathbf{x}  \\end{align} \\]\nwhere \\(A\\) is for example (an already familiar matrix):\n\\[ A=\\begin{pmatrix}1 & 2 & 2 & 2 \\\\2 & 4 & 6 & 8 \\\\3 & 6 & 8 & 10 \\end{pmatrix} \\]\nIts shape tell us all, \\(4\\) columns ready to multiply the \\(4\\) coefficients in \\((x,y,z,w)\\) returning a \\(3\\) entry vector \\(A\\mathbf{x}\\).\nNotice how the property \\(A(\\alpha \\mathbb{x} +\\beta \\mathbf{y})=\\alpha A\\mathbf{x} +\\beta A \\mathbf{y}\\) leads to \\(f\\) being linear as well\\(f(\\alpha \\mathbb{x} +\\beta \\mathbf{y})=\\alpha f(\\mathbf{x}) +\\beta f (\\mathbf{y})\\).\nEssentially, what we want with the introduction of this \\(f\\) is to name an idea already present in \\(A\\mathbf{x}=\\mathbf{b}\\), the idea that the \\(A\\mathbf{x}\\) part of the equation is a function that maps \\(\\mathbf{x}\\) into the vector \\(A\\mathbf{x}\\). With this idea in mind, the system of equations encoded \\(A\\mathbf{x}=\\mathbf{b}\\) is like asking what \\(\\mathbf{x}\\) in the domain of \\(f\\) is mapped into the fixed \\(\\mathbf{b}\\) in the codomain.\n\\[ A\\mathbf{x}=\\mathbf{b} \\iff f(\\mathbf{x})=\\mathbf{b} \\]"
  },
  {
    "objectID": "linear_functions.html#mathematical-motivation",
    "href": "linear_functions.html#mathematical-motivation",
    "title": "Linear functions",
    "section": "(Mathematical) Motivation",
    "text": "(Mathematical) Motivation\nWhy are these linear functions important?\nWe will answer this question in two ways: a zoom in version and a zoom out version (later)\nZoom in version: The rule says something important about the map \\(f\\), on the lhs we see an l.c. of elements of \\(\\mathbb{R}^n\\), i.e., \\(\\nu\\bf{u}\\) and \\(\\lambda \\bf{v}\\) are mapped into \\(\\nu\\bf{u}+\\lambda\\bf{v}\\in \\mathbb{R}^n\\). This vector in turn is mapped, now under \\(f\\), into the element \\(f(\\nu\\bf{u}+\\lambda\\bf{v})\\in\\mathbb{R}^m\\). On the rhs we see a l.c. of elements of \\(\\mathbb{R}^m\\): \\(f (\\bf{u})\\) is being combined with \\(f (\\bf{v})\\) with the coefficients \\(\\nu\\) and \\(\\lambda\\).\n\nOn the picture we see a l.c. diagram-\\((\\nu,\\lambda)\\) on the left and another on the right whose coefficients are the same. These diagrams and all the other are what we can the connectivity structure of the vector space. Notice, the elements being combined are different - on the left we have \\(\\bf{u}\\) with \\(n\\) dimensions while \\(\\bf{u}'\\) has \\(m\\) dimensions.\nThe equality \\(f(\\nu \\bf{u}+\\lambda \\bf{v}) = \\nu f (\\bf{u}) + \\lambda f(\\bf{v})\\) says the three points of diagram on the left are “connected” to a diagram on the right involving the same coefficients. As a result, the connection on the left is preserved because it is reproduced on the right. Its image under \\(f\\) does not change.\n\n\n\n\n\n\nCommentary\n\n\n\nAn example of a non-preserving function was the \\(\\square\\) function whose behavior is diagrammatically akin to this:\n\nThe image of the l.c. diagram on the left does not have a corresponding l.c. diagram (with the same coefficients) on the right. Thus we say \\(\\square\\) does not preserve the diagram during is mapping action."
  },
  {
    "objectID": "linear_functions.html#kernel-of-a-linear-function",
    "href": "linear_functions.html#kernel-of-a-linear-function",
    "title": "Linear functions",
    "section": "Kernel of a linear function",
    "text": "Kernel of a linear function\n\nDefinition 2 Let \\(f:\\mathbb{R}^n\\overset{\\sim}{\\longrightarrow} \\mathbb{R}^m\\), then the kernel or nullspace of \\(f\\) is the subset of the domain \\(\\mathbb{R}^n\\):\n\\[\n\\ker f = \\{\\bf{u}\\in\\mathbb{R}^n\\,\\,|\\,\\,f(\\bf{u})=\\bf{0}\\}\n\\]\nMoreover, it is in fact a subspace.\n\n\nExample 1 (cont):\nThe kernel of \\(f(x)=ax\\) is the zeros of the function:\n\\[\n\\ker f = \\{x\\in \\mathbb{R}\\,\\,|\\,\\,ax =0\\}=\\{0\\}\n\\]\nIt is just one point in the domain, geometrically where the line intercept the x-axis.\n\n\nExample 2 (cont):\nReturning to \\(g(x)=(2x,x)\\), its nullspace is the subspace:\n\\[\n\\ker g = \\{x\\in \\mathbb{R}\\,\\,|\\,\\,(2x,x) =(0,0)\\}=\\{0\\}\n\\]\nAgain, just one vector.\n\n\nExample 3 (cont):\n\\[\n\\ker \\pi = \\{(x,y)\\in \\mathbb{R}\\,\\,|\\,\\,\\pi(x,y)=x=0\\}=\\{(0,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y\\in \\mathbb{R}\\}\n\\]\n\nAny vector along the subspace [0y axis] of the domain \\(\\mathbb{R}^2\\) is always projected in the \\(0\\) of the codomain. This vertical axis is the kernel of \\(\\pi\\) (determined by \\(\\pi\\) itself)\n\n\nExample 4 (cont):\nThe kernel of \\(\\Phi(x,y)=(2x-y,x+y)\\) is:\n\\[\n\\ker \\Phi = \\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\,(2x-y,x+y)=(0,0)\\}\n\\]\nWe have to solve the system of equations:\n\\[\n\\begin{cases}\n2x-y=0\\\\\nx+y=0\n\\end{cases}\n\\leftrightsquigarrow\n\\begin{pmatrix}\n2 & -1\\\\\n1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx\\\\y\n\\end{pmatrix}\n=\\begin{pmatrix}\n0\\\\0\n\\end{pmatrix}\n\\leftrightsquigarrow\n\\begin{pmatrix}2 & -1 &\\bigm|0\\\\1 & 1 &\\bigm| 0\\end{pmatrix}\n\\]\nBy inspection, the columns are independent and thus the solution is \\((0,0)\\).\n\n\nExample 5 (cont):\n\\[\n\\ker \\Xi = \\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\,(2x-2y,x-y)=(0,0)\\}\n\\]\nThe corresponding system to solve is\n\\[\n\\begin{pmatrix}2 & -2 &\\bigm|0\\\\1 & -1 &\\bigm| 0\\end{pmatrix}\n\\]\nwhose solution is \\(c(1,1)\\). The kernel is the whole line along \\((1,1)\\)."
  },
  {
    "objectID": "linear_functions.html#image-of-the-domain-range-of-linear-functions",
    "href": "linear_functions.html#image-of-the-domain-range-of-linear-functions",
    "title": "Linear functions",
    "section": "Image of the domain (range of linear functions)",
    "text": "Image of the domain (range of linear functions)\n\nDefinition 3 Let \\(f:\\mathbb{R}^n\\overset{\\sim}{\\longrightarrow} \\mathbb{R}^m\\), then the range of \\(f\\) is the subset of the domain \\(\\mathbb{R}^m\\):\n\\[\nf(\\mathbb{R}^n) = \\{f(\\bf{u})\\in\\mathbb{R}^m\\,\\,|\\,\\,\\bf{u}\\in\\mathbb{R}^n\\}\n\\]\nMoreover, it is also a subspace.\n\nThe range of \\(f\\) can is hard to compute, the concept is what is most important here.\nHow do we compute the image of the domain?\nExample 1 (cont):\nIs it possible to indentify any element \\(y\\) of the codomain \\(\\mathbb{R}\\) by some label \\(x\\) of the domain \\(\\mathbb{R}\\)? In other words, can we solve for \\(x\\), the equation:\n\\[\nax = y\n\\]\ngiven an arbitrary \\(y\\)?\nAnswer is yes, the form of the element of the domain assigned to \\(y\\) is \\(y/a\\).\nWe conclude \\(f(\\mathbb{R})=\\mathbb{R}\\).\nExample 2 (cont):\nIs there for every \\((u,v)\\in \\mathbb{R}^2\\) a corresponding element \\(x\\) in the domain \\(\\mathbb{R}\\)? We answer by solving for \\(x\\):\n\\[\n(2x,x)=(u,v) \\implies \\begin{cases}2x=u\\\\x=v\\end{cases}\\implies \\begin{cases}x=u/2\\\\x=v\\end{cases}\n\\]\nThe answer is no. We cannot choose \\(u,v\\) arbitrarily and find an \\(x\\), the system only has solution \\(x\\) for specific pairs of \\(u,v\\), namely those that lie along the line \\(y=x/2\\).\nComputing the image of the domain is \\(f(\\mathbb{R})=[\\text{line with equation}\\,\\, y=x/2]\\) we can see that it is a subset of the codomain \\(\\mathbb{R}^2\\) and thus not everyone of its elements (those outside the line in question) are not assigned to some real \\(x\\). The function is therefore not surjective.\nExample 3 (cont):\nThe codomain is \\(\\mathbb{R}\\) is it true or false that every one of its elements has a corresponding \\((x,y)\\) living in the domain \\(\\mathbb{R}^2\\).\nLets solve for \\((x,y)\\) the equation:\n\\[\n\\pi(x,y) = u\\implies x=u\n\\]\nThere is indeed a solution for this equation, in fact many of them, of the form \\((u,y)\\) where \\(y\\) is any real number.\nThe image of the domain under \\(\\pi\\) is the same set as the codomain - \\(f(\\mathbb{R}^2)=\\mathbb{R}\\) - and thus the function is surjective.\nExample 4 (cont):\nWe seek, given any \\((u,y)\\) of the codomain, the corresponding \\((x,y)\\) in the domain:\n\\[\n\\Phi(x,y)=(u,v)\\implies \\begin{cases}2x-y=u\\\\x+y=v\\end{cases}\\implies\\begin{cases}x=(u+v)/3\\\\y=(2v-u)/3\\end{cases}\n\\]\nAnother way to check this is to compute the image of the domain: \\(\\Phi(\\mathbb{R}^2)=\\mathbb{R}^2\\), because the column of the matrix are independent. The \\(\\Phi\\) function is surjective.\nExample 5 (cont):\nThe function \\(\\Xi\\) maps \\((x,y)\\in\\mathbb{R}^2\\) into \\((u,v)\\in\\mathbb{R}^2\\). Is it true that every \\((u,y)\\) comes from some \\((x,y)\\)? Lets solve the adequate equation:\n\\[\n\\Xi(x,y)=(u,v) \\implies \\begin{cases}2x-2y=u\\\\x-y=v\\end{cases}\\implies \\begin{cases}x=u/2+y\\\\v=2u\\end{cases}\n\\]\nThe equation \\(l_2\\) tells us that we cannot choose any pair \\(u,v\\) hence not every element of the codomain \\(\\mathbb{R}^2\\) is assigned to some \\((x,y)\\).\nAnother way is to compute the image of the domain, notice as \\(x,y\\) ranges in \\(\\mathbb{R}\\), then \\(x-y\\) ranges in \\(\\mathbb{R}\\) as well, and thus the \\(\\Xi\\) function is just like \\(\\Xi(\\spadesuit) = (2\\spadesuit,\\spadesuit)\\), see example 2.\nThus\n\\(\\Xi(\\mathbb{R}^2) = [\\text{line with equation}\\,\\, y=x/2]\\) again. And the function is not surjective."
  },
  {
    "objectID": "linear_functions.html#maps-involved-in-matrix-multiplication-composition",
    "href": "linear_functions.html#maps-involved-in-matrix-multiplication-composition",
    "title": "Linear functions",
    "section": "Maps involved in matrix multiplication (composition)",
    "text": "Maps involved in matrix multiplication (composition)"
  },
  {
    "objectID": "matrices.html",
    "href": "matrices.html",
    "title": "Matrices",
    "section": "",
    "text": "A matrix is simply put an array of numbers with \\(n\\) rows and \\(m\\) columns. Here are some examples:\n\\[\n\\left(\\begin{matrix}1 \\\\2\\\\ 100  \\end{matrix}\\right)\n\\qquad\n\\left(\\begin{matrix}1 & 1 & -1\\\\2 & -1 & 2  \\end{matrix}\\right)\n\\qquad\n\\left(\\begin{matrix}1 & 0 & 0\\\\0 & 1 & 0\\\\0 & 0 & 1  \\end{matrix}\\right)\n\\qquad\n\\left(\\begin{matrix}1 & 1 \\\\2 & -1\\\\0 & 0  \\end{matrix}\\right)\n\\qquad\n\\left[(-1)^{i+j}\\right]\n\\]\nWhat can we do with matrices?\nAnswer: linear combinations! [Comment: provided their shape is compatible]\nBut this time we’ll introduce more: matrix multiplication, transpose, inverses, determinants.",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Matrices"
    ]
  },
  {
    "objectID": "matrices.html#example-1",
    "href": "matrices.html#example-1",
    "title": "Matrices",
    "section": "Example 1:",
    "text": "Example 1:\nWe want to compute the entries of the matrix \\(AB-BA\\) in terms of the entries of the matrices \\(A\\) and \\(B\\).\n\\[\n(AB)_{ij} = \\sum_k A_{ik}B_{kj} \\qquad (BA)_{ij} = \\sum_k B_{ik}A_{kj}\n\\]\nThus:\n\\[\n(AB-BA)_{ij} = (AB)_{ij} - (BA)_{ij} = \\sum_k A_{ik}B_{kj} - \\sum_k B_{ik}A_{kj}=\\sum_k (A_{ik}B_{kj} -B_{ik}A_{kj})\n\\]",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Matrices"
    ]
  },
  {
    "objectID": "matrices.html#example-2",
    "href": "matrices.html#example-2",
    "title": "Matrices",
    "section": "Example 2:",
    "text": "Example 2:\nCompute all matrices that commute (\\(AB-BA=0I\\)) with the matrix \\(A_{ij}=1\\) with \\(i,j=1,2\\).\nWe see the solution \\(B\\) for the problem:\n\\[\n0=\\sum_{k=1,2} (A_{ik}B_{kj} -B_{ik}A_{kj})\n\\]\nSince the entries of \\(A\\) are all ones this equation reduces to:\n\\[\n0=\\sum_{k=1,2}(B_{kj}-B_{ik}) \\iff 0= B_{1j}+B_{2j}-B_{i1}-B_{i2} \\iff B_{1j}+B_{2j}=B_{i1}+B_{i2}\n\\]\nIn words: \\(B\\) must be in such a way what, the sum of the entries in any of its columns must be the sum of the entries in any of its rows. After some guess work we arrive at the general form for \\(B\\):\n\\[\n\\left(\\begin{matrix}\\alpha & \\beta \\\\\\beta & \\alpha  \\end{matrix}\\right)\n\\]\nExercises: 2.1.All",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Matrices"
    ]
  },
  {
    "objectID": "polynomials.html",
    "href": "polynomials.html",
    "title": "Getting the feet wet with polynomials",
    "section": "",
    "text": "Polynomials are the simplest functions we can study and are an ideal arena to put into practice the two key views on functions:\n\nRelation Point of View\nProcedure Point of View\n\n\n\nUnder the relation perspective, we specify a function by listing all ordered pairs that make up its graph. Here are four examples of polynomial functions:\n\\[\n\\begin{align}&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y=100\\}\\\\&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y=3x+1\\}\\\\&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y=x^2+3\\}\\\\&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y=2x^3-x^2+0x+1\\}\\end{align}\n\\tag{1}\\]\nHow should we interpret this notation?\n\nThese are sets because they appear between \\(\\{\\) and \\(\\}\\).\nStating that \\((x,y)\\in \\mathbb{R}^2\\), just says two things at once:\n\nthe elements of the set are ordered pairs \\((x,y)\\)\nthat \\(x\\in\\mathbb{R}\\) and \\(y\\in \\mathbb{R}\\), in other words, \\(x\\) and \\(y\\) are reals.\n\nthe bar \\(|\\) we see in Equation 1, reads as “such that”, after which we write some rule/property/equation that we wish these ordered pairs of reals to have. For example the equation \\(y=100\\) is the rule we want for the first polynomial in Equation 1, therefore this function is composed by all ordered pairs \\((x,y)\\) with \\(y\\) equal to \\(100\\), while \\(x\\) ranges on all reals.\nTo interpret the notation as a whole, take \\(\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y=3x+1\\}\\). We read is as a “set that contains ordered reals, such that they obey the equation \\(y=3x+1\\)”. As an example, the pair \\((1.3,2.9)\\) obeys the equation because \\(2.9 = 3\\times 1.3 +1\\) is true, therefore it belong to the set, but \\((1,3)\\) does not since \\(3=3\\times 1 +1\\) is false.\n\nThe equations we wrote after \\(|\\) allow us to compute from a given \\(x\\) a unique value \\(y\\). As an example of this fact, choose the equation of the second polynomial, \\(y=3x+1\\), plug in \\(x=0\\), what do we get? We get \\(y=3\\cdot 0 +1\\), that is, just the number \\(y=1\\). Notice we do not get other possible value for \\(y\\), we just get the \\(y=1\\), no other possibility; in fact the same happens for any other \\(x\\) we choose. This is such an obvious fact we actually skip by it. But it is important, because the fact that the equations in Equation 1 have that property ensures us that each possible value of \\(x\\) does not appear two or more times within these set. In such situation we have in our hands more than a set, we have a function!\n\n\n\n\n\n\nCommentary\n\n\n\nThe set \\(\\{(1,2),(1,3),(2,4)\\}\\) have \\(1\\) connected to \\(2\\) and \\(3\\), thus it is not a function. The set\\(\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, x\\in \\mathbb{R} \\land y\\in[2,3]\\}\\) have each real \\(x\\) connected to every real between \\(2\\) and \\(3\\), its also not a function. The set \\(\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y^2=x^2\\}\\) is not a function, if I specify \\(x=1\\) then we get \\(y^2=1\\) whose solution is either \\(y=\\pm 1\\). Therefore we have two y-numbers \\(\\pm1\\) attached to \\(x=1\\).\nThese sets are not functions, but still, they make connections between the values of \\(x\\) and \\(y\\) . Because they relate the values \\(x\\) and \\(y\\), we call these sets relations. Note that functions are also relations.\n\n\nObservation of the right hand side of the rules in Equation 1 show they are all some combination of a power of \\(x\\) times some number (positive or negative), the jargon for this is to say we have a linear combination of powers of \\(x\\). From Equation 1 we conclude the general form of these rules (equations) is:\n\\[\ny=a_nx^n+a_{n-1}x^{n-1}+\\dots+a_1 x+a_0\n\\tag{2}\\]\nwhere the coefficients are labeled by \\(a_0\\) through \\(a_n\\) are reals.\n\n\n\n\n\n\nRecall\n\n\n\nA Cartesian product of the sets \\(X=\\{1,2,3\\}\\) and \\(Y=\\{4,5\\}\\) is defined as:\n\\[\nX\\times Y =\\{(1,4),(1,5),(2,4),(2,5),(3,4),(3,5)\\}\n\\]\n\n\nThe truth set of the statement Equation 2 is the function:\n\\[\nP_n := \\{(x,y)\\in X\\times Y\\,\\,|\\,\\, y=a_nx^n+a_{n-1}x^{n-1}+\\dots+a_1 x+a_0\\}\n\\]\nWhich reads: from the pairs in the Cartesian product of the set \\(X\\) with the set \\(Y\\) we choose only the elements such that the rule \\(y=a_nx^n+a_{n-1}x^{n-1}+\\dots+a_1 x+a_0\\) is obeyed; it is useful to think about it as if we were choosing one by one the elements of \\(X\\times Y\\) which are of the form \\((x,y)\\) and then verifying whether that rules applies or not (True or False), see Figure 1 .\n\n\n\n\n\n\nFigure 1: Step by step construction of a polynomial function.\n\n\n\nThe highest power of \\(x\\) is \\(n\\) and is called the degree of the polynomial. We can and will use it to classify the polynomial functions, that is the reason why we decided to label the set by the symbol \\(P_n\\).\n\n\n\nRather then the view of Figure 1 on how to build the function \\(P_n\\) we adopt a slightly different strategy:\n\n\n\n\n\n\nFigure 2: An alternative way to construct a polynomial function.\n\n\n\nRather than to choose, test and verify each element of the Cartesian product we can instead choose from \\(X\\) the elements one by one and compute the corresponding \\(y\\) by the calculation \\(a_nx^n+a_{n-1}x^{n-1}+\\dots+a_1 x+a_0\\); then we just include the so obtained ordered pair in the set \\(P_n\\). The perceptive in Figure 2 is expressed in mathematical notation as:\n\\[\n\\begin{align}\np_n:X&\\longrightarrow Y\\\\x &\\longmapsto p_n(x):= a_nx^n+a_{n-1}x^{n-1}+\\dots+a_1 x+a_0\n\\end{align}\n\\tag{3}\\]\nLets unpack what does this mean:\n\nOn the first row we find symbols that stand for:\nThe procedure labeled by the symbol \\(p_n\\) maps elements of \\(X\\) into elements in \\(Y\\)\nThis row of information immediately tells us what sets are involved, we can picture it as\n\n\n\n\n\n\n\nFigure 3: Sets involved in the construction of the function, as well as the name of procedure that maps elements of the first into the second. From the first row we still do not know how the map works, this is specified in the second row.\n\n\n\n\\(X\\) is called the domain of the function, while \\(Y\\) is the codomain.\n\nThe second row explains many aspects of the procedure between these sets: the symbol \\(x\\) stands for a generic element of the set \\(X\\) while the symbol \\(p_n(x)\\) represents the element in \\(Y\\), which is assigned to \\(x\\) (notice the \\(\\mapsto\\) arrow), when the procedure \\(p_n\\) acts on it. This row also tell us what is the element \\(p_n(x)\\), by telling what \\(p_n\\) does with the number \\(x\\)! The string of symbols:\n\\[\np_n(x):= a_nx^n+a_{n-1}x^{n-1}+\\dots+a_1 x+a_0\n\\]\nmust now be well understood:\ni) \\(a_nx^n+a_{n-1}x^{n-1}+\\dots+a_1 x+a_0\\) is the LONG symbol for the element of \\(Y\\) which is assigned (connected) to \\(x\\);\nii) it is LONG because it explains how the number comes about. We usually abbreviate the LONG symbol by a short symbol such as \\(p_n(x)\\). The symbol \\(:=\\) means “by definition is equal to”.\n\n\nThe entire definition Equation 3 is the definition of a function, out of convince we name the function after the procedure, and refer to the function just as the “\\(p_n\\) function”. Observe that the symbol \\(p_n\\) is our choice, we can use any symbol as we wish, traditionally we use just one letter \\(f\\), \\(g\\), \\(h\\), etc.\n\n\n\n\n\n\nComment\n\n\n\nThe largest domain supported by the procedure \\(p_n\\) is \\(X=\\mathbb{R}\\) irrespective of the degree \\(n\\), because with any real \\(x\\) we can do the calculation \\(p_n(x)\\); as usual the \\(Y\\) is the codomain and could either be chosen as \\(\\mathbb{R}\\) or the range of the function. The range of a function depends on the degree \\(n\\), being \\(\\mathbb{R}^+_0\\) if \\(n\\) is even and \\(\\mathbb{R}\\) is \\(n\\) is odd integer.\n\n\n\n\n\nConsider the functions:\n\\[\n\\begin{align}\n&F:=\\{(x,y)\\in\\mathbb{R}\\times\\mathbb{R}\\,\\,|\\,\\,y=3x+1\\}\\\\\n&G:=\\{(x,y)\\in\\mathbb{R}^+\\times\\mathbb{R}\\,\\,|\\,\\,y=2x^3-x^2+0x+1\\}\n\\end{align}\n\\]\nThe domain of \\(F\\) is \\(X=\\mathbb{R}\\) while that of \\(G\\) is \\(X=\\mathbb{R}^+\\), the codomains are both \\(Y=\\mathbb{R}\\).\nWe can rewrite the functions \\(F\\) and \\(G\\) using the procedure notation as\n\\[ \\begin{align} f:\\mathbb{R}&\\longrightarrow \\mathbb{R}\\\\ x &\\longmapsto f(x):= 3x+1 \\end{align}  \\tag{4}\\]\n\\[ \\begin{align} g:\\mathbb{R}^+&\\longrightarrow \\mathbb{R}\\\\ x &\\longmapsto g(x):= 2x^3-x^2+1 \\end{align} \\]\nThe range is the set of all outputs of a function, conveniently named by the symbols \\(f(\\mathbb{R})\\) and \\(g(\\mathbb{R})\\). What are these sets? Unfortunately, this requires tools not yet introduced in these notes, to circumvent that, we will simply plot the functions and guess what we think the range is:\n\n\n\n\n\n\n\n\n\nWe can see that the outputs of the \\(F\\) function increase/decrease without bound as \\(x\\) increases, thus the range is \\(\\mathbb{R}\\) in both functions. For the function \\(G\\), as \\(x\\) increase, the outputs do as well; observe \\(x\\) is never zero! Thus the range is \\(\\mathbb{R}^+\\).\n\n\n\n\n\n\nCommentary\n\n\n\nWhen one wishes to refer to functions defined as Equation 3 we simply say “the function \\(p_n(x)\\)”. For example if I want to say Equation 4 has property \\(A\\), then we should say “the function \\(f(x)\\) has property \\(A\\)” or more simply “the function \\(f\\) has property \\(A\\)” or “the function \\(3x+1\\) has property \\(A\\)” or even “the function \\(y=3x+1\\) has property \\(A\\)”. Even though the later are not accurate ( \\(f(x)\\) is a variable symbol, \\(f\\) labels the procedure and \\(y=3x+1\\) is an equation) we will still use it. We sacrifice a bit of accuracy for the sake of brevity.\n\n\n\n\n\nDefine Equation 3 for the first and third function sets in Equation 1. Be careful in specifying what is the range of either function.\n\n\n\nFrom the set:\n\\[\n\\{(x,y)\\in \\mathbb{R}\\times\\mathbb{R}\\,\\,|\\,\\, y=100\\}\n\\]\nwe identify the domain and codomain as the first and second sets in \\(\\mathbb{R}\\times\\mathbb{R}\\), thus the domain is \\(\\mathbb{R}\\) and the codomain is \\(\\mathbb{R}\\) aswell. Meanwhile the range is just is \\(l(\\mathbb{R})=\\{100\\}\\). We can write this function using the procedure notation as:\n\\[\n\\begin{align}\nl:\\mathbb{R}&\\longrightarrow \\{1\\}\\subset \\mathbb{R}\\\\\nx &\\longmapsto l(x):= 100\n\\end{align}\n\\]\nThe second function-set:\n\\[\n\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y=x^2+3\\}\n\\]\nIs translated into:\n\\[\n\\begin{align}\nc:\\mathbb{R}&\\longrightarrow [3,+\\infty[\\subset \\mathbb{R}\\\\\nx &\\longmapsto c(x):= x^2+3\n\\end{align}\n\\]\nThe domain and range are both \\(\\mathbb{R}\\) again but the range is trickier. We want to compute the set \\(c(\\mathbb{R})\\), to do it, we note by inspecting \\(x^2+3\\) that its minimum value occurs when \\(x=0\\) and the larger the \\(x\\), the larger is \\(x^2+3\\), thus \\(c(\\mathbb{R})=[3,+\\infty[\\), which is a subset of the codomain \\(\\mathbb{R}\\)."
  },
  {
    "objectID": "polynomials.html#relation-definition-of-polynomial-functions",
    "href": "polynomials.html#relation-definition-of-polynomial-functions",
    "title": "Getting the feet wet with polynomials",
    "section": "",
    "text": "Under the relation perspective, we specify a function by listing all ordered pairs that make up its graph. Here are four examples of polynomial functions:\n\\[\n\\begin{align}&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y=100\\}\\\\&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y=3x+1\\}\\\\&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y=x^2+3\\}\\\\&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y=2x^3-x^2+0x+1\\}\\end{align}\n\\tag{1}\\]\nHow should we interpret this notation?\n\nThese are sets because they appear between \\(\\{\\) and \\(\\}\\).\nStating that \\((x,y)\\in \\mathbb{R}^2\\), just says two things at once:\n\nthe elements of the set are ordered pairs \\((x,y)\\)\nthat \\(x\\in\\mathbb{R}\\) and \\(y\\in \\mathbb{R}\\), in other words, \\(x\\) and \\(y\\) are reals.\n\nthe bar \\(|\\) we see in Equation 1, reads as “such that”, after which we write some rule/property/equation that we wish these ordered pairs of reals to have. For example the equation \\(y=100\\) is the rule we want for the first polynomial in Equation 1, therefore this function is composed by all ordered pairs \\((x,y)\\) with \\(y\\) equal to \\(100\\), while \\(x\\) ranges on all reals.\nTo interpret the notation as a whole, take \\(\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y=3x+1\\}\\). We read is as a “set that contains ordered reals, such that they obey the equation \\(y=3x+1\\)”. As an example, the pair \\((1.3,2.9)\\) obeys the equation because \\(2.9 = 3\\times 1.3 +1\\) is true, therefore it belong to the set, but \\((1,3)\\) does not since \\(3=3\\times 1 +1\\) is false.\n\nThe equations we wrote after \\(|\\) allow us to compute from a given \\(x\\) a unique value \\(y\\). As an example of this fact, choose the equation of the second polynomial, \\(y=3x+1\\), plug in \\(x=0\\), what do we get? We get \\(y=3\\cdot 0 +1\\), that is, just the number \\(y=1\\). Notice we do not get other possible value for \\(y\\), we just get the \\(y=1\\), no other possibility; in fact the same happens for any other \\(x\\) we choose. This is such an obvious fact we actually skip by it. But it is important, because the fact that the equations in Equation 1 have that property ensures us that each possible value of \\(x\\) does not appear two or more times within these set. In such situation we have in our hands more than a set, we have a function!\n\n\n\n\n\n\nCommentary\n\n\n\nThe set \\(\\{(1,2),(1,3),(2,4)\\}\\) have \\(1\\) connected to \\(2\\) and \\(3\\), thus it is not a function. The set\\(\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, x\\in \\mathbb{R} \\land y\\in[2,3]\\}\\) have each real \\(x\\) connected to every real between \\(2\\) and \\(3\\), its also not a function. The set \\(\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y^2=x^2\\}\\) is not a function, if I specify \\(x=1\\) then we get \\(y^2=1\\) whose solution is either \\(y=\\pm 1\\). Therefore we have two y-numbers \\(\\pm1\\) attached to \\(x=1\\).\nThese sets are not functions, but still, they make connections between the values of \\(x\\) and \\(y\\) . Because they relate the values \\(x\\) and \\(y\\), we call these sets relations. Note that functions are also relations.\n\n\nObservation of the right hand side of the rules in Equation 1 show they are all some combination of a power of \\(x\\) times some number (positive or negative), the jargon for this is to say we have a linear combination of powers of \\(x\\). From Equation 1 we conclude the general form of these rules (equations) is:\n\\[\ny=a_nx^n+a_{n-1}x^{n-1}+\\dots+a_1 x+a_0\n\\tag{2}\\]\nwhere the coefficients are labeled by \\(a_0\\) through \\(a_n\\) are reals.\n\n\n\n\n\n\nRecall\n\n\n\nA Cartesian product of the sets \\(X=\\{1,2,3\\}\\) and \\(Y=\\{4,5\\}\\) is defined as:\n\\[\nX\\times Y =\\{(1,4),(1,5),(2,4),(2,5),(3,4),(3,5)\\}\n\\]\n\n\nThe truth set of the statement Equation 2 is the function:\n\\[\nP_n := \\{(x,y)\\in X\\times Y\\,\\,|\\,\\, y=a_nx^n+a_{n-1}x^{n-1}+\\dots+a_1 x+a_0\\}\n\\]\nWhich reads: from the pairs in the Cartesian product of the set \\(X\\) with the set \\(Y\\) we choose only the elements such that the rule \\(y=a_nx^n+a_{n-1}x^{n-1}+\\dots+a_1 x+a_0\\) is obeyed; it is useful to think about it as if we were choosing one by one the elements of \\(X\\times Y\\) which are of the form \\((x,y)\\) and then verifying whether that rules applies or not (True or False), see Figure 1 .\n\n\n\n\n\n\nFigure 1: Step by step construction of a polynomial function.\n\n\n\nThe highest power of \\(x\\) is \\(n\\) and is called the degree of the polynomial. We can and will use it to classify the polynomial functions, that is the reason why we decided to label the set by the symbol \\(P_n\\)."
  },
  {
    "objectID": "polynomials.html#procedure-definition-of-polynomials-functions",
    "href": "polynomials.html#procedure-definition-of-polynomials-functions",
    "title": "Getting the feet wet with polynomials",
    "section": "",
    "text": "Rather then the view of Figure 1 on how to build the function \\(P_n\\) we adopt a slightly different strategy:\n\n\n\n\n\n\nFigure 2: An alternative way to construct a polynomial function.\n\n\n\nRather than to choose, test and verify each element of the Cartesian product we can instead choose from \\(X\\) the elements one by one and compute the corresponding \\(y\\) by the calculation \\(a_nx^n+a_{n-1}x^{n-1}+\\dots+a_1 x+a_0\\); then we just include the so obtained ordered pair in the set \\(P_n\\). The perceptive in Figure 2 is expressed in mathematical notation as:\n\\[\n\\begin{align}\np_n:X&\\longrightarrow Y\\\\x &\\longmapsto p_n(x):= a_nx^n+a_{n-1}x^{n-1}+\\dots+a_1 x+a_0\n\\end{align}\n\\tag{3}\\]\nLets unpack what does this mean:\n\nOn the first row we find symbols that stand for:\nThe procedure labeled by the symbol \\(p_n\\) maps elements of \\(X\\) into elements in \\(Y\\)\nThis row of information immediately tells us what sets are involved, we can picture it as\n\n\n\n\n\n\n\nFigure 3: Sets involved in the construction of the function, as well as the name of procedure that maps elements of the first into the second. From the first row we still do not know how the map works, this is specified in the second row.\n\n\n\n\\(X\\) is called the domain of the function, while \\(Y\\) is the codomain.\n\nThe second row explains many aspects of the procedure between these sets: the symbol \\(x\\) stands for a generic element of the set \\(X\\) while the symbol \\(p_n(x)\\) represents the element in \\(Y\\), which is assigned to \\(x\\) (notice the \\(\\mapsto\\) arrow), when the procedure \\(p_n\\) acts on it. This row also tell us what is the element \\(p_n(x)\\), by telling what \\(p_n\\) does with the number \\(x\\)! The string of symbols:\n\\[\np_n(x):= a_nx^n+a_{n-1}x^{n-1}+\\dots+a_1 x+a_0\n\\]\nmust now be well understood:\ni) \\(a_nx^n+a_{n-1}x^{n-1}+\\dots+a_1 x+a_0\\) is the LONG symbol for the element of \\(Y\\) which is assigned (connected) to \\(x\\);\nii) it is LONG because it explains how the number comes about. We usually abbreviate the LONG symbol by a short symbol such as \\(p_n(x)\\). The symbol \\(:=\\) means “by definition is equal to”.\n\n\nThe entire definition Equation 3 is the definition of a function, out of convince we name the function after the procedure, and refer to the function just as the “\\(p_n\\) function”. Observe that the symbol \\(p_n\\) is our choice, we can use any symbol as we wish, traditionally we use just one letter \\(f\\), \\(g\\), \\(h\\), etc.\n\n\n\n\n\n\nComment\n\n\n\nThe largest domain supported by the procedure \\(p_n\\) is \\(X=\\mathbb{R}\\) irrespective of the degree \\(n\\), because with any real \\(x\\) we can do the calculation \\(p_n(x)\\); as usual the \\(Y\\) is the codomain and could either be chosen as \\(\\mathbb{R}\\) or the range of the function. The range of a function depends on the degree \\(n\\), being \\(\\mathbb{R}^+_0\\) if \\(n\\) is even and \\(\\mathbb{R}\\) is \\(n\\) is odd integer."
  },
  {
    "objectID": "polynomials.html#examples",
    "href": "polynomials.html#examples",
    "title": "Getting the feet wet with polynomials",
    "section": "",
    "text": "Consider the functions:\n\\[\n\\begin{align}\n&F:=\\{(x,y)\\in\\mathbb{R}\\times\\mathbb{R}\\,\\,|\\,\\,y=3x+1\\}\\\\\n&G:=\\{(x,y)\\in\\mathbb{R}^+\\times\\mathbb{R}\\,\\,|\\,\\,y=2x^3-x^2+0x+1\\}\n\\end{align}\n\\]\nThe domain of \\(F\\) is \\(X=\\mathbb{R}\\) while that of \\(G\\) is \\(X=\\mathbb{R}^+\\), the codomains are both \\(Y=\\mathbb{R}\\).\nWe can rewrite the functions \\(F\\) and \\(G\\) using the procedure notation as\n\\[ \\begin{align} f:\\mathbb{R}&\\longrightarrow \\mathbb{R}\\\\ x &\\longmapsto f(x):= 3x+1 \\end{align}  \\tag{4}\\]\n\\[ \\begin{align} g:\\mathbb{R}^+&\\longrightarrow \\mathbb{R}\\\\ x &\\longmapsto g(x):= 2x^3-x^2+1 \\end{align} \\]\nThe range is the set of all outputs of a function, conveniently named by the symbols \\(f(\\mathbb{R})\\) and \\(g(\\mathbb{R})\\). What are these sets? Unfortunately, this requires tools not yet introduced in these notes, to circumvent that, we will simply plot the functions and guess what we think the range is:\n\n\n\n\n\n\n\n\n\nWe can see that the outputs of the \\(F\\) function increase/decrease without bound as \\(x\\) increases, thus the range is \\(\\mathbb{R}\\) in both functions. For the function \\(G\\), as \\(x\\) increase, the outputs do as well; observe \\(x\\) is never zero! Thus the range is \\(\\mathbb{R}^+\\).\n\n\n\n\n\n\nCommentary\n\n\n\nWhen one wishes to refer to functions defined as Equation 3 we simply say “the function \\(p_n(x)\\)”. For example if I want to say Equation 4 has property \\(A\\), then we should say “the function \\(f(x)\\) has property \\(A\\)” or more simply “the function \\(f\\) has property \\(A\\)” or “the function \\(3x+1\\) has property \\(A\\)” or even “the function \\(y=3x+1\\) has property \\(A\\)”. Even though the later are not accurate ( \\(f(x)\\) is a variable symbol, \\(f\\) labels the procedure and \\(y=3x+1\\) is an equation) we will still use it. We sacrifice a bit of accuracy for the sake of brevity."
  },
  {
    "objectID": "polynomials.html#exercise",
    "href": "polynomials.html#exercise",
    "title": "Getting the feet wet with polynomials",
    "section": "",
    "text": "Define Equation 3 for the first and third function sets in Equation 1. Be careful in specifying what is the range of either function."
  },
  {
    "objectID": "polynomials.html#answer",
    "href": "polynomials.html#answer",
    "title": "Getting the feet wet with polynomials",
    "section": "",
    "text": "From the set:\n\\[\n\\{(x,y)\\in \\mathbb{R}\\times\\mathbb{R}\\,\\,|\\,\\, y=100\\}\n\\]\nwe identify the domain and codomain as the first and second sets in \\(\\mathbb{R}\\times\\mathbb{R}\\), thus the domain is \\(\\mathbb{R}\\) and the codomain is \\(\\mathbb{R}\\) aswell. Meanwhile the range is just is \\(l(\\mathbb{R})=\\{100\\}\\). We can write this function using the procedure notation as:\n\\[\n\\begin{align}\nl:\\mathbb{R}&\\longrightarrow \\{1\\}\\subset \\mathbb{R}\\\\\nx &\\longmapsto l(x):= 100\n\\end{align}\n\\]\nThe second function-set:\n\\[\n\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y=x^2+3\\}\n\\]\nIs translated into:\n\\[\n\\begin{align}\nc:\\mathbb{R}&\\longrightarrow [3,+\\infty[\\subset \\mathbb{R}\\\\\nx &\\longmapsto c(x):= x^2+3\n\\end{align}\n\\]\nThe domain and range are both \\(\\mathbb{R}\\) again but the range is trickier. We want to compute the set \\(c(\\mathbb{R})\\), to do it, we note by inspecting \\(x^2+3\\) that its minimum value occurs when \\(x=0\\) and the larger the \\(x\\), the larger is \\(x^2+3\\), thus \\(c(\\mathbb{R})=[3,+\\infty[\\), which is a subset of the codomain \\(\\mathbb{R}\\)."
  },
  {
    "objectID": "pol_division_PT.html",
    "href": "pol_division_PT.html",
    "title": "pol_division",
    "section": "",
    "text": "The goal of polynomial division is relate two polynomials.\nFor example:\n\nThe equation that relates and is\nThe equation that relates and is\n\nWhy compute relations of this form? This relation breaks the polynomial into simpler polynomials. It is easier to analyse the simpler parts.\n\nThe algorithm for division of polynomials is analogous to the algorithm of long division learned in elementary school. We shall start by understanding the basic ideas behind long division and then use them in the division of polynomials."
  },
  {
    "objectID": "pol_division_PT.html#exemplo-1-223",
    "href": "pol_division_PT.html#exemplo-1-223",
    "title": "pol_division",
    "section": "Exemplo 1: \\(22/3\\)",
    "text": "Exemplo 1: \\(22/3\\)\n\n\nO múltiplo de \\(3\\) mais perto de \\(22\\) é \\(7\\).\nComo \\(7\\times 3 = 21\\), a subtracção \\(22-21\\) dá-nos \\(1\\).\nDado que \\(1\\) não é divisível por \\(3\\), este é considerado o resto da divisão.\nConcluímos que: \\(22 = 7\\times 3 +1\\).\n\n\nUma forma alternativa de calcular \\(22/3\\)\nO quociente de inteiros \\(p/d\\) é o número que nos diz quantas vezes \\(d\\) cabe em \\(p\\), esta é a ideia chave!\nPor sua vez este número diz-nos quem é maior, \\(p\\) ou \\(d\\)?\n\nQuando \\(p/d&lt;1\\) então \\(d\\) é maior.\nQuando \\(p/d=1\\) são o mesmo número.\nQuando o quociente é maior do que \\(1\\), significa que \\(p\\) é maior.\n\nFoque-mo-nos no último caso.\nConsidera \\(22\\) e \\(3\\), repara que, \\(3\\), cabe \\(7\\) vezes em \\(22\\) pois \\(3\\times 7=21\\). Concluímos então que \\(22=7\\cdot 3+1\\), o resto \\(1\\) é a pequena correcção que falta a \\(3\\times 7\\) para chegar a \\(22\\). A formula\n\\[\n22=7\\cdot 3 +1\n\\]\nrelacciona os números \\(22\\) e \\(3\\). Observa, no lado esquerdo um único número \\(22\\) e no lado direito \\(7\\times 3 +1\\) as peças que o compõem.\nUsando em geral esta decomposição tem as seguintes características:\nSejam \\(p\\) e \\(d\\) dois interiros. Então existe um inteiro \\(r\\) tal que \\(0\\leq r&lt;d\\) e um inteiro \\(q\\geq 0\\) tal que:\n\\[\np=qd+r\n\\]\nA ideia chave para realizar o calculo \\(22/3\\) é ver quantas vezes \\(3\\) cabe dentro de \\(22\\), por outras palavras, a ideia chave é encontrar o número \\(q\\) que garante a correcção \\(r\\) mais pequena (neste caso inferior a \\(3\\)), ou seja:\n\\[\n22=q\\cdot 3+r \\qquad 0\\leq r&lt;3\n\\]\nComo é que encontramos este número \\(q\\)? Resposta: Adivinhamos a resposta. Eis alguns exemplos:\n\nE se \\(q=6\\)?\nSubstituindo na equação obtemos \\(22=6\\cdot 3+r\\) o que é equivalente a\\(r=22-18=4\\). Da do que o resto não está entre \\(0\\) e \\(3\\), temos de parti-lo em partes mais pequenas:\n\\[\n4 = q'\\cdot 3 + r' \\qquad 0\\leq r' &lt; 3\n\\]\nAdivinhando que a resposta é \\(q'=1\\), obtemos \\(r'=1\\).\nEm conclusão:\n\\[\n22 = 6\\times 3 + 1\\times 3 + +1\n\\]\nOu seja:\n\\[\n22 = 7\\times 3 +1\n\\]\nE se \\(q=7\\)?\nSubstituindo obtemos\n\\[\n22 = 7\\times 3 + r\n\\]\no que é equivalente a\n\\[\nr = 22-21 = 1\n\\]\nConcluímos que\n\\[\n22 = 7\\times 3 +1\n\\]\nE se \\(q=8\\) ?\n\\[\n22 = 8\\times 3 + r \\iff r=22-24 =-2\n\\]\nEntão\n\\[\n22 = 8\\times 3 -2\n\\]\nComo o resto é negativo e precisamos dele positivo, vamos corrigir esta expressão adicionado \\(0=3-3\\):\n\\[\n22 = 8\\times 3 -2 +3-3 = (8-1)\\times 3 +(3-2) = 7\\times 3 +1\n\\]"
  },
  {
    "objectID": "pol_division_PT.html#example-2-100112",
    "href": "pol_division_PT.html#example-2-100112",
    "title": "pol_division",
    "section": "Example 2: \\(1001/12\\)",
    "text": "Example 2: \\(1001/12\\)\nLet us consider now a more sophisticated division \\(1001/12\\), how do we argue as in elementary school?\nWe would first ask:\n\n\n\n\n\nImplicit in this question is the observation that \\(1001 = 100\\times 10 +1\\).\nThe answer is obviously \\(8\\):\n\n\n\n\n\nLook the position of the \\(8\\) is at the \\(10\\)’s place (under the \\(1\\) in \\(12\\)).\nMultiplying now \\(8\\times 12 = 96\\) we place it under the \\(10\\)’s place of \\(1001\\):\n\n\n\n\n\nNow we remove from \\(1001\\) the value \\(96 \\times 10\\) since \\(96\\) sits under the \\(10\\)’s place, the result is a \\(4\\) under the \\(10\\)’s place:\n\n\n\n\n\nRemoving \\(96\\times 10\\) from \\(1000\\) yields the \\(4 \\times 10\\), but removing \\(96\\times 10\\) from \\(1001\\) gives \\(4\\times 10 +1\\), thus we drop the \\(1\\) in \\(1001\\) and ask:\n\n\n\n\n\nThe answer is \\(3\\), we place it under the \\(1\\)’s place (under the \\(2\\) in \\(12\\)):\n\n\n\n\n\nMultiplying \\(3\\times 12\\) we obtain \\(36\\), we write it under the \\(1\\)’s place of \\(41\\):\n\n\n\n\n\nRemoving from the current remainder \\(41\\) this \\(36\\) gives:\n\n\n\n\n\nCombining all these removals from \\(1001\\) we conclude:\n\n\n\n\n\nWhat we have been computing in elementary school with this dividion algorithm is the solution \\(q\\) and \\(r\\) of the equation:\n\\[\n1001 = q \\times 12 + r \\qquad 0\\leq r&lt;12\n\\]\nThere are many paths and we follow then in stages.\nThe extreme of simplicity is to immediately guess the answer as \\(q=83\\) and \\(r=5\\), you do this if you are a genius, a path of a single step.\nDepending on the quality of our guesses we increase the number of steps.\nGuess 1:\nObserving that \\(1001=600+401\\) and \\(5\\times 12 =60\\) we guess that actual solution \\(q\\) is close to \\(10\\times 5\\):\n\\[\n600+401 = \\overbrace{10 \\times 5}^{q} \\times 12 +r\n\\]\nwith the remainder \\(r=401\\). Since it is not between \\(0\\) and \\(12\\) we can do better. Lets break \\(401\\) by guessing what is \\(q'\\) and \\(r'\\) such that:\n\\[\n401 = q'\\times 12 + r' \\qquad 0\\leq r'&lt;12\n\\]\nWe think \\(q'=3\\times10\\) is a good solution since \\(q'\\times 12 = 360\\), as a result the remainder is \\(r'=401-360=41\\)\nNot ideal, we want it at most \\(12\\). Therefore we again break into pieces:\n\\[\n41 = q''\\times 12 + r'' \\qquad 0\\leq r''&lt;12\n\\]\nWith the guess \\(q''=3\\) and the current remainder is just \\(r''=41-36=5\\).\nNote, finally!! We have a remainder smaller than \\(12\\).\nCollecting out calculations we have:\n\\[\n1001 = \\overbrace{(10 \\times 5 + 3\\times 10 + 3)}^{q=83}\\times 12 + 5\n\\]\n\n\n\n\n\n\nCommentary\n\n\n\nWhen guessing \\(q\\) it is useful to break \\(p\\) (in the example above \\(p=1001\\)) into a large and a smaller part.\n\\[\np=[\\text{large part}]+[\\text{small part}]\n\\]\nAnd then guess the best you can the \\(q\\) that cancels the large part.\nThere are many ways to break \\(p\\), experience will tell you what is ideal or not."
  },
  {
    "objectID": "question_q_13.html",
    "href": "question_q_13.html",
    "title": "asymptotic to MARS",
    "section": "",
    "text": "Exercise 1 Consider the sets \\(A\\coloneqq \\{(x,y) \\in \\mathbb{R}\\times\\mathbb{R}\\,|\\,\\, y=1\\}\\) and \\(B\\coloneqq \\{(x,y) \\in \\mathbb{R}\\times \\mathbb{R}\\,|\\,\\, x=1\\}\\). Convince yourself whether it is or not a function using the Definition 1. For both specify the domain, range and codomain. What about \\(B'\\coloneqq \\{(x,y) \\in \\mathbb{R}\\times \\{2\\}\\,|\\,\\, x=1\\}\\)?\n\n\nSolution 1. \\(A\\) is a function because all real \\(x\\) is assigned to \\(1\\), so for all \\(x\\) is paired uniquely with \\(1\\). \\(B\\) is not a function as \\(x=1\\) is paired with every real \\(y\\), so the pairs where \\(x\\) occur are not unique. \\(D_A=\\mathbb{R}\\), \\(R_A=\\{1\\}\\), \\(D_B=\\{1\\}\\), \\(R_B=\\mathbb{R}\\), the codomain is equal in both cases to \\(\\mathbb{R}\\). The set \\(B'=\\{(1,2)\\}\\) is a function, although an weird one, because it is just one ordered pair. \\(D_{B'}=\\{1\\}\\), \\(R_B'=\\{2\\}\\). The starting and codomain can be red from \\(\\mathbb{R}\\times \\{2\\}\\)."
  },
  {
    "objectID": "rutherfords atom.html",
    "href": "rutherfords atom.html",
    "title": "rutherfords atom",
    "section": "",
    "text": "Átomo era indivisível (Maxwell 1872)\n1899 - Thomson nos seus estudos da condução de electricidade observa que átomo é divisível, a corrente eléctrica evolve movimento livre de partes (electrões) de um átomo\nExperiências com radioactividade realizadas por Marie Curie (1867- 1934), Ernest Rutherford (1871-1937) e Frederick Soddy apontam também para divisibilidade do átomo. Descobriu-se que a transformação radioactiva dos átomos envolve a emissão de partículas pelos átomos (partículas subatómicas)\nEm 1910 as experiências de Lenard mostraram que electrões conseguem atravessar folhas de metal finas, mostrando assim que átomos são porosos.\nDescoberta que cada elemento tem-lhe associado uma serie espectral, i.e., um conjunto de comprimentos de onda de radiação emitida. Tal sugere que deve haver movimento de cargas no interior dos átomos.\n\nTodas estas descobertas levaram ao desenvolvimento de modelos do átomo:\n\nModelo de Thomson (1898) - Átomos são esferas de cargas positivas e negativas distribuídas uniformemente (passas no pudim). Este modelo evita o colapso do átomo, mas não explica as series espectrais dos elementos.\nModelo de Rutherford, Hans Geiger (1882-1945), Enerst Marsden (1899-1970) através de experiências realizadas entre 1909 e 1914 conduzem à descoberta que átomos têm estrutura interna, um núcleo onde a maior parte da massa está concentrada e onde está também localizada toda a carga positiva do átomo. A sua experiência consiste em:\n\nO seu modelo de átomo:\n\n\nThe theoretical framework that fits his experimental observations lead to the famous Rutherford formula."
  },
  {
    "objectID": "special relativity.html",
    "href": "special relativity.html",
    "title": "Special Relativity",
    "section": "",
    "text": "Key Ideas of Taylor Chap. 15"
  },
  {
    "objectID": "special relativity.html#galilean-transformation",
    "href": "special relativity.html#galilean-transformation",
    "title": "Special Relativity",
    "section": "Galilean Transformation",
    "text": "Galilean Transformation\nConsider a set of reference frames that move at relative constant velocity. Let us deduce a particular and simple case of Galilean transformations between two frames, this simple case is a consequence of the following assumption on the two chosen frames:\n\nThe frames S and S’ are oriented in such a way that the x,y,z-axes are parallel and they move with velocity \\(V\\) along the x axis, see Fig. 15.1\nS and S’ are equipped with a clock, to measure time and both clocks measure \\(t=t'=0\\) when initially both frames’s origin coincide.\n\nAs a result we have the following relation between measurements \\((x,y,z,t)\\) and \\((x',y',z',t')\\):\n\\[\n\\begin{cases}\nx' =x-Vt\\\\\ny'=y\\\\\nz'=z\\\\\nt'=t\n\\end{cases}\n\\]\nA generalization of these formulas for two frames where \\(V\\) has any direction \\(\\mathbf{V}\\) is\n\\[\n\\mathbf{r}'=\\mathbf{r}-\\mathbf{V}t \\qquad t'=t\n\\tag{1}\\]\nFurther generalizations are possible if we allow the two frame’s axes to be non-parallel but rotated, but the formula above suffices.\nDifferentiating the relation we find:\n\\[\n\\mathbf{v}'=\\mathbf{v}-\\mathbf{V}\n\\tag{2}\\]"
  },
  {
    "objectID": "special relativity.html#measurement-of-time-in-a-single-frame",
    "href": "special relativity.html#measurement-of-time-in-a-single-frame",
    "title": "Special Relativity",
    "section": "Measurement of Time in a Single Frame",
    "text": "Measurement of Time in a Single Frame\nAttached to any frame S is a square grid, at each vertex there is someone ready measure \\((x,y,z,t)\\) of a nearby event."
  },
  {
    "objectID": "special relativity.html#time-dilation",
    "href": "special relativity.html#time-dilation",
    "title": "Special Relativity",
    "section": "Time Dilation",
    "text": "Time Dilation\nConsider the train situation with the two frames S and S’.\nS is inertial and thus S’ is inertial, therefore the law distance = velocity \\(\\times\\) time holds in both. In particular for S’\n\\[\n\\Delta t' =\\frac{2h}{c}\n\\]\nUsing again the rule distance = velocity \\(\\times\\) time in S we find the lengths of the sides of the triangle, which in turn are related by\n\\[\n(\\frac{c\\Delta t}{2})^2=h^2+(\\frac{V\\Delta t}{2})^2\n\\]\nwhich is equivalent to:\n\\[\n\\Delta t = \\frac{2h}{c}\\frac{1}{\\sqrt{1-\\beta^2}} \\qquad \\beta = \\frac{V}{c}\n\\]\nWe conclude:\n\\[\n\\Delta t = \\frac{\\Delta t'}{\\sqrt{1-\\beta^2}}\n\\]\nTwo events occurring at the same place in S’ separated by \\(\\Delta t'\\) seconds are seen in S as being separated by \\(\\Delta t\\) seconds.\nWe want to reformulate this expression, define\n\\[\n\\gamma = \\frac{1}{\\sqrt{1-\\beta^2}}\n\\]\nand \\(\\Delta t_0:= \\Delta t'\\) the time elapsed in the reference frame (in this case S’) where the event occurred at the same place.\nThus:\n\\[\n\\Delta t = \\gamma \\Delta t_0\\geq \\Delta t_0\n\\]\nsince \\(\\gamma \\geq 1\\).\nWe should think about this formula as:\n\\[\n[\\text{time elaspsed measured in two places}] = \\gamma\\times [\\text{time elapsed measured at same place}]\n\\tag{3}\\]"
  },
  {
    "objectID": "systems_of_equations.html",
    "href": "systems_of_equations.html",
    "title": "Systems of equations",
    "section": "",
    "text": "Lets review the method you use in high-school to solve systems of equations - the elimination method - and then rewrite it under a new notation - the matrix and vector notation.\nA few examples will be given.\nKey concepts: systems of equations in matrix-vector notation, elimination, pivot, rank, conditions for 1,0 or infinite solutions.",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Solving systems using elimination"
    ]
  },
  {
    "objectID": "systems_of_equations.html#matrix-vector-notation-for-a-system",
    "href": "systems_of_equations.html#matrix-vector-notation-for-a-system",
    "title": "Systems of equations",
    "section": "Matrix-vector notation for a system",
    "text": "Matrix-vector notation for a system\n\n\n\n\n\n\nVideo\n\n\n\nTranslating between notation\n\n\nConsider the system with two equations, called \\(l_1\\) and \\(l_2\\):\n\\[\n\\begin{cases}\nx-4y =2\\\\\n2x-6y = 5\n\\end{cases}\n\\tag{1}\\]\nIn Equation 1 we find two equations and two unknowns \\(x\\) and \\(y\\). We want their values such that both equations are satisfied. (this system represents the interception of two lines)\nUsing the matrix-vector notation we can write Equation 1 as:\n\\[\n\\begin{pmatrix} 1 & -4\\\\ 2 & -6 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}2\\\\5\\end{pmatrix}\n\\tag{2}\\]\nLets read Equation 2 in words: the \\(2\\) by \\(2\\) matrix of coefficients is multiplied by the column vector \\(\\begin{pmatrix}x\\\\y\\end{pmatrix}\\) , the result is \\(\\begin{pmatrix}2\\\\5\\end{pmatrix}\\). This is one equation with one unknown, the column vector \\(\\begin{pmatrix}x\\\\y\\end{pmatrix}\\). Traditionally we write Equation 2 as \\(A \\mathbf{x} =\\mathbf{b}\\).\nHow do we multiply a vector by a matrix?\nAnswer:\n\\[\n\\overbrace{\\begin{pmatrix} 1 & -4\\\\ 2 & -6 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}}^\\text{matrix-vector mult.} =\\overbrace{\\begin{pmatrix}1\\cdot x-4\\cdot y \\\\ 2\\cdot x-6\\cdot y\\end{pmatrix}}^\\text{scalar mult.}\n\\]\nMatrix times a vector on the lhs is just a super-compact way of writing the vector on the rhs. Moreover, notice the shapes of the matrix and vectors, this is very, very important. A \\(2\\) by \\(2\\) matrix times a \\(2\\) by \\(1\\) column vector yields a \\(2\\) by \\(1\\) column vector! If you understand this it should not be a problem to see what shapes are or not compatible, check this:\n\n[\\(2\\times2\\)][ \\(3\\times 1\\)] \\(=\\) Nonsense\n[\\(3\\times2\\)][ \\(2\\times 1\\)] \\(=\\) [\\(3\\times 1\\)]\n[\\(2\\times3\\)][ \\(3\\times 1\\)] \\(=\\) [\\(2\\times 1\\)]\n[\\(3\\times2\\)][ \\(3\\times 1\\)] \\(=\\) Nonsense\n[\\(1\\times3\\)][ \\(3\\times 1\\)] \\(=\\) [\\(1\\times 1\\)]",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Solving systems using elimination"
    ]
  },
  {
    "objectID": "systems_of_equations.html#solving-the-system-using-the-elimination-method",
    "href": "systems_of_equations.html#solving-the-system-using-the-elimination-method",
    "title": "Systems of equations",
    "section": "Solving the system using the Elimination method",
    "text": "Solving the system using the Elimination method\n\n\n\n\n\n\nVideo\n\n\n\nIdeas behind elimination method\n\n\nLets solve the system Equation 1 using the traditional rules we already know from high-school, recall:\n\nyou can replace an equation by itself times some constant.\nyou can replace any one equation by the sum both equations.\nyou can isolate \\(x\\) or \\(y\\) in one equation and substitute in the other equation.\n\nIn other words, 1. and 2., just say this: you can replace \\(l_1\\) or \\(l_2\\) by some convenient combination \\(al_1+bl_2\\). Rule 3. is known as back-substitution.\nApplying any one of these operations yields another and equivalent system of equations.\nThe central idea of the Elimination method is use linear combination of equations (1. and 2.) to eliminate variables and thus giving us an equivalent and easier to solve system. To eliminate variables we make clever use of rules \\(1\\) and \\(2\\). Once the system is simple enough we can use rule \\(3\\). How do you know what is or not a good combination? We’ll see that with examples. But the guiding principle is to use the pivots.\nThis recaps what you know, now lets use these rules to solve the Equation 1 and in parallel see the corresponding matrix-vector version.\n\nstep 1: Replace equation \\(l_2\\) by, \\(l_2\\) minus twice the equation \\(l_1\\), i.e., make the new second equation \\(l_2'\\) into \\(l_2-2l_1\\). This gives us:\n\\[\n\\begin{cases}x-4y =2\\\\2x-6y = 5\\end{cases} \\overset{l_2'=l_2-2l_1}{\\longrightarrow}\\begin{cases} x-4y =2\\\\ 2y = 1\\end{cases}\n\\]\nCorrespondingly we subtract from row \\(l_2\\) twice the row \\(l_1\\) in Equation 2, giving us\n\\[\n\\begin{pmatrix} 1 & -4\\\\ 2 & -6 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}2\\\\5\\end{pmatrix}\\overset{l_2'=l_2-2l_1}{\\longrightarrow} \\begin{pmatrix} 1 & -4\\\\ 0 & 2 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}2\\\\1\\end{pmatrix}\n\\]\nA good way to look at this is to focus on using the \\(1\\), to eliminate the \\(2\\). This entry of the matrix we focus on is called a pivot entry!\nstep 2: Multiply equation \\(l_2\\) by \\(1/2\\):\n\\[\n\\begin{cases} x-4y =2\\\\ 2y = 1\\end{cases}\\overset{l_2'=1/2l_2}{\\longrightarrow} \\begin{cases} x-4y =2\\\\ y = 1/2\\end{cases}\n\\]\nIn matrix-vector notation we find:\n\\[\n\\begin{pmatrix} 1 & -4\\\\ 0 & 2 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}2\\\\1\\end{pmatrix}\\overset{l_2'=1/2l_2}{\\longrightarrow} \\begin{pmatrix} 1 & -4\\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}2\\\\1/2\\end{pmatrix}\n\\]\nstep 3: Focusing on the second pivot entry, we eliminate the entry, \\(-4\\), by replacing \\(l_1\\) by \\(l_1\\) plus four times \\(l_2\\):\n\\[\n\\begin{cases} x -4y =2\\\\ y = 1/2\\end{cases}\\overset{l_1'=l_1+4l_2}{\\longrightarrow}\\begin{cases} x =4\\\\ y = 1/2\\end{cases}\n\\]\nIn matrix-vector notation we find:\n\\[\n\\begin{pmatrix} 1 & -4\\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}2\\\\1/2\\end{pmatrix}\\overset{l_1'=l_1+4l_2}{\\longrightarrow} \\begin{pmatrix} 1 & 0\\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}4\\\\1/2\\end{pmatrix}\n\\]\nFrom which we can read the final result \\(x=4\\) and \\(y=1/2\\).\n\nBetter notation: Going through the three steps again we notice we can improve our matrix-vector notation by suppressing from it the column \\(\\begin{pmatrix}x\\\\y\\end{pmatrix}\\) and writing instead the steps as:\n\\[\n\\left(\\begin{matrix} 1 & -4 \\\\ 2 & -6 \\end{matrix}\\;\\middle|\\; \\begin{matrix} 2\\\\5\\end{matrix}\\right)\n\\overset{l_2'=l_2-2l_1}{\\longrightarrow}\n\\left(\\begin{matrix} 1 & -4 \\\\ 0 & 2 \\end{matrix}\\;\\middle|\\; \\begin{matrix} 2\\\\1\\end{matrix}\\right)\n\\overset{l_2'=1/2l_2}{\\longrightarrow}\n\\left(\\begin{matrix} 1 & -4 \\\\ 0 & 1 \\end{matrix}\\;\\middle|\\; \\begin{matrix} 2\\\\1/2\\end{matrix}\\right)\n\\overset{l_1'=l_1+4l_2}{\\longrightarrow}\n\\left(\\begin{matrix} 1 & 0 \\\\ 0 & 1 \\end{matrix}\\;\\middle|\\; \\begin{matrix} 4\\\\1/2\\end{matrix}\\right)\n\\]\nFrom now on we’ll adopt this way of writing systems of equations, its called the extended matrix notation, because we appended a new column to right side of the \\(2\\times2\\) matrix. From the extended matrix we read the solution as follows\n\\[\n\\left(\\begin{matrix} 1 & 0 \\\\ 0 & 1 \\end{matrix}\\;\\middle|\\; \\begin{matrix} 4\\\\1/2\\end{matrix}\\right)\\longrightarrow \\begin{pmatrix} 1 & 0\\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}4\\\\1/2\\end{pmatrix}\\longrightarrow \\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}4\\\\1/2\\end{pmatrix}\n\\]\nwhere in the last step we multiplied the vector by the matrix.\nThe vector\n\\[\n\\begin{pmatrix}4\\\\1/2\\end{pmatrix}\n\\]\nis the solution of\n\\[\n\\begin{pmatrix} 1 & -4\\\\ 2 & -6 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}2\\\\5\\end{pmatrix}\n\\]\nMeaning, the solution of\n\\[\n\\begin{cases}x-4y =2\\\\2x-6y = 5\\end{cases}\n\\]\nis:\n\\[\n\\begin{cases}x =4\\\\y=1/2\\end{cases}\n\\]",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Solving systems using elimination"
    ]
  },
  {
    "objectID": "systems_of_equations.html#more-examples-of-the-elimination-method",
    "href": "systems_of_equations.html#more-examples-of-the-elimination-method",
    "title": "Systems of equations",
    "section": "More examples of the elimination method",
    "text": "More examples of the elimination method\nThe following examples will be given:\n\nA system with one solution\nA system with no solution\nA system with many solutions\nAnother system with many solutions\n\n\nExample 1: A system with one solution\n\n\n\n\n\n\nVideo\n\n\n\nthe example below in video\n\n\nConsider the system, which we write in three different notations.\n\\[\n\\begin{cases}\nx+y-z=1\\\\\n2x-y+2z = 9\\\\\n2y=-x+z\n\\end{cases}\n\\leftrightsquigarrow\n\\begin{pmatrix}\n1 & 1 & -1\\\\\n2 & -1 & 2\\\\\n1 & 2 &-1\n\\end{pmatrix}\n\\begin{pmatrix}\nx\\\\\ny\\\\\nz\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1\\\\\n9\\\\\n0\n\\end{pmatrix}\n\\leftrightsquigarrow\n\\left(\n\\begin{matrix}\n1 & 1 & -1\\\\\n2 & -1 & 2\\\\\n1 & 2 & -1\n\\end{matrix}\n\\;\\middle|\\;\n\\begin{matrix}\n1\\\\\n9\\\\\n0\n\\end{matrix}\n\\right)\n\\tag{3}\\]\n(This system represents the interception of three planes at one point, each row of the matrix is a vector perpendicular to a plane)\nAgain we’ll use the extended notation during the elimination algorithm, because we don’t want to carry around the \\(x\\), \\(y\\) and \\(z\\) at each step.\n\\[\n\\begin{align}\n&\\left(\\begin{matrix}1 & 1 & -1\\\\2 & -1 & 2 \\\\1 & 2 & -1 \\end{matrix}\\;\\middle|\\;\\begin{matrix}1\\\\9\\\\0 \\end{matrix}\\right)\n\\overset{l_2'=l_2-2l_1\\\\l_3'=l_3-l_1}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 1 & -1 \\\\0 & -3 & 4 \\\\0 & 1 & 0 \\end{matrix}\\;\\middle|\\;\\begin{matrix}1\\\\7\\\\-1\\end{matrix}\\right)\n\\overset{l_2 \\leftrightarrow l_3}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 1 & -1 \\\\0 & 1 & 0\\\\0 & -3 & 4\\end{matrix}\\;\\middle|\\;\\begin{matrix}1\\\\-1\\\\7\\end{matrix}\\right)\\\\\n\\overset{l_3'=l_3+3l_2}{\\longrightarrow}\n&\\left(\\begin{matrix}1 & 1 & -1 \\\\0 & 1 & 0 \\\\0 & 0 & 4 \\end{matrix}\\;\\middle|\\;\\begin{matrix}1\\\\-1\\\\4\\end{matrix}\\right)\n\\overset{l_3'=1/4l_3}{\\longrightarrow}\\left(\\begin{matrix}1 & 1 & -1 \\\\0 & 1 & 0 \\\\0 & 0 & 1 \\end{matrix}\\;\\middle|\\;\\begin{matrix}1\\\\-1\\\\1\\end{matrix}\\right)\n\\overset{l_1'=l_1+l_3}{\\longrightarrow}\\left(\\begin{matrix}1 & 1 & 0 \\\\0 & 1 & 0 \\\\0 & 0 & 1 \\end{matrix}\\;\\middle|\\;\\begin{matrix}2\\\\-1\\\\1\\end{matrix}\\right) \\\\\n\\overset{l_1'=l_1-l_2}{\\longrightarrow}\n&\\left(\\begin{matrix}1 & 0 & 0 \\\\0 & 1 & 0 \\\\0 & 0 & 1 \\end{matrix} \\;\\middle|\\;\\begin{matrix}3\\\\-1\\\\1\\end{matrix}\\right)\n\\end{align}\n\\]\nEach step has its own commentary:\n\nOn the first step we use the pivot \\(A_{11}=1\\) to eliminate the entries \\(2\\) and \\(1\\) bellow.\nThe second step we switched equations, so as to bring a pivot \\(1\\) at position \\(A_{22}\\).\nThe third step consists in using this pivot to eliminate the entry \\(-3\\) below.\nIn step four we divided \\(l_3\\) by \\(4\\) so that that the pivot \\(A_{33}\\) is \\(1\\) instead of \\(4\\).\nAt step five we use \\(A_{33}\\) to eliminate the entry \\(-1\\) above it.\nStep six we just used the pivot \\(A_{22}\\), to eliminate the entry \\(1\\) at \\(A_{12}\\).\n\nAfter simplification, it is time to go back to the original notation - we find:\n\\[\n\\begin{cases}\nx = 3\\\\y=-1\\\\z=1\n\\end{cases}\n\\]\nwhich is the solution of the system of equations! Correspondingly, the solution for the matrix-vector notation is the following vector\n\\[\n\\begin{pmatrix}\n2\\\\\n-1\\\\\n1\n\\end{pmatrix}\n\\]\nImportant observations (that we’ll come very useful later): the form of the matrix after all these l.c. of rows, it has the form:\n\\[\n\\begin{pmatrix}I\\end{pmatrix}\n\\]\nand its rank is \\(3\\), meaning, the number of pivots if \\(3\\). Note as well that if we consider each column of the matrix as a vector, then we find \\(3\\) independent vectors. We write \\(r=3\\).\n\nDefinition 1 [rank \\(r\\) of a matrix \\(A\\)] = \\(r\\) = [# of pivots] =[# of indep columns of \\(A\\)] = [# of indep rows of \\(A\\)]\n\nMoreover, notice that the rank of the extended matrix \\(r^*\\) is also \\(3\\). And that the number of columns \\(n=3\\) as well.\nFrom this example we see something that happens in general:\nA system of equations \\(A\\mathbf{x}=\\mathbf{b}\\) have one solution provided \\(r=r^*=n\\).\n\n\nExample 2: A system with no solution\nThe system this time is:\n\\[\n\\begin{cases}\n2x-y=8\\\\\ny+2x = 4\\\\\nx=-y-1\n\\end{cases}\n\\leftrightsquigarrow\n\\begin{pmatrix}\n2 & -1\\\\\n2 & 1\\\\\n1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx\\\\\ny\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n8\\\\\n4\\\\\n-1\n\\end{pmatrix}\n\\leftrightsquigarrow\n\\left(\n\\begin{matrix}\n2 & -1 \\\\\n2 & 1  \\\\\n1 & 1\n\\end{matrix}\n\\;\\middle|\\;\n\\begin{matrix}\n8\\\\\n4\\\\\n-1\n\\end{matrix}\n\\right)\n\\tag{4}\\]\nTo find how many solutions \\(\\begin{pmatrix}x\\\\y\\end{pmatrix}\\) are there, we try to compute them using the Elimination algorithm:\n\\[\n\\begin{align}\n&\\left(\\begin{matrix}2 & -1 \\\\2 & 1 \\\\1 & 1  \\end{matrix}\\;\\middle|\\;\\begin{matrix}8\\\\4\\\\-1\\end{matrix}\\right)\n\\overset{l_1\\leftrightarrow l_3}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 1\\\\ 2 & 1 \\\\2 & -1 \\end{matrix} \\;\\middle|\\;\\begin{matrix}-1\\\\4\\\\8\\end{matrix}\\right)\n\\overset{l_2'=l_2-2l_1\\\\l_3'=l_3-2l_1}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 1 \\\\0 & -1 \\\\0 & -3 \\end{matrix}\\;\\middle|\\;\\begin{matrix}-1\\\\6\\\\10\\end{matrix}\\right)\\\\\n\\overset{l_2'=-l_2\\\\l_3'=l_3-3l_2}{\\longrightarrow}\n&\\left(\\begin{matrix}1 & 1 \\\\0 & 1 \\\\0 & 0 \\end{matrix} \\;\\middle|\\; \\begin{matrix}-1\\\\-6\\\\-8\\end{matrix}\\right)\n\\overset{l_1'=l_1-l_2}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 0 \\\\0 & 1 \\\\0 & 0 \\end{matrix} \\;\\middle|\\;\\begin{matrix}5\\\\-6\\\\-8\\end{matrix}\\right)\n\\end{align}\n\\]\nThis means:\n\\[\n\\left(\\begin{matrix}1 & 0 \\\\0 & 1 \\\\0 & 0 \\end{matrix} \\;\\middle|\\;\\begin{matrix}5\\\\-6\\\\-8\\end{matrix}\\right)\n\\leftrightsquigarrow\n\\begin{pmatrix}1 & 0 \\\\0 & 1 \\\\0 & 0 \\end{pmatrix} \\begin{pmatrix}x\\\\y\\end{pmatrix}=\\begin{pmatrix}5\\\\-6\\\\-8\\end{pmatrix}\n\\]\nNow that most simplification is done, lets convert back to the system’s notation:\n\\[\n\\begin{cases}\nx=5\\\\\ny=-6\\\\\n0=-8\n\\end{cases}\n\\tag{5}\\]\nIt is clearly impossible. No choice of \\(x\\) or \\(y\\) makes this three statements true simultaneously! Since Equation 4 is equivalent to Equation 5, therefore our original system Equation 4 has no solution as well.\nObservations: Notice the form of the matrix is\n\\[\n\\begin{pmatrix}I\\\\\\mathbf{0}\\end{pmatrix}\n\\]\nthe fact that \\(r=2\\), \\(r^*=3\\) and \\(n=2\\). We see in this example something which happens in general and which we’ll justify later:\nA system of equations \\(A\\mathbf{x}=\\mathbf{b}\\) have no solution provided \\(r&lt;r^*\\).\n\n\nExample 3: A system with many solutions\nThe system is:\n\\[\n\\begin{cases}\nx+y-z=0\\\\\n2x-y+2z = 0\\\\\n\\end{cases}\n\\leftrightsquigarrow\n\\begin{pmatrix}\n1 & 1 & -1\\\\\n2 & -1 & 2\\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nx\\\\\ny\\\\\nz\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0\\\\\n0\n\\end{pmatrix}\n\\leftrightsquigarrow\n\\left(\n\\begin{matrix}\n1 & 1 & -1\\\\\n2 & -1 & 2 \\\\\n\\end{matrix}\n\\;\\middle|\\;\n\\begin{matrix}\n0\\\\0\n\\end{matrix}\n\\right)\n\\tag{6}\\]\nSolving:\n\\[\n\\begin{align}\n&\\begin{pmatrix}1 & 1 & -1 &\\bigm| & 0\\\\2 & -1 & 2 &\\bigm| & 0\\\\\\end{pmatrix}\n\\overset{l_2'=l_2-2l_1}{\\longrightarrow}\n\\begin{pmatrix}1 & 1 & -1 &\\bigm| & 0\\\\0 & -3 & 4 &\\bigm| & 0\\end{pmatrix}\n\\overset{l_2'=-1/3l_2}{\\longrightarrow}\n\\begin{pmatrix}1 & 1 & -1 &\\bigm| & 0\\\\0 & 1 & -4/3 &\\bigm| & 0\\end{pmatrix}\\\\\n\\overset{l_1'=l_1-l_2}{\\longrightarrow}\n&\\begin{pmatrix}1 & 0 & 1/3 &\\bigm| & 0\\\\0 & 1 & -4/3 &\\bigm| & 0\\end{pmatrix}\n\\end{align}\n\\]\nOnce again, this notation means:\n\\[\n\\begin{pmatrix}1 & 0 & 1/3 &\\bigm| & 0\\\\0 & 1 & -4/3 &\\bigm| & 0\\end{pmatrix}\n\\leftrightsquigarrow\n\\begin{pmatrix}1 & 0 & 1/3\\\\0 & 1 & -4/3 \\end{pmatrix}\n\\begin{pmatrix}x\\\\y\\\\z\\end{pmatrix}\n=\n\\begin{pmatrix}0\\\\0\\end{pmatrix}\n\\tag{7}\\]\nConverting back to the system notation have:\n\\[\n\\begin{cases}\nx+1/3z =0\\\\\ny-4/3z=0\n\\end{cases}\n\\iff\n\\begin{cases}\nx=-1/3z\\\\\ny=4/3z\n\\end{cases}\n\\tag{8}\\]\nAsking what \\((x,y,z)\\in \\mathbb{R}^3\\) that satisfy the equation Equation 6 is equivalent to ask, what is \\(x,y,z\\in \\mathbb{R}\\) that satisfy Equation 8 . Each real \\(z\\) we choose gives us the corresponding \\(x\\) and \\(y\\); as a consequence we have many solution. In other words, the solution is\n\\[\n\\begin{pmatrix}\nx\\\\\ny\\\\\nz\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-1/3z\\\\\n4/3z\\\\\nz\n\\end{pmatrix}\n\\tag{9}\\]\nObservations: The aspect of the final matrix is:\n\\[\n\\begin{pmatrix}I \\,\\,F\\end{pmatrix}\n\\]\nwhere the \\(F\\) block is the third column \\((1/3, -4/3)^\\intercal\\), while \\(I\\) is the \\(2\\) by \\(2\\) identity.\nAdditionally, \\(r=2\\), \\(r^*=2\\) and \\(n=3\\). A general rule (to be explained later) is:\nA system of equations \\(A\\mathbf{x}=\\mathbf{b}\\) have infinite solutions provided \\(r=r^*&lt;n\\).\n\n\nExample 4: Another system with many solutions\nWhat is the solution \\((x,y, z, w)\\) for the following system:\n\\[\n\\begin{cases}\n&x&+ &2 y&+&2 z &+&2w &= 1\\\\\n&2x&+&4y&+&6z&+&8w &= 2\\\\\n&3x&+&6y&+&8z&+&10w &=3\n\\end{cases}\n\\]\nUsing elimination algorithm we make linear combinations of the equation with the goal of eliminating variables, here is one way to go\n\\[\n\\begin{align}\n&\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm|1\\\\\n2 & 4 & 6 & 8 &\\bigm|2\\\\\n3 & 6 & 8 & 10 &\\bigm| 3\n\\end{pmatrix}\n\\overset{l_2' = l_2-2l_1}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| 1\\\\\n0 & 0 & 2 & 4 &\\bigm| 0\\\\\n3 & 6 & 8 & 10 &\\bigm| 3\n\\end{pmatrix}\n\\overset{l_3' = l_3-3l_1}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| 1\\\\\n0 & 0 & 2 & 4 &\\bigm| 0\\\\\n0 & 0 & 2 & 4 &\\bigm| 0\n\\end{pmatrix}\\\\\n&\\overset{l_3'=l_3-l_2}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| 1\\\\\n0 & 0 & 2 & 4 &\\bigm| 0\\\\\n0 & 0 & 0 & 0 &\\bigm| 0\n\\end{pmatrix}\n\\overset{l_2'=1/2l_2}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| 1\\\\\n0 & 0 & 1 & 2 &\\bigm| 0\\\\\n0 & 0 & 0 & 0 &\\bigm| 0\n\\end{pmatrix}\n\\overset{l_1'=l_1-2l_2}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 0 & -2 &\\bigm| 1\\\\\n0 & 0 & 1 & 2 &\\bigm| 0\\\\\n0 & 0 & 0 & 0 &\\bigm| 0\n\\end{pmatrix}\n\\end{align}\n\\tag{10}\\]\nWe simplified it as much as we can, going back to the system’s notation we have:\n\\[\n\\begin{cases}\nx + 2 y & &- &2w &=1\\\\\n&z &+ &2w &=0\n\\end{cases}\n\\]\nNow, promote \\(y\\) and \\(w\\) into parameters and express \\(x\\) and \\(z\\) in term of them (if why we take this step is not natural, it will be soon)\n\\[\n\\begin{cases}\nx + 2 y & &- &2w &=1\\\\\n&z &+ &2w &=0\n\\end{cases}\n\\longrightarrow\n\\begin{cases}\nx &=1-2y&-&2w\\\\\nz &=&-&2w\n\\end{cases}\n\\tag{11}\\]\nFor each values we assign to \\(y\\) and \\(w\\) we get one solution! There’s an infinite number of them.\nThis example is given so that you can see, that the final aesthetic of the simplified \\(A\\) may have the \\(I\\) and \\(F\\) blocks mixed!\nThis is also a matrix with form:\n\\[\n\\begin{pmatrix}I \\,\\,F\\end{pmatrix}\n\\]\nThe rank is \\(r=2\\), the rank of the extended matrix is \\(r^*=2\\), meanwhile \\(n=4\\). Since \\(r=r^*&lt;n\\) we have an infinite number of solutions.\n\n\nExample 5: A system that may have one, none or many solutions\nWhat is the solution of:\n\\[\n\\begin{cases}\n&x&+ &k y&+&2 z &= 1\\\\\n&x&+&y&+&(k+1)z &= k\\\\\n&-x&-&y&-&z &=k+1\n\\end{cases}\n\\]\nUsing the extended matrix formulation we use the pivots to eliminate entries:\n\\[ \\begin{align} &\\begin{pmatrix} 1 & k & 2  &\\bigm|1\\\\ 1 & 1 & k+1 &\\bigm|k\\\\ -1 & -1 & -1 &\\bigm| k+1 \\end{pmatrix} \\overset{l_2' = l_2-l_1\\\\l_3'=l_3+l_1}{\\longrightarrow} \\begin{pmatrix} 1 & k & 2 &\\bigm| 1\\\\ 0 & 1-k & k-1 &\\bigm| k-1\\\\ 0 & k-1 & 1 &\\bigm| k+2 \\end{pmatrix} \\overset{l_3' = l_3+l_2}{\\longrightarrow} \\begin{pmatrix} 1 & k & 2  &\\bigm| 1\\\\ 0 & 1-k & k-1 &\\bigm| k-1\\\\ 0 & 0 & k &\\bigm| 2k+1 \\end{pmatrix} \\end{align} \\]\nNow, depending upon on the value \\(k\\):\nFor \\(k=1\\) then\n\\[\n\\begin{pmatrix} 1 & 1 & 2  &\\bigm|1\\\\ 0 & 0 & 0 &\\bigm|0\\\\ 0 & 0 & 1 &\\bigm| 3 \\end{pmatrix} \\overset{l_2 \\leftrightarrow l_3}{\\longrightarrow} \\begin{pmatrix} 1 & 1 & 2  &\\bigm|1\\\\ 0 & 0 & 1 &\\bigm|3\\\\ 0 & 0 & 0 &\\bigm| 0 \\end{pmatrix}\\overset{l_1'=l_1-2l_2 }{\\longrightarrow}\\begin{pmatrix} 1 & 1 & 0  &\\bigm|-5\\\\ 0 & 0 & 1 &\\bigm|3\\\\ 0 & 0 & 0 &\\bigm| 0 \\end{pmatrix}\n\\tag{12}\\]\nwhich has the form:\n\\[\n\\begin{pmatrix}I\\,\\,F \\end{pmatrix}\n\\]\nalso: \\(r=2\\), \\(r^*=2\\) and \\(n=3\\).\nSince \\(r=r^*&lt;n\\) we conclude we have an infinite number of solutions as we readily check by actually computing them. From Equation 12 we know:\n\\[\n\\begin{cases}\nx + y = -5\\\\\nz=3\n\\end{cases}\n\\]\nSince we have two equations and three unknowns, we promote one of them, let it be \\(x\\), to the status of a parameter \\(x_0\\), hence:\n\\[\n\\begin{pmatrix}x\\\\y\\\\z\\end{pmatrix}=\\begin{pmatrix}x_0\\\\-5-x_0\\\\3\\end{pmatrix}=\\begin{pmatrix}0\\\\-5\\\\3\\end{pmatrix}+x_0\\begin{pmatrix}1\\\\-1\\\\0\\end{pmatrix}\n\\]\nFor each \\(x_0\\) of our choice we have a distinct solution. There is infinite of them.\nFor \\(k=0\\)\n\\[\n\\begin{pmatrix} 1 & 0 & 2  &\\bigm|1\\\\ 0 & 1 & -1 &\\bigm|-1\\\\ 0 & 0 & 0 &\\bigm| 1 \\end{pmatrix}\n\\]\nThe system has the form:\n\\[\n\\begin{pmatrix}\nI & F\\\\\n\\mathbf{0} &\\mathbf{0}\n\\end{pmatrix}\n\\]\nAnd note: \\(r=2\\), \\(r^*=3\\) and \\(n=3\\), thus \\(r&lt;r^*=n\\) which tells us there is no solution. In fact we can see why by rolling back to:\n\\[\n\\begin{cases}\nx+2z=1\\\\\ny-z =-1\\\\\n0=1\n\\end{cases}\n\\]\nThere are no \\(x\\), \\(y\\) and \\(z\\) that satisfies these three equations simultaneously.\nIf \\(k\\) is not \\(0,1\\) then we always have \\(r=r^*=n\\), thus, there is always a unique solution.\nIn summary:\n\nA system of equations \\(A\\mathbf{x}=\\mathbf{b}\\) have infinite solutions provided \\(r=r^*&lt;n\\).\nA system of equations \\(A\\mathbf{x}=\\mathbf{b}\\) have one solution provided \\(r=r^*=n\\).\nA system of equations \\(A\\mathbf{x}=\\mathbf{b}\\) have no solutions provided \\(r&lt;r^*\\).\n\n\n\n\n\n\n\nCommentaries\n\n\n\n\nFor the moment assume this summary as a fact of life, later we’ll justify in detail the meaning of this triple equality, for know just focus on learning elimination and testing the triple equality\nHow do we know how many pivots are there? Using l.c. of rows, simplify the matrix by eliminating as many entries as possible using the pivots.\nA matrix where all entries below the pivots were killed is in triangular form. Back substitution can already be used at this stage. (we did not do this in the examples, but we could) If we proceed and also kill all entries above the pivots, we get the best matrix (this is what we did in the examples). A matrix is then said to be in reduced row echelon form. Back substitution can also used at this final stage.\nWith these three statements we can decide whether a system has one, none or infinite solution. But thinking about how did we get \\(r\\) and \\(r^*\\)? We did use elimination, which is a labor intensive process, to bring \\(A\\mathbf{x}=\\mathbf{b}\\) into a simplified form. So much work was done, one might as well solve the entire thing we from the solution itself evaluate whether there is one, none or infinite solutions. Here comes a very important point, these three statements are worthless as a practical point of view to decide in which case are we. These three are like a tip of an iceberg, and what’s underneath is beautiful way to understand why some systems have one, none or infinite solution. For the moment, we use them as facts of life, later we uncover what is going on. These statements are only useful in situations where the matrix depend on some parameter \\(k\\) and we which, after putting the matrix in triangular form, we want to check for which values there is one, none or infinite solutions.",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Solving systems using elimination"
    ]
  },
  {
    "objectID": "vector_spaces.html",
    "href": "vector_spaces.html",
    "title": "Vector spaces",
    "section": "",
    "text": "Instead of telling you what a vector is lets see what can you do with them. So, for the moment we’ll consider a vector, or better said - a column vector - an array of numbers which we write as:\n\\[\n\\begin{pmatrix}1\\\\5\\end{pmatrix}, \\begin{pmatrix}1\\\\-5\\\\0.1\\end{pmatrix},\\begin{pmatrix}x\\\\y\\\\z\\end{pmatrix}\n\\]\nWhat can you do with vectors? Answer: linear combinations! A linear combination is a computation that looks like this:\n\\[\n3 \\mathbf{u}+2\\mathbf{v} = 2\\begin{pmatrix} 1\\\\3\\end{pmatrix}+4\\begin{pmatrix} 1\\\\-1\\end{pmatrix}\n\\]\nFirst multiply the vectors by the scalars, then add the vectors by adding the entries, the result is:\n\\[\n\\begin{pmatrix} 2\\\\6\\end{pmatrix}+\\begin{pmatrix} 4\\\\-4\\end{pmatrix} = \\begin{pmatrix} 6\\\\2 \\end{pmatrix}\n\\]\nA generic linear combination looks like this:\n\\[\na \\mathbf{u}+b\\mathbf{v} = a\\begin{pmatrix} 1\\\\5\\end{pmatrix}+b\\begin{pmatrix} 2\\\\10\\end{pmatrix} = \\begin{pmatrix}a +2b\\\\5a +10b \\end{pmatrix}\n\\]\nwhere \\(a\\) and \\(b\\) are scalars.\nIf you know how to combine two vectors, you know how to combine three and so on.\nThe geometrical addition of vectors is performed using the paralelogram rule:"
  },
  {
    "objectID": "vector_spaces.html#matrix-vector-notation-for-a-system",
    "href": "vector_spaces.html#matrix-vector-notation-for-a-system",
    "title": "Vector spaces",
    "section": "Matrix-vector notation for a system",
    "text": "Matrix-vector notation for a system\nConsider the system with two equations, called \\(l_1\\) and \\(l_2\\):\n\\[\n\\begin{cases}\nx-4y =2\\\\\n2x-6y = 5\n\\end{cases}\n\\tag{1}\\]\nIn Equation 1 we find two equations and two unknowns \\(x\\) and \\(y\\). We want their values such that both equations are satisfied.\nUsing the matrix-vector notation we can write Equation 1 as:\n\\[\n\\begin{pmatrix} 1 & -4\\\\ 2 & -6 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}2\\\\5\\end{pmatrix}\n\\tag{2}\\]\nLets read Equation 2 in words: the \\(2\\) by \\(2\\) matrix of coefficients is multiplied by the column vector \\(\\begin{pmatrix}x\\\\y\\end{pmatrix}\\) , the result is \\(\\begin{pmatrix}2\\\\5\\end{pmatrix}\\). This is one equation, and the unknown is the column vector \\(\\begin{pmatrix}x\\\\y\\end{pmatrix}\\).\nHow do we multiply a vector by a matrix?\nAnswer:\n\\[\n\\overbrace{\\begin{pmatrix} 1 & -4\\\\ 2 & -6 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}}^\\text{matrix-vector mult.} =\\overbrace{\\begin{pmatrix}1\\cdot x-4\\cdot y \\\\ 2\\cdot x-6\\cdot y\\end{pmatrix}}^\\text{scalar mult.}\n\\]\nMatrix times a vector on the lhs is just a super-compact way of writing the vector on the rhs. Moreover, notice the shapes of the matrix and vectors, this is very, very important. A \\(2\\) by \\(2\\) matrix times a \\(2\\) by \\(1\\) column vector yields a \\(2\\) by \\(1\\) column vector! If you understand this it should not be a problem to see what shapes are or not compatible, check this:\n\n[\\(2\\times2\\)][ \\(3\\times 1\\)] \\(=\\) Nonsense\n[\\(3\\times2\\)][ \\(2\\times 1\\)] \\(=\\) [\\(3\\times 1\\)]\n[\\(2\\times3\\)][ \\(3\\times 1\\)] \\(=\\) [\\(2\\times 1\\)]\n[\\(3\\times2\\)][ \\(3\\times 1\\)] \\(=\\) Nonsense\n[\\(1\\times3\\)][ \\(3\\times 1\\)] \\(=\\) [\\(1\\times 1\\)]"
  },
  {
    "objectID": "vector_spaces.html#solving-the-system-using-the-elimination-method",
    "href": "vector_spaces.html#solving-the-system-using-the-elimination-method",
    "title": "Vector spaces",
    "section": "Solving the system using the Elimination method",
    "text": "Solving the system using the Elimination method\nLets solve the system Equation 1 using the traditional rules we already know from high-school, recall:\n\nyou can replace an equation by itself times some constant.\nyou can replace any one equation by the sum both equations.\nyou can isolate \\(x\\) or \\(y\\) in one equation and substitute in the other equation.\n\nIn other words, 1. and 2., just say this: you can replace \\(l_1\\) or \\(l_2\\) by some convenient combination \\(al_1+bl_2\\).\nThe central idea of the Elimination method is use combination of equations to eliminate variables and thus giving us an equivalent and easier to solve system. To eliminate variables we use clever use of rules \\(1\\) and \\(2\\). Once the system is simple enough we can use rule \\(3\\). How do you know what is or not a good combination? Just practice and see.\nThis recaps what you know, now lets use these rules to solve the Equation 1 and in parallel see the corresponding matrix-vector version.\n\nstep 1: Replace equation \\(l_2\\) by, \\(l_2\\) minus twice the equation \\(l_1\\), i.e., make the new second equation \\(l_2'\\) into \\(l_2-2l_1\\). This gives us:\n\\[\n\\begin{cases}x-4y =2\\\\2x-6y = 5\\end{cases} \\overset{l_2'=l_2-2l_1}{\\longrightarrow}\\begin{cases} x-4y =2\\\\ 2y = 1\\end{cases}\n\\]\nCorrespondingly we subtract from row \\(l_2\\) twice the row \\(l_1\\) in Equation 2, giving us\n\\[\n\\begin{pmatrix} 1 & -4\\\\ 2 & -6 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}2\\\\5\\end{pmatrix}\\overset{l_2'=l_2-2l_1}{\\longrightarrow} \\begin{pmatrix} 1 & -4\\\\ 0 & 2 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}2\\\\1\\end{pmatrix}\n\\]\nstep 2: Multiply equation \\(l_2\\) by \\(1/2\\):\n\\[\n\\begin{cases} x-4y =2\\\\ 2y = 1\\end{cases}\\overset{l_2'=1/2l_2}{\\longrightarrow} \\begin{cases} x-4y =2\\\\ y = 1/2\\end{cases}\n\\]\nIn matrix-vector notation we find:\n\\[\n\\begin{pmatrix} 1 & -4\\\\ 0 & 2 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}2\\\\1\\end{pmatrix}\\overset{l_2'=1/2l_2}{\\longrightarrow} \\begin{pmatrix} 1 & -4\\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}2\\\\1/2\\end{pmatrix}\n\\]\nstep 3: Since the system is simple enough, there are two way to go at this stage. You either substitute the equation \\(y=1/2\\) into the first equation \\(x-4y=2\\) and solve for \\(x\\), this gives us the answer \\(x=4\\) and \\(y=1/2\\). The second way to to go about it, is to simplify even further the system of equations, we do that by replacing \\(l_1\\) by, \\(l_1\\) plus four times \\(l_2\\):\n\\[\n\\begin{cases} x-4y =2\\\\ y = 1/2\\end{cases}\\overset{l_1'=l_1+4l_2}{\\longrightarrow}\\begin{cases} x =4\\\\ y = 1/2\\end{cases}\n\\]\nIn matrix-vector notation we find:\n\\[\n\\begin{pmatrix} 1 & -4\\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}2\\\\1/2\\end{pmatrix}\\overset{l_1'=l_1+4l_2}{\\longrightarrow} \\begin{pmatrix} 1 & 0\\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}4\\\\1/2\\end{pmatrix}\n\\]\nFrom which we can read the final result \\(x=4\\) and \\(y=1/2\\).\n\nGoing through the three steps again we notice we can improve our matrix-vector notation by suppressing from it the column \\(\\begin{pmatrix}x\\\\y\\end{pmatrix}\\) and writing instead the steps as:\n\\[\n\\begin{pmatrix} 1 & -4 &\\bigm| & 2 \\\\ 2 & -6 &| & 5 \\end{pmatrix}\\overset{l_2'=l_2-2l_1}{\\longrightarrow} \\begin{pmatrix} 1 & -4 &\\bigm| & 2\\\\ 0 & 2 &\\bigm| & 1\\end{pmatrix} \\overset{l_2'=1/2l_2}{\\longrightarrow} \\begin{pmatrix} 1 & -4 &\\bigm| & 2\\\\ 0 & 1 &\\bigm| & 1/2\\end{pmatrix}\\overset{l_1'=l_1+4l_2}{\\longrightarrow} \\begin{pmatrix} 1 & 0 &\\bigm| & 4\\\\ 0 & 1 &\\bigm| & 1/2\\end{pmatrix}\n\\]\nFrom now on we’ll adopt this way of writing systems of equations, its called the extended matrix notation, because we appended a new column to right side of the \\(2\\times2\\) matrix. From the extended matrix we read the solution as follows\n\\[\n\\begin{pmatrix} 1 & 0 &\\bigm| & 4\\\\ 0 & 1 &\\bigm| & 1/2\\end{pmatrix}\\longrightarrow \\begin{pmatrix} 1 & 0\\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}4\\\\1/2\\end{pmatrix}\\longrightarrow \\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}4\\\\1/2\\end{pmatrix}\n\\]\nwhere in the last step we multiplied the vector by the matrix.\nThe vector\n\\[\n\\begin{pmatrix}4\\\\1/2\\end{pmatrix}\n\\]\nis the solution of\n\\[\n\\begin{pmatrix} 1 & -4\\\\ 2 & -6 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}2\\\\5\\end{pmatrix}\n\\]\nMeaning, the solution of\n\\[\n\\begin{cases}x-4y =2\\\\2x-6y = 5\\end{cases}\n\\]\nis:\n\\[\n\\begin{cases}x =4\\\\y=1/2\\end{cases}\n\\]"
  },
  {
    "objectID": "vector_spaces.html#more-examples-of-the-elimination-method",
    "href": "vector_spaces.html#more-examples-of-the-elimination-method",
    "title": "Vector spaces",
    "section": "More examples of the elimination method",
    "text": "More examples of the elimination method\nThe following examples will be given:\n\nA system with one solution\nA system with no solution\nA system with many solutions\nAnother system with many solutions\n\n\nExample 1: A system with one solution\nConsider the system, which we write in three different notations.\n\\[\n\\begin{cases}\nx+y-z=1\\\\\n2x-y+2z = 9\\\\\n2y=-x+z\n\\end{cases}\n\\leftrightsquigarrow\n\\begin{pmatrix}\n1 & 1 & -1\\\\\n2 & -1 & 2\\\\\n1 & 2 &-1\n\\end{pmatrix}\n\\begin{pmatrix}\nx\\\\\ny\\\\\nz\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1\\\\\n9\\\\\n0\n\\end{pmatrix}\n\\leftrightsquigarrow\n\\begin{pmatrix}\n1 & 1 & -1 &\\bigm| & 1\\\\\n2 & -1 & 2 &\\bigm| & 9\\\\\n1 & 2 & -1 &\\bigm| & 0\n\\end{pmatrix}\n\\tag{3}\\]\nAgain we’ll use the later notation during the elimination algorithm, because we don’t have to carry around the \\(x\\), \\(y\\) and \\(z\\) at each step.\n\\[\n\\begin{align}\n&\\begin{pmatrix}1 & 1 & -1 &\\bigm| & 1\\\\2 & -1 & 2 &\\bigm| & 9\\\\1 & 2 & -1 &\\bigm| & 0\\end{pmatrix}\n\\overset{l_2'=l_2-2l_1\\\\l_3'=l_3-l_1}{\\longrightarrow}\n\\begin{pmatrix}1 & 1 & -1 &\\bigm| & 1\\\\0 & -3 & 4 &\\bigm| & 7\\\\0 & 1 & 0 &\\bigm| & -1\\end{pmatrix}\n\\overset{l_2 \\leftrightarrow l_3}{\\longrightarrow}\n\\begin{pmatrix}1 & 1 & -1 &\\bigm| & 1\\\\0 & 1 & 0 &\\bigm| & -1\\\\0 & -3 & 4 &\\bigm| & 7\\end{pmatrix}\\\\\n\\overset{l_3'=l_3+3l_2}{\\longrightarrow}\n&\\begin{pmatrix}1 & 1 & -1 &\\bigm| & 1\\\\0 & 1 & 0 &\\bigm| & -1\\\\0 & 0 & 4 &\\bigm| & 4\\end{pmatrix}\n\\overset{l_3'=1/4l_3}{\\longrightarrow}\n\\begin{pmatrix}1 & 1 & -1 &\\bigm| & 1\\\\0 & 1 & 0 &\\bigm| & -1\\\\0 & 0 & 1 &\\bigm| & 1\\end{pmatrix}\n\\overset{l_1'=l_1+l_3}{\\longrightarrow}\n\\begin{pmatrix}1 & 1 & 0 &\\bigm| & 2\\\\0 & 1 & 0 &\\bigm| & -1\\\\0 & 0 & 1 &\\bigm| & 1\\end{pmatrix} \\\\\n\\overset{l_1'=l_1-l_2}{\\longrightarrow}\n&\\begin{pmatrix}1 & 0 & 0 &\\bigm| & 2\\\\0 & 1 & 0 &\\bigm| & -1\\\\0 & 0 & 1 &\\bigm| & 1\\end{pmatrix}\n\\end{align}\n\\]\nMost of the system simplification (by elimination of variables) is done in this matrix-vector notation, looking at we find we went as far as we can, thus it is time to go back to the original notation - we find:\n\\[\n\\begin{cases}\nx = 3\\\\y=-1\\\\z=1\n\\end{cases}\n\\]\nwhich is the solution of the system of equations! Correspondingly, the solution for the matrix-vector notation is the following vector\n\\[\n\\begin{pmatrix}\n2\\\\\n-1\\\\\n1\n\\end{pmatrix}\n\\]\n\n\nExample 2: A system with no solution\nThe system this time is:\n\\[\n\\begin{cases}\n2x-y=8\\\\\ny+2x = 4\\\\\nx=-y-1\n\\end{cases}\n\\leftrightsquigarrow\n\\begin{pmatrix}\n2 & -1\\\\\n2 & 1\\\\\n1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx\\\\\ny\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n8\\\\\n4\\\\\n-1\n\\end{pmatrix}\n\\leftrightsquigarrow\n\\begin{pmatrix}\n2 & -1 &\\bigm| & 8\\\\\n2 & 1  &\\bigm| & 4\\\\\n1 & 1  &\\bigm| & -1\n\\end{pmatrix}\n\\]\nTo find how many solutions \\(\\begin{pmatrix}x\\\\y\\end{pmatrix}\\) are there, we try to compute them using the Elimination algorithm:\n\\[\n\\begin{align}\n&\\begin{pmatrix}2 & -1 &\\bigm| & 8\\\\2 & 1  &\\bigm| & 4\\\\1 & 1  &\\bigm| & -1\\end{pmatrix}\n\\overset{l_1\\leftrightarrow l_3}{\\longrightarrow}\n\\begin{pmatrix}1 & 1 &\\bigm| & -1\\\\2 & 1 &\\bigm| & 4\\\\2 & -1 &\\bigm| & 8\\end{pmatrix}\n\\overset{l_2'=l_2-2l_1\\\\l_3'=l_3-2l_1}{\\longrightarrow}\n\\begin{pmatrix}1 & 1 &\\bigm| & -1\\\\0 & -1 &\\bigm| & 6\\\\0 & -3 &\\bigm| & 10\\end{pmatrix}\\\\\n\\overset{l_2'=-l_2\\\\l_3'=l_3-3l_2}{\\longrightarrow}\n&\\begin{pmatrix}1 & 1 &\\bigm| & -1\\\\0 & 1 &\\bigm| & -6\\\\0 & 0 &\\bigm| & 8\\end{pmatrix}\n\\overset{l_1'=l_1-l_2}{\\longrightarrow}\n\\begin{pmatrix}1 & 0 &\\bigm| & 5\\\\0 & 1 &\\bigm| & -6\\\\0 & 0 &\\bigm| & -8\\end{pmatrix}\n\\end{align}\n\\]\nThis means:\n\\[\n\\begin{pmatrix}1 & 0 &\\bigm| & 5\\\\0 & 1 &\\bigm| & -6\\\\0 & 0 &\\bigm| & -8\\end{pmatrix}\n\\leftrightsquigarrow\n\\begin{pmatrix}1 & 0 \\\\0 & 1 \\\\0 & 0 \\end{pmatrix} \\begin{pmatrix}x\\\\y\\end{pmatrix}=\\begin{pmatrix}5\\\\-6\\\\-8\\end{pmatrix}\n\\]\nNow that most simplification is done, lets convert back to the system’s notation:\n\\[\n\\begin{cases}\nx=5\\\\\ny=-6\\\\\n0=-8\n\\end{cases}\n\\tag{4}\\]\nIt is clearly impossible. No choice of \\(x\\) or \\(y\\) makes this three statements true simultaneously! Since Equation 3 is equivalent to Equation 4, therefore our original system Equation 3 has no solution as well.\n\n\nExample 3: A system with many solutions\nThe system is:\n\\[\n\\begin{cases}\nx+y-z=0\\\\\n2x-y+2z = 0\\\\\n\\end{cases}\n\\leftrightsquigarrow\n\\begin{pmatrix}\n1 & 1 & -1\\\\\n2 & -1 & 2\\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nx\\\\\ny\\\\\nz\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0\\\\\n0\n\\end{pmatrix}\n\\leftrightsquigarrow\n\\begin{pmatrix}\n1 & 1 & -1 &\\bigm| & 0\\\\\n2 & -1 & 2 &\\bigm| & 0\\\\\n\\end{pmatrix}\n\\tag{5}\\]\nSolving:\n\\[\n\\begin{align}\n&\\begin{pmatrix}1 & 1 & -1 &\\bigm| & 0\\\\2 & -1 & 2 &\\bigm| & 0\\\\\\end{pmatrix}\n\\overset{l_2'=l_2-2l_1}{\\longrightarrow}\n\\begin{pmatrix}1 & 1 & -1 &\\bigm| & 0\\\\0 & -3 & 4 &\\bigm| & 0\\end{pmatrix}\n\\overset{l_2'=-1/3l_2}{\\longrightarrow}\n\\begin{pmatrix}1 & 1 & -1 &\\bigm| & 0\\\\0 & 1 & -4/3 &\\bigm| & 0\\end{pmatrix}\\\\\n\\overset{l_1'=l_1-l_2}{\\longrightarrow}\n&\\begin{pmatrix}1 & 0 & 1/3 &\\bigm| & 0\\\\0 & 1 & -4/3 &\\bigm| & 0\\end{pmatrix}\n\\end{align}\n\\]\nOnce again, this notation means:\n\\[\n\\begin{pmatrix}1 & 0 & 1/3 &\\bigm| & 0\\\\0 & 1 & -4/3 &\\bigm| & 0\\end{pmatrix}\n\\leftrightsquigarrow\n\\begin{pmatrix}1 & 0 & 1/3\\\\0 & 1 & -4/3 \\end{pmatrix}\n\\begin{pmatrix}x\\\\y\\\\z\\end{pmatrix}\n=\n\\begin{pmatrix}0\\\\0\\end{pmatrix}\n\\tag{6}\\]\nConverting back to the system notation have: \\[\n\\begin{cases}\nx+1/3z =0\\\\\ny-4/3z=0\n\\end{cases}\n\\]\nSubstituting the \\(z\\) of the second equation into the \\(z\\) of the first we find:\n\\[\n\\begin{cases}\nx+1/3z =0\\\\\ny-4/3z=0\n\\end{cases}\n\\iff\n\\begin{cases}\nx=-1/3z\\\\\ny=4/3z\n\\end{cases}\n\\tag{7}\\]\nAsking what \\(x,y,z\\in \\mathbb{R}\\) that satisfy the equation Equation 5 is equivalent to ask, what is \\(x,y,z\\in \\mathbb{R}\\) that satisfy Equation 7 . Each real \\(z\\) we choose gives us the corresponding \\(x\\) and \\(y\\); as a consequence we have many solution. In other words, the solution is\n\\[\n\\begin{pmatrix}\nx\\\\\ny\\\\\nz\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-1/3z\\\\\n4/3z\\\\\nz\n\\end{pmatrix}\n\\tag{8}\\]\n\n\nExample 4: Another system with many solutions\nWhat is the solution \\((x,y, z, w)\\) for the following system:\n\\[\n\\begin{cases}\n&x&+ &2 y&+&2 z &+&2w &= 1\\\\\n&2x&+&4y&+&6z&+&8w &= 2\\\\\n&3x&+&6y&+&8z&+&10w &=3\n\\end{cases}\n\\]\nUsing elimination algorithm we make linear combinations of the equation with the goal of eliminating variables, here is one way to go\n\\[\n\\begin{align}\n&\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm|1\\\\\n2 & 4 & 6 & 8 &\\bigm|2\\\\\n3 & 6 & 8 & 10 &\\bigm| 3\n\\end{pmatrix}\n\\overset{l_2' = l_2-2l_1}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| 1\\\\\n0 & 0 & 2 & 4 &\\bigm| 0\\\\\n3 & 6 & 8 & 10 &\\bigm| 3\n\\end{pmatrix}\n\\overset{l_3' = l_3-3l_1}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| 1\\\\\n0 & 0 & 2 & 4 &\\bigm| 0\\\\\n0 & 0 & 2 & 4 &\\bigm| 0\n\\end{pmatrix}\\\\\n&\\overset{l_3'=l_3-l_2}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| 1\\\\\n0 & 0 & 2 & 4 &\\bigm| 0\\\\\n0 & 0 & 0 & 0 &\\bigm| 0\n\\end{pmatrix}\n\\overset{l_2'=1/2l_2}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| 1\\\\\n0 & 0 & 1 & 2 &\\bigm| 0\\\\\n0 & 0 & 0 & 0 &\\bigm| 0\n\\end{pmatrix}\n\\overset{l_1'=l_1-2l_2}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 0 & -2 &\\bigm| 1\\\\\n0 & 0 & 1 & 2 &\\bigm| 0\\\\\n0 & 0 & 0 & 0 &\\bigm| 0\n\\end{pmatrix}\n\\end{align}\n\\tag{9}\\]\nWe simplified it as much as we can, going back to the system’s notation we have:\n\\[\n\\begin{cases}\nx + 2 y & &- &2w &=1\\\\\n&z &+ &2w &=0\n\\end{cases}\n\\]\nNow, promote \\(y\\) and \\(w\\) into parameters and express \\(x\\) and \\(z\\) in term of them (if why we take this step is not natural, it will be soon)\n\\[\n\\begin{cases}\nx + 2 y & &- &2w &=1\\\\\n&z &+ &2w &=0\n\\end{cases}\n\\longrightarrow\n\\begin{cases}\nx &=1-2y&-&2w\\\\\nz &=&-&2w\n\\end{cases}\n\\tag{10}\\]\nFor each values we assign to \\(y\\) and \\(w\\) we get one solution! There’s an infinite number of them."
  },
  {
    "objectID": "vector_spaces.html#matrix-multiplication-and-linear-combination-of-rows",
    "href": "vector_spaces.html#matrix-multiplication-and-linear-combination-of-rows",
    "title": "Vector spaces",
    "section": "Matrix multiplication and linear combination of rows",
    "text": "Matrix multiplication and linear combination of rows\nThe first step in Equation 9 is\n\\[\n\\begin{pmatrix}1 & 2 & 2 & 2\\\\2 & 4 & 6 & 8\\\\3 & 6 & 8 & 10 \\end{pmatrix}\\overset{l_2' = l_2-2l_1}{\\longrightarrow}\\begin{pmatrix}1 & 2 & 2 & 2 \\\\0 & 0 & 2 & 4 \\\\3 & 6 & 8 & 10 \\end{pmatrix}\n\\]\nand\n\\[\n\\begin{pmatrix}\n1\\\\2\\\\3\n\\end{pmatrix}\n\\overset{l_2' = l_2-2l_1}{\\longrightarrow}\n\\begin{pmatrix}\n1\\\\0\\\\3\n\\end{pmatrix}\n\\]\nImplicit was the following matrix multiplication:\n\\[\n\\begin{pmatrix}1 & 0 & 0 \\\\-2 & 1 & 0 \\\\0 & 0 & 1 \\end{pmatrix}\\begin{pmatrix}1 & 2 & 2 & 2 \\\\2 & 4 & 6 & 8 \\\\3 & 6 & 8 & 10\\end{pmatrix} = \\begin{pmatrix}1 & 2 & 2 & 2 \\\\0 & 0 & 2 & 4 \\\\3 & 6 & 8 & 10 \\end{pmatrix}\n\\]\nand\n\\[\n\\begin{pmatrix}1 & 0 & 0 \\\\-2 & 1 & 0 \\\\0 & 0 & 1 \\end{pmatrix}\\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix}=\\begin{pmatrix}1\\\\0\\\\3\\end{pmatrix}\n\\]\nHow do we do these calculations?\nThe general rule is:\n\n\n\n\n\n\nFigure 1: Matrix multiplication of a 3x4 matrix by a 3x3 matrix.\n\n\n\nwhere, for example:\n\\[\n\\begin{align}\na_{22}'= e_{21}a_{12} + e_{22}a_{22} +e_{23}a_{32}\\\\\na_{23}'= e_{21}a_{13} + e_{22}a_{23} +e_{23}a_{33}\\\\\n\\end{align}\n\\tag{11}\\]\nLets name the matrices involved, the \\(e\\)’s matrix is called \\(E_1\\) and the \\(a\\)’s matrix is called \\(A\\), the result of the multiplication is called \\(A'\\).\nObserving Figure 1 and Equation 11 we see two important aspects about \\(E_1 A\\)\n\nThe shape of \\(E_1\\) is \\(3\\times3\\),the shape of \\(A\\) is \\(3\\times 4\\) and the shape of \\(A'\\) is \\(3\\times4\\).\nThe matrix \\(E_1\\) needs to have as many columns as there are rows in \\(A\\) (in this case \\(3\\)), otherwise it is not possible to do matrix multiplication. In other words, the compatible shapes for matrix multiplication are of the form \\([m\\times r][r\\times n] = [m\\times n]\\), for any integers \\(m,n,r\\). For example, the case above is \\([3\\times 3][3\\times 4]\\) and thus \\(m=3\\), \\(r=3\\) and \\(n=4\\).\nThe general formula for the entries of \\(A'\\) is the complicated formula:\n\\[\na_{ij}'=\\sum_{k=1}^re_{ik}a_{kj}\n\\]"
  },
  {
    "objectID": "vector_spaces.html#inverse-matrix",
    "href": "vector_spaces.html#inverse-matrix",
    "title": "Vector spaces",
    "section": "Inverse Matrix",
    "text": "Inverse Matrix\nOnly square matrices can have inverses.\nNot every square matrix will have an inverse. For example \\(E_1\\) is square, thus it may or not have an inverse \\(E_1^{-1}\\), on the other hand the matrix \\(A\\) is rectangular, thus no inverse exists.\nHow do we compute an inverse of a matrix?\nThe \\(E\\)’s matrices have inverses, always and thus:\n\\[\nAx=b \\iff EAx=Eb \\iff A'x=b'\n\\]\nare equivalent and and as a result their truth sets are equal! Finding the truth set of the later is easier than the former."
  },
  {
    "objectID": "vector_spaces.html#two-special-subspaces",
    "href": "vector_spaces.html#two-special-subspaces",
    "title": "Vector spaces",
    "section": "Two special subspaces",
    "text": "Two special subspaces\nHere we focus on the column space and nullspace of a matrix.\nThe column space is the vector space that is generated by taking all linear combinations of the columns of a matrix. For example, the column space of the matrix\n\\[\nA=\\begin{pmatrix}1 & 2 & 2 & 2 \\\\2 & 4 & 6 & 8 \\\\3 & 6 & 8 & 10 \\end{pmatrix}\n\\tag{12}\\]\nfrom example 4 called \\(C(A)\\) and is generated by\n\\[\na\\begin{pmatrix}\n1\\\\2\\\\3\n\\end{pmatrix}\n+\nb\\begin{pmatrix}2\\\\4\\\\6\\end{pmatrix}\n+\nc\\begin{pmatrix}2\\\\6\\\\8\\end{pmatrix}\n+\nd\\begin{pmatrix}2\\\\8\\\\10\\end{pmatrix}\n\\]\nIt may happen, and it is the case as we shall see, that we need not all the columns of \\(A\\) to generate \\(C(A)\\), the reason being, some of the column vectors may depend on other columns, thus not providing no additional information. At any rate, \\(C(A)\\) always comes as a result of combining all the columns, it is just that some columns are redundant and we can generate the same with less.\nThe nullspace of a matrix, take again Equation 12, is the vector space generated by all solutions \\(\\mathbf{x}_N\\) of the equation:\n\\[\nA\\mathbf{x}_N=\\mathbf{0}\n\\]\nThe concept of column space and nullspace will be key from now on."
  },
  {
    "objectID": "vector_spaces.html#systems-of-equations-from-a-new-view",
    "href": "vector_spaces.html#systems-of-equations-from-a-new-view",
    "title": "Vector spaces",
    "section": "Systems of equations from a new view",
    "text": "Systems of equations from a new view\nThe system of equations used in example 4 is:\n\\[\nA\\mathbf{x}=\\mathbf{b}\n\\leftrightsquigarrow\n\\begin{pmatrix}1 & 2 & 2 & 2 \\\\2 & 4 & 6 & 8 \\\\3 & 6 & 8 & 10 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\\\z\\\\w\\end{pmatrix}=\\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix}\n\\leftrightsquigarrow\n\\begin{pmatrix}1 & 2 & 2 & 2 &\\bigm|1\\\\2 & 4 & 6 & 8 &\\bigm|2\\\\3 & 6 & 8 & 10 &\\bigm| 3\\end{pmatrix}\n\\tag{13}\\]\nWe can rewrite the system of equation in, yet, another manner:\n\\[\n\\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix}x+\\begin{pmatrix}2\\\\4\\\\6\\end{pmatrix}y+\\begin{pmatrix}2\\\\6\\\\8\\end{pmatrix}z+\\begin{pmatrix}2\\\\8\\\\10\\end{pmatrix}w = \\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix}\n\\tag{14}\\]\nIn other words, a matrix times a column vector is a linear combination of the columns of the matrix. Under the Equation 13 we would ask, what is the column vector \\(\\mathbf{x}\\) which when multiplied by the matrix \\(A\\) yields the column vector \\(\\mathbf{b}\\).\nUnder the Equation 14 we would ask: what is the linear combination of the columns that leads to the vector \\(\\mathbf{b}\\) on the rhs. But now note: this combination is only possible, provided the \\(\\mathbf{b}\\) vector is in the column space!\nThis crucial observation can be paraphrased differently; going back to Equation 13, focus on the extended matrix version of the system of the equations and note, that, when we extend the matrix, i.e., when we append the \\(\\mathbf{b}\\) vector on the rhs, the column space must not change, for otherwise the \\(\\mathbf{b}\\) was not in the column space.\nMoreover, one more way, to paraphrase the paragraph above, the \\(\\mathbf{b}\\) vector must be dependent on the columns of \\(A\\)."
  },
  {
    "objectID": "vector_spaces.html#putting-all-together",
    "href": "vector_spaces.html#putting-all-together",
    "title": "Vector spaces",
    "section": "Putting all together",
    "text": "Putting all together\nBecause the \\(E_k\\) matrices are invertible we have the following equivalence:\n\\(A\\mathbf{x}=\\mathbf{b} \\iff A'\\mathbf{x}=\\mathbf{b}'\\)\nAnd thus the solutions of both equations are the same, in particular, following example 4 we have (using the extended-matrix notation) \\[\n\\begin{align}\n&\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm|1\\\\\n2 & 4 & 6 & 8 &\\bigm|2\\\\\n3 & 6 & 8 & 10 &\\bigm| 3\n\\end{pmatrix}\n\\iff\n\\begin{pmatrix}\n1 & 2 & 0 & -2 &\\bigm| 1\\\\\n0 & 0 & 1 & 2 &\\bigm| 0\\\\\n0 & 0 & 0 & 0 &\\bigm| 0\n\\end{pmatrix}\n\\end{align}\n\\]\nWe can rewrite both sides of the equivalence as the follows:\n\\[\n\\begin{align}\n&\\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix}x+\\begin{pmatrix}2\\\\4\\\\6\\end{pmatrix}y+\\begin{pmatrix}2\\\\6\\\\8\\end{pmatrix}z+\\begin{pmatrix}2\\\\8\\\\10\\end{pmatrix}w = \\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix}\\\\\n&\\iff\n\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}x+\\begin{pmatrix}2\\\\0\\\\0\\end{pmatrix}y+\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}z+\\begin{pmatrix}-2\\\\2\\\\0\\end{pmatrix}w = \\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}\n\\end{align}\n\\tag{15}\\]\nWhat this says, is that the coefficients that combine the columns of \\(A\\) to give us the \\(\\mathbf{b}\\) is exactly the same as the coefficients that, when combine the columns of \\(A'\\) give us the vector \\(\\mathbf{b}'\\).\nWe already know how to find the solution for this problem, see Section, what we want to do now to find the solution using the matrix-vector notation. The way you do it in this notation, provides you with deeper insight about when there is one, none or many solutions.\nHow would we find \\(x,y,z,w\\) that satisfy the equations above?\n\nSimplify the system as much as you can, we already did this. The lhs of Equation 15 is complicated, but the rhs (full of zeros and many ones) is simple.\nIdentify the dependent and independent columns of the simple matrix. We do this by visual inspection:\nThe first and third columns can be linearly combined to generate the second and fourth columns:\n\\[\n\\text{col}_2 =2\\text{col}_1\\qquad \\text{col}_4=2\\text{col}_3-2\\text{col}_1\n\\]\nNotice the independent columns have an entry \\(1\\) and the remaining entries are zeros, the independent columns are also called the pivot columns. The dependent columns are called free columns.\nThe \\(y\\) and \\(w\\) unknowns are the free unknowns.\nTo find a particular solution of the system, set the free unknowns to zero:\n\\[\ny=0\\qquad w=0\n\\]\n\\[\n\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}x\n+\\begin{pmatrix}2\\\\0\\\\0\\end{pmatrix}0\n+\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}z\n+\\begin{pmatrix}-2\\\\2\\\\0\\end{pmatrix}0 = \\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}\n\\]\nNow find \\(x\\) and \\(z\\), by inspection:\n\\[\nx=1 \\qquad z=0\n\\]\nThe particular solution is\n\\[ \\begin{pmatrix} 1\\\\0\\\\0\\\\0 \\end{pmatrix} \\]\nTo find the general solution, compute the nullspace by solving \\(A'\\mathbf{x}_N=\\mathbf{0}\\) and add it to the particular solution\n\\[\n\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}x_N+\\begin{pmatrix}2\\\\0\\\\0\\end{pmatrix}y_N\n+\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}z_N+\\begin{pmatrix}-2\\\\2\\\\0\\end{pmatrix}w_N = \\begin{pmatrix}0\\\\0\\\\0\\end{pmatrix}\n\\tag{16}\\]\nReminding ourselves which columns are independent and which ones are dependent tells us how to find the solution. The column two and four can be built out from approriate linear combinations of column one and three.\nTo solve the equation we choose the free variables \\(y_N\\) and \\(w_N\\) as we wish and then solve for the \\(x_N\\) and \\(z_N\\). When choosing freely, at least choose something that simplify you calculations, for example set \\(y_N=1\\) and \\(w_N=0\\) in Equation 16 and guess what is \\(x_N\\) and \\(z;_N\\) we get:\n\\[\n\\mathbf{x}_N =\\begin{pmatrix}-2\\\\1\\\\0\\\\0 \\end{pmatrix}\n\\]\nNow substitute \\(y_N=0\\) and \\(w_N=1\\) into Equation 16 and guess the corresponding \\(x_N\\) and \\(z_N\\); the answer gives us:\n\\[\n\\mathbf{x}_N=\\begin{pmatrix}2\\\\0\\\\-2\\\\1\\end{pmatrix}\n\\]\nThe nullspace of the matrix \\(A\\) is composed by all linear combinations:\n\\[\na\\begin{pmatrix}-2\\\\1\\\\0\\\\0 \\end{pmatrix} +b\\begin{pmatrix}2\\\\0\\\\-2\\\\1 \\end{pmatrix}\n\\]\nSo far we know a particular solution for the problem \\(A\\mathbf{x}_p=\\mathbf{b}\\) and we know the nullspace of \\(A\\), the key observations is this:\nIf \\(\\mathbf{x}_P\\) is a particular solution of \\(A\\mathbf{x}=\\mathbf{b}\\) then adding to it any \\(\\mathbf{x}_N\\) will not make a difference.\nJustification: \\(A(\\mathbf{x}_P+\\mathbf{x}_N)=A\\mathbf{x}_P+A\\mathbf{x}_N=\\mathbf{b}+\\mathbf{0}=\\mathbf{b}\\)\nThus, the system \\(A\\mathbf{x} =\\mathbf{b}\\) has an infinite number of solutions, all of the form \\(\\mathbf{x}=\\mathbf{x}_P+\\mathbf{x}_N\\).\nFor the matrix in question, the solutions are these:\n\\[\n\\mathbf{x}=\\begin{pmatrix}1\\\\0\\\\0\\\\0\\end{pmatrix}+a\\begin{pmatrix}-2\\\\1\\\\0\\\\0 \\end{pmatrix} +b\\begin{pmatrix}2\\\\0\\\\-2\\\\1 \\end{pmatrix}\n\\tag{17}\\]\n\nIts important to compare the solutions Equation 10 and Equation 17.\nRearranging Equation 10 we have:\n\\[\n\\begin{cases}x &=1-2y&-&2w\\\\z &=&-&2w\\end{cases}\n\\iff\n\\begin{cases}\nx &= &1&+& &-&2a&+& &-&2b\\\\\ny &= & &+& &+&a&+& &+& \\\\\nz &= & &+& &+& &+& &-&2b\\\\\nw &= & &+& &+& &+& &+&b\n\\end{cases}\n\\]\nThe rhs of this equivalence is exactly what Equation 17 means!"
  },
  {
    "objectID": "vector_spaces.html#one-none-or-many-solutions",
    "href": "vector_spaces.html#one-none-or-many-solutions",
    "title": "Vector spaces",
    "section": "One, none or many solutions?",
    "text": "One, none or many solutions?\nWith the above discussion understood we have the following cases:\n\nCase: One solution\nWhen the matrix is square and when its rref is the identity, then there is only one solution!\nExample:\n\nThe solution of this example is \\(\\mathbf{x} =(3,-1,1)^T\\). The nullspace of the matrix is just the zero vector.\n\n\nCase: One or none solution\nWhen the matrix is rectangular \\(r=n&lt;m\\) and its rref is like:\n\nthen, there is no solution! The \\(\\mathbf{b}=(5,-6,-8)^T\\) does not belong to the column space.\nIf the system is instead like\n\nthen, there is one solution, \\(\\mathbf{x}=(5,-6)^T\\). By making the last entry of \\(\\mathbf{b}\\) zero, now it belongs to the columns space, \\(5\\) times the first column minus \\(6\\) times the second column gives us this \\(\\mathbf{b}\\).\n\n\nCase: Infinite solutions\nConsider a matrix that looks like this, identity columns and free column separated\n\nor like this, with the identity and free blocks mixed:\n\nIn this situation there is an infinite number of solutions. The first system has solutions of the form\n\\[\n\\begin{pmatrix}\nx\\\\\ny\\\\\nz\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-1/3z_0\\\\\n4/3z_0\\\\\nz_0\n\\end{pmatrix}\n\\]\nwhile the second has them of the form\n\\[\n\\begin{pmatrix}\nx\\\\\ny\\\\\nz\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-1/3y_0\\\\\ny_0\\\\\n4/3y_0\n\\end{pmatrix}\n\\]\nThese solutions also constitute the nullspaces.\n\n\nCase zero or infinite solutions\nLets look again Equation 6, we identify three unknown and two equations, if we imagine we have one more equation provided by ourselves the system would have a solution, assume what equation is just \\(z=z_0\\) for some value of \\(z_0\\) of our liking then the system would have been\n\\[\n\\begin{pmatrix}1 & 0 & 1/3\\\\0 & 1 & -4/3\\\\0 & 0 & 1\\end{pmatrix}\\begin{pmatrix}x\\\\y\\\\z\\end{pmatrix}=\\begin{pmatrix}0\\\\0\\\\z_0\\end{pmatrix}\n\\]\nThe solution is:\n\\[\n\\begin{pmatrix}\nx\\\\\ny\\\\\nz\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-1/3z_0\\\\\n4/3z_0\\\\\nz_0\n\\end{pmatrix}\n\\]\nJust like we computed above. I’m showing this example for you to see that we have to have as many independent equations as unknowns, only then we have a unique solution. The key take away is that a parameter dependent solution like Equation 8 is equivalent to adding an equation ourselves."
  },
  {
    "objectID": "vector_spaces.html#subspaces",
    "href": "vector_spaces.html#subspaces",
    "title": "Vector spaces",
    "section": "Subspaces",
    "text": "Subspaces"
  },
  {
    "objectID": "vector_spaces.html#dot-product",
    "href": "vector_spaces.html#dot-product",
    "title": "Vector spaces",
    "section": "Dot product",
    "text": "Dot product\nFalar novamente dos subespaços do secundário, que já mensionei acima na sec dos subespaços, mas desta vez dar usar a equação com produto interno tal como fazes nos teus resumos de GA do 11o ano."
  },
  {
    "objectID": "vector_spaces.html#transpose-of-a-matrix",
    "href": "vector_spaces.html#transpose-of-a-matrix",
    "title": "Vector spaces",
    "section": "Transpose of a matrix",
    "text": "Transpose of a matrix"
  },
  {
    "objectID": "vector_spaces.html#rank-of-a-matrix",
    "href": "vector_spaces.html#rank-of-a-matrix",
    "title": "Vector spaces",
    "section": "Rank of a matrix",
    "text": "Rank of a matrix"
  },
  {
    "objectID": "vector_spaces.html#basis",
    "href": "vector_spaces.html#basis",
    "title": "Vector spaces",
    "section": "Basis?",
    "text": "Basis?"
  },
  {
    "objectID": "vector_spaces.html#matrices-as-linear-operators",
    "href": "vector_spaces.html#matrices-as-linear-operators",
    "title": "Vector spaces",
    "section": "Matrices as linear Operators?",
    "text": "Matrices as linear Operators?"
  },
  {
    "objectID": "vector_spaces.html#four-subspaces-of-a-matrix",
    "href": "vector_spaces.html#four-subspaces-of-a-matrix",
    "title": "Vector spaces",
    "section": "Four subspaces of a matrix",
    "text": "Four subspaces of a matrix\n\\[\n\\begin{align}\n&\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| &b_1\\\\\n2 & 4 & 6 & 8 &\\bigm| &b_2\\\\\n3 & 6 & 8 & 10 &\\bigm| &b_3\n\\end{pmatrix}\n\\overset{l_2' = l_2-2l_1}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| &b_1\\\\\n0 & 0 & 2 & 4 &\\bigm| &b_2-2b_1\\\\\n3 & 6 & 8 & 10 &\\bigm| &b_3\n\\end{pmatrix}\\\\\n&\\overset{l_3' = l_3-3l_1}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| &b_1\\\\\n0 & 0 & 2 & 4 &\\bigm| &b_2-2b_1\\\\\n0 & 0 & 2 & 4 &\\bigm| &b_3-3b_1\n\\end{pmatrix}\n\\overset{l_3'=l_3-l_2}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| &b_1\\\\\n0 & 0 & 2 & 4 &\\bigm| &b_2-2b_1\\\\\n0 & 0 & 0 & 0 &\\bigm| &b_3-b_2-b_1\n\\end{pmatrix}\\\\\n&\\overset{l_2'=1/2l_2}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| &b_1\\\\\n0 & 0 & 1 & 2 &\\bigm| &b_2/2-b_1\\\\\n0 & 0 & 0 & 0 &\\bigm| &b_3-b_2-b_1\n\\end{pmatrix}\n\\overset{l_1'=l_1-2l_2}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 0 & -2 &\\bigm| &3b_1-b_2\\\\\n0 & 0 & 1 & 2 &\\bigm| &b_2/2-b_1\\\\\n0 & 0 & 0 & 0 &\\bigm| &b_3-b_2-b_1\n\\end{pmatrix}\n\\end{align}\n\\]"
  }
]