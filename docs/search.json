[
  {
    "objectID": "basis_of_vector_spaces.html",
    "href": "basis_of_vector_spaces.html",
    "title": "Basis of vector spaces",
    "section": "",
    "text": "The concept of basis is a natural one after the discussion on vector spaces so far.\n\nDefinition 1 Consider a generic vector space \\(\\mathbb{V}\\) (eg. \\(\\mathbb{R}^n\\)), then a subset \\(\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\}\\) is called a basis of \\(\\mathbb{V}\\) if:\n\n\\(span\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\} = \\mathbb{V}\\)\n\\(\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\) are linearly independent\n\n\nFor example take the vector space \\(\\mathbb{R}^2\\), we can promote the vectors \\(\\begin{pmatrix}1 \\\\0\\end{pmatrix}=:\\mathbf{e}_1\\) and \\(\\begin{pmatrix}0\\\\1\\end{pmatrix}=:\\mathbf{e}_2\\) to the status of basis vectors. Pictorially they look like the pink vectors:\n\nand by visual inspection alone it is clear they are independent, using analytics we confirm their independence as well by solving:\n\\[\n\\alpha\\mathbf{e}_1 +\\beta\\mathbf{e}_2 = \\mathbf{0} \\implies \\alpha\\begin{pmatrix}1 \\\\0\\end{pmatrix} +\\beta \\begin{pmatrix}0 \\\\1\\end{pmatrix} = \\begin{pmatrix}0 \\\\0\\end{pmatrix}\\implies \\alpha=\\beta=0\n\\]\nThe only solution are zeros, thus \\(\\mathbf{e}_1\\) and \\(\\mathbf{e}_2\\) are independent.\nWith \\(\\{\\mathbf{e}_1,\\mathbf{e}_2\\}\\) we can write any vector of \\(\\mathbb{R}^2\\) as some appropriate l.c. (choice of \\(c_1\\) and \\(c_2\\)):\n\\[\n\\mathbf{v} = c_1\\mathbf{e}_1+c_2\\mathbf{e}_2\\\n\\]\nReading this formula from left to right we can see that \\(\\mathbf{v}\\) is decomposed into two pieces, reading from right to left we see we are constructing the vector \\(\\mathbf{v}\\) . Given a basis, each vector in our vector space can be decomposed in a unique manner!\nThe numbers \\(c_1\\) and \\(c_2\\) are called the components of the vector \\(\\mathbf{v}\\) wrt to the chosen basis. How do we compute them is the subject of a later section.\nNotice that \\(\\{\\mathbf{e}_1,\\mathbf{e}_2\\}\\) is not the only basis we can choose for \\(\\mathbb{R}^2\\), in fact any two (and not three or four or…) vectors with distinct directions can be used, for example:\n\\[\n\\mathbb{R}^2 = span\\{\\mathbf{e}_1,\\mathbf{e}_2\\}=span\\{\\mathbf{e}'_1,\\mathbf{e}'_2\\}=span\\{\\mathbf{e}''_1,\\mathbf{e}''_2\\}=\\dots\n\\]\nWhich implies \\(\\mathbf{v}\\) can be written as:\n\\[\n\\mathbf{v} = c_1 \\mathbf{e}_1+c_2\\mathbf{e}_2=c_1' \\mathbf{e}_1'+c_2'\\mathbf{e}_2'=c_1'' \\mathbf{e}_1''+c_2''\\mathbf{e}_2''=\\dots\n\\]\nThe same \\(\\mathbf{v}\\) and multiple points of view suggests the components of different basis must be related, the subject of the Section on Change of Basis.\nObserve that:\n\nfor a basis of \\(\\mathbb{R}^2\\), we need two vectors, because the vector space is \\(2-\\)dimensional. The dimension of the vector space, \\(\\dim \\mathbb{R}^2\\) is equal to the number of basis vectors needed.\nThe basis need not be orthogonal, though we prefer it to be.\nThe subspace x axis of \\(\\mathbb{R}^2\\) has basis \\(\\{\\mathbf{e}_1\\}\\) or \\(\\{2\\mathbf{e}_1\\}\\) or \\(\\{-\\mathbf{e}_1\\}\\) or etc.\nThe subspace of \\(\\mathbb{R}^2\\) given by \\(\\{\\mathbf{e}_1+\\mathbf{e}_2\\}\\) is the line with \\(45^\\circ\\).\n\n\n\n\n\n\n\nCommentary\n\n\n\nWriting a vector wrt to a basis is analogous to decompose ones emotion by comparing with reference emotions (´◡`) and (o_O)?\n\\[\nHarry = c_1 \\,\\,\\text{Happy} + c_2 \\,\\,\\text{Confused}\n\\]\nOther reference emotions are possible. When you text emojis, you are texting a l.c.\n\n\n\n\nLooking at our definition above we can see that any set of independent vectors constitute a basis of some vector space. Essentially to determine whether a set is a basis or not is the same as asking whether the vectors are or not independent, and this question was already addressed before.\nFor the moment lets see two examples.\nExample 1: Imagine the following spans:\n\\[\nA=\\{\\begin{pmatrix} 1\\\\2\\end{pmatrix},\\begin{pmatrix} 3\\\\0\\end{pmatrix},\\begin{pmatrix} 0\\\\3\\end{pmatrix}\\}\\qquad B=\\{\\begin{pmatrix} 1\\\\2\\end{pmatrix},\\begin{pmatrix} 3\\\\0\\end{pmatrix}\\}\n\\]\nOnly the \\(B\\) has the status of a basis. Why? See the definition above and checking the two criteria, \\(B\\) fulfills both but \\(A\\) does not fulfill the independence criteria.\nWe can check this by solving:\n\\[\n\\alpha \\begin{pmatrix} 1\\\\2\\end{pmatrix} +\\beta \\begin{pmatrix} 1\\\\0\\end{pmatrix}+\\gamma\\begin{pmatrix} 0\\\\1\\end{pmatrix}=\\begin{pmatrix} 0\\\\0\\end{pmatrix}\n\\]\nwhose solution is: \\(\\gamma=2\\), \\(\\alpha=1\\) and \\(\\beta= -1\\). The \\(B\\) basis, expands a space with dimension \\(2\\).\nExample 2: Consider now the st:\n\\[ C=\\{( 1,0,1),(0,0,1)\\}\\qquad D=\\{( 1,0,1),( 0,0,1),( 1,0,3)\\} \\]\nThe set \\(C\\) is a basis that expands a two dimensional space, because we have two vectors. We can actually say more, because these vectors live in \\(\\mathbb{R}^3\\) and hence this space is in fact a subspace of it. The set \\(D\\) is not a basis because the vectors are not independent; the solution is \\(\\alpha=1\\), \\(\\beta=2\\) and \\(\\gamma=-1\\). Note, by the criteria 1. of a basis, \\(C\\) is not a basis of \\(\\mathbb{R}^3\\).\n\n\n\n\n\n\nCommentary\n\n\n\n\nTo test the (in)dependence of a candidate set to basis vectors will be equivalent to solving a problem like \\(A\\mathbf{x}=\\mathbf{0}\\).\nIf we have a set of independent vectors, then they generate a vector space. We describe the space as: \\(A = span\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\}\\) and \\(\\dim A =n\\). Later we’ll see another way to describe the set: also through the equation of the form \\(\\tilde{A}\\mathbf{x}=\\mathbf{0}\\), whose solutions are the \\(\\mathbf{v}_1,\\dots\\mathbf{v}_n\\).\n\nSo, the same kind of problem \\(A\\mathbf{x}=\\mathbf{0}\\) will, as we shall see, occur in two situations: test (in)dependence of sets of vectors, generate a basis for a subspace.\n\n\nExercises: 1.5.1,2\n\n\n\nExample 1: Imagine we have the following set \\(A=\\{(1,1,0),(1,0,1)\\}\\) how do we create from it a basis for \\(\\mathbb{R}^3\\)? Well, we have to append to this set one new vectors that point in different direction. That way we would have three independent vectors living in \\(\\mathbb{R}^3\\) which would allow us to cover the entire space by l.c. The new vector cannot be a l.c. of the vectors already present in \\(A\\).\nTo find the new vector in a systematic manner, we notice, a key aspect of it, since the missing vector belongs to \\(\\mathbb{R}^3\\), then it has the form \\((a,b,c)\\) for \\(a,b,c\\in \\mathbb{R}\\). Our problem, is thus to make the third column of the following matrix\n\\[\n\\begin{pmatrix}1 & 1 & a\\\\1 & 0 & b\\\\0 & 1 & c\\end{pmatrix}\n\\]\nindependent of the first two, to achieve that we simplify it through elimination:\n\\[\n\\begin{pmatrix}\n1 & 1 & a\\\\\n1 & 0 & b\\\\\n0 & 1 & c\n\\end{pmatrix}\n\\overset{l_2'=l_2-l_1}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 1 & a\\\\\n0 & -1 & b-a\\\\\n0 & 1 & c\\end{pmatrix}\n\\overset{l_2\\leftrightarrow l_3}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 1 & a\\\\\n0 & 1 & c\\\\\n0 & -1 & b-a\n\\end{pmatrix}\n\\overset{l_3'=l_3+l_2}{\\longrightarrow}\n\\begin{pmatrix}\n\\mathbf{1} & 1 & a\\\\\n0 & \\mathbf{1} & c\\\\\n0 & 0 & b-a+c\n\\end{pmatrix}\n\\]\nWe identify pivots in the first two columns, meaning they are independent, we want now to choose \\(a,b,c\\in \\mathbb{R}\\) so such that we gain a third pivot at position \\(A_{33}\\). For example: \\(b=1\\) and \\(a=c =0\\) guarantees that \\(b-a+c\\not =0\\), a new pivot emerged!\nWe conclude the set \\(A'=\\{(1,1,0),(1,0,1),(0,1,0\\}\\) has only independent vectors and is a basis of \\(\\mathbf{R}^3\\).\nExample 2: Assume not the set \\(A=\\{(0,0,1)\\}\\) and we want to complete \\(A\\) so as to turn it into a basis of the subspace \\(\\{(x,y,z)\\in\\mathbb{R}^3\\,\\,|\\,\\, y=2x\\}\\); notice this is our [pink-blue plane]. Since the missing vector belongs to this plane, then it must have the form \\((a,2a,b)\\). [Note: \\((0,0,1)\\) fits this form.]\nTo answer, we have to simplify:\n\\[\n\\begin{pmatrix}\n0 & a\\\\\n0 & 2a\\\\\n1 & b\n\\end{pmatrix}\n\\overset{l_1\\leftrightarrow l_3}{\\longrightarrow}\n\\begin{pmatrix}\n\\mathbf{1} & b\\\\\n0 & 2a\\\\\n0 & a\n\\end{pmatrix}\n\\]\nWe have one pivot in column 1 and want a second pivot in column 2. A possible solution to the candidate \\(2a\\) into a pivot is choose \\(b=0\\) and \\(a=1\\). Thus \\(A'=\\{(0,0,1),(1,2,0)\\}\\) is a basis for the [pink-blue subspace].\nExample 3: Suppose we have the following vector space \\(\\mathbb{V}=\\{(x,y)\\,\\,|\\,\\, x+2y-3z=0\\}\\). Note it is a two dimensional space because the three entries of the vector are constrained by the equation; another way to arrive at this conclusion is to recognize that the equation is the equation of plane perpendicular to \\((1,2,-3)\\) that passes (as any vector space should) through the origin. Choosing values for two of the variables and solving for the third yields the desired vectors, for example, let \\(z=0\\) and \\(y=-1/2\\), then \\(x=1\\); choosing \\(y=0\\) and \\(z=1\\) gives us \\(x=3\\). These calculation gave us two basis vectors:\n\\[\nC=\\{(1,-1/2,0),(3,0,1)\\}\n\\]\nwhich are clearly independent.\nExercises: 1.5.3,6,7",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Basis of vector spaces"
    ]
  },
  {
    "objectID": "basis_of_vector_spaces.html#is-it-a-basis-or-not-of-which-vector-space",
    "href": "basis_of_vector_spaces.html#is-it-a-basis-or-not-of-which-vector-space",
    "title": "Basis of vector spaces",
    "section": "",
    "text": "Looking at our definition above we can see that any set of independent vectors constitute a basis of some vector space. Essentially to determine whether a set is a basis or not is the same as asking whether the vectors are or not independent, and this question was already addressed before.\nFor the moment lets see two examples.\nExample 1: Imagine the following spans:\n\\[\nA=\\{\\begin{pmatrix} 1\\\\2\\end{pmatrix},\\begin{pmatrix} 3\\\\0\\end{pmatrix},\\begin{pmatrix} 0\\\\3\\end{pmatrix}\\}\\qquad B=\\{\\begin{pmatrix} 1\\\\2\\end{pmatrix},\\begin{pmatrix} 3\\\\0\\end{pmatrix}\\}\n\\]\nOnly the \\(B\\) has the status of a basis. Why? See the definition above and checking the two criteria, \\(B\\) fulfills both but \\(A\\) does not fulfill the independence criteria.\nWe can check this by solving:\n\\[\n\\alpha \\begin{pmatrix} 1\\\\2\\end{pmatrix} +\\beta \\begin{pmatrix} 1\\\\0\\end{pmatrix}+\\gamma\\begin{pmatrix} 0\\\\1\\end{pmatrix}=\\begin{pmatrix} 0\\\\0\\end{pmatrix}\n\\]\nwhose solution is: \\(\\gamma=2\\), \\(\\alpha=1\\) and \\(\\beta= -1\\). The \\(B\\) basis, expands a space with dimension \\(2\\).\nExample 2: Consider now the st:\n\\[ C=\\{( 1,0,1),(0,0,1)\\}\\qquad D=\\{( 1,0,1),( 0,0,1),( 1,0,3)\\} \\]\nThe set \\(C\\) is a basis that expands a two dimensional space, because we have two vectors. We can actually say more, because these vectors live in \\(\\mathbb{R}^3\\) and hence this space is in fact a subspace of it. The set \\(D\\) is not a basis because the vectors are not independent; the solution is \\(\\alpha=1\\), \\(\\beta=2\\) and \\(\\gamma=-1\\). Note, by the criteria 1. of a basis, \\(C\\) is not a basis of \\(\\mathbb{R}^3\\).\n\n\n\n\n\n\nCommentary\n\n\n\n\nTo test the (in)dependence of a candidate set to basis vectors will be equivalent to solving a problem like \\(A\\mathbf{x}=\\mathbf{0}\\).\nIf we have a set of independent vectors, then they generate a vector space. We describe the space as: \\(A = span\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\}\\) and \\(\\dim A =n\\). Later we’ll see another way to describe the set: also through the equation of the form \\(\\tilde{A}\\mathbf{x}=\\mathbf{0}\\), whose solutions are the \\(\\mathbf{v}_1,\\dots\\mathbf{v}_n\\).\n\nSo, the same kind of problem \\(A\\mathbf{x}=\\mathbf{0}\\) will, as we shall see, occur in two situations: test (in)dependence of sets of vectors, generate a basis for a subspace.\n\n\nExercises: 1.5.1,2",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Basis of vector spaces"
    ]
  },
  {
    "objectID": "basis_of_vector_spaces.html#creating-the-appropriate-basis-for-a-vector-space",
    "href": "basis_of_vector_spaces.html#creating-the-appropriate-basis-for-a-vector-space",
    "title": "Basis of vector spaces",
    "section": "",
    "text": "Example 1: Imagine we have the following set \\(A=\\{(1,1,0),(1,0,1)\\}\\) how do we create from it a basis for \\(\\mathbb{R}^3\\)? Well, we have to append to this set one new vectors that point in different direction. That way we would have three independent vectors living in \\(\\mathbb{R}^3\\) which would allow us to cover the entire space by l.c. The new vector cannot be a l.c. of the vectors already present in \\(A\\).\nTo find the new vector in a systematic manner, we notice, a key aspect of it, since the missing vector belongs to \\(\\mathbb{R}^3\\), then it has the form \\((a,b,c)\\) for \\(a,b,c\\in \\mathbb{R}\\). Our problem, is thus to make the third column of the following matrix\n\\[\n\\begin{pmatrix}1 & 1 & a\\\\1 & 0 & b\\\\0 & 1 & c\\end{pmatrix}\n\\]\nindependent of the first two, to achieve that we simplify it through elimination:\n\\[\n\\begin{pmatrix}\n1 & 1 & a\\\\\n1 & 0 & b\\\\\n0 & 1 & c\n\\end{pmatrix}\n\\overset{l_2'=l_2-l_1}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 1 & a\\\\\n0 & -1 & b-a\\\\\n0 & 1 & c\\end{pmatrix}\n\\overset{l_2\\leftrightarrow l_3}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 1 & a\\\\\n0 & 1 & c\\\\\n0 & -1 & b-a\n\\end{pmatrix}\n\\overset{l_3'=l_3+l_2}{\\longrightarrow}\n\\begin{pmatrix}\n\\mathbf{1} & 1 & a\\\\\n0 & \\mathbf{1} & c\\\\\n0 & 0 & b-a+c\n\\end{pmatrix}\n\\]\nWe identify pivots in the first two columns, meaning they are independent, we want now to choose \\(a,b,c\\in \\mathbb{R}\\) so such that we gain a third pivot at position \\(A_{33}\\). For example: \\(b=1\\) and \\(a=c =0\\) guarantees that \\(b-a+c\\not =0\\), a new pivot emerged!\nWe conclude the set \\(A'=\\{(1,1,0),(1,0,1),(0,1,0\\}\\) has only independent vectors and is a basis of \\(\\mathbf{R}^3\\).\nExample 2: Assume not the set \\(A=\\{(0,0,1)\\}\\) and we want to complete \\(A\\) so as to turn it into a basis of the subspace \\(\\{(x,y,z)\\in\\mathbb{R}^3\\,\\,|\\,\\, y=2x\\}\\); notice this is our [pink-blue plane]. Since the missing vector belongs to this plane, then it must have the form \\((a,2a,b)\\). [Note: \\((0,0,1)\\) fits this form.]\nTo answer, we have to simplify:\n\\[\n\\begin{pmatrix}\n0 & a\\\\\n0 & 2a\\\\\n1 & b\n\\end{pmatrix}\n\\overset{l_1\\leftrightarrow l_3}{\\longrightarrow}\n\\begin{pmatrix}\n\\mathbf{1} & b\\\\\n0 & 2a\\\\\n0 & a\n\\end{pmatrix}\n\\]\nWe have one pivot in column 1 and want a second pivot in column 2. A possible solution to the candidate \\(2a\\) into a pivot is choose \\(b=0\\) and \\(a=1\\). Thus \\(A'=\\{(0,0,1),(1,2,0)\\}\\) is a basis for the [pink-blue subspace].\nExample 3: Suppose we have the following vector space \\(\\mathbb{V}=\\{(x,y)\\,\\,|\\,\\, x+2y-3z=0\\}\\). Note it is a two dimensional space because the three entries of the vector are constrained by the equation; another way to arrive at this conclusion is to recognize that the equation is the equation of plane perpendicular to \\((1,2,-3)\\) that passes (as any vector space should) through the origin. Choosing values for two of the variables and solving for the third yields the desired vectors, for example, let \\(z=0\\) and \\(y=-1/2\\), then \\(x=1\\); choosing \\(y=0\\) and \\(z=1\\) gives us \\(x=3\\). These calculation gave us two basis vectors:\n\\[\nC=\\{(1,-1/2,0),(3,0,1)\\}\n\\]\nwhich are clearly independent.\nExercises: 1.5.3,6,7",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Basis of vector spaces"
    ]
  },
  {
    "objectID": "vector_spaces_and_subspaces.html",
    "href": "vector_spaces_and_subspaces.html",
    "title": "Vector spaces and subspaces",
    "section": "",
    "text": "The operation of linear combination allow us to construct vector spaces and vector subspaces. In this section we want to look at some figures and from them deduce how to construct spaces. In later sections we will adopt a new approach: to describe the space by an equation or system of equations \\(A\\mathbf{x}=\\mathbf{0}\\).\nKey concepts: vectors space, subspace, span, form of the elements, direct sum.\n\n\nConsider the vectors \\(\\begin{pmatrix} 1\\\\2\\end{pmatrix}\\) and \\(\\begin{pmatrix} 3\\\\0\\end{pmatrix}\\). Now multiply both by every possible scalar and then add them; the end result is a set of vectors. All these vectors constitute a two dimensional plane, which we call \\(\\mathbb{R}^2\\):\n\nFrom the picture, it is easy to convince yourself that any linear combination of two vectors of the plane \\(\\mathbb{R}^2\\) is another vector in \\(\\mathbb{R}^2\\), in particular from the l.c. \\(0\\begin{pmatrix} 1\\\\2\\end{pmatrix}+0\\begin{pmatrix} 3\\\\0\\end{pmatrix}\\) we get the origin. Hence we write:\n\\[\n\\mathbb{R}^2 = span\\{\\begin{pmatrix} 1\\\\2\\end{pmatrix},\\begin{pmatrix} 3\\\\0\\end{pmatrix}\\}\n\\tag{1}\\] Another important way to write this set is to identify the form of its elements, the form of its elements is:\n\\[\na(1,2)+b(3,0)\n\\]\nor written more compactly \\((a+3b,2a)\\); the set of this elements is\n\\[\n\\{(a+3b,2a)\\,\\,|\\,\\,a,b\\in\\mathbb{R}\\}=\\mathbb{R}^2\n\\]\nThe idea of a set of things (in this case column vectors) where any l.c. of its elements gives us again an element of the set (we say the set is closed under l.c.) is very important and will appear countless times in this course. Hence we give it a special name: vector space.\n\nDefinition 1 [Vector space] := a set of vectors with the property of being closed under linear combinations\n\n\\(\\mathbb{R}^2\\) is a vector space.\nNotice the same space comes about if we consider \\(\\begin{pmatrix} 1\\\\0\\end{pmatrix}\\) and \\(\\begin{pmatrix} 0\\\\1\\end{pmatrix}\\) as our starting vectors. More, any other pair of vectors of \\(\\mathbb{R}^2\\) that point in distinct directions, do the job. Hence we write\n\\[\n\\mathbb{R}^2 = span\\{\\begin{pmatrix} 1\\\\2\\end{pmatrix},\\begin{pmatrix} 3\\\\0\\end{pmatrix}\\} = span\\{\\begin{pmatrix} 1\\\\0\\end{pmatrix},\\begin{pmatrix} 0\\\\1\\end{pmatrix}\\}=\\dots\n\\]\nThe form of elements of \\(\\mathbb{R}^2\\) appears in many way, either \\((a+3b,2a)\\) or \\((a,b)\\), etc, depending on our choice of vectors to span.\n\n\n\nConsider the vector \\(\\begin{pmatrix} 1\\\\2\\end{pmatrix}\\), this is our starting point. Now imagine multiplying it by every possible scalar; the end result is a set of vectors which lie along the same line:\n\nFrom the picture, it is easy to convince yourself that any linear combination of two (or more) vectors of the line gives us another vector in the line.\n\\[\n\\textbf{line} = span\\{\\begin{pmatrix}1\\\\2 \\end{pmatrix}\\}=\\{(a,2a)\\,\\,|\\,\\, a\\in\\mathbb{R}\\}\n\\tag{2}\\]\nIn particular if you multiply any of its vectors by \\(0\\), we get the origin \\(\\begin{pmatrix} 0\\\\0\\end{pmatrix}\\). Thus the line we are referring passes through the origin and we can say \\(\\mathbf{0} \\in \\textbf{line}\\).\nOnce again, the line is a set closed under l.c., which means it is a vector space.\nAdditionally, notice that the line lives inside the plane \\(\\mathbb{R}^2\\), because of that we say it is a subspace of \\(\\mathbb{R}^2\\).\n\nDefinition 2 [subspace (of a vector space \\(A\\))] := a vector space which is a subset of \\(A\\).\n(A subspace is also closed under linear combinations of its vectors.)\n\nNote that: the same line we see in the picture also results if you consider as your starting vector, the vector \\(\\begin{pmatrix} 2\\\\4\\end{pmatrix}\\), and then multiply it by all possible numbers (i.e. all l.c. of this vector). Thus this line can be created in many ways:\n\\[\n\\textbf{line} = span\\{\\begin{pmatrix}1\\\\2 \\end{pmatrix}\\} = span\\{\\begin{pmatrix}2\\\\4 \\end{pmatrix}\\}=\\dots\n\\]\n\n\n\n\n\n\nCommentary\n\n\n\n\nMultiplying a vector by a scalar is just a particular example of a linear combination:\n\n\\[ a\\begin{pmatrix}1\\\\2\\end{pmatrix}+b\\begin{pmatrix}0\\\\0\\end{pmatrix} \\]\n\nAny two vectors on the line are parallel, i.e., \\(u||v\\) if they are proportional.\n\n\n\n\n\n\nConsider the vectors \\((1,2,0)^\\intercal\\) and \\((1,0,1)^\\intercal\\) and \\((0,0,1)^\\intercal\\). Linear combine them with every scalar and you obtain a set of vectors. All these vectors fill the three dimensional space \\(\\mathbb{R}^3\\).\n\nIn special notation we have:\n\\[\n\\mathbb{R}^3 = span\\{(1,2,0)^\\intercal,(1,0,1)^\\intercal,(0,0,1)^\\intercal\\}\n\\]From the picture here can identify some subspaces of \\(\\mathbb{R}^3\\), we also write the equation that describes them, take them as facts for now, later, when we introduce matrices we justify them.\n\\[\n[\\textbf{pink-blue plane}] = span\\{(1,2,0), (0,0,1)\\}=\\{(a,2a,b)\\,\\,|\\,\\,a,b\\in\\mathbb{R}\\}\n\\]\n\\[\n[\\textbf{xy plane}] = span \\{(1,0,0),(0,1,0\\}=\\{(a,b,0)\\,\\,|\\,\\,a,b\\in\\mathbb{R}\\}\n\\]\n\\[\n[\\textbf{yellow line}] = span \\{(1,0,1)\\}=\\{(a,0,a)\\,\\,|\\,\\, a\\in\\mathbb{R}\\}\n\\]\nObservation 1: The vector space \\(\\mathbb{R}^3\\) results from summing vectors from the x0y plane and z axis subspaces. Thus any vector in \\(\\mathbb{R}^3\\) can be written as a sum of a vector from the first plus one of the second. We have special notation for this:\n\nDefinition 3 Assume we have two subspaces \\(U\\) and \\(W\\) of \\(\\mathbb{R}^3\\) (eg. the xoy plane and z axis) where the only common element is the origin \\(\\mathbf{0}\\). Then any vector \\(\\mathbf{r}\\) in \\(\\mathbb{R}^3\\) can be written by some appropriate sum of \\(\\mathbf{u}\\) and \\(\\mathbf{w}\\):\n\\[\n\\mathbf{r} = \\mathbf{u}+\\mathbf{w}\n\\]\nIn math language:\n\\[\n\\mathbb{R}^3 = \\{\\mathbf{u}+\\mathbf{v}\\,\\,|\\,\\, \\mathbf{u}\\in U \\,\\,\\text{and} \\,\\,\\mathbf{v}\\in V\\}\n\\]\nThe nick-name of this set is: \\(U\\oplus V\\). The direct sum. Reading \\(\\mathbb{R}^3=U\\oplus V\\) from right to left we see what we said above; reading from left to right we see the space being decomposed in subspaces, clearly this can be done in many ways:\n\\[\n\\mathbb{R}^3 = [\\textbf{xy plane}] \\oplus [\\textbf{0z line}] = [\\textbf{yellow line}]\\oplus [\\textbf{plane perpendicular to yellow line}]=\\dots\n\\]\n\nObservation 2: Let \\(A\\) and \\(B\\) be two (non parallel) planes containing the yellow-line, then the intersection of both yields the yellow line:\n\\[\n[\\textbf{yellow-line}] = A \\cap B\n\\]\nThis is important because it tells us that interception of vectors spaces yields new vector spaces.\n\n\n\nConsider the vectors \\(\\begin{pmatrix} 1\\\\2\\end{pmatrix}\\) , \\(\\begin{pmatrix} 3\\\\0\\end{pmatrix}\\) and \\(\\begin{pmatrix} 0\\\\1\\end{pmatrix}\\). All linear combinations of these three vectors always yield a vector in \\(\\mathbb{R}^2\\), a two-dimensional vector space and not three-dimensional!\n\nNotice one of the three vectors is redundant.\n\\[\n\\mathbb{R}^2 = span\\{\\begin{pmatrix} 1\\\\2\\end{pmatrix},\\begin{pmatrix} 3\\\\0\\end{pmatrix},\\begin{pmatrix} 0\\\\3\\end{pmatrix}\\}=span\\{\\begin{pmatrix} 1\\\\2\\end{pmatrix},\\begin{pmatrix} 3\\\\0\\end{pmatrix}\\}\n\\]\n\n\n\n\n\n\nCommentary\n\n\n\nIn general a vector space \\(\\mathbb{V}\\) is a set of things with which remains invariant under linear combinations. Those things are what we call vectors. A vector is simply a member of this set. An array of numbers, like a column vector, is such an example; matrices, functions are other examples as we’ll see later.",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Vector spaces and subspaces"
    ]
  },
  {
    "objectID": "vector_spaces_and_subspaces.html#example-1",
    "href": "vector_spaces_and_subspaces.html#example-1",
    "title": "Vector spaces and subspaces",
    "section": "",
    "text": "Consider the vectors \\(\\begin{pmatrix} 1\\\\2\\end{pmatrix}\\) and \\(\\begin{pmatrix} 3\\\\0\\end{pmatrix}\\). Now multiply both by every possible scalar and then add them; the end result is a set of vectors. All these vectors constitute a two dimensional plane, which we call \\(\\mathbb{R}^2\\):\n\nFrom the picture, it is easy to convince yourself that any linear combination of two vectors of the plane \\(\\mathbb{R}^2\\) is another vector in \\(\\mathbb{R}^2\\), in particular from the l.c. \\(0\\begin{pmatrix} 1\\\\2\\end{pmatrix}+0\\begin{pmatrix} 3\\\\0\\end{pmatrix}\\) we get the origin. Hence we write:\n\\[\n\\mathbb{R}^2 = span\\{\\begin{pmatrix} 1\\\\2\\end{pmatrix},\\begin{pmatrix} 3\\\\0\\end{pmatrix}\\}\n\\tag{1}\\] Another important way to write this set is to identify the form of its elements, the form of its elements is:\n\\[\na(1,2)+b(3,0)\n\\]\nor written more compactly \\((a+3b,2a)\\); the set of this elements is\n\\[\n\\{(a+3b,2a)\\,\\,|\\,\\,a,b\\in\\mathbb{R}\\}=\\mathbb{R}^2\n\\]\nThe idea of a set of things (in this case column vectors) where any l.c. of its elements gives us again an element of the set (we say the set is closed under l.c.) is very important and will appear countless times in this course. Hence we give it a special name: vector space.\n\nDefinition 1 [Vector space] := a set of vectors with the property of being closed under linear combinations\n\n\\(\\mathbb{R}^2\\) is a vector space.\nNotice the same space comes about if we consider \\(\\begin{pmatrix} 1\\\\0\\end{pmatrix}\\) and \\(\\begin{pmatrix} 0\\\\1\\end{pmatrix}\\) as our starting vectors. More, any other pair of vectors of \\(\\mathbb{R}^2\\) that point in distinct directions, do the job. Hence we write\n\\[\n\\mathbb{R}^2 = span\\{\\begin{pmatrix} 1\\\\2\\end{pmatrix},\\begin{pmatrix} 3\\\\0\\end{pmatrix}\\} = span\\{\\begin{pmatrix} 1\\\\0\\end{pmatrix},\\begin{pmatrix} 0\\\\1\\end{pmatrix}\\}=\\dots\n\\]\nThe form of elements of \\(\\mathbb{R}^2\\) appears in many way, either \\((a+3b,2a)\\) or \\((a,b)\\), etc, depending on our choice of vectors to span.",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Vector spaces and subspaces"
    ]
  },
  {
    "objectID": "vector_spaces_and_subspaces.html#example-2",
    "href": "vector_spaces_and_subspaces.html#example-2",
    "title": "Vector spaces and subspaces",
    "section": "",
    "text": "Consider the vector \\(\\begin{pmatrix} 1\\\\2\\end{pmatrix}\\), this is our starting point. Now imagine multiplying it by every possible scalar; the end result is a set of vectors which lie along the same line:\n\nFrom the picture, it is easy to convince yourself that any linear combination of two (or more) vectors of the line gives us another vector in the line.\n\\[\n\\textbf{line} = span\\{\\begin{pmatrix}1\\\\2 \\end{pmatrix}\\}=\\{(a,2a)\\,\\,|\\,\\, a\\in\\mathbb{R}\\}\n\\tag{2}\\]\nIn particular if you multiply any of its vectors by \\(0\\), we get the origin \\(\\begin{pmatrix} 0\\\\0\\end{pmatrix}\\). Thus the line we are referring passes through the origin and we can say \\(\\mathbf{0} \\in \\textbf{line}\\).\nOnce again, the line is a set closed under l.c., which means it is a vector space.\nAdditionally, notice that the line lives inside the plane \\(\\mathbb{R}^2\\), because of that we say it is a subspace of \\(\\mathbb{R}^2\\).\n\nDefinition 2 [subspace (of a vector space \\(A\\))] := a vector space which is a subset of \\(A\\).\n(A subspace is also closed under linear combinations of its vectors.)\n\nNote that: the same line we see in the picture also results if you consider as your starting vector, the vector \\(\\begin{pmatrix} 2\\\\4\\end{pmatrix}\\), and then multiply it by all possible numbers (i.e. all l.c. of this vector). Thus this line can be created in many ways:\n\\[\n\\textbf{line} = span\\{\\begin{pmatrix}1\\\\2 \\end{pmatrix}\\} = span\\{\\begin{pmatrix}2\\\\4 \\end{pmatrix}\\}=\\dots\n\\]\n\n\n\n\n\n\nCommentary\n\n\n\n\nMultiplying a vector by a scalar is just a particular example of a linear combination:\n\n\\[ a\\begin{pmatrix}1\\\\2\\end{pmatrix}+b\\begin{pmatrix}0\\\\0\\end{pmatrix} \\]\n\nAny two vectors on the line are parallel, i.e., \\(u||v\\) if they are proportional.",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Vector spaces and subspaces"
    ]
  },
  {
    "objectID": "vector_spaces_and_subspaces.html#example-3",
    "href": "vector_spaces_and_subspaces.html#example-3",
    "title": "Vector spaces and subspaces",
    "section": "",
    "text": "Consider the vectors \\((1,2,0)^\\intercal\\) and \\((1,0,1)^\\intercal\\) and \\((0,0,1)^\\intercal\\). Linear combine them with every scalar and you obtain a set of vectors. All these vectors fill the three dimensional space \\(\\mathbb{R}^3\\).\n\nIn special notation we have:\n\\[\n\\mathbb{R}^3 = span\\{(1,2,0)^\\intercal,(1,0,1)^\\intercal,(0,0,1)^\\intercal\\}\n\\]From the picture here can identify some subspaces of \\(\\mathbb{R}^3\\), we also write the equation that describes them, take them as facts for now, later, when we introduce matrices we justify them.\n\\[\n[\\textbf{pink-blue plane}] = span\\{(1,2,0), (0,0,1)\\}=\\{(a,2a,b)\\,\\,|\\,\\,a,b\\in\\mathbb{R}\\}\n\\]\n\\[\n[\\textbf{xy plane}] = span \\{(1,0,0),(0,1,0\\}=\\{(a,b,0)\\,\\,|\\,\\,a,b\\in\\mathbb{R}\\}\n\\]\n\\[\n[\\textbf{yellow line}] = span \\{(1,0,1)\\}=\\{(a,0,a)\\,\\,|\\,\\, a\\in\\mathbb{R}\\}\n\\]\nObservation 1: The vector space \\(\\mathbb{R}^3\\) results from summing vectors from the x0y plane and z axis subspaces. Thus any vector in \\(\\mathbb{R}^3\\) can be written as a sum of a vector from the first plus one of the second. We have special notation for this:\n\nDefinition 3 Assume we have two subspaces \\(U\\) and \\(W\\) of \\(\\mathbb{R}^3\\) (eg. the xoy plane and z axis) where the only common element is the origin \\(\\mathbf{0}\\). Then any vector \\(\\mathbf{r}\\) in \\(\\mathbb{R}^3\\) can be written by some appropriate sum of \\(\\mathbf{u}\\) and \\(\\mathbf{w}\\):\n\\[\n\\mathbf{r} = \\mathbf{u}+\\mathbf{w}\n\\]\nIn math language:\n\\[\n\\mathbb{R}^3 = \\{\\mathbf{u}+\\mathbf{v}\\,\\,|\\,\\, \\mathbf{u}\\in U \\,\\,\\text{and} \\,\\,\\mathbf{v}\\in V\\}\n\\]\nThe nick-name of this set is: \\(U\\oplus V\\). The direct sum. Reading \\(\\mathbb{R}^3=U\\oplus V\\) from right to left we see what we said above; reading from left to right we see the space being decomposed in subspaces, clearly this can be done in many ways:\n\\[\n\\mathbb{R}^3 = [\\textbf{xy plane}] \\oplus [\\textbf{0z line}] = [\\textbf{yellow line}]\\oplus [\\textbf{plane perpendicular to yellow line}]=\\dots\n\\]\n\nObservation 2: Let \\(A\\) and \\(B\\) be two (non parallel) planes containing the yellow-line, then the intersection of both yields the yellow line:\n\\[\n[\\textbf{yellow-line}] = A \\cap B\n\\]\nThis is important because it tells us that interception of vectors spaces yields new vector spaces.",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Vector spaces and subspaces"
    ]
  },
  {
    "objectID": "vector_spaces_and_subspaces.html#example-4",
    "href": "vector_spaces_and_subspaces.html#example-4",
    "title": "Vector spaces and subspaces",
    "section": "",
    "text": "Consider the vectors \\(\\begin{pmatrix} 1\\\\2\\end{pmatrix}\\) , \\(\\begin{pmatrix} 3\\\\0\\end{pmatrix}\\) and \\(\\begin{pmatrix} 0\\\\1\\end{pmatrix}\\). All linear combinations of these three vectors always yield a vector in \\(\\mathbb{R}^2\\), a two-dimensional vector space and not three-dimensional!\n\nNotice one of the three vectors is redundant.\n\\[\n\\mathbb{R}^2 = span\\{\\begin{pmatrix} 1\\\\2\\end{pmatrix},\\begin{pmatrix} 3\\\\0\\end{pmatrix},\\begin{pmatrix} 0\\\\3\\end{pmatrix}\\}=span\\{\\begin{pmatrix} 1\\\\2\\end{pmatrix},\\begin{pmatrix} 3\\\\0\\end{pmatrix}\\}\n\\]\n\n\n\n\n\n\nCommentary\n\n\n\nIn general a vector space \\(\\mathbb{V}\\) is a set of things with which remains invariant under linear combinations. Those things are what we call vectors. A vector is simply a member of this set. An array of numbers, like a column vector, is such an example; matrices, functions are other examples as we’ll see later.",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Vector spaces and subspaces"
    ]
  },
  {
    "objectID": "vector_spaces_and_subspaces.html#example-3-cont",
    "href": "vector_spaces_and_subspaces.html#example-3-cont",
    "title": "Vector spaces and subspaces",
    "section": "Example 3 (cont)",
    "text": "Example 3 (cont)\nFrom the pictures above we deduced what vectors we need to span to generate that space and from this choice we identified the form of the elements. Now lets assume we are given some form of the vectors, how do we know they constitute a vector space?\nWhether they do or do not form one is a matter of checking the definition of space, recall that a space is a collection of things which remain invariant under l.c.\nTo see this concretely, consider pink-blue plane:\n\\[\n\\{(a,2a,b)^\\intercal\\,\\,|\\,\\, a,b\\in\\mathbb{R}\\}\n\\]\nIs it a vector space?\nLet \\((a_1,2a_1,b_1)\\) and \\((a_2,2a_2,b_2)\\) be two generic vectors of the set, now take a generic l.c. \\(\\alpha(a_1,2a_1,b_1)+\\beta(a_2,2a_2,b_2)\\). Is this again an element of that set? If it is, it must have the form \\((a,2a,b)\\). Lets check if this is true:\n\\[\n\\alpha(a_1,2a_1,b_1)+\\beta(a_2,2a_2,b_2) = (\\alpha a_1+\\beta a_2,2(\\alpha a_1 +\\beta a_2),\\alpha b_1+\\beta b_2) = (\\spadesuit,2\\spadesuit,\\clubsuit)\n\\]\nIt has.\nBecause, the chosen elements and the l.c. were arbitrary we can conclude that this set is closed under l.c. and therefore constitute a vector space.",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Vector spaces and subspaces"
    ]
  },
  {
    "objectID": "vector_spaces_and_subspaces.html#exercises",
    "href": "vector_spaces_and_subspaces.html#exercises",
    "title": "Vector spaces and subspaces",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1 Solve 1.1 &gt; 3: If a span of \\(3v\\), \\((2,-1)\\) and \\(1/2(-2,4)\\) what can you say?\nSolve 1.1 &gt; 4 &gt; (e) Same question.\nSolve 1.4 &gt; 1",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Vector spaces and subspaces"
    ]
  },
  {
    "objectID": "two_special_subspaces.html",
    "href": "two_special_subspaces.html",
    "title": "Two special subspaces",
    "section": "",
    "text": "From a system of equation we obtained a matrix. In this section we will see how matrices define two important subspaces: the column space and nullspace of a matrix.\n\nNullspace of a matrix (kernel of a linear function)\nThe nullspace of the matrix is the vector space generated by all solutions \\(\\mathbf{x}_N\\) of the equation:\n\\[ A\\mathbf{x}_N=\\mathbf{0} \\]\nThis equation is exactly the same as out test for independence/dependence!\nConsider again the matrix\n\\[\nA=\\begin{pmatrix}1 & 2 & 2 & 2 \\\\2 & 4 & 6 & 8 \\\\3 & 6 & 8 & 10 \\end{pmatrix}\n\\]\nSolving we find:\n\\[ \\begin{align}&\\begin{pmatrix}1 & 2 & 2 & 2 &\\bigm|0\\\\2 & 4 & 6 & 8 &\\bigm|0\\\\3 & 6 & 8 & 10 &\\bigm| 0\\end{pmatrix}\\overset{l_2' = l_2-2l_1}{\\longrightarrow}\\begin{pmatrix}1 & 2 & 2 & 2 &\\bigm| 0\\\\0 & 0 & 2 & 4 &\\bigm| 0\\\\3 & 6 & 8 & 10 &\\bigm| 0\\end{pmatrix}\\overset{l_3' = l_3-3l_1}{\\longrightarrow}\\begin{pmatrix}1 & 2 & 2 & 2 &\\bigm| 0\\\\0 & 0 & 2 & 4 &\\bigm| 0\\\\0 & 0 & 2 & 4 &\\bigm| 0\\end{pmatrix}\\\\&\\overset{l_3'=l_3-l_2}{\\longrightarrow}\\begin{pmatrix}1 & 2 & 2 & 2 &\\bigm| 0\\\\0 & 0 & 2 & 4 &\\bigm| 0\\\\0 & 0 & 0 & 0 &\\bigm| 0\\end{pmatrix}\\overset{l_2'=1/2l_2}{\\longrightarrow}\\begin{pmatrix}1 & 2 & 2 & 2 &\\bigm| 0\\\\0 & 0 & 1 & 2 &\\bigm| 0\\\\0 & 0 & 0 & 0 &\\bigm| 0\\end{pmatrix}\\overset{l_1'=l_1-2l_2}{\\longrightarrow}\\begin{pmatrix}\\mathbf{1} & 2 & 0 & -2 &\\bigm| 0\\\\0 & 0 & \\mathbf{1} & 2 &\\bigm| 0\\\\0 & 0 & 0 & 0 &\\bigm| 0\\end{pmatrix}\\end{align}  \\tag{1}\\]\nBy looking at the simplified system \\(A'\\) we see column 2 and column 4 depend on column 1 and 3. (focus on the pivots)\nImportant observation: the same column dependence found in \\(A'\\) holds for the columns of \\(A\\).\nWe can solve this problem in two other ways:\n\nway 1: In summary, use back substitution and promote the necessary variables to parameters.As an example, translate Equation 1 into the old notation set the free variables \\(y_N\\) and \\(w_N\\) as the parameters \\(a\\) and \\(b\\) respectively then solve for the dependent variables \\(x_N\\) and \\(z_N\\):\n\\[ \\begin{cases} x_N+2y_N-2w_N=0\\\\ z_N+2w_N =0\\\\ y_N=a\\\\ w_N = b \\end{cases} \\implies \\begin{cases} x_N=-2a+2b\\\\ z_N = -2b\\\\y_N=a\\\\ w_N = b \\end{cases} \\implies \\begin{pmatrix} x_N\\\\y_N\\\\z_N\\\\w_N \\end{pmatrix} = a \\begin{pmatrix} -2\\\\1\\\\0\\\\0 \\end{pmatrix} + b \\begin{pmatrix} 2\\\\0\\\\-2\\\\1 \\end{pmatrix} \\]\nway 2: To find solutions of this system, choose (any) values for the free variables \\(y_N\\) and \\(w_N\\) and then solve for \\(x_N\\) and \\(z_N\\). Why is this a good strategy? Since the \\(col_2\\) and \\(col_4\\) are dependent on \\(col_1\\) and \\(col_3\\) then \\(y_N\\times col_1\\) and \\(w_n \\times col_4\\) are also dependent, and thus we can find the appropriate \\(x_N\\) and \\(z_N\\) to cancel them.\nWhen choosing freely, at least choose something that simplify your calculations, for example set \\(y_N=1\\) and \\(w_N=0\\) and guess what is \\(x_N\\) and \\(z_N\\), we get:\n\\[ \\mathbf{x}_N =\\begin{pmatrix}-2\\\\1\\\\0\\\\0 \\end{pmatrix} \\]\nNow substitute \\(y_N=0\\) and \\(w_N=1\\) and guess the corresponding \\(x_N\\) and \\(z_N\\); the answer gives us:\n\\[ \\mathbf{x}_N=\\begin{pmatrix}2\\\\0\\\\-2\\\\1\\end{pmatrix} \\]\nNotice we did this twice because there are two free columns (dependent columns to cancel with the independent one)\nThe nullspace of the matrix \\(A\\) is composed by all linear combinations:\n\\[ a\\begin{pmatrix}-2\\\\1\\\\0\\\\0 \\end{pmatrix} +b\\begin{pmatrix}2\\\\0\\\\-2\\\\1 \\end{pmatrix} \\]\nThese two vectors constitute a basis for it.\nEither way 1 or 2 gives the same answer:\n\n\\[ a\\begin{pmatrix}-2\\\\1\\\\0\\\\0\\end{pmatrix}+b\\begin{pmatrix}2\\\\0\\\\-2\\\\1\\end{pmatrix} \\]\nThe nullspace of \\(A\\) and its basis:\n\\[ N(A) = span\\{(-2,1,0,0),(2,0,-2,1)\\} \\]\n\n\n\n\n\n\nCommentary\n\n\n\nThe way 1 and 2 are essentially the same, on 2 we choose specific values for the free variables and compute the corresponding non-free variables to get specific solution, which are then multiplied by \\(a\\) and \\(b\\); on way 1 we choose arbitrary values for the free variables and then solve for the non-free.\n\n\n\n\nColumns Space of a Matrix (image of a linear function)\nThe column space is the vector space that is generated by taking all linear combinations of the columns of a matrix. For example, the column space \\(C(A)\\) of the matrix:\n\\[\nA=\\begin{pmatrix}1 & 2 & 2 & 2 \\\\2 & 4 & 6 & 8 \\\\3 & 6 & 8 & 10 \\end{pmatrix}\n\\tag{2}\\]\nis generated by\n\\[\nC(A)=span\\{\\begin{pmatrix}\n1\\\\2\\\\3\n\\end{pmatrix}\n,\n\\begin{pmatrix}2\\\\4\\\\6\\end{pmatrix}\n,\n\\begin{pmatrix}2\\\\6\\\\8\\end{pmatrix}\n,\n\\begin{pmatrix}2\\\\8\\\\10\\end{pmatrix}\\}\n\\]\nIt may happen, and it is the case as we shall see, that we need not all the columns of \\(A\\) to generate \\(C(A)\\), the reason being, some of the column vectors may depend on other columns, thus not providing no additional information. In other words, the columns of \\(A\\) may or not constitute a basis for \\(C(A)\\). Thus we also do not know, yet, the dimensionality of this space.\nTo construct a basis for \\(C(A)\\) we have to identify which column or columns are dependent and which are independent, we found by inspecting the \\(\\text{rref} A\\) in Equation 1, we know the column 1 and 3 are independent, hence the basis for the column space:\n\\[\nC(A)=span\\{\n\\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix},\\begin{pmatrix}2\\\\6\\\\8\\end{pmatrix}\\}\n\\]\n\n\n\n\n\n\nCommentary\n\n\n\nThe column space of \\(A\\) is different from that of \\(\\text{rref}\\,A\\).\n\n\n\n\nConclusion\n\nThe nullspace is the set of solutions of \\(A\\mathbf{x}_N=\\mathbf{0}\\), which we compute by simplifying the matrix \\(A\\) to the point \\(A'\\) (=rref of \\(A\\)) where we can see the dependent columns.\nBy knowing in \\(A'\\) which columns are dependent, we know which ones are independent, the corresponding columns in \\(A\\) in turn span the column space.\n\n\n\n\n\n\n\nCommentary\n\n\n\n\n\\(\\dim C(A) +\\dim N(A) = m\\), where \\(m\\) is the number of columns. It will be better intuitively to write \\(\\dim C(A^\\intercal) +\\dim N(A) = m\\) as we shall see later.\n\n\nExercises: 1.5.11,12",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Column space and nullspace"
    ]
  },
  {
    "objectID": "systems_from_a_new_view.html",
    "href": "systems_from_a_new_view.html",
    "title": "Lines, planes and hyperplanes",
    "section": "",
    "text": "We now to return to the observations:\nGoal: To understand the reason behind these inequalities. In doing so we will simultaneously understand how \\(r=r^*&lt;n\\) is related with lines, planes and hyperplanes.\nHow? we’ll look carefully at the structure of the computations involved in the equation \\(A\\mathbf{x}=\\mathbf{b}\\) (or \\(A'\\mathbf{x}=\\mathbf{b}'\\), \\(A'=\\text{rref}\\, A\\))",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "System from a new point of view"
    ]
  },
  {
    "objectID": "systems_from_a_new_view.html#looking-at-amathbfxmathbfb-from-a-new-point-of-view",
    "href": "systems_from_a_new_view.html#looking-at-amathbfxmathbfb-from-a-new-point-of-view",
    "title": "Lines, planes and hyperplanes",
    "section": "Looking at \\(A\\mathbf{x}=\\mathbf{b}\\) from a new point of view",
    "text": "Looking at \\(A\\mathbf{x}=\\mathbf{b}\\) from a new point of view\nReturn to the equations:\n\\[\nA\\mathbf{x}=\\mathbf{b}\n\\leftrightsquigarrow\n\\begin{pmatrix}1 & 2 & 2 & 2 \\\\2 & 4 & 6 & 8 \\\\3 & 6 & 8 & 10 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\\\z\\\\w\\end{pmatrix}=\\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix}\n\\leftrightsquigarrow\n\\begin{pmatrix}1 & 2 & 2 & 2 &\\bigm|1\\\\2 & 4 & 6 & 8 &\\bigm|2\\\\3 & 6 & 8 & 10 &\\bigm| 3\\end{pmatrix}\n\\tag{1}\\]\nLooking at Equation 1 we would ask: what is the column vector \\(\\mathbf{x}\\) which when multiplied by the matrix \\(A\\) yields the column vector \\(\\mathbf{b}\\)?\nNow rewrite the system of equation in a different point of view:\n\\[\n\\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix}x+\\begin{pmatrix}2\\\\4\\\\6\\end{pmatrix}y+\\begin{pmatrix}2\\\\6\\\\8\\end{pmatrix}z+\\begin{pmatrix}2\\\\8\\\\10\\end{pmatrix}w = \\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix}\n\\tag{2}\\]\nLooking at Equation 2 we would ask: what is the linear combination of the columns that leads to the vector \\(\\mathbf{b}\\) on the rhs? [Not every columns is independent!]\nThis second perspective on a system of equations allow us to make the following:\n\nKey observation: this linear combination is only possible, provided the \\(\\mathbf{b}\\) vector is in the column space of \\(A\\)! Meaning \\(\\mathbf{b}\\in C(A)\\).\nKey observation paraphrased differently: going back to Equation 1, focus on the extended matrix version of the system of the equations and note, that, when we extend the matrix, i.e., when we append the \\(\\mathbf{b}\\) vector on the rhs, the column space must not change, for otherwise the \\(\\mathbf{b}\\) was not in the column space. This is to say:\n\n\\[\nr =r^*\n\\]\nThis same key observation can be obtained from \\(A'\\mathbf{x}=\\mathbf{b}'\\):\n\n\\[\n\\begin{align}\n\\begin{pmatrix}\n1 & 2 & 0 & -2 &\\bigm| 1\\\\\n0 & 0 & 1 & 2 &\\bigm| 0\\\\\n0 & 0 & 0 & 0 &\\bigm| 0\n\\end{pmatrix}\n\\end{align}\n\\tag{3}\\]\nIf \\(\\mathbf{b}'\\in C(A')\\), that is to say when \\(r=r^*\\), then there is a solution. Thus if \\(\\mathbf{b}\\in C(A)\\) then \\(\\mathbf{b}'\\in C(A')\\). Note how much easier is to check the later.\nHow would we find \\(x,y,z,w\\) that satisfy the equations above?\nWay 1: Use back substitution. And promoting the necessary variables to parameters.\nWay 2: Follow these three step procedure:\n\nFirst, find one particular solution \\(\\mathbf{x}_P\\) for the system.\nSecond, find the \\(N(A)\\) by solving \\(A\\mathbf{x}_N=\\mathbf{0}\\).\nThird, The complete solution is of the form \\(\\mathbf{x}_P+\\mathbf{x}_N\\).",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "System from a new point of view"
    ]
  },
  {
    "objectID": "systems_from_a_new_view.html#first-how-to-compute-mathbfx_p",
    "href": "systems_from_a_new_view.html#first-how-to-compute-mathbfx_p",
    "title": "Lines, planes and hyperplanes",
    "section": "First, how to compute \\(\\mathbf{x}_P\\)?",
    "text": "First, how to compute \\(\\mathbf{x}_P\\)?\n\nSimplify the system as much as you can using Elimination and the pivots, we already did this and obtained Equation 3.\nIdentify the dependent and independent columns of \\(A'\\). We do this by visual inspection of the simplified system: The independent column are where the pivots lie. The dependent columns (also known as free columns) are the remaining ones.\nThe unknowns multiplying the dependent columns in Equation 3 are \\(y\\) and \\(w\\); these are known as free unknowns.\nTo find a particular solution \\(\\mathbf{x}_P\\) of the system, set the free unknowns to zero:\n\\[\ny=0\\qquad w=0\n\\]\n\\[\n\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}x_P\n+\\begin{pmatrix}2\\\\0\\\\0\\end{pmatrix}0\n+\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}z_P\n+\\begin{pmatrix}-2\\\\2\\\\0\\end{pmatrix}0 = \\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}\n\\tag{4}\\]\nNow solve Equation 4 for \\(x_P\\) and \\(z_P\\); by inspection:\n\\[\nx_P=1 \\qquad z_P=0\n\\]\nThe particular solution is\n\\[ \\mathbf{x}_P=\\begin{pmatrix} 1\\\\0\\\\0\\\\0 \\end{pmatrix} \\]",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "System from a new point of view"
    ]
  },
  {
    "objectID": "systems_from_a_new_view.html#second-how-to-compute-mathbfx_n",
    "href": "systems_from_a_new_view.html#second-how-to-compute-mathbfx_n",
    "title": "Lines, planes and hyperplanes",
    "section": "Second, how to compute \\(\\mathbf{x}_N\\)?",
    "text": "Second, how to compute \\(\\mathbf{x}_N\\)?\nCompute the nullspace of \\(A\\) by solving \\(A\\mathbf{x}_N=\\mathbf{0}\\), after simplification (through elimination) we arrive at \\(A'\\mathbf{x}_N=\\mathbf{0}\\), i.e.\n\\[\n\\begin{pmatrix}1 & 2 & 0 & -2 &\\bigm| 0\\\\0 & 0 & 1 & 2 &\\bigm| 0\\\\0 & 0 & 0 & 0 &\\bigm| 0\\end{pmatrix}\n\\tag{5}\\]\nSolving (done previously) we obtain the nullspace of the matrix \\(A\\) which is composed by all linear combinations:\n\\[\na\\begin{pmatrix}-2\\\\1\\\\0\\\\0 \\end{pmatrix} +b\\begin{pmatrix}2\\\\0\\\\-2\\\\1 \\end{pmatrix}\n\\]\nGeometrically: this is plane containing the origin living in four-dimension space. This is a vector space.",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "System from a new point of view"
    ]
  },
  {
    "objectID": "systems_from_a_new_view.html#third-the-complete-solution-of-amathbfxmathbfb",
    "href": "systems_from_a_new_view.html#third-the-complete-solution-of-amathbfxmathbfb",
    "title": "Lines, planes and hyperplanes",
    "section": "Third, the complete solution of \\(A\\mathbf{x}=\\mathbf{b}\\)",
    "text": "Third, the complete solution of \\(A\\mathbf{x}=\\mathbf{b}\\)\nSo far we know a particular solution \\(A\\mathbf{x}_P=\\mathbf{b}\\) and we know the nullspace \\(A\\mathbf{x}_N=\\mathbf{0}\\).\nThus the solution of \\(A\\mathbf{x}=\\mathbf{b}\\) are of the form:\n\\[\n\\begin{pmatrix} 1\\\\0\\\\0\\\\0 \\end{pmatrix} +a\\begin{pmatrix}-2\\\\1\\\\0\\\\0 \\end{pmatrix} +b\\begin{pmatrix}2\\\\0\\\\-2\\\\1 \\end{pmatrix}\n\\tag{6}\\]\nand the solution set is:\n\\[\n\\begin{pmatrix} 1\\\\0\\\\0\\\\0 \\end{pmatrix} +span\\{\\begin{pmatrix}-2\\\\1\\\\0\\\\0 \\end{pmatrix} ,\\begin{pmatrix}2\\\\0\\\\-2\\\\1 \\end{pmatrix}\\}\n\\]\nGeometrically: this is a plane in four-dimension space that does not contain the origin, thus it is not a vector space.\nWhy is this the solution?\nIf \\(\\mathbf{x}_P\\) is a particular solution of \\(A\\mathbf{x}=\\mathbf{b}\\) then adding to it any \\(\\mathbf{x}_N\\) will not make a difference.\nJustification: \\(A(\\mathbf{x}_P+\\mathbf{x}_N)=A\\mathbf{x}_P+A\\mathbf{x}_N=\\mathbf{b}+\\mathbf{0}=\\mathbf{b}\\)\nThus, the system \\(A\\mathbf{x} =\\mathbf{b}\\) has an infinite number of solutions, all of the form \\(\\mathbf{x}=\\mathbf{x}_P+\\mathbf{x}_N\\).",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "System from a new point of view"
    ]
  },
  {
    "objectID": "systems_from_a_new_view.html#a-system-with-one-solution",
    "href": "systems_from_a_new_view.html#a-system-with-one-solution",
    "title": "Lines, planes and hyperplanes",
    "section": "A system with one solution",
    "text": "A system with one solution\nUsing elimination we showed:\n\\[\n\\begin{pmatrix}1 & 1 & -1 &\\bigm| & 1\\\\2 & -1 & 2 &\\bigm| & 9\\\\1 & 2 & -1 &\\bigm| & 0\\end{pmatrix} \\iff \\begin{pmatrix}1 & 0 & 0 &\\bigm| & 2\\\\0 & 1 & 0 &\\bigm| & -1\\\\0 & 0 & 1 &\\bigm| & 1\\end{pmatrix}\n\\]\nClearly there are no free column and thus no free coefficients to set to zero. The matrix has the form:\n\\[\n\\begin{pmatrix}I\\end{pmatrix}\n\\]Solving for the remaining we get:\n\\[\n\\mathbf{x}_P=\\begin{pmatrix}2\\\\-1\\\\1\\end{pmatrix}\n\\]\nThe nullspace only contains the \\(\\mathbf{0}\\) vector. There is no \\(F\\) block, since \\(n-r=0\\).\nThe solution of the system is just the particular solution.\n\\[\n\\mathbf{x}=\\mathbf{x}_P+\\mathbf{x}_N=\\begin{pmatrix}2\\\\-1\\\\1\\end{pmatrix}+\\begin{pmatrix}0\\\\0\\\\0\\end{pmatrix}=\\begin{pmatrix}2\\\\-1\\\\1\\end{pmatrix}\n\\]",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "System from a new point of view"
    ]
  },
  {
    "objectID": "systems_from_a_new_view.html#a-system-with-no-solution",
    "href": "systems_from_a_new_view.html#a-system-with-no-solution",
    "title": "Lines, planes and hyperplanes",
    "section": "A system with no solution",
    "text": "A system with no solution\nPreviously we computed:\n\\[\n\\begin{pmatrix}2 & -1 &\\bigm| & 8\\\\2 & 1  &\\bigm| & 4\\\\1 & 1  &\\bigm| & -1\\end{pmatrix}\\iff \\begin{pmatrix}1 & 0 &\\bigm| & 5\\\\0 & 1 &\\bigm| & -6\\\\0 & 0 &\\bigm| & -8\\end{pmatrix}\n\\]\nThere is no particular solution because \\(\\mathbf{b}'\\not\\in C(A)\\), meaning \\(r&lt;r^*\\). The null space is \\(\\{\\mathbf{0}\\}\\). The system as has no solution",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "System from a new point of view"
    ]
  },
  {
    "objectID": "sol_to_exer_withperequisites.html",
    "href": "sol_to_exer_withperequisites.html",
    "title": "solution",
    "section": "",
    "text": "\\(A(x)\\coloneqq [\\text{$x$ is a vowel}]\\) makes it true; \\(A(x)\\coloneqq [\\text{$x$ is a consonant}]\\) makes it false.\n\\(A(x)\\coloneqq [\\text{$x$ is a consonat}]\\) is a solution of \\(\\exists x : A(x)\\). but \\(A(x)\\coloneqq [\\text{$x$ is $v$}]\\) is not.\n\\([\\text{$x$ is below $i$}]\\) for true and \\([\\text{$x$ is above $a$}]\\) for false.\nFor the second part of the exercise we find:\n\\[\n\\begin{equation}\\begin{split}&\\forall x \\in \\{a,b,c\\}:\\exists ! y\\in \\{x,y\\}\\in F\\\\&\\iff\\\\&\\left(((a,1)\\in F)\\dot{\\lor}((a,2)\\in F)\\right) \\land \\left(((b,1)\\in F)\\dot{\\lor}((a,2)\\in F)\\right)\\land\\left(((c,1)\\in F)\\dot{\\lor}((c,2)\\in F)\\right)\\end{split}\\end{equation}\n\\]\nThe true values are: (1) True, (2) True, (3) False, (4) True"
  },
  {
    "objectID": "qu_131.html",
    "href": "qu_131.html",
    "title": "asymptotic to MARS",
    "section": "",
    "text": "Exercise 1  \n\nWith \\(X\\coloneqq \\{\\alpha,\\beta,\\gamma,\\delta\\}\\) and \\(Y\\coloneqq \\{1,2,3,4\\}\\) create relations, two that are not functions and two that are. Make sure you create function, which use all \\(X\\) and others that do not.\n\n\n\nSolution 1. The main point of this exercise is to show that \\(X\\) and \\(Y\\) are the starting assumptions from which many relations stem, some are functions while others are not."
  },
  {
    "objectID": "pratical_guide_cm.html",
    "href": "pratical_guide_cm.html",
    "title": "A Pratical Guide to Condensed Matter Physics",
    "section": "",
    "text": "Einstein Model"
  },
  {
    "objectID": "pratical_guide_cm.html#topics",
    "href": "pratical_guide_cm.html#topics",
    "title": "A Pratical Guide to Condensed Matter Physics",
    "section": "",
    "text": "Einstein Model"
  },
  {
    "objectID": "pol_division.html",
    "href": "pol_division.html",
    "title": "Polynomial division",
    "section": "",
    "text": "Look at the two divisions below:\n\\[\n\\begin{equation}\n\\frac{x^3+1}{x+2}\\qquad \\text{vs}\\qquad \\frac{1001}{12}\n\\end{equation}\n\\tag{1}\\]\nWhat do they have in common?\nIn this notes we see 9 important ideas behind division of polynomials and integers. We’ll start by integers since its easier.",
    "crumbs": [
      "Bried Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Polynomial division"
    ]
  },
  {
    "objectID": "pol_division.html#analogy-between-polynomial-notation-and-integer-notation",
    "href": "pol_division.html#analogy-between-polynomial-notation-and-integer-notation",
    "title": "Polynomial division",
    "section": "1) Analogy between polynomial notation and integer notation",
    "text": "1) Analogy between polynomial notation and integer notation\nMany of you may have forgotten, but the position of watch integer in the symbol \\(1001\\) has a meaning, specifically:\n\\[\n1001 = 1\\cdot 10 ^3+0\\cdot10^2+0\\cdot10^1+1\\cdot10^0\n\\]\nThe left hand side is just a super compact way of writing the right side.\nSimilarly, \\(12\\) is just a symbol for \\(1\\cdot 10^1+2\\cdot 10^0\\).\nWith this reminder we can see clearly the connection between the numerators and denominators in Equation 1 .\n\\[\n\\begin{cases}\nx^3+1 = 1\\cdot x^3+0\\cdot x^2 + 0 \\cdot x^1 + 1\\cdot x^0\\\\\n1001 = 1\\cdot 10 ^3+0\\cdot10^2+0\\cdot10^1+1\\cdot10^0\n\\end{cases}\n\\]\nand\n\\[\n\\begin{cases}\nx+2 = 1 \\cdot x^1 + 2\\cdot x^0\\\\\n12 = 1\\cdot10^1+2\\cdot10^0\n\\end{cases}\n\\]\nThe connection is: powers of \\(10\\) are replaced by powers of \\(x\\).\nWith such a bridge, we may expect the practical realization of this calculation will also be similar. You could have done polynomial division on the 4th grade.",
    "crumbs": [
      "Bried Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Polynomial division"
    ]
  },
  {
    "objectID": "pol_division.html#to-divide-two-integers-is-to-not-only-find-the-quotient-but-also-find-the-remainder",
    "href": "pol_division.html#to-divide-two-integers-is-to-not-only-find-the-quotient-but-also-find-the-remainder",
    "title": "Polynomial division",
    "section": "2) To divide two integers is to NOT ONLY find the quotient but also find the remainder!!",
    "text": "2) To divide two integers is to NOT ONLY find the quotient but also find the remainder!!\nLets see this with a picture.\n\nImagine a chocolate bar of length \\(p=22\\) with grooves at each unit. You want to divide it, cutting only at the grooves, by your \\(d=3\\) friends. You don’t want to cheat anyone (do you?), so you must give pieces of the same length and, of course, also give them the maximum possible piece-length.\nYou might immediately think to measure the chocolate bar length, pull a calculator and compute the division\n\\[\n\\frac{22}{3}\\approx 7.3\n\\]\nThen measure and mark the cuts at \\(7.3\\) and \\(14.6\\); then cut the thing and give then. See picture above.\nWith minimal error in the calculation, because the division is only approximately \\(7.3\\) and error in the actual measurements, you succeeded. Congratulations….but this is not what was being asked!\nI remind you: you can only cut at the grooves that already came with the chocolate, they are spaced by \\(1\\)unit.\nIn this case a question naturally arises, what is the maximum piece-length \\(q\\), you can give your friends? Certainly it is not the real number \\(7.3\\), the length \\(q\\) must be an integer.\nThe following picture answers the problem (think about it before looking at it)\n\nThe unique solution is to cut at \\(7\\) and \\(14\\) and \\(21\\) units. Give your \\(d=3\\) friends chocolate pieces of length \\(q=7\\); a small one of length \\(1\\) remains, you can eat that as a prize.\nThe takeaway is that when you divide two things and the result you desire must be an integer thing, then a remaining thing is left (for you).\nI would like now to make things more complicated and write the statement above in proper math language, its a theorem:\n\n\n\n\n\n\nTheorem\n\n\n\nLet \\(p\\) and \\(d\\) be two integers where \\(p\\geq d\\). Then there exists an integer \\(r\\) such that \\(0\\leq r&lt;d\\) and an integer \\(q\\geq 0\\) such that\n\\[ p=qd+r \\]\n\n\nLets break down the meaning of this theorem. We start by requiring that \\(p\\) and \\(d\\) be integers (a chocolate bar has an integer number of pieces and you have an integer number of friends - right?) and thus positive, we also require that \\(p\\) is larger or equal to \\(d\\).\nIf we imagine \\(p\\) as the length of a line segment (our chocolate bar), this length is the sum of two length\n\\[\np=qd + r\n\\]\nThe \\(qd\\) length, is the length of each piece you give your friends times the number of friends, thus \\(qd\\) is the part of the chocolate bar you give away.\nThe \\(r\\) length, is the length of the remaining piece, you keep that!\nSumming both, \\(qd +r\\) yields the original chocolate bar length \\(p\\).\n\nTo divide \\(p\\) by \\(d\\) subjected to the fact we want an integer result, means to find the integer result \\(q\\) and then the integer remainder \\(r\\).\nIn practice, when we seek for the value of \\(q\\), we seek for the highest possible \\(q\\), that way ensures the smallest length \\(r\\). By smallest I mean, its length \\(r\\) is not divisible by \\(d\\) - Please STOP here and ensure you understand this!\nWe can guarantee that \\(r\\) is not divisible by \\(d\\) by stating that \\(r\\) must be smaller than \\(d\\), thus the requirement \\(0\\leq r&lt;d\\) in the theorem above.",
    "crumbs": [
      "Bried Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Polynomial division"
    ]
  },
  {
    "objectID": "pol_division.html#the-key-strategy-behind-computing-the-division-of-integers-and-an-example",
    "href": "pol_division.html#the-key-strategy-behind-computing-the-division-of-integers-and-an-example",
    "title": "Polynomial division",
    "section": "3) The key strategy behind computing the division of integers and an example",
    "text": "3) The key strategy behind computing the division of integers and an example\nTo compute the division of the integer \\(1001\\) by \\(12\\) under the constrain that the result is also an integer is to compute the values of \\(q\\) and \\(r\\) that obey the following equation:\n\\[ 1001 = q \\times 12 + r \\qquad 0\\leq r&lt;12 \\]\nHow do we proceed in finding these numbers? The key strategy is very simple: guess them!\nIts not a random guess, this is not a lottery, its an educated guess; educated by clues provided by the problem itself.\nSee the following line of reasoning (and guesses) to get the idea:\n\nStep 1: decompose \\(1001\\) and \\(12\\) into powers of \\(10\\). This a clue that will to aid (educate) our guesses!\n\n\\[\n\\begin{cases}\n1001 = 1\\cdot 10^3+0\\cdot 10^2 +0\\cdot 10^1+1\\cdot 10^0 = 1\\cdot 10^3 +1\\cdot 10^0\\\\\n12 = 1\\cdot 10^1 + 2\\cdot 10^0\n\\end{cases}\n\\]\n\nStep 2: Observe both sides of the equation:\n\\[ 1\\cdot 10^3 +1 = q\\cdot(10+2)+r  \\tag{2}\\]\nIt is very useful to identify what are the largest numbers on both sides of the equation: the largest value on the left side is \\(1\\cdot 10^3\\), the largest value on the right side is \\(q\\cdot 10\\). Hence we expect that whatever \\(q\\) is, it must, when multiplied by \\(10\\), to give a number with is close or equal to \\(1\\cdot 10^3\\), for otherwise, balancing the sides of the equation is impossible. We should balance first these two terms, then we deal with the smaller ones.\nOur guess of \\(q\\) must be guided by this observation.\nStep 3: Guess a value of \\(q\\) which makes large terms similar or equal, i.e., \\(q\\cdot 10\\) close to \\(1\\cdot 10^3\\). There are many possible guesses and all of them would work! Here is one possibility \\(q=10^2\\).\nStep 4: Now that we have a value for \\(q\\) lets see how large is the remainder.\nSubstituting \\(q=10^2\\) into Equation 2 gives:\n\\[ \\begin{align} &1\\cdot 10^3 +1 = 1\\cdot 10^3 +2 \\cdot 10^2 + r\\\\ \\implies&r=-2\\cdot10^2+1 \\end{align} \\]\nThe remainder is negative and this presents no problem, what we really want is the final remainder to be positive. The remainder is also larger (in absolute value) than the divisor \\(12\\). This means the remainder is still divisible by \\(12\\), hence we proceed into computing the division of \\(-2\\cdot 10 +1\\) by \\(12\\), this is what we’ll do in step 5. To end step 4 let us summarize in a single expression what we know so far:\n\\[\n1\\cdot 10^3+1=\\overbrace{10^2}^q\\cdot(10+2)+\\overbrace{(-2\\cdot 10^2+1)}^r\n\\tag{3}\\]\nStep 5: Write the division of the remainder \\(-2\\cdot 10+1\\) by \\(12\\):\n\\[\n-2\\cdot 10^2+1 = q'\\cdot(10+2)+r'\n\\]\nand analyse the equation.\nOur goal now is to guess \\(q'\\) that makes \\(r'\\) as small as possible, we achieve that by balancing the largest terms on both sides of the equation, in this case we want to guess a \\(q'\\) that when multiplied by \\(10\\) is close to \\(-2\\cdot 10^2\\), the largest number on the left.\nStep 6: Guess \\(q'=-2\\cdot 10\\) and then substitute into the equation:\n\\[ \\begin{align} &-2\\cdot 10^2+1 = -2\\cdot 10^2 -4\\cdot 10 + r'\\\\ \\implies &r'=4\\cdot 10+1 \\end{align} \\]\nThe remainder is \\(4\\cdot 10+1\\) and is still larger than \\(12\\), thus divisible by \\(12\\), we’ll do this calculation in step 7. Let’s end this step by summarizing what we know so far:\n\\[\n\\begin{cases}\n1\\cdot 10^3+1=\\overbrace{10^2}^q\\cdot(10+2)+\\overbrace{(-2\\cdot 10^2+1)}^r\\\\\n-2\\cdot10^2+1 =\\overbrace{(-2\\cdot10)}^{q'}\\cdot(10+2)+\\overbrace{(4\\cdot10+1)}^{r'}\n\\end{cases}\n\\]\nStep 7: We repeat the process. Write the equation for the division of the current remainder \\(4\\cdot 10+1\\) by \\(12\\), analyse it and guess the quotient \\(q''\\) that makes \\(r''\\) as small as possible:\n\\[\n4\\cdot 10 +1 = q''\\cdot (10+2)+r''\n\\tag{4}\\]\nGuessing \\(q''=4\\) we balance the largest term on the lhs. The corresponding remainder is:\n\\[\nr'' = -7\n\\]\nSince the remainder is smaller than \\(12\\) the guessing process ends here.\nOnce again we summarize everything we know so far:\n\\[\n\\begin{cases}\n1\\cdot 10^3+1=\\overbrace{10^2}^q\\cdot(10+2)+\\overbrace{(-2\\cdot 10^2+1)}^r\\\\\n-2\\cdot10^2+1 =\\overbrace{(-2\\cdot10)}^{q'}\\cdot(10+2)+\\overbrace{(4\\cdot10+1)}^{r'}\\\\\n4\\cdot 10+1 = \\overbrace{4}^{q''}\\cdot(10+2)+\\overbrace{(-7)}^{r''}\n\\end{cases}\n\\tag{5}\\]\nStep 7: Use Equation 5 to write\n\\[\n1\\cdot 10^3 +1 = (10^2-2\\cdot 10+4)\\cdot 12 -7\n\\]\nwhich simplifies into:\n\\[\n1001=84\\cdot 12 -7\n\\tag{6}\\]\nStep 8: Our final remainder, the \\(-7\\) in Equation 6 is negative, to make it positive we resort to a trick: add \\(0=12-12\\) to the rhs:\n\\[ 1001=(84-1)\\cdot 12+(12-7) \\]\nIn conclusion:\n\\[ 1001=83\\cdot 12 +5 \\]\nThe division is complete, our quotient is \\(q=83\\) and the remainder is the positive number \\(5\\).\n\n\n\n\n\n\n\nComment\n\n\n\nThe division of \\(1001\\) by \\(12\\) can be done in many ways, all ending in the result \\(1001=83\\cdot 12 +5\\). What distinguishes the different ways are the guesses of \\(q\\), \\(q''\\), etc one makes. Among all this approaches we find one very well known by you, the long division of integers, which you learned in elementary school.\nThe particular sequence of steps we followed above was chosen so that it can be compared with the sequence of step in dividing \\(x^3+1\\) by \\(x+2\\).",
    "crumbs": [
      "Bried Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Polynomial division"
    ]
  },
  {
    "objectID": "pol_division.html#to-divide-two-polynomials-is-to-not-only-find-the-quotient-but-also-find-the-remainder",
    "href": "pol_division.html#to-divide-two-polynomials-is-to-not-only-find-the-quotient-but-also-find-the-remainder",
    "title": "Polynomial division",
    "section": "4) To divide two polynomials is to NOT ONLY find the quotient but also find the remainder!!",
    "text": "4) To divide two polynomials is to NOT ONLY find the quotient but also find the remainder!!\nThe division of polynomials is analogous to the division of integers, the division of \\(1001/12\\) made us to seek the \\(q\\) and \\(r\\) that satisfy the equation \\(1001 = q\\times 12 +r\\), while having the \\(r\\) as small as possible, which means \\(0\\leq r &lt;12\\), so that the remainder is not divisible by \\(12\\).\nThe division of polynomials, for example \\((x^3+1)/(x+2)\\) , requires to find the function \\(q(x)\\) and \\(r(x)\\) that obey\n\\[ x^3+1=q(x)\\times(x+2)+r(x) \\qquad 0\\leq\\deg r &lt; 1   \\tag{7}\\]\nThe constraint \\(0\\leq\\deg r&lt;1\\) ensures the degree of the remainder is the smallest, meaning in this case, not divisible by \\(x+2\\) whose degree is \\(1\\).\nThe following theorem summarizes the problem to solve:\n\n\n\n\n\n\nTheorem\n\n\n\nLet \\(f\\) and \\(g\\) be non-zero polynomials where \\(\\deg f \\geq \\deg g\\) (this assumption is analogous to saying \\(1001&gt;12\\)). Then there exist polynomials \\(q\\), \\(r\\) such that:\n\\[ f(x)=q(x)g(x)+r(x) \\qquad 0\\leq\\deg r &lt; \\deg g \\quad\\text{(Analogous to $0\\leq r &lt; 12$)} \\]\n\n\nThe ideas and strategies presented above for division of integers can be employed to the division of polynomials.",
    "crumbs": [
      "Bried Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Polynomial division"
    ]
  },
  {
    "objectID": "pol_division.html#the-key-strategy-behind-computing-the-division-of-polynomials-and-an-example",
    "href": "pol_division.html#the-key-strategy-behind-computing-the-division-of-polynomials-and-an-example",
    "title": "Polynomial division",
    "section": "5) The key strategy behind computing the division of polynomials and an example",
    "text": "5) The key strategy behind computing the division of polynomials and an example\nAs usual we guess what \\(q(x)\\) might be and then compute the corresponding remainder \\(r(x)\\). By checking the constraint \\(0\\leq \\deg r&lt;1\\) at each stage of the guessing process, we decide whether to terminate it or keep going. Let us see this in practice:\n\nStep 1: Observe both sides of the equation:\n\\[ x^3+1=q(x)\\times(x+2)+r(x) \\qquad 0\\leq\\deg r &lt; 1   \\tag{8}\\]\nand identify the largest powers: on the lhs we have \\(x^3\\) and on the rhs we have \\(q(x) \\cdot x\\). This is useful information, because we want to match the largest powers on either side of the equation. This is the only way the equal sign will hold.\nStep 2: We want to guess what \\(q(x)\\) based on this matching observation we did at step 1, hence \\(q(x)=x^2\\) :\nSubstituting into Equation 8, we compute the remainder of the current guess:\n\\[ \\begin{align} &x^3+1=x^2\\times(x+2)+r(x)\\\\ \\implies &r(x) = -2x^2+1 \\end{align} \\]\nIs the constraint that the remainder is as “small” as possible, \\(0\\leq \\deg r &lt;1\\), satisfied by this remainder? We clearly see from the \\(x^2\\) power that its degree is \\(2\\), the constraint is not satisfied and that means this remainder is divisible by \\(x+2\\).\nAs we did for the integer division we summarize our calculations:\n\\[\nx^3+1 = x^2\\cdot(x+2)+\\overbrace{(-2x^2+1)}^r\n\\]\n\n\n\nStep 3: The division \\((-2x^2+1)/(x+2)\\) is done by guessing the quotient \\(q'(x)\\) and the remainder \\(r'(x)\\):\n\\[ -2x^2+1 = q'(x)(x+2)+r'(x)  \\tag{9}\\]\nWe proceed as we always did above and analyze the equation before attempting any guess. The highest degree terms in either side of the equation are \\(-2x^2\\) and \\(q'(x)\\cdot x\\), we must match these, this will guide our guess.\nStep 4: To match the highest powers of the equation, set \\(q'(x)=-2x\\)\nSubstituting into Equation 9 we compute the remainder:\n\\[ \\begin{align} &-2x^2 +1 =-2x(x+2)+r'(x)\\\\ \\implies &r'(x) = 4x+1 \\end{align} \\]\nAgain, the degree of the current remainder still does not obey \\(0\\leq \\deg r' &lt;1\\), thus the remainder is divisible by \\(x+2\\). Before doing this calculation we summarize calculations:\n\\[\n\\begin{cases}\nx^3+1 = x^2\\cdot(x+2)+\\overbrace{(-2x^2+1)}^r\\\\\n-2x^2+1 = -2x(x+2)+\\overbrace{(4x+1)}^{r'}\n\\end{cases}\n\\]\nStep 5: Division of \\(4x+1\\) by \\(x+2\\) means to solve the equation:\n\\[ 4x+1 = q''(x)(x+2)+r''(x)  \\tag{10}\\]\nWe should match \\(q''(x)\\cdot x\\) with \\(4x\\) - the higher power term on the lhs.\nStep 6: Guess \\(q''(x)=4\\) and substitute into Equation 10 to get:\n\\[ \\begin{align} &4x+1 = 4(x+2)+r''(x)\\\\ \\implies & r''(x)=-7 \\end{align} \\]\nFinally we arrive at \\(\\deg(-7)=0\\) as we needed by the condition \\(0\\leq\\deg r &lt; 1\\), the process of division end here.\nCollecting results we have:\n\\[\n\\begin{cases}\nx^3+1 = x^2\\cdot(x+2)+\\overbrace{(-2x^2+1)}^r\\\\\n-2x^2+1 = -2x(x+2)+\\overbrace{(4x+1)}^{r'}\\\\\n4x+1 = 4(x+2)+\\overbrace{(-7)}^{r''}\n\\end{cases}\n\\tag{11}\\]\nStep 7: From Equation 11 we arrive at the result of our division:\n\\[ \\begin{align} x^3+1&=\\overbrace{x^2}^{q(x)}\\times(x+2)+\\overbrace{(-2x^2+1)}^{r(x)}\\\\ &=\\overbrace{x^2}^{q(x)}\\times (x+2) + \\overbrace{(-2x)}^{q'(x)}(x+2) +\\overbrace{(4x+1)}^{r'(x)}\\\\ &=\\overbrace{x^2}^{q(x)}\\times (x+2) + \\overbrace{(-2x)}^{q'(x)}(x+2)+\\overbrace{(+4)}^{q''(x)}(x+2) +\\overbrace{(-7)}^{r''(x)}\\\\ &=(x^2-2x+4)(x+2)-7 \\end{align} \\]\n\nWe conclude that:\n\\[ x^3+1 = (x^2-2x+4)(x+2)-7 \\]\nCompare this result with Equation 6 !!!",
    "crumbs": [
      "Bried Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Polynomial division"
    ]
  },
  {
    "objectID": "pol_division.html#polynomial-division-is-useful-in-polynomial-factoring",
    "href": "pol_division.html#polynomial-division-is-useful-in-polynomial-factoring",
    "title": "Polynomial division",
    "section": "6) Polynomial division is useful in polynomial factoring",
    "text": "6) Polynomial division is useful in polynomial factoring\nThe decomposition\n\\[ f(x)=q(x)g(x)+r(x) \\]is key in factoring polynomials.\n\nFactoring polynomials\nConsider \\(f(x) = x^2-1\\). We know by inspection that \\(f(1)=0\\) and using our division procedures above we find:\n\\[ \\overbrace{x^2-1}^f = \\overbrace{(x+1)}^q\\overbrace{(x-1)}^g+\\overbrace{0}^r \\]\nThis simple example is a particular case of the following fact:\n\nA very important observation: If somehow we know a zero of \\(f\\) the division of \\(f(x)\\) by \\(x-\\text{zero of $f$ }\\) yields a zero remainder!\nNow, consider a more complicated polynomial \\(f\\):\\[ f(x) = x^4 -x^3-3x^2+5x-2 \\]\nImagine, that we know it has a zero at \\(x=1\\), an observation which could be arrived by inspecting its graph. Using polynomial division we find, as expected, a complete division, nothing remains, \\(r=0\\):\n\\[ \\overbrace{x^4 -x^3-3x^2+5x-2}^f = \\overbrace{(x^3-3x+2)}^q(x-1)+\\overbrace{0}^r \\]\nBut inspection of \\(q\\) reveals that \\(q(1)=0\\), the quotient has a zero as well, using again polynomial division we find \\(r'=0\\):\n\\[ \\overbrace{x^3-3x+2}^q = \\overbrace{(x^2+x-2)}^{q'}(x-1)+\\overbrace{0}^{r'} \\]\nIn general if we know a zero of \\(q\\) then again, division of \\(q(x)\\) by \\(x-\\text{zero of $q$}\\) yields again a zero remainder:\n\nTherefore:\n\nAnd the same story continues with \\(q'\\).\n\n\n\n\n\n\nCommentary\n\n\n\nNotice that a zero of \\(q\\) is automatically a zero of \\(f\\). And a zero of \\(q'\\) is a zero of \\(f\\) as well.\n\n\nNow repeat the process and inspect the prime quotient \\(x^2+x-2\\), what do we find? We find that \\(q'(1)=0\\), using again polynomial division, yes! A zero remainder again:\n\\[ \\overbrace{x^2+x-2}^{q'} =\\overbrace{(x+2)}^{q''}(x-1)+\\overbrace{0}^{r''} \\]\nHere is a summary of the sequence of steps:\n\nCollecting terms we found:\n\\[ x^4-x^3-3x^2+5x-2 =(x+2)(x-1)(x-1)(x-1)+0 \\]\nThe lower the degree of the polynomial the easier it is to find its zeros by inspecting the function. Figuring out the original zero (the zero of \\(f\\)) is the hardest part, a graph might help in our guess work, if nothing else, just use trial and error. If it is found, congrats yourself, you just made one step of \\(n\\) steps.",
    "crumbs": [
      "Bried Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Polynomial division"
    ]
  },
  {
    "objectID": "pol_division.html#why-factoring-polynomials-is-cool",
    "href": "pol_division.html#why-factoring-polynomials-is-cool",
    "title": "Polynomial division",
    "section": "7) Why factoring polynomials is cool?",
    "text": "7) Why factoring polynomials is cool?\nOne key aspect one should keep in mind, is that finding the zeros of a polynomial and factored polynomial goes hand in hand. As we seen, the zeros of \\(x^4-x^3-3x^2+5x-2\\) are the singlet \\(-2\\), and the triplet of \\(1\\); knowing this allowed us to write this polynomial as \\((x+2)(x-1)^3\\). Going the other way around, starting from a polynomial given by \\((x+2)(x-1)^3\\) we know, just by looking at this formula, the zeros are \\(-2\\) and a triplet \\(1\\). Knowing the zeros is important or should I say knowing the factored form of a polynomial is important because, in-between those zeros, the polynomial must either positive (above the x-axis) or negative (below the x-axis). Additionally, note that the factored form of a polynomial is a product of atomic polynomials, in the current case it the product of four terms of degree one, \\(x+2\\) and three \\(x-1\\); the sign (positive or negative) of each is easily computed, they are just lines in the plane! As a result the sign of the product polynomial can also be easily computed from the sign of the a product of the terms. It goes without saying: The good polynomial is the factored polynomial.\nWhat we intend to do now is precisely to find the sign (positive or negative) of the product polynomial in-between the zeros (\\(-2\\) and \\(1\\)), in other words, we want to compute the following sets:\n\\[ \\begin{align} \\{x\\in\\mathbb{R}\\,\\,|\\,\\,x^4-x^3-3x^2+5x-1&gt;0\\}\\\\ \\{x\\in\\mathbb{R}\\,\\,|\\,\\,x^4-x^3-3x^2+5x-1&lt;0\\} \\end{align}  \\tag{12}\\] We already know the zeros of \\(x ^ 4-x ^ 3-3x^2+5x-1\\) which are embedded in the terms\n\n\\(x+2\\) is zero at \\(x=-2\\)\n\\(x-1\\) has a zero at \\(x=1\\)\n\nNow, to compute the sign of the term, draw a table as follows: in the first column put the factors, the first row is suppose to represent the real line, in it we highlight key points, which in the present case are the zeros of the factors, the point at \\(-2\\) and \\(1\\) , and as reference we should include the infinities (“end points” of the real line) as well:\n\n\n\nReal line\n\\(-\\infty\\)\n\n\\(-2\\)\n\n\\(1\\)\n\n\\(\\infty\\)\n\n\n\n\n\\(x+2\\)\n\\(-\\)\n\\(-\\)\n\\(0\\)\n\\(+\\)\n\\(+\\)\n\\(+\\)\n\\(+\\)\n\n\n\\(x-1\\)\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\\(0\\)\n\\(+\\)\n\\(+\\)\n\n\n\\(x-1\\)\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\\(0\\)\n\\(+\\)\n\\(+\\)\n\n\n\\(x-1\\)\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\\(0\\)\n\\(+\\)\n\\(+\\)\n\n\n\\((x+2)(x-1)^3\\)\n\\(+\\)\n\\(+\\)\n\\(0\\)\n\\(-\\)\n\\(0\\)\n\\(+\\)\n\\(+\\)\n\n\n\nHere an equivalent but clearer picture of this table, which I prefer:\n\nIn the rows \\(x+2\\) and \\(x-1\\) fill the zeros as \\(x=-2\\) and \\(x=1\\) respectively. Now, take the \\(x+2\\) row as an example, there is only one zero, hence before that zero we have a definite sign and after this zero we have also a definite sign, to find these signs we can just pick any two \\(x\\)-values, picking \\(x=-3\\) and \\(x=1\\) we compute \\(-3+2 = -1\\) which is negative and thus the region \\((-\\infty,-2)\\) must be negative; computing \\(1+2=3\\) we get a positive value, thus the region \\((-2,\\infty)\\) is positive. We repeat the same approach and pick two numbers, one before and other after \\(x=1\\) the zero of \\(x-1\\) to check its sign, the results are shown above.\nThe last row is also easy to fill from the signs of \\(x+2\\) and \\((x-1)^3\\). Notice the cube power, being an odd number, does not change the sign of \\(x-1\\).\nFrom the last row we can read the answer for our question Equation 12 , we conclude:\n\\[ \\begin{align} &\\{x\\in\\mathbb{R}\\,\\,|\\,\\,x^4-x^3-3x^2+5x-1&gt;0\\}=(-\\infty,-2)\\cup (1,+\\infty)\\\\ &\\{x\\in\\mathbb{R}\\,\\,|\\,\\,x^4-x^3-3x^2+5x-1&lt;0\\}=(-2,1) \\end{align} \\] The table method to find the signs of a complicated polynomial is fine and good, but I would like to emphasize the central idea behind it. What the table essentially is, is a representation of the graph of the factors with just the signs singled out. As such an alternative way, which I actually prefer, to find the answer to Equation 12 is to just graph the terms of the product polynomial:\n\nAnd from it read the signs from the lines:\nCool ! :)",
    "crumbs": [
      "Bried Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Polynomial division"
    ]
  },
  {
    "objectID": "pol_division.html#method-of-undetermined-coefficients-for-integer-division",
    "href": "pol_division.html#method-of-undetermined-coefficients-for-integer-division",
    "title": "Polynomial division",
    "section": "8) Method of undetermined coefficients for integer division",
    "text": "8) Method of undetermined coefficients for integer division\nA general strategy to solve the problem:\n\\[ 1001=q\\cdot 12 +r\\qquad 0\\leq q&lt;12  \\tag{13}\\]\nstarts by splitting the parameters of the equation into powers of \\(10\\)\n\\[ \\begin{split}&1001=1\\cdot 10^3+0\\cdot 10^2+0\\cdot 10^1+1\\cdot 10^0\\\\&12=1\\cdot 10^1+2\\cdot 10^0\\\\\\end{split} \\]\nand the unknowns \\(q\\) and \\(r\\) as well:\n\\[ \\begin{split}&q=a\\cdot 10^2+b\\cdot 10^2+c\\cdot 10^0\\\\&r=\\alpha \\cdot 10^0\\end{split} \\]\nfor some real numbers \\(a\\), \\(b\\), \\(c\\) and \\(\\alpha\\). Note that \\(q\\) must be at most a power \\(10^2\\) to guarantee a \\(10^3\\) of \\(1001\\), meanwhile the remainder \\(r\\) which must be between \\(0\\) and \\(12\\) has to be a number proportional to \\(10^0=1\\).\nSubstituting we find\n\\[ 1\\cdot 10^3+0\\cdot 10^2+0\\cdot 10^1+1\\cdot 10^0=(a\\cdot 10^2+b\\cdot 10^1+c\\cdot 10^0)\\cdot (1\\cdot 10^1+2\\cdot 10^0) +\\alpha \\cdot 10^0 \\]\nRearranging, we factor the same powers on the right side, which gives us:\n\\[ 1\\cdot 10^3+0\\cdot 10^2+0\\cdot 10^1+1\\cdot 10^0=a\\cdot 10^3+(2a+b)\\cdot 10^2+(2b+c)\\cdot 10^1+(2c+\\alpha)\\cdot 10^0  \\tag{14}\\]\nThe goal of determining what is \\(q\\) and \\(r\\) that satisfy the conditions Equation 13 becomes the goal of determining \\(a\\), \\(b\\), \\(c\\) and \\(\\alpha\\) that satisfy Equation 14.\nBoth sides of Equation 14 are equal provided the coefficients of the same powers of \\(10\\) are equal, that yields the following system of equations to solve:\n\\[ \\begin{cases}10^3:\\,\\,\\,1=a\\\\10^2:\\,\\,\\,0=2a+b\\\\10^1:\\,\\,\\,0=2b+c\\\\10^0:\\,\\,\\,1=2c+\\alpha\\end{cases} \\]\nwith the additional constraint \\(0\\leq \\alpha &lt;12\\).\nThe solution of this system of equations is \\(a=1\\), \\(b=-2\\), \\(c=4\\) and \\(\\alpha=-7\\) . Thus:\n\\[ \\begin{split}&q=1\\cdot 10^2-2\\cdot 10^1+4\\cdot 10^0=84\\\\&r=-7\\cdot 10^0=-7\\end{split} \\]\nHence, \\(1001=84\\cdot 12-7\\), observe the remainder is negative, therefore we modify the aesthetics of this result in order to make it positive and below \\(12\\). Adding \\(0=12-12\\) to the right hand side we find:\n\\[ \\begin{split}&1001=84\\cdot 12-7+12-12\\\\\\implies&1001=83\\cdot 12+5\\end{split} \\]\nThe final result is \\(1001=83\\cdot 12+5\\) as we computed through other methods.",
    "crumbs": [
      "Bried Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Polynomial division"
    ]
  },
  {
    "objectID": "pol_division.html#method-of-undetermined-coefficients-for-polynomials-division",
    "href": "pol_division.html#method-of-undetermined-coefficients-for-polynomials-division",
    "title": "Polynomial division",
    "section": "9) Method of undetermined coefficients for polynomials division",
    "text": "9) Method of undetermined coefficients for polynomials division\nHere we present our problem by casting it as a system of equations to solve. Some students might prefer this approach.\nComparing the lhs and rhs of Equation 7 we notice:\n\nthe polynomial \\(q(x)\\) must have a degree equal to \\(2\\)\nthe degree of \\(r(x)\\) is \\(0\\)\n\nFrom this we know the general from of these polynomials to be:\n\\[ \\begin{split}&q(x) :=  ax^2+bx+c\\\\&r(x) :=  \\alpha \\end{split} \\]\nSubstituting we find:\n\\[ x^3+1=(ax^2+bx+c)(x+2)+\\alpha  \\]\nExpanding the rhs and lumping the same powers of \\(x\\) we get:\n\\[ x^3+1=a x^3+(2a+b) x^2+ (2b +c)x^1+(2c+\\alpha) \\]\nThe polynomial on the right is equal to the the left if the coefficients of the same powers are the same, i.e., they satisfy the following system of equations\n\\[ \\begin{cases}x^3:\\,\\,\\,1=a\\\\x^2:\\,\\,\\,0=2a+b\\\\x^1:\\,\\,\\,0=2b+c\\\\x^0:\\,\\,\\,1=2c+\\alpha\\end{cases} \\]\nThe solution is \\(a=1\\), \\(b=-2\\), \\(c=4\\) and \\(\\alpha=-7\\):\n\\[ x^3+1=(x^2-2x+4)(x+2)-7 \\]\nThe similarities between the division of integers and polynomials should be obvious, since we just replace the powers of \\(10\\) by powers of \\(x\\).",
    "crumbs": [
      "Bried Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Polynomial division"
    ]
  },
  {
    "objectID": "matrix_multiplication.html",
    "href": "matrix_multiplication.html",
    "title": "Matrix multiplication and linear combination of rows",
    "section": "",
    "text": "Consider the following matrix operation (we found this operation in step 1 of Example 4):\n\\[\n\\begin{pmatrix}1 & 2 & 2 & 2\\\\2 & 4 & 6 & 8\\\\3 & 6 & 8 & 10 \\end{pmatrix}\\overset{l_2' = l_2-2l_1}{\\longrightarrow}\\begin{pmatrix}1 & 2 & 2 & 2 \\\\0 & 0 & 2 & 4 \\\\3 & 6 & 8 & 10 \\end{pmatrix}\n\\]\nand corresponding vector operation\n\\[\n\\begin{pmatrix}\n1\\\\2\\\\3\n\\end{pmatrix}\n\\overset{l_2' = l_2-2l_1}{\\longrightarrow}\n\\begin{pmatrix}\n1\\\\0\\\\3\n\\end{pmatrix}\n\\]\nImplicit was the following matrix multiplication:\n\\[\n\\begin{pmatrix}1 & 0 & 0 \\\\-2 & 1 & 0 \\\\0 & 0 & 1 \\end{pmatrix}\\begin{pmatrix}1 & 2 & 2 & 2 \\\\2 & 4 & 6 & 8 \\\\3 & 6 & 8 & 10\\end{pmatrix} = \\begin{pmatrix}1 & 2 & 2 & 2 \\\\0 & 0 & 2 & 4 \\\\3 & 6 & 8 & 10 \\end{pmatrix}\n\\]\nand\n\\[\n\\begin{pmatrix}1 & 0 & 0 \\\\-2 & 1 & 0 \\\\0 & 0 & 1 \\end{pmatrix}\\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix}=\\begin{pmatrix}1\\\\0\\\\3\\end{pmatrix}\n\\]\nHow do we do these calculations?\nThe general rule is:\n\n\n\n\n\n\nFigure 1: Matrix multiplication of a 3x4 matrix by a 3x3 matrix.\n\n\n\nwhere, for example:\n\\[\n\\begin{align}\na_{22}'= e_{21}a_{12} + e_{22}a_{22} +e_{23}a_{32}\\\\\na_{23}'= e_{21}a_{13} + e_{22}a_{23} +e_{23}a_{33}\\\\\n\\end{align}\n\\tag{1}\\]\nLets name the matrices involved, the \\(e\\)’s matrix is called \\(E_1\\) and the \\(a\\)’s matrix is called \\(A\\), the result of the multiplication is called \\(A'\\).\nObserving Figure 1 and Equation 1 we see two important aspects about \\(E_1 A\\)\n\nThe shape of \\(E_1\\) is \\(3\\times3\\),the shape of \\(A\\) is \\(3\\times 4\\) and the shape of \\(A'\\) is \\(3\\times4\\).\nThe matrix \\(E_1\\) needs to have as many columns as there are rows in \\(A\\) (in this case \\(3\\)), otherwise it is not possible to do matrix multiplication. In other words, the compatible shapes for matrix multiplication are of the form \\([m\\times r][r\\times n] = [m\\times n]\\), for any integers \\(m,n,r\\). For example, the case above is \\([3\\times 3][3\\times 4]\\) and thus \\(m=3\\), \\(r=3\\) and \\(n=4\\).\nThe general formula for the entries of \\(A'\\) is the complicated formula:\n\\[\na_{ij}'=\\sum_{k=1}^re_{ik}a_{kj}\n\\]",
    "crumbs": [
      "Bried Notes",
      "Linear Algebra",
      "Things you can do with matrices",
      "Matrix multiplication"
    ]
  },
  {
    "objectID": "macros.html",
    "href": "macros.html",
    "title": "macros",
    "section": "",
    "text": "::: {.hidden} $$ \\newcommand{\\foo}{E=mc^{2}} $$ :::"
  },
  {
    "objectID": "linear_dependence_and_independence.html",
    "href": "linear_dependence_and_independence.html",
    "title": "Linear dependence and independence",
    "section": "",
    "text": "Exercise 1 (Motivation exercise) Solve 1.2 &gt; 1 &gt; c with \\(w=(1,10,3,4)\\) instead, then compute \\(2u-3v+u\\). The later gives the zero vector, what can you conclude?\n\nFirst lets see what is linear (in)dependence, then how to check it. As we will see throughout the course there are many ways of doing it.\nLinear (in)dependence is seen best with examples.\n\nExample 1: Just consider two parallel vectors\n\nOne is just twice the other (i.e. proportional). They are dependent because you can create one from the other:\n\\[ \\begin{pmatrix} 2\\\\4\\end{pmatrix} = 2 \\begin{pmatrix} 1\\\\2\\end{pmatrix} \\qquad\\text{or}\\qquad \\begin{pmatrix} 1\\\\2\\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 2\\\\4\\end{pmatrix} \\]\nExample 2: More difficult. Consider again the vectors \\(\\begin{pmatrix} 1\\\\2\\end{pmatrix}\\), \\(\\begin{pmatrix} 1\\\\0\\end{pmatrix}\\) and \\(\\begin{pmatrix} 0\\\\1\\end{pmatrix}\\). If you discard the first vector, nothing is lost because through the linear combination of the second and third vectors we can recover it. Here is how:\n\\[ \\begin{pmatrix} 1\\\\0\\end{pmatrix}+2\\begin{pmatrix} 0\\\\1\\end{pmatrix} = \\begin{pmatrix} 1\\\\2\\end{pmatrix} \\]\nIn other words the first vector depends on the second and third vector through the linear combination above, it can be fully build from these pieces and thus it does not bring anything new.\n\nWhat may not be so obvious is the fact that any vector from the three could be discarded, for example drop the second one! No problem the first and third vectors can construct it:\n\\[ \\begin{pmatrix} 1\\\\2\\end{pmatrix}-2\\begin{pmatrix} 0\\\\1\\end{pmatrix} = \\begin{pmatrix} 1\\\\0\\end{pmatrix} \\]\n\nFrom these examples we conclude that the following statements are equivalent:\n\nVectors from a list are linearly independent if you cannot construct any one of them from the remaining ones. (they point in distinct directions):\nVectors from a list are linearly independent when there exist scalars that combine them, yielding the zero vector. Otherwise they are dependent;\n\nThere two statements in English translates to Mathematical language:\n\nDefinition 1 [\\(\\mathbf{u},\\mathbf{v},\\mathbf{w}\\) are independent] := [the only solution of \\(\\alpha \\mathbf{u} +\\beta \\mathbf{v} + \\gamma \\mathbf{w} = \\mathbf{0}\\) is \\(\\alpha=\\beta=\\gamma=0\\)]\n[\\(\\mathbf{u},\\mathbf{v},\\mathbf{w}\\) are dependent] := [there are solutions of \\(\\alpha \\mathbf{u} +\\beta \\mathbf{v} + \\gamma \\mathbf{w} = \\mathbf{0}\\) where \\(\\alpha,\\beta,\\gamma\\) need not be \\(0\\)]\n[\\(\\mathbf{w}\\) is dependent on \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\)] := [\\(\\mathbf{w} = \\alpha \\mathbf{u} + \\beta\\mathbf{w}\\) for some \\(\\alpha,\\beta\\)] = [\\(\\mathbf{w} \\in span\\{\\mathbf{u},\\mathbf{v}\\}\\)]\n\n\n\n\n\n\n\nCommentaries\n\n\n\n\nWhat the dependence definition is really telling us, is that there is at least one vector that can be written in terms of the others. And that “writing in terms of the others” is accomplished just by rearranging \\(\\alpha \\mathbf{u} +\\beta \\mathbf{v} + \\gamma \\mathbf{w} = \\mathbf{0}\\).\nFor example, if \\(\\mathbf{u} +2\\mathbf{v} - \\mathbf{w} = \\mathbf{0}\\), then isolating \\(\\mathbf{w}\\) we obtain \\(\\mathbf{w}=\\mathbf{u} + 2\\mathbf{v}\\), telling \\(\\mathbf{w}\\) depends on \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\); similarly isolating \\(\\mathbf{u}\\) or \\(\\mathbf{v}\\) tells us they are also dependent on the remaining. Geometrically the vectors lie in the same plane. (see example 2)\nIn a later section we’ll see the definition above is in fact solving \\(A\\bf{x}=\\bf{0}\\). To check whether one vector is dependent or not on other vectors is a matter of whether \\(A\\mathbf{x}=\\mathbf{b}\\) has or not a solution.\n\n\n\n\n\nConsider again example 2 above. We were given the vectors: \\(\\begin{pmatrix} 1\\\\2\\end{pmatrix}\\) , \\(\\begin{pmatrix} 1\\\\0\\end{pmatrix}\\) and \\(\\begin{pmatrix} 0\\\\1\\end{pmatrix}\\) . Are they dependent or independent?\nIn other words, using the definition: what is the solution \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\) of the equation:\n\\[\n\\alpha \\begin{pmatrix} 1\\\\2\\end{pmatrix} +\\beta \\begin{pmatrix} 1\\\\0\\end{pmatrix}+\\gamma\\begin{pmatrix} 0\\\\1\\end{pmatrix}=\\begin{pmatrix} 0\\\\0\\end{pmatrix}\n\\]\nWell, rewriting it, we obtain a system of equations to solve:\n\\[\n\\begin{cases}\n\\alpha + \\beta =0\\\\\n2\\alpha +\\gamma =0\n\\end{cases}\n\\implies\n\\begin{cases}\n\\alpha = \\beta =0\\\\\n\\alpha =\\gamma/2 =0\n\\end{cases}\n\\]\nThere are solutions for this system, for example \\(\\gamma=2\\), \\(\\alpha=1\\) and \\(\\beta= -1\\). This shows some of the three vectors can be constructed from the remaining.\n\n\n\n\n\n\nCommentary\n\n\n\nWhen the number of vectors in \\(\\mathbb{R}^n\\) being spanned is larger than the dimension \\(n\\) of the space, then some are dependent vectors. In the example above, \\(n=2\\) and there are \\(3\\) vectors being spanned.\n\n\n\nExercise 2 Solve 1.2 &gt; 5 &gt; (d)\n\n\n\n\nIs \\(\\begin{pmatrix} 1\\\\2\\end{pmatrix}\\) dependent on \\(\\begin{pmatrix} 1\\\\0\\end{pmatrix}\\) and \\(\\begin{pmatrix} 0\\\\1\\end{pmatrix}\\) ? In other words, is it true or false that \\(\\begin{pmatrix} 1\\\\2\\end{pmatrix} \\in span \\{\\begin{pmatrix} 1\\\\0\\end{pmatrix},\\begin{pmatrix} 0\\\\1\\end{pmatrix}\\}\\)\nUsing the definition above we seek for solutions of\n\\[\n\\begin{pmatrix} 1\\\\2\\end{pmatrix}=\\beta\\begin{pmatrix} 1\\\\0\\end{pmatrix}+\\gamma\\begin{pmatrix} 0\\\\1\\end{pmatrix}\n\\]\n(we set \\(\\alpha=-1\\))\nThe solutions are obtained by solving the system:\n\\[\n\\begin{cases}\n1=\\beta\\\\\n2=\\gamma\n\\end{cases}\n\\]\nwhich is immediate! So yes, the vector is dependent.\n\nExercise 3 Solve 1.2 &gt; 3 &gt; (b), (c);",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Linear dependence and independence"
    ]
  },
  {
    "objectID": "linear_dependence_and_independence.html#how-to-check-whether-a-set-of-vectors-is-or-not-independent",
    "href": "linear_dependence_and_independence.html#how-to-check-whether-a-set-of-vectors-is-or-not-independent",
    "title": "Linear dependence and independence",
    "section": "",
    "text": "Consider again example 2 above. We were given the vectors: \\(\\begin{pmatrix} 1\\\\2\\end{pmatrix}\\) , \\(\\begin{pmatrix} 1\\\\0\\end{pmatrix}\\) and \\(\\begin{pmatrix} 0\\\\1\\end{pmatrix}\\) . Are they dependent or independent?\nIn other words, using the definition: what is the solution \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\) of the equation:\n\\[\n\\alpha \\begin{pmatrix} 1\\\\2\\end{pmatrix} +\\beta \\begin{pmatrix} 1\\\\0\\end{pmatrix}+\\gamma\\begin{pmatrix} 0\\\\1\\end{pmatrix}=\\begin{pmatrix} 0\\\\0\\end{pmatrix}\n\\]\nWell, rewriting it, we obtain a system of equations to solve:\n\\[\n\\begin{cases}\n\\alpha + \\beta =0\\\\\n2\\alpha +\\gamma =0\n\\end{cases}\n\\implies\n\\begin{cases}\n\\alpha = \\beta =0\\\\\n\\alpha =\\gamma/2 =0\n\\end{cases}\n\\]\nThere are solutions for this system, for example \\(\\gamma=2\\), \\(\\alpha=1\\) and \\(\\beta= -1\\). This shows some of the three vectors can be constructed from the remaining.\n\n\n\n\n\n\nCommentary\n\n\n\nWhen the number of vectors in \\(\\mathbb{R}^n\\) being spanned is larger than the dimension \\(n\\) of the space, then some are dependent vectors. In the example above, \\(n=2\\) and there are \\(3\\) vectors being spanned.\n\n\n\nExercise 2 Solve 1.2 &gt; 5 &gt; (d)",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Linear dependence and independence"
    ]
  },
  {
    "objectID": "linear_dependence_and_independence.html#how-to-check-whether-a-vector-depends-on-two-other-vectors",
    "href": "linear_dependence_and_independence.html#how-to-check-whether-a-vector-depends-on-two-other-vectors",
    "title": "Linear dependence and independence",
    "section": "",
    "text": "Is \\(\\begin{pmatrix} 1\\\\2\\end{pmatrix}\\) dependent on \\(\\begin{pmatrix} 1\\\\0\\end{pmatrix}\\) and \\(\\begin{pmatrix} 0\\\\1\\end{pmatrix}\\) ? In other words, is it true or false that \\(\\begin{pmatrix} 1\\\\2\\end{pmatrix} \\in span \\{\\begin{pmatrix} 1\\\\0\\end{pmatrix},\\begin{pmatrix} 0\\\\1\\end{pmatrix}\\}\\)\nUsing the definition above we seek for solutions of\n\\[\n\\begin{pmatrix} 1\\\\2\\end{pmatrix}=\\beta\\begin{pmatrix} 1\\\\0\\end{pmatrix}+\\gamma\\begin{pmatrix} 0\\\\1\\end{pmatrix}\n\\]\n(we set \\(\\alpha=-1\\))\nThe solutions are obtained by solving the system:\n\\[\n\\begin{cases}\n1=\\beta\\\\\n2=\\gamma\n\\end{cases}\n\\]\nwhich is immediate! So yes, the vector is dependent.\n\nExercise 3 Solve 1.2 &gt; 3 &gt; (b), (c);",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Linear dependence and independence"
    ]
  },
  {
    "objectID": "linear_approximation.html",
    "href": "linear_approximation.html",
    "title": "Linear approximation",
    "section": "",
    "text": "The basic idea is this: start with the graph of a function \\(f\\), then draw a line tangent to a chosen point \\((x_0,y_0)\\) on the function. This line is special, because the points \\((x,y)\\) on the line around the point \\((x_0,y_0)\\) and the points \\((x,f(x))\\) also around \\((x_0,y_0)\\) are very close and as a result we can use them interchangeably, at the price of a small error. Lets express this mathematically, for \\(x\\) close to \\(x_0\\) we have:\n\\[\nf(x) \\sim m(x-x_0)+y_0\n\\tag{1}\\]\nThe equation of the line is \\(y=m(x-x_0)+y_0\\), we want to find it! Because the line is tangent to the function \\(f\\), somehow, the parameters \\(m\\) and \\(y_0\\) are related to that function (\\(x_0\\) is chosen by us). From the picture we know that:\n\\[\nm = f'(x_0)\\qquad y_0 = f(x_0)\n\\]\nIn conclusion, the images of \\(x\\)’s around \\(x_0\\) through the function \\(f\\) are mapped into the corresponding \\(f(x)\\)’s, and in turn this values are approximately of the form \\(f'(x_0)(x-x_0)+f(x_0)\\).\nYou may wonder, why this approximation is useful (for a physicist it is extremely useful!), only though example we’ll see that, but just from the formula Equation 1 you can see that for a complicated function \\(f(x)\\), the rhs, is just a simple and each to handle polynomial of degree one. I would rather use that than the exact value \\(f(x)\\), if the error committed is negligible for the problem at hand.\n\nDiferentials\nThe idea of differentials is closely connected to the idea of linear approximations.\nAbove we seen that the values of \\(f(x)\\) are very close to the values \\(f'(x_0)(x-x_0)+f(x_0)\\) as long as \\(x\\) is close to \\(x_0\\), the farther \\(x\\) is, the worst is the difference between both.\nAs the name differentials suggests, we want to compute differences, differences between points on the function, for example, we want to compute \\(f(x)-f(x_0)\\).\nThe idea behind linear approximations gives us a way to evaluate this diference easily, with better and better accuracy the closer and closer is \\(x\\) to the given \\(x_0\\); just evaluate \\(f(x)\\) approximately using \\(f'(x_0)(x-x_0)+f(x_0)\\). Thus:\n\\[\nf(x)-f(x_0) \\sim f'(x_0)(x-x_0)+f(x_0) -f(x_0) = f'(x_0)(x-x_0)\n\\tag{2}\\]\nOn the lhs we have the difference between the images of \\(x\\) and \\(x_0\\) thought the function, meanwhile on the rhs we find the corresponding difference by though the tangent line. If \\(x\\) is very close to \\(x_0\\), then \\(x-x_0\\) is close to zero and \\(f(x)-f(x_0)\\) is close to zero as well, as it should; see the picture.\nIn fact we have special notation for formula Equation 2:\n\n\\(f(x)-f(x_0) =: df\\) is called the differential of the function \\(f\\) around \\(x_0\\).\n\\(x-x_0=:dx\\) is called the differential of the independent variable \\(x\\), around, again, the given value of \\(x_0\\). The differential \\(dx\\) is any real number!\n\nThus,\n\\[\ndf \\sim f'(x_0) dx\n\\]\nThe close \\(x\\) is to \\(x_0\\), the closer \\(dx\\) is to zero and the better is the approximation. If \\(dx\\) is extremely small we would rather use the equal sign rather than the approximate sign: \\(df = f'(x_0) dx\\) if \\(dx\\longrightarrow 0\\)."
  },
  {
    "objectID": "inner_product.html",
    "href": "inner_product.html",
    "title": "Inner Product",
    "section": "",
    "text": "Linear combinations we also have a inner product operation \\(\\cdot\\) which is an operation between vectors and yields a number, it is defined as:\n\nDefinition 1 Let \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) be two vectors from the same vector space \\(\\mathbb{R}^n\\) (i.e. \\(\\mathbf{u}=(u_1,\\dots,u_n)\\) and \\(\\mathbf{v}=(v_1,\\dots,v_n)\\)). The inner product of these two vectors is the real number of the form:\n\\[\n[\\text{inner product between}\\,\\, \\mathbf{u}\\,\\, \\text{and} \\,\\,\\mathbf{v}]:=\\mathbf{u}\\cdot\\mathbf{v} = u_1v_1+\\dots+u_nv_n =: \\sum_{i=1}^nu_iv_i\n\\]\nAnother notation for this number/operation is \\(\\mathbf{u}^\\intercal\\mathbf{v}\\).\nThe inner product also results from the calculation \\(\\mathbf{u}\\cdot\\mathbf{v} = uv\\cos\\theta\\).\n\nProperties:\n\nit obeys the property of linearity\n\\[ \\mathbf{u} \\cdot (\\mathbf{v} + \\lambda \\cdot \\mathbf{w}) = \\mathbf{u}\\cdot \\mathbf{v} + \\lambda \\mathbf{u}\\cdot \\mathbf{w} \\] where \\(\\lambda\\) is any real number. The scalar product \\(\\mathbf{u}\\cdot \\mathbf{v}\\) is a number, \\(\\mathbf{u}\\cdot \\mathbf{w}\\) is a number, the right hand side of this formula is a number, as is the left.\nThe order of the vectors being dotted is irrelevant \\(\\mathbf{u}\\cdot\\mathbf{v} = \\mathbf{v}\\cdot\\mathbf{u}\\) and \\(\\mathbf{u}^\\intercal\\mathbf{v} = \\mathbf{v}^\\intercal \\mathbf{u}\\).\nSince \\(-1\\leq\\cos\\theta\\leq 1\\), then \\(-uv\\leq\\mathbf{u}\\cdot\\mathbf{v} \\leq uv\\).\n(Pythagoras theorem) If two vectors are orthogonal, then \\(|\\mathbf{u}+\\mathbf{v}|^2=u^2+v^2\\). [Commentary: Two orthogonal vectors can come about when decomposing a single vector \\(\\mathbf{w}\\in\\mathbb{R}^n\\) in two complementary spaces \\(\\mathbb{R}^n=U\\oplus U^\\perp\\).]\n(Cauchy-Schwarz inequality aka triangular inequality) For any vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) we have the following: \\(|\\mathbf{u}+\\mathbf{v}|\\leq u+v\\). Equality holding when the vectors are parallel and pointing in the same sense.\n\nUsefulness: The inner product operation is a tool to gather geometrical information about vectors, and to describe planes and hyperplanes.\nI.e. it allows answering the following questions:\n\n\nAnswer: Compute the inner product of a vector with itself.\n\\[\n\\mathbf{u}^\\intercal\\mathbf{u} = u_1^2+\\dots+u_n^2\n\\]\nWhose rhs we interpret, using the Pythagoras theorem, as the length square of \\(\\mathbf{u}\\). The norm \\(|\\mathbf{u}|\\) of the vector is the square-root of this value:\n\\[\n|\\mathbf{u}|:=\\sqrt{u_1^2+\\dots+u_n^2}\n\\]\nNotation: we can call the norm \\(|\\mathbf{u}|\\) just by \\(u\\).\n\n\nThe length squared of the vector \\(\\mathbf{u}=(1,2)\\) is:\n\\[\n|\\mathbf{u}|^2 = \\mathbf{u}^\\intercal\\mathbf{u} = u_1^2+u_2^2 = 1^2+2^2=1+4=5\n\\]\nThe norm of the vector is obtained by taking the square root of \\(5\\):\n\\[\nu=\\sqrt{5}\n\\]\n\nAnother way to compute the norm-squared of a vector is to consider \\(\\mathbb{R}^2=[\\textbf{x axis}]\\oplus[\\textbf{y axis}]\\) and write \\(\\mathbf{u} = (0,1)+(2,0)\\) and then use the pythagoras theorem \\(|\\mathbf{w}+\\mathbf{v}|^2=w^2+v^2\\) to compute:\n\\[\n|\\mathbf{u}|^2=|(0,1)+(2,0)|^2=(0^2+1^2)^2+(2^2+0^2)^2=1+4=5\n\\]\nThen take the square-root.\nExercises: 1.6.1a\n\n\n\n\nStarting with the vector \\(\\mathbf{u}=(u_x,u_y)\\) we can define a parallel unit vector by dividing it by it length:\n\\[\n\\mathbf{e}_u = \\frac{\\mathbf{u}}{|\\mathbf{u}|}\n\\]\nJustification: Compute its norm \\(|\\mathbf{e}_u|^2 = \\frac{\\mathbf{u}}{|\\mathbf{u}|}\\cdot \\frac{\\mathbf{u}}{|\\mathbf{u}|}=\\frac{|\\mathbf{u}|^2}{|\\mathbf{u}|^2}=1\\)\n\n\nThe unit vector along \\(\\mathbf{u}=-(1,2)\\) is:\n\\[\n\\mathbf{e}_u = \\frac{-(1,2)}{\\sqrt{1+2^2}}=-\\frac{1}{\\sqrt{5}}(1,2)\n\\]\nClearly: \\(|\\mathbf{e}_u|^2 = -\\frac{1}{\\sqrt{5}}(1,2)\\cdot (-\\frac{1}{\\sqrt{5}}(1,2))=\\frac{1}{5}(1+2^2)=1\\)\n\n\n\n\nExample: Given again the vector \\(\\mathbf{n}=(1,2)\\), a vector \\(\\mathbf{r}\\) perpendicular to it if it obeys \\(\\mathbf{n}^\\intercal \\mathbf{r}=0\\). Solving this equation for \\(\\mathbf{r}\\) we find:\n\\[\n(1,2)\\cdot (x,y)=0 \\implies x+2y=0\n\\]\n[Comment: This is a very simple \\(A\\mathbf{x}=\\mathbf{0}\\) type of problem, its already fully simplified] One equation with two unknowns, we must promote one to parameter: choose \\(x=-2\\), then \\(y=1\\). Thus \\((-2,1)\\) is perpendicular to \\((1,2)\\). This is not the only solution.\n\n\n\nCircle and spheres: A circle in \\(\\mathbb{R}^2\\) or a sphere in \\(\\mathbb{R}^3\\) centered at \\(\\mathbf{0}\\) satisfy the equation: \\(|\\mathbf{r}|^2=R^2\\) where \\(R\\) is the radius. Centered at \\(\\mathbf{r}_0\\) they obey \\(|\\mathbf{r}-\\mathbf{r}_0|^2=R^2\\). The geometry of the situation should clearly justify the equations.\nLine in \\(\\mathbb{R}^2\\): A line perpendicular to \\(\\mathbf{n}=(1,2)\\) is the set of all solutions (not just one) to the equation \\(\\mathbf{n}^\\intercal \\mathbf{r}=0\\), the answer is:\n\\[\n[\\textbf{line}\\,\\perp\\,\\textbf{to}\\,\\,(1,2)]=\\{(-2c,c)\\,\\,|\\,\\,c\\in \\mathbb{R}\\}\n\\]\nLine in \\(\\mathbb{R}^3\\): A line (which crosses the origin) is the interception of two non-parallel planes (which cross it as well); a plane is the subspace perpendicular to a vector. Let \\(\\mathbf{n}_1=(1,0,1)\\) and \\(\\mathbf{n}_2=(1,2,0)\\) define the two planes \\(\\mathbf{n}_{1,2}^\\intercal \\mathbf{r}=0\\). The interception is the set of vectors common to both planes, this means we have to find the \\(\\mathbf{r}=(x,y,z)\\in\\mathbb{R}^3\\) such that both plane equations are satisfied, i.e.\n\\[\n\\begin{cases}\n\\mathbf{n}_{1}^\\intercal \\mathbf{r}=0\\\\\n\\mathbf{n}_{2}^\\intercal \\mathbf{r}=0\n\\end{cases}\n\\leftrightsquigarrow\n\\left(\\begin{matrix}1 & 0 & 1\\\\1 & 2 & 0 \\end{matrix}\\;\\middle|\\;\\begin{matrix}0\\\\0 \\end{matrix}\\right)\n\\overset{l_2'=l_2-l_1}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 0 & 1\\\\0 & 2 & -1 \\end{matrix}\\;\\middle|\\;\\begin{matrix}0\\\\0 \\end{matrix}\\right)\n\\overset{l_2'=1/2l_2}{\\longrightarrow}\\left(\\begin{matrix}1 & 0 & 1\\\\0 & 1 & -1/2 \\end{matrix}\\;\\middle|\\;\\begin{matrix}0\\\\0 \\end{matrix}\\right)\n\\tag{1}\\]\nThere are two pivots in the first two columns and a dependent column, one solution is \\((-1,1/2,1)\\), in general we have:\n\\[\n\\mathbf{x}_N = c\\left(\\begin{matrix}-1\\\\1/2\\\\1\\end{matrix}\\right)\n\\]\nwhich is the form of the elements along a line!\n\\[\n[\\textbf{line}\\,\\perp\\, \\textbf{to}\\,\\, \\mathbf{n}_{1,2}] = \\{(-c,c/2,c)\\,\\,|,\\, c\\in \\mathbb{R}\\}\n\\]\nCaptain obvious: If we know the process of finding all vector (the line) perpendicular to these two vector, I can find just one vector perpendicular to both.\nA plane in \\(\\mathbb{R}^3\\): The subspace perpendicular to \\((1,0,1)\\) is the solution of \\((1,0,1)\\cdot (x,y,z)=0\\), because there are two free columns and one pivot we expect null space \\(N(1,0,1)\\) with two dimensions. Setting the free variable \\(y=1\\) and \\(z=0\\) we find \\(x=-1\\), hence \\(\\mathbf{x}_{N,1}=(-1,1,0)\\); setting \\(y=0\\) and \\(z=1\\) we arrive at \\(\\mathbf{x}_{N,2}=(-1,0,1)\\). Any l.c. of these solutions is a solution of the equation:\n\\[\n\\mathbf{x}_N=\\alpha\\left(\\begin{matrix}-1\\\\1\\\\0 \\end{matrix}\\right)+\\beta\\left(\\begin{matrix}-1\\\\0\\\\1 \\end{matrix}\\right)\n\\]\nwhich defines a plane containing \\(\\mathbf{0}\\).\nHyperplane: The idea is similar, just define the appropriate interception of subspaces perpendicular to the given vectors, then arrange the equations as an \\(A\\mathbf{r}=\\mathbf{0}\\) problem.\n\n\n\n\n\nThe formula \\(\\mathbf{u}\\cdot\\mathbf{v} = uv\\cos\\theta\\) is of great importance because it allow us to compute the angle between \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\). Let \\(\\mathbf{u} = (1,2)\\) and \\(\\mathbf{v}=(-2,3)\\):\n\nThen the angle, is obtained by solving the trigonometric equation:\n\\[\n\\cos \\theta = \\frac{\\mathbf{u}^\\intercal\\mathbf{v}}{uv}=\\frac{1\\cdot (-2)+2\\cdot 3}{\\sqrt{1+2^2}\\sqrt{2^2+3^2}}=\\frac{4}{\\sqrt{65}}\\implies \\theta \\approx 1\\,\\, rad\n\\]\n[write \\(\\frac{1\\cdot (-2)+2\\cdot 3}{(1+2^2)(2+3^2)}\\) on white board rather than \\(\\frac{1\\cdot (-2)+2\\cdot 3}{\\sqrt{(1+2^2)(2+3^2)}}\\) and ask for where is the mistake.]\nAnother way to evaluate similarity is through the value of the cosine, in this case \\(\\cos \\theta\\) is \\(4/\\sqrt{85}\\) which is about \\(1/2\\).\n\nSince the \\(\\theta\\) is smaller than \\(\\pi/2\\), then the angle acute which means the vectors are not very similar, but at least point in the same direction.\n\n\n\nIf we now let \\(\\mathbf{u} = (1,-2)\\) and \\(\\mathbf{v}=(-2,3)\\), then the angle is: \\(\\theta \\approx 0.95 \\pi \\,\\,rad\\). Almost \\(180^\\circ\\), very different vectors. The inner product by being negative already tells us that the vectors point in almost opposite directions.\nFor this \\(\\mathbb{R}^2\\) examples, the calculation of the angle or the cosine is rather useless because we can easily draw both vectors and check whether they are or not similar. A harder task is when we try to do this in three, four, etc dimensions.\n\n\n\nLet \\(\\mathbf{u} = (1,2)\\) and the unitary vector \\(\\mathbf{e}_v=(1,-1)/\\sqrt{2}\\):\n\nComputing the inner product two ways we find:\n\\[\n\\mathbf{e}_v\\cdot \\mathbf{u} = -\\frac{1}{\\sqrt{2}}=5\\cos \\theta\n\\]\nOn the rhs we see using trigonometry, the projection (shadow) of the vector \\(\\mathbf{u}\\) along the vector \\(\\mathbf{e}_v\\), which is this case is negative, implying the vectors point in different directions.\nThus the matrix \\(\\mathbf{v}^\\intercal/v\\) is a projector along \\(\\mathbf{v}\\).\n\n\n\nThis time we start with \\(\\mathbf{e}_x\\) and \\(\\mathbf{e}_x\\), the unitary vector \\((1,0)\\) or \\((0,1)\\), and keep the usual \\(\\mathbf{u}=(1,2)\\). The inner products yields:\n\\[\n\\begin{cases}\n\\mathbf{e}_x^\\intercal \\mathbf{u}=1\\\\\n\\mathbf{e}_y^\\intercal \\mathbf{u}=2\\\\\n\\end{cases}\n\\]\nMoreover:\n\\[\n\\begin{cases}\n\\mathbf{e}_x^\\intercal \\mathbf{u}=5\\cos\\theta_x\\\\\n\\mathbf{e}_y^\\intercal \\mathbf{u}=5\\cos\\theta_y=5 \\sin\\theta_x\n\\end{cases}\n\\]\nWhich means geometrically:\n\nDotting a vector with an unitary vector yields \\(5\\cos\\theta_x\\) and \\(5\\cos\\theta_y\\) which using trigonometry are the projections of \\(\\mathbf{u}\\) along those directions, which in turn is are the entries \\(1\\) and \\(2\\) of the vector along the unitary vectors.\nWe conclude that, given a basis \\(B=\\{\\mathbf{e}_x, \\mathbf{e}_y\\}\\) we can write the vector \\(\\mathbf{u}=(1,2)\\) as:\n\\[\n\\mathbf{u} = (\\mathbf{e}_x\\cdot\\mathbf{u})\\mathbf{e}_x+(\\mathbf{e}_y\\cdot\\mathbf{u})\\mathbf{e}_y= 1\\mathbf{e}_x+2 \\mathbf{e}_y\n\\]\nWhich means: \\([\\mathbf{u}]_B=(\\mathbf{e}_x\\cdot\\mathbf{u},\\mathbf{e}_y\\cdot\\mathbf{u})\\)\nExercises: 1.7.5",
    "crumbs": [
      "Bried Notes",
      "Linear Algebra",
      "Inner product and Orthogonal spaces"
    ]
  },
  {
    "objectID": "inner_product.html#what-is-the-length-of-a-vector",
    "href": "inner_product.html#what-is-the-length-of-a-vector",
    "title": "Inner Product",
    "section": "",
    "text": "Answer: Compute the inner product of a vector with itself.\n\\[\n\\mathbf{u}^\\intercal\\mathbf{u} = u_1^2+\\dots+u_n^2\n\\]\nWhose rhs we interpret, using the Pythagoras theorem, as the length square of \\(\\mathbf{u}\\). The norm \\(|\\mathbf{u}|\\) of the vector is the square-root of this value:\n\\[\n|\\mathbf{u}|:=\\sqrt{u_1^2+\\dots+u_n^2}\n\\]\nNotation: we can call the norm \\(|\\mathbf{u}|\\) just by \\(u\\).\n\n\nThe length squared of the vector \\(\\mathbf{u}=(1,2)\\) is:\n\\[\n|\\mathbf{u}|^2 = \\mathbf{u}^\\intercal\\mathbf{u} = u_1^2+u_2^2 = 1^2+2^2=1+4=5\n\\]\nThe norm of the vector is obtained by taking the square root of \\(5\\):\n\\[\nu=\\sqrt{5}\n\\]\n\nAnother way to compute the norm-squared of a vector is to consider \\(\\mathbb{R}^2=[\\textbf{x axis}]\\oplus[\\textbf{y axis}]\\) and write \\(\\mathbf{u} = (0,1)+(2,0)\\) and then use the pythagoras theorem \\(|\\mathbf{w}+\\mathbf{v}|^2=w^2+v^2\\) to compute:\n\\[\n|\\mathbf{u}|^2=|(0,1)+(2,0)|^2=(0^2+1^2)^2+(2^2+0^2)^2=1+4=5\n\\]\nThen take the square-root.\nExercises: 1.6.1a",
    "crumbs": [
      "Bried Notes",
      "Linear Algebra",
      "Inner product and Orthogonal spaces"
    ]
  },
  {
    "objectID": "inner_product.html#how-to-build-a-unit-vector",
    "href": "inner_product.html#how-to-build-a-unit-vector",
    "title": "Inner Product",
    "section": "",
    "text": "Starting with the vector \\(\\mathbf{u}=(u_x,u_y)\\) we can define a parallel unit vector by dividing it by it length:\n\\[\n\\mathbf{e}_u = \\frac{\\mathbf{u}}{|\\mathbf{u}|}\n\\]\nJustification: Compute its norm \\(|\\mathbf{e}_u|^2 = \\frac{\\mathbf{u}}{|\\mathbf{u}|}\\cdot \\frac{\\mathbf{u}}{|\\mathbf{u}|}=\\frac{|\\mathbf{u}|^2}{|\\mathbf{u}|^2}=1\\)\n\n\nThe unit vector along \\(\\mathbf{u}=-(1,2)\\) is:\n\\[\n\\mathbf{e}_u = \\frac{-(1,2)}{\\sqrt{1+2^2}}=-\\frac{1}{\\sqrt{5}}(1,2)\n\\]\nClearly: \\(|\\mathbf{e}_u|^2 = -\\frac{1}{\\sqrt{5}}(1,2)\\cdot (-\\frac{1}{\\sqrt{5}}(1,2))=\\frac{1}{5}(1+2^2)=1\\)",
    "crumbs": [
      "Bried Notes",
      "Linear Algebra",
      "Inner product and Orthogonal spaces"
    ]
  },
  {
    "objectID": "inner_product.html#how-to-build-a-perpendicular-vector",
    "href": "inner_product.html#how-to-build-a-perpendicular-vector",
    "title": "Inner Product",
    "section": "",
    "text": "Example: Given again the vector \\(\\mathbf{n}=(1,2)\\), a vector \\(\\mathbf{r}\\) perpendicular to it if it obeys \\(\\mathbf{n}^\\intercal \\mathbf{r}=0\\). Solving this equation for \\(\\mathbf{r}\\) we find:\n\\[\n(1,2)\\cdot (x,y)=0 \\implies x+2y=0\n\\]\n[Comment: This is a very simple \\(A\\mathbf{x}=\\mathbf{0}\\) type of problem, its already fully simplified] One equation with two unknowns, we must promote one to parameter: choose \\(x=-2\\), then \\(y=1\\). Thus \\((-2,1)\\) is perpendicular to \\((1,2)\\). This is not the only solution.",
    "crumbs": [
      "Bried Notes",
      "Linear Algebra",
      "Inner product and Orthogonal spaces"
    ]
  },
  {
    "objectID": "inner_product.html#how-to-define-a-circles-lines-planes-and-hyperplane",
    "href": "inner_product.html#how-to-define-a-circles-lines-planes-and-hyperplane",
    "title": "Inner Product",
    "section": "",
    "text": "Circle and spheres: A circle in \\(\\mathbb{R}^2\\) or a sphere in \\(\\mathbb{R}^3\\) centered at \\(\\mathbf{0}\\) satisfy the equation: \\(|\\mathbf{r}|^2=R^2\\) where \\(R\\) is the radius. Centered at \\(\\mathbf{r}_0\\) they obey \\(|\\mathbf{r}-\\mathbf{r}_0|^2=R^2\\). The geometry of the situation should clearly justify the equations.\nLine in \\(\\mathbb{R}^2\\): A line perpendicular to \\(\\mathbf{n}=(1,2)\\) is the set of all solutions (not just one) to the equation \\(\\mathbf{n}^\\intercal \\mathbf{r}=0\\), the answer is:\n\\[\n[\\textbf{line}\\,\\perp\\,\\textbf{to}\\,\\,(1,2)]=\\{(-2c,c)\\,\\,|\\,\\,c\\in \\mathbb{R}\\}\n\\]\nLine in \\(\\mathbb{R}^3\\): A line (which crosses the origin) is the interception of two non-parallel planes (which cross it as well); a plane is the subspace perpendicular to a vector. Let \\(\\mathbf{n}_1=(1,0,1)\\) and \\(\\mathbf{n}_2=(1,2,0)\\) define the two planes \\(\\mathbf{n}_{1,2}^\\intercal \\mathbf{r}=0\\). The interception is the set of vectors common to both planes, this means we have to find the \\(\\mathbf{r}=(x,y,z)\\in\\mathbb{R}^3\\) such that both plane equations are satisfied, i.e.\n\\[\n\\begin{cases}\n\\mathbf{n}_{1}^\\intercal \\mathbf{r}=0\\\\\n\\mathbf{n}_{2}^\\intercal \\mathbf{r}=0\n\\end{cases}\n\\leftrightsquigarrow\n\\left(\\begin{matrix}1 & 0 & 1\\\\1 & 2 & 0 \\end{matrix}\\;\\middle|\\;\\begin{matrix}0\\\\0 \\end{matrix}\\right)\n\\overset{l_2'=l_2-l_1}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 0 & 1\\\\0 & 2 & -1 \\end{matrix}\\;\\middle|\\;\\begin{matrix}0\\\\0 \\end{matrix}\\right)\n\\overset{l_2'=1/2l_2}{\\longrightarrow}\\left(\\begin{matrix}1 & 0 & 1\\\\0 & 1 & -1/2 \\end{matrix}\\;\\middle|\\;\\begin{matrix}0\\\\0 \\end{matrix}\\right)\n\\tag{1}\\]\nThere are two pivots in the first two columns and a dependent column, one solution is \\((-1,1/2,1)\\), in general we have:\n\\[\n\\mathbf{x}_N = c\\left(\\begin{matrix}-1\\\\1/2\\\\1\\end{matrix}\\right)\n\\]\nwhich is the form of the elements along a line!\n\\[\n[\\textbf{line}\\,\\perp\\, \\textbf{to}\\,\\, \\mathbf{n}_{1,2}] = \\{(-c,c/2,c)\\,\\,|,\\, c\\in \\mathbb{R}\\}\n\\]\nCaptain obvious: If we know the process of finding all vector (the line) perpendicular to these two vector, I can find just one vector perpendicular to both.\nA plane in \\(\\mathbb{R}^3\\): The subspace perpendicular to \\((1,0,1)\\) is the solution of \\((1,0,1)\\cdot (x,y,z)=0\\), because there are two free columns and one pivot we expect null space \\(N(1,0,1)\\) with two dimensions. Setting the free variable \\(y=1\\) and \\(z=0\\) we find \\(x=-1\\), hence \\(\\mathbf{x}_{N,1}=(-1,1,0)\\); setting \\(y=0\\) and \\(z=1\\) we arrive at \\(\\mathbf{x}_{N,2}=(-1,0,1)\\). Any l.c. of these solutions is a solution of the equation:\n\\[\n\\mathbf{x}_N=\\alpha\\left(\\begin{matrix}-1\\\\1\\\\0 \\end{matrix}\\right)+\\beta\\left(\\begin{matrix}-1\\\\0\\\\1 \\end{matrix}\\right)\n\\]\nwhich defines a plane containing \\(\\mathbf{0}\\).\nHyperplane: The idea is similar, just define the appropriate interception of subspaces perpendicular to the given vectors, then arrange the equations as an \\(A\\mathbf{r}=\\mathbf{0}\\) problem.",
    "crumbs": [
      "Bried Notes",
      "Linear Algebra",
      "Inner product and Orthogonal spaces"
    ]
  },
  {
    "objectID": "inner_product.html#how-much-a-vector-looks-like-another-vector",
    "href": "inner_product.html#how-much-a-vector-looks-like-another-vector",
    "title": "Inner Product",
    "section": "",
    "text": "The formula \\(\\mathbf{u}\\cdot\\mathbf{v} = uv\\cos\\theta\\) is of great importance because it allow us to compute the angle between \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\). Let \\(\\mathbf{u} = (1,2)\\) and \\(\\mathbf{v}=(-2,3)\\):\n\nThen the angle, is obtained by solving the trigonometric equation:\n\\[\n\\cos \\theta = \\frac{\\mathbf{u}^\\intercal\\mathbf{v}}{uv}=\\frac{1\\cdot (-2)+2\\cdot 3}{\\sqrt{1+2^2}\\sqrt{2^2+3^2}}=\\frac{4}{\\sqrt{65}}\\implies \\theta \\approx 1\\,\\, rad\n\\]\n[write \\(\\frac{1\\cdot (-2)+2\\cdot 3}{(1+2^2)(2+3^2)}\\) on white board rather than \\(\\frac{1\\cdot (-2)+2\\cdot 3}{\\sqrt{(1+2^2)(2+3^2)}}\\) and ask for where is the mistake.]\nAnother way to evaluate similarity is through the value of the cosine, in this case \\(\\cos \\theta\\) is \\(4/\\sqrt{85}\\) which is about \\(1/2\\).\n\nSince the \\(\\theta\\) is smaller than \\(\\pi/2\\), then the angle acute which means the vectors are not very similar, but at least point in the same direction.\n\n\n\nIf we now let \\(\\mathbf{u} = (1,-2)\\) and \\(\\mathbf{v}=(-2,3)\\), then the angle is: \\(\\theta \\approx 0.95 \\pi \\,\\,rad\\). Almost \\(180^\\circ\\), very different vectors. The inner product by being negative already tells us that the vectors point in almost opposite directions.\nFor this \\(\\mathbb{R}^2\\) examples, the calculation of the angle or the cosine is rather useless because we can easily draw both vectors and check whether they are or not similar. A harder task is when we try to do this in three, four, etc dimensions.\n\n\n\nLet \\(\\mathbf{u} = (1,2)\\) and the unitary vector \\(\\mathbf{e}_v=(1,-1)/\\sqrt{2}\\):\n\nComputing the inner product two ways we find:\n\\[\n\\mathbf{e}_v\\cdot \\mathbf{u} = -\\frac{1}{\\sqrt{2}}=5\\cos \\theta\n\\]\nOn the rhs we see using trigonometry, the projection (shadow) of the vector \\(\\mathbf{u}\\) along the vector \\(\\mathbf{e}_v\\), which is this case is negative, implying the vectors point in different directions.\nThus the matrix \\(\\mathbf{v}^\\intercal/v\\) is a projector along \\(\\mathbf{v}\\).\n\n\n\nThis time we start with \\(\\mathbf{e}_x\\) and \\(\\mathbf{e}_x\\), the unitary vector \\((1,0)\\) or \\((0,1)\\), and keep the usual \\(\\mathbf{u}=(1,2)\\). The inner products yields:\n\\[\n\\begin{cases}\n\\mathbf{e}_x^\\intercal \\mathbf{u}=1\\\\\n\\mathbf{e}_y^\\intercal \\mathbf{u}=2\\\\\n\\end{cases}\n\\]\nMoreover:\n\\[\n\\begin{cases}\n\\mathbf{e}_x^\\intercal \\mathbf{u}=5\\cos\\theta_x\\\\\n\\mathbf{e}_y^\\intercal \\mathbf{u}=5\\cos\\theta_y=5 \\sin\\theta_x\n\\end{cases}\n\\]\nWhich means geometrically:\n\nDotting a vector with an unitary vector yields \\(5\\cos\\theta_x\\) and \\(5\\cos\\theta_y\\) which using trigonometry are the projections of \\(\\mathbf{u}\\) along those directions, which in turn is are the entries \\(1\\) and \\(2\\) of the vector along the unitary vectors.\nWe conclude that, given a basis \\(B=\\{\\mathbf{e}_x, \\mathbf{e}_y\\}\\) we can write the vector \\(\\mathbf{u}=(1,2)\\) as:\n\\[\n\\mathbf{u} = (\\mathbf{e}_x\\cdot\\mathbf{u})\\mathbf{e}_x+(\\mathbf{e}_y\\cdot\\mathbf{u})\\mathbf{e}_y= 1\\mathbf{e}_x+2 \\mathbf{e}_y\n\\]\nWhich means: \\([\\mathbf{u}]_B=(\\mathbf{e}_x\\cdot\\mathbf{u},\\mathbf{e}_y\\cdot\\mathbf{u})\\)\nExercises: 1.7.5",
    "crumbs": [
      "Bried Notes",
      "Linear Algebra",
      "Inner product and Orthogonal spaces"
    ]
  },
  {
    "objectID": "inner_product.html#how-to-define-a-basis-for-a-subspace-orthogonal-to-another-given-subspace",
    "href": "inner_product.html#how-to-define-a-basis-for-a-subspace-orthogonal-to-another-given-subspace",
    "title": "Inner Product",
    "section": "How to define a basis for a subspace orthogonal to another given subspace?",
    "text": "How to define a basis for a subspace orthogonal to another given subspace?\nEssentially what is being asked is to solve \\(U\\oplus U^\\perp=\\mathbb{R}^n\\) given \\(U\\). And since \\(U^\\perp\\) has all its vectors perpendicular to \\(U\\), it is the nullspace of a matrix whose rows are the basis of \\(U\\).\nExample: consider the subspace given by \\(U=span\\{(1,0,1),(1,2,0)\\}\\). Define a basis for \\(U^\\perp\\).\nIf we find the set of vectors perpendicular to these basis vectors, we found the set perpendicular to every element of this span. But finding this set is the same as solving Equation 1 ! Thus\n\\[ U^\\perp=[\\textbf{line}\\,\\perp\\, \\textbf{to}\\,\\, \\mathbf{n}_{1,2}] = \\{(-c,c/2,c)\\,\\,|,\\, c\\in \\mathbb{R}\\} \\]\nA basis for \\(U^\\perp\\) is for example \\((-1,1/2,1)\\). If we want it normalized we have \\((-1,1/2,1)/\\sqrt{(-1)^2+(1/2)^2+1^2}\\).\nExercise: 1.7.3",
    "crumbs": [
      "Bried Notes",
      "Linear Algebra",
      "Inner product and Orthogonal spaces"
    ]
  },
  {
    "objectID": "inner_product.html#four-very-important-subspaces",
    "href": "inner_product.html#four-very-important-subspaces",
    "title": "Inner Product",
    "section": "Four very important subspaces",
    "text": "Four very important subspaces\nFor a given matrix \\(A\\) with shape \\(n\\times m\\) we already defined the column space \\(C(A)\\) which is a subspace of \\(\\mathbb{R}^n\\) and the nullspace \\(N(A)\\) which is a subspace of \\(\\mathbb{R}^m\\). Recall the matrix\n\\[\nA=\\begin{pmatrix}1 & 2 & 2 & 2 \\\\2 & 4 & 6 & 8 \\\\3 & 6 & 8 & 10 \\end{pmatrix}\n\\]\nThe column space is the first and third column (because they are independent); its dimension is the number of pivots, in this case \\(2\\); it is a subspace of \\(\\mathbb{R}^3\\).\n\\[\nC(A)=span\\{\\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix},\\begin{pmatrix}2\\\\6\\\\8\\end{pmatrix}\\}\n\\]\nWhile the nullspace was obtained by solving \\(A\\mathbf{x}_N=\\mathbf{0}\\), its dimension is the number of dependent columns, which is the number of columns \\(m=4\\) minus the number of pivots \\(r=2\\) (number of independent columns); it is a subspace of \\(\\mathbb{R}^4\\).\n\\[\nN(A) = span\\{(-2,1,0,0),(2,0,-2,1)\\}\n\\]\nNow we’ll introduce their orthogonal complements in the \\(\\mathbb{R}^3\\) and \\(\\mathbb{R}^4\\) respectively.\n\nLeft Nullspace\n\\(C(A)^\\perp\\) is the set of all vectors perpendicular to every vector in \\(C(A)\\). It is the nullspace of \\(A^\\intercal\\), aka, left nullspace \\(N(A^\\intercal)\\).\nWhy is this so?\n\n\n\nRow space\n\\(N(A)^\\perp\\) is the set of all vectors perpendicular to every vector in \\(N(A)\\). It is the column space of \\(A^\\intercal\\), aka, row space.\nWhy is this so?\n\n\n\nObservations:\nWe know then:\n\\[\n\\mathbb{R}^n = C(A) \\oplus N(A^\\intercal)\\qquad \\mathbb{R}^m=C(A^\\intercal)\\oplus N(A)\n\\]\nSince \\(\\dim C(A) = \\dim C(A^\\intercal) =r\\), \\(\\dim N(A) =n-r\\) and \\(\\dim N(A^\\intercal) =m-r\\) we can verify that:\n\\[\nn = \\dim C(A) + \\dim N(A^\\intercal) \\qquad m = \\dim C(A^\\intercal)+\\dim N(A)\n\\]\nPictorially we find:\n\nAny vector \\(\\mathbf{x}\\) lives in \\(\\mathbb{R}^m\\), it has as many entries as there are columns in the matrix it multiplies. Since this space is broken down into two complements \\(\\mathbb{R}^m=C(A^\\intercal)\\oplus N(A)\\) we can write \\(\\mathbf{x}=\\mathbf{x}_\\text{row}+\\mathbf{x}_\\text{null}\\), the entries of each being in the canonical basis. We can see now why finding a particular solution we chose the free variables as zero. In that way, the particular solution lives solely in the row space (though it need not to)\nWhat happens when we multiply \\(\\mathbf{x}\\) by \\(A\\)? Answer: We “project” \\(\\mathbf{x}\\) written in the canonical basis into each row the row space (we use “project” because, the inner product is being made with most probably non orthogonal and non normalized rows), the components of that projection is the vector \\(\\mathbf{b}\\). The same “projection” would be obtained if we use \\(\\mathbf{x}_\\text{row}\\) instead, since this is written in the canonical basis, we just compute its components (times the row length) along each row.",
    "crumbs": [
      "Bried Notes",
      "Linear Algebra",
      "Inner product and Orthogonal spaces"
    ]
  },
  {
    "objectID": "how_many_solutions.html",
    "href": "how_many_solutions.html",
    "title": "One, none or many solutions?",
    "section": "",
    "text": "With the above discussion understood we have the following cases:",
    "crumbs": [
      "Bried Notes",
      "Linear Algebra",
      "Things you can do with matrices",
      "How many solutions a system has?"
    ]
  },
  {
    "objectID": "how_many_solutions.html#subspaces",
    "href": "how_many_solutions.html#subspaces",
    "title": "One, none or many solutions?",
    "section": "Subspaces",
    "text": "Subspaces",
    "crumbs": [
      "Bried Notes",
      "Linear Algebra",
      "Things you can do with matrices",
      "How many solutions a system has?"
    ]
  },
  {
    "objectID": "how_many_solutions.html#dot-product",
    "href": "how_many_solutions.html#dot-product",
    "title": "One, none or many solutions?",
    "section": "Dot product",
    "text": "Dot product\nFalar novamente dos subespaços do secundário, que já mensionei acima na sec dos subespaços, mas desta vez dar usar a equação com produto interno tal como fazes nos teus resumos de GA do 11o ano.",
    "crumbs": [
      "Bried Notes",
      "Linear Algebra",
      "Things you can do with matrices",
      "How many solutions a system has?"
    ]
  },
  {
    "objectID": "how_many_solutions.html#transpose-of-a-matrix",
    "href": "how_many_solutions.html#transpose-of-a-matrix",
    "title": "One, none or many solutions?",
    "section": "Transpose of a matrix",
    "text": "Transpose of a matrix",
    "crumbs": [
      "Bried Notes",
      "Linear Algebra",
      "Things you can do with matrices",
      "How many solutions a system has?"
    ]
  },
  {
    "objectID": "how_many_solutions.html#rank-of-a-matrix",
    "href": "how_many_solutions.html#rank-of-a-matrix",
    "title": "One, none or many solutions?",
    "section": "Rank of a matrix",
    "text": "Rank of a matrix",
    "crumbs": [
      "Bried Notes",
      "Linear Algebra",
      "Things you can do with matrices",
      "How many solutions a system has?"
    ]
  },
  {
    "objectID": "how_many_solutions.html#basis",
    "href": "how_many_solutions.html#basis",
    "title": "One, none or many solutions?",
    "section": "Basis?",
    "text": "Basis?",
    "crumbs": [
      "Bried Notes",
      "Linear Algebra",
      "Things you can do with matrices",
      "How many solutions a system has?"
    ]
  },
  {
    "objectID": "how_many_solutions.html#matrices-as-linear-operators",
    "href": "how_many_solutions.html#matrices-as-linear-operators",
    "title": "One, none or many solutions?",
    "section": "Matrices as linear Operators?",
    "text": "Matrices as linear Operators?",
    "crumbs": [
      "Bried Notes",
      "Linear Algebra",
      "Things you can do with matrices",
      "How many solutions a system has?"
    ]
  },
  {
    "objectID": "how_many_solutions.html#four-subspaces-of-a-matrix",
    "href": "how_many_solutions.html#four-subspaces-of-a-matrix",
    "title": "One, none or many solutions?",
    "section": "Four subspaces of a matrix",
    "text": "Four subspaces of a matrix\n\\[\n\\begin{align}\n&\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| &b_1\\\\\n2 & 4 & 6 & 8 &\\bigm| &b_2\\\\\n3 & 6 & 8 & 10 &\\bigm| &b_3\n\\end{pmatrix}\n\\overset{l_2' = l_2-2l_1}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| &b_1\\\\\n0 & 0 & 2 & 4 &\\bigm| &b_2-2b_1\\\\\n3 & 6 & 8 & 10 &\\bigm| &b_3\n\\end{pmatrix}\\\\\n&\\overset{l_3' = l_3-3l_1}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| &b_1\\\\\n0 & 0 & 2 & 4 &\\bigm| &b_2-2b_1\\\\\n0 & 0 & 2 & 4 &\\bigm| &b_3-3b_1\n\\end{pmatrix}\n\\overset{l_3'=l_3-l_2}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| &b_1\\\\\n0 & 0 & 2 & 4 &\\bigm| &b_2-2b_1\\\\\n0 & 0 & 0 & 0 &\\bigm| &b_3-b_2-b_1\n\\end{pmatrix}\\\\\n&\\overset{l_2'=1/2l_2}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| &b_1\\\\\n0 & 0 & 1 & 2 &\\bigm| &b_2/2-b_1\\\\\n0 & 0 & 0 & 0 &\\bigm| &b_3-b_2-b_1\n\\end{pmatrix}\n\\overset{l_1'=l_1-2l_2}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 0 & -2 &\\bigm| &3b_1-b_2\\\\\n0 & 0 & 1 & 2 &\\bigm| &b_2/2-b_1\\\\\n0 & 0 & 0 & 0 &\\bigm| &b_3-b_2-b_1\n\\end{pmatrix}\n\\end{align}\n\\]",
    "crumbs": [
      "Bried Notes",
      "Linear Algebra",
      "Things you can do with matrices",
      "How many solutions a system has?"
    ]
  },
  {
    "objectID": "functions-introduction.html",
    "href": "functions-introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nIt is remarkable how the world we see around us can be understood with two simple concepts: the concept of set and the concept of function. These are two fundamental and natural ways of organizing the information we gather from the world.\nSets are essentially lists, with them we can conveniently gather things that share some common property: for example the set of your friends, the set of videos you liked in the last month, the set of fruits in your local supermarket, the set of letters we use to write this phrase. With our ability to group things together allow us to start thinking about the sets themselves rather than their elements, liberating us of mental “ram” memory to focus on other tasks.\nGrouping things is not enough to organize information. Because things in the world interact, we must have some way to describe that interaction and the dependencies that emerge. The concept of relation is essential in that description; by relation we mean a way to connect elements of distinct sets: The sets of fruits and the set of positive real numbers are related, either by the label they carry or their price; the set of fruits and the set of friends are related, because each one prefer some fruits over other fruits; the set of letter in the alphabet and the set of fruit is also related, just let each fruit be assigned a letter, say the first in their name.\nIn particular we will focus on a special kind of relations, namely functions, in Sec. \\(\\ref{sec:asarelation}\\) we introduce why are functions a particular case of relations. In Sec. \\(\\ref{sec:4ways}\\) we will show how a function can be manifested as a list of pairs, a diagram, a table or a graph; all equivalent points of view, with theirs advantages and disadvantages. Alternatively we can view functions as a procedure that transforms (map) elements of one set into the elements of the other, we introduce this notion in Sec. \\(\\ref{sec:asaprocedure}\\), which is also the most difficult. The relation and procedure points of view on functions are equivalent, we will stress that in Sec. \\(\\ref{sec:equiv}\\).\nThe great diversity in ways we can connect elements of two sets shows in diversity of functions we will encounter in this chapter and others, it will turn out to be important to classify theirs global behavior as , or , because some functions are better for describing certain aspects of the world than others. These notions are introduced in Sec. \\(\\ref{sec:sib}\\).\nFinally, if we can assign an element of a given set to another element of a different set, then we can assign many, this “super assignment” called the image of a set is introduced in Sec. \\(\\ref{sec:defs}\\) with some special examples which will be useful in later chapters; additionally we introduce in this section the reverse of this super assignment, the so called pre-image of a set.",
    "crumbs": [
      "Bried Notes",
      "Basic Maths",
      "Functions",
      "Introduction"
    ]
  },
  {
    "objectID": "exer_withperequisites.html",
    "href": "exer_withperequisites.html",
    "title": "Exercise",
    "section": "",
    "text": "In this exercise we’ll make sure you understand the statement \\(\\forall x : \\exists ! y :(x,y)\\in F\\). There are two key perquisites\n\nFirst, consider \\(x\\in\\{a,i,o\\}\\) and \\(A(x)\\) an \\(x\\)-dependent proposition. Then the generic statement \\(\\forall x : A(x)\\) is a compact version of \\(A(a)\\land A(b)\\land A(c)\\), we write:\n\\[ \\begin{equation}\\forall x : A(x) \\iff A(a)\\land A(i)\\land A(o)\\end{equation} \\]\nFor some propositions \\(A\\) this statement is true for other it will be false, for example: \\(A(x)\\coloneqq [\\text{$x$ is not $z$}]\\) makes \\(\\forall x : A(x)\\) true, while \\(A(x)\\coloneqq [\\text{$x$ is below $p$}]\\) makes it false. Give one example of an \\(A(x)\\) that makes (\\(\\ref{eq:thestat2}\\)) true and other that makes it false.\nSecond, we have the following equivalence:\n\\[ \\begin{equation}\\exists x : A(x) \\iff A(a) \\lor A(i) \\lor A(o)\\end{equation} \\]\nConsider \\(x\\in \\{a,b,c\\}\\). When \\(A(x)\\coloneqq [\\text{$x$ is $d$}]\\) the statement is false, but when \\(A(x)\\coloneqq [\\text{$x$ is a vowel}]\\) it is true; moreover since there are two vowels we have:\n\\[ \\begin{equation}A(a)\\lor A(i)\\lor A(o) \\longrightarrow T \\lor T \\lor F\\end{equation} \\]\nThere are two true proposition in that string of \\(or\\)’s. Give examples of \\(A(x)\\) that make it true and other false.\nThird, \\(\\exists ! x : A(x)\\) can only be true when either \\(A(a)\\) or \\(A(i)\\) or \\(A(o)\\) is true, exclusively:\n\\[ \\begin{equation}\\exists ! x : A(x) \\iff A(a)\\dot{\\lor} A(i)\\dot{\\lor} A(o)\\end{equation} \\]\nThat means that \\(A(x)\\coloneqq [\\text{$x$ is a vowel}]\\) no longer makes the statement true, but \\([\\text{$x$ is a consonat}]\\) will make it true. Again, give examples that make the statement true and false.\n\nWith these prerequisites at hand all we have to do is to see the statement \\(\\forall x : \\exists ! y :(x,y)\\in F\\) as \\(\\forall x : B(x)\\) where \\(B(x)\\coloneqq [\\exists ! y :(x,y)\\in F]\\). Let \\(x\\in \\{a,b,c\\}\\) and \\(y\\in\\{1,2\\}\\). Unpack the meaning of \\(B(x)\\) for each \\(x\\) using \\((c)\\) and the then the meaning of \\(\\forall x : B(x)\\) using prerequisite \\((a)\\), you should get a statement with only one unknown, which is \\(F\\).\nNow, consider these possible \\(F\\)’s:\n\n\\(\\{(a,1),(b,2)\\}\\)\n\\(\\{(a,1),(b,1),(c,1)\\}\\)\n\\(\\{(a,1),(b,1),(c,1),(a,2)\\}\\)\n\\(\\{(c,2)\\}\\)\n\nFor which ones the statement \\(\\forall x : \\exists ! y :(x,y)\\in F\\) is true and for which ones it is false, if any.\nSolution to exer:withperequisites"
  },
  {
    "objectID": "Example_of_pol_division_4.html",
    "href": "Example_of_pol_division_4.html",
    "title": "Example_of_pol_division_4",
    "section": "",
    "text": "The elementary school algorithm we seen above is just one of many ways in which we can compute the division os \\(1001\\) by \\(12\\). Lets follow, this time, a slightly different path.\nRecall the main question: What is the solution \\(q\\) and \\(r\\) of the equation:\n\\[ 1001 = q \\times 12 + r \\qquad 0\\leq r&lt;12 \\]\nThere are many paths and depending on the quality of our guesses for \\(q\\) the number of steps is different.\nThe shortest path is to immediately guess the answer, just one step, though a difficult one. We would have to guess just glancing ?@eq-question that \\(q=83\\) and \\(r=5\\). That is hard or you are luck.\nLets decompose \\(1001\\) into powers of \\(10\\) to aid our guesses (other decomposition are possible, we seen another in the elementary school’s approach; the powers of \\(10\\) is the basic one because it is exactly what the symbol \\(1001\\) means)\n\\[ 1001 = 1\\cdot 10^3+0\\cdot 10^2 +0\\cdot 10^1+1\\cdot 10^0 = 1\\cdot 10^3 +1\\cdot 10^0 \\]\nNoting \\(12=10+2\\), lets guess the solution for:\n\\[ 1\\cdot 10^3 +1 = q\\cdot(10+2)+r \\]\nGuess: \\(q=8\\cdot 10\\) guarantees that \\(q\\cdot 10\\) is close to \\(1\\cdot 10^3\\) (notice \\(8\\) is almost \\(10\\))\nSubstitution gives:\n\\[ \\begin{align} &1\\cdot 10^3 +1 = 8\\cdot 10^2 +16 \\cdot 10 + r\\\\ \\implies&1\\cdot 10^3 +1 = 9\\cdot10^2+6\\cdot10+r\\\\ \\implies & r=1\\cdot 10^2-6\\cdot 10+1 \\\\ \\implies&r=4\\cdot10+1 \\end{align} \\]\nThe remainder is positive and larger than the divisor \\(12\\) so we keep going."
  },
  {
    "objectID": "Example_of_pol_division_4.html#example4",
    "href": "Example_of_pol_division_4.html#example4",
    "title": "Example_of_pol_division_4",
    "section": "",
    "text": "The elementary school algorithm we seen above is just one of many ways in which we can compute the division os \\(1001\\) by \\(12\\). Lets follow, this time, a slightly different path.\nRecall the main question: What is the solution \\(q\\) and \\(r\\) of the equation:\n\\[ 1001 = q \\times 12 + r \\qquad 0\\leq r&lt;12 \\]\nThere are many paths and depending on the quality of our guesses for \\(q\\) the number of steps is different.\nThe shortest path is to immediately guess the answer, just one step, though a difficult one. We would have to guess just glancing ?@eq-question that \\(q=83\\) and \\(r=5\\). That is hard or you are luck.\nLets decompose \\(1001\\) into powers of \\(10\\) to aid our guesses (other decomposition are possible, we seen another in the elementary school’s approach; the powers of \\(10\\) is the basic one because it is exactly what the symbol \\(1001\\) means)\n\\[ 1001 = 1\\cdot 10^3+0\\cdot 10^2 +0\\cdot 10^1+1\\cdot 10^0 = 1\\cdot 10^3 +1\\cdot 10^0 \\]\nNoting \\(12=10+2\\), lets guess the solution for:\n\\[ 1\\cdot 10^3 +1 = q\\cdot(10+2)+r \\]\nGuess: \\(q=8\\cdot 10\\) guarantees that \\(q\\cdot 10\\) is close to \\(1\\cdot 10^3\\) (notice \\(8\\) is almost \\(10\\))\nSubstitution gives:\n\\[ \\begin{align} &1\\cdot 10^3 +1 = 8\\cdot 10^2 +16 \\cdot 10 + r\\\\ \\implies&1\\cdot 10^3 +1 = 9\\cdot10^2+6\\cdot10+r\\\\ \\implies & r=1\\cdot 10^2-6\\cdot 10+1 \\\\ \\implies&r=4\\cdot10+1 \\end{align} \\]\nThe remainder is positive and larger than the divisor \\(12\\) so we keep going."
  },
  {
    "objectID": "Example_of_pol_division_4.html#commentary",
    "href": "Example_of_pol_division_4.html#commentary",
    "title": "Example_of_pol_division_4",
    "section": "Commentary",
    "text": "Commentary\nNote that a guesses \\(q=9\\cdot 10\\) or \\(q=10^2\\) would lead to a negative remainders. These two guesses would be fine since we only need the final remainder to be positive, not the intermediate ones. Our choice of \\(q=8\\) is motivated by convenience.\nWe have to solve:\n\\[ 4\\cdot 10+1 = q'\\cdot(10+2)+r' \\]\nfor \\(q'\\) and \\(r'\\), because \\(41\\) is larger than \\(12\\) and thus still divisible by \\(12\\), or said in another way, \\(41\\) is not the smallest remainder (which must be between \\(0\\) and \\(12\\)) we can get.\nGuess’: \\(q'=3\\)\nSubstitute:\n\\[ \\begin{align} &4\\cdot 10+1 = 3\\cdot 10 + 6 + r'\\\\ \\implies &r'=1\\cdot 10-6+1 = 4+1=5 \\end{align} \\]\nFrom ?@eq-step1II and ?@eq-step2II we conclude that:\n\\[ 1001=(8\\cdot 10+3)\\cdot 12+5 \\]\nwhich simplifies into:\n\\[ 1001=83\\cdot 12+5 \\]\nEXERCISE: Making your initial guess \\(q=10^3\\) and arrive at this result.\nSOLUTION:\nIn this solution we present the steps to solve the problem in two parallel ways: using guesswork to solve the equations and the table-like approach (see the pictures).\nThe basic problem at hand can be presented in the following fashion:\n\nGuess: \\(q=10^2\\) so that \\(q\\cdot (10+2)\\) has a \\(10^3\\) term to cancel the \\(1\\cdot 10^3\\) from \\(1001\\).\nThis basic step can be presented as:\n\nSubstituting we find:\n\\[ \\begin{align} &1\\cdot 10^3 + 1 = 10^2(10+2)+r\\\\ \\implies & r=-2\\cdot 10^2+1 \\end{align} \\]\nWhich is represented by:\n\nThe current remainder is negative but is still divisible by \\(12\\) (given it is larger than \\(12\\)):\n\\[ -2\\cdot 10^2+1 = q'\\cdot (10+2) +r' \\]\nGuess’: \\(q' = -2\\cdot 10\\) so that the \\(-2\\cdot 10^2\\) is canceled on the lhs.\nThis guess step can be represented as:\n\nSubstitute:\n\\[ \\begin{align} &-2\\cdot 10^2 +1 = -2\\cdot 10^2 -4\\cdot 10 +r'\\\\ \\implies & r' = 4\\cdot 10 +1 \\end{align} \\]\nPresented as:\n\nSince the remainder \\(4\\cdot 10+1\\) is divisible by \\(12\\) we write:\n\\[ 4\\cdot 10 +1 = q''\\cdot (10+2)+r'' \\]\nwhich leads to our next guess.\nGuess’’: \\(q''=4\\) to cancel the lhs \\(4\\cdot 10\\)\nSubstitution:\n\\[ \\begin{align} &4\\cdot 10 +1 =4\\cdot 10+8 + r''\\\\ \\implies& r'' = -7 \\end{align} \\]\nShow in:\n\nCollecting terms:\n\\[ 1001=(10^2-2\\cdot 10+4)\\cdot 12 -7 \\]\nOur remainder in negative, hence add \\(0=12-12\\) to the rhs:\n\\[ 1001=(10^2-2\\cdot 10+4-1)\\cdot 12+(12-7) \\]\nIn conclusion:\n\\[ 1001=83\\cdot 12 +5 \\]"
  },
  {
    "objectID": "examples_of_pol_part_4.html",
    "href": "examples_of_pol_part_4.html",
    "title": "Second order polynomials are parabolas",
    "section": "",
    "text": "The relation definitions of these types of polynomials is:\n\\[\n\\{(x,y)\\in\\mathbb{R}^2\\,\\,|\\,\\, y = a_2 x^2+a_1 x+a_0\\}\n\\tag{1}\\]\nfor some real parameters \\(a_0\\), \\(a_1\\) and \\(a_2\\), having \\(a_2 \\not = 0\\) guarantees this is a second order polynomial.\nHere are some choices of parameters and the corresponding graphs:\n\n\n\n\n\n\n\n\n\nThe graph above exhibits the windows on the set Equation 1 where interesting behavior of these functions occur, by interesting, I mean, we see see the x-axis and y-axis intersections as well as minima and maxima; outside of the picture, these function either increase or decrease monotonically toward infinity, that’s uninteresting since nothing else happens there.\nTo know these function even better it is useful to compute by hand (analytically) some important characteristics:\n\ny-axis intercept: we want to know the coordinates on Equation 1 of the form \\((0,y_0)\\).\n\\[\n(0,y_0)\\in \\{(x,y)\\in\\mathbb{R}^2\\,\\,|\\,\\, y = a_2 x^2+a_1 x+a_0\\} \\iff y = a_2 \\times 0 +a_1\\times 0+ a_0 \\iff y=a_0\n\\]\nThus the point \\((0,a_0)\\) is the interception of the polynomial with the vertical axis.\nx-axis intercept: seeking point of the form \\((x_0,0)\\) requires us to solve the equation\n\\[\n(x_0,0)\\in \\{(x,y)\\in\\mathbb{R}^2\\,\\,|\\,\\, y = a_2 x^2+a_1 x+a_0\\}\\iff 0 = a_2 x_0^2+a_1 x_0+a_0\n\\]\nTo solve the second order equation we may use the Resolvent Formula if a solution exists. More on this later.\nwhen \\(x\\) is large (far from \\(1\\)): the process of progressively increasing \\(x\\) and seeing what happens to the corresponding \\(y\\)’s.\nThe idea is to “solve” the dynamical problem:\n\\[\n\\begin{cases}y = a_2 x^2+a_1 x+a_0\\\\\nx\\longrightarrow \\pm\\infty \\end{cases}\n\\tag{2}\\]\nby computing how \\(y\\) behaves when \\(x\\) increases without bound.\nWe solve this problem using our knowledge of how the atomic polynomials behave individually, see the last ordering relation in ?@eq-relations_of_atoms. We know that when \\(x\\) is far from \\(1\\) we must have:\n\\[\na_0 \\ll a_1 x \\ll a_2x^2\n\\]\nSince \\(a_2x^2\\) term is much larger than \\(a_1 x\\) and \\(a_0\\), we can drop them from the equation \\(y=a_2x^2+a_1 x+a_0\\) , doing so giving us an approximate equation, \\(y \\sim a_2 x^2\\), which is simple and thus easy to graph. The graph of both functions is very similar.\n\n\n\nExact equation for all \\(x\\)\nApproximate equation when \\(x\\) is large\n\n\n\n\n\\(y = a_2 x^2+a_1 x+a_0\\)\n\\(y \\sim a_2 x^2\\)\n\n\n\nThat means, if you know how to solve\n\\[\n\\begin{cases}y \\sim a_2 x^2\\\\x\\longrightarrow \\pm\\infty \\end{cases}\n\\tag{3}\\]\nyou know how to solve Equation 2 . But Equation 3 is easy, since it is just an atomic polynomial: if \\(x\\) gets larger and larger then \\(y\\) gets larger and larger, the solution is \\(y\\longrightarrow +\\infty\\), and we conclude the same thing must happen in Equation 2. Problem solved, but solving a similar but simpler problem!",
    "crumbs": [
      "Bried Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Second order polynomials"
    ]
  },
  {
    "objectID": "examples_of_pol_part_2.html",
    "href": "examples_of_pol_part_2.html",
    "title": "Molecular Polynomials",
    "section": "",
    "text": "Molecular Polynomials\nCombining atomic polynomials through linear combinations we obtain molecular polynomials.\n\n\n\n\n\n\nRecall\n\n\n\nA linear combination is an expression where things are multiplied by constants and then added. Here is an example\n\\[\n4x^3+5x+2\n\\]\nThe constants \\(4\\), \\(5\\) and \\(2\\) multiply \\(x^3\\), \\(x^1\\) and \\(x^0\\); which are then summed. Notice how \\(x^0\\) is hidden, to find it, remember that \\(2 =2\\times 1\\) and \\(1=x^0\\).",
    "crumbs": [
      "Bried Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Molecular Polynomials"
    ]
  },
  {
    "objectID": "examples_of_pol_part_1.html",
    "href": "examples_of_pol_part_1.html",
    "title": "Atomic Polynomials",
    "section": "",
    "text": "The simplest polynomial involve procedures of the form \\(1\\), \\(x\\), \\(x^2\\), \\(x^3\\), etc. These functions are given by\n\\[\n\\begin{align}\ne_n:\\,\\, \\mathbb{R} &\\longrightarrow\\mathbb{R}\\\\x &\\longmapsto e_n(x):= x^n\n\\end{align}\n\\tag{1}\\]\nfor an integer \\(n\\) of our choice. The function Equation 1 can also be given under the relation notation by:\\[\nE_n:= \\{(x,y)\\in\\mathbb{R}^2\\,\\,|\\,\\, y=x^n\\}\n\\tag{2}\\]\nThe graphical version of these functions for \\(n=0,1,2,3\\) is:\nThe graph shows the slice of the functions \\(E_n\\) where \\(x\\) runs from \\(-1.5\\) up to \\(1.5\\) and not the whole function, which according to Equation 1 or Equation 2 has the domain ranging on all reals \\(\\mathbb{R}\\). The slice was chosen because it exhibits interesting part of the function: the region where it intercept the axes (the zeros of the function) and shows maxima/minima.",
    "crumbs": [
      "Bried Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Atomic Polynomials"
    ]
  },
  {
    "objectID": "examples_of_pol_part_1.html#properties-of-the-atomic-polynomials",
    "href": "examples_of_pol_part_1.html#properties-of-the-atomic-polynomials",
    "title": "Atomic Polynomials",
    "section": "Properties of the atomic polynomials",
    "text": "Properties of the atomic polynomials\nRather than showing a picture of a small slice of the function as de did in Figure 1 we can instead zoom into some aspects of it.\n\nProperties of the connectivity between domain and codomain:\n\nRange: From inspection of Equation 1 (or Equation 2) we know the domain and codomain of these four functions is \\(\\mathbb{R}\\). The range of these functions is the image of the domain under the action of the function, we can infer what this image is by inspecting Figure 1, for example, the image of \\(\\mathbb{R}\\) under the action of \\(e_2\\) is the set of all positive reals as we check by the red curve hence we write \\(e_2(\\mathbb{R})=\\mathbb{R}^+_0\\). Similarly we find:\n\n\\[\ne_0(\\mathbb{R})=\\{1\\} \\qquad e_1(\\mathbb{R})=e_3(\\mathbb{R})=\\mathbb{R}\\qquad e_2(\\mathbb{R})=\\mathbb{R}^+_0\n\\]\n\nOnto: We also observe that for even powers of \\(n\\) such as \\(e_0\\) and \\(e_2\\), not all elements in the codomain \\(\\mathbb{R}\\) are hit by outputs, i.e., these are not onto function; as an example, [is \\(0\\) hit by some \\(e_0(x)\\) ? Is \\(-1\\) hit by \\(e_2(x)\\), for some \\(x\\) in its domain?] We can answer these question by trying to solve the equations \\(e_0(x)=0\\) and \\(e_2(x)=-1\\). To no avail! Neither have solutions \\(x\\) in the domain \\(\\mathbb{R}\\).\n1-1: Additionally nor are these even powers 1-1. Why? Because we can clearly see from Figure 1 that \\(e_0\\) is in fact the worst possible “labeller”, where all “fruits” \\(x\\) have exactly the same label \\(1\\), no other “label” in its codomain \\(\\mathbb{R}\\) is put into good use by this \\(e_0\\); the function \\(e_2\\) is not as bad since for each label we find just two elements in \\(\\mathbb{R}\\). The functions \\(e_1\\) and \\(e_3\\) are 1-1 since each element \\(x\\) in their domain have a unique element \\(y\\) in the codomain, hence if we choose a label \\(y\\) we uniquely know the \\(x\\). Summarizing:\n\\[\n\\begin{cases}e_0 \\qquad &\\text{worst possible labeler (one label for all inputs)}\\\\e_2 \\qquad  &\\text{bad labeller (one label for each pair of inputs)}\\\\e_1\\,\\, \\text{and}\\,\\, e_3 \\qquad &\\text{perfect (one label, one input)}\\end{cases}\n\\]\n\n\n\nProperties at interesting regions of the function:\n\nWho is bigger or smaller? The Figure 1 reveals something funny:\n\nfor points on the horizontal axis close to the origin, that is \\(-1&lt;x&lt;1\\), the outputs of the four functions have the following rank \\(x^3 &lt; x^2 &lt; x^1 &lt; x^0\\);\nprecisely at \\(x=1\\) we find they are all the same \\(x^0=x^1=x^2=x^3=1\\);\nmeanwhile for the remaining points, those in the region \\(x&lt;-1 \\lor 1&lt;x\\), the absolute value of these functions follow the reversed rank! \\(x^0&lt;x^1&lt;x^2&lt;x^3\\).\nFor \\(x\\) very far from \\(1\\) we find \\(x^0\\ll x^1\\ll x^2\\ll x^3\\), which means, for example, that \\(|x^3|\\) is much, much, much, much larger then \\(|x^2|\\), and so on.\n\nIn summary:\n\\[\n\\begin{cases}\nx^3 &lt; x^2 &lt; x^1 &lt; x^0 \\qquad &\\text{for $x$ close to 1}\\\\x^0=x^1=x^2=x^3 \\qquad &\\text{for $x=1$}\\\\x^0\\ll x^1\\ll x^2\\ll x^3 \\qquad &\\text{for $x$ far from $1$}\n\\end{cases}\n\\tag{3}\\]\nThis summary will be important when we study the limiting behavior of molecular polynomials.\nZeros: The zeros of a function are the coordinates of the points which intercept the axes. Observing Figure 1 we see polynomial \\(1\\) intercepts the y-axis at \\((0,1)\\) while all other polynomials intercept the x-axis and y-axis exactly at the origin \\((0,0)\\).\nMaxima and minima: By inspection of Figure 1, we observe the polynomials \\(1\\) has no maxima or minima, it is always flat; \\(x\\) and \\(x^3\\) on the other hand is a ramp, a never ending one, and thus also does not have a maximum or minimum. The only function exhibiting a minimum is \\(x^2\\), coincidentally, at \\((0,0)\\). Only when we study derivatives we’ll have the computational means to find these extrema.\n\n\n\n\n\n\n\nCommentary\n\n\n\nThe previous list of properties was already embedded in Figure 1 (or if you think about it, already at Equation 2), only by close observation we made it explicit. Of course, the list is not exhaustive, many other properties remain hidden in that picture, waiting to be dug out. This is why I said this list are a zoom into the picture, when we get closer we see more detail.\nAs additional examples we could also ask how fast a polynomial of degree one increases, i.e., what is its slope; for polynomials of degree two it would make sense to ask if it its graph is a cup or a cap, it all depends on \\(a_2\\) parameter.\nWe usually do not list the properties just as we did above, we just acknowledge in our mind that they exist and compute them as needed, since many times some of them are so bluntly obvious just by looking at the graph or the function formula, that no need is required for a calculation.",
    "crumbs": [
      "Bried Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Atomic Polynomials"
    ]
  },
  {
    "objectID": "examples_of_functions.html",
    "href": "examples_of_functions.html",
    "title": "Examples_of_functions",
    "section": "",
    "text": "Examples of functions\nSome examples illustrate again Definition 1 in practice:\n\nExample 1 Let \\(X=\\{a,b,c\\}\\) and \\(Y=\\{1,2\\}\\). All possible pairs are listed in the cartesian product \\(X \\times Y=\\{(a,1),(a,2),(b,1),(b,2),(c,1),(c,2)\\}\\). Relations and functions are subsets of the cartesian product, give some examples.\n\n\nSolution 1. A special subset of these pairs is the function \\(F\\coloneqq \\{(a,1),(b,1),(c,2)\\}\\). The domain of this function is \\(\\{a,b,c\\}\\) since these are all the \\(x\\)’s in \\(F\\); the codomain is \\(Y\\) and the range which contains all \\(y\\)’s of \\(F\\) is \\(R_F=\\{1,2\\}\\). Another subset of \\(X \\times Y\\) could be \\(G\\coloneqq \\{(a,1),(b,1),(c,2),(a,2)\\}\\), it is not special since the element \\(a\\) is associated with two distinct elements \\(1\\) and \\(2\\), thus appearing twice, rendering Equation 3 a false statement. This subset is not a function, but it is a relation whose domain is \\(\\{a,b,c\\}\\) and range \\(\\{1,2\\}\\).\nThe list \\(H\\coloneqq\\{(a,1),(b,1)\\}\\) is a function whose domain is \\(\\{a,b\\}\\) which is a subset of \\(X\\) and range is just \\(\\{1\\}\\) which is a subset of the codomain \\(Y\\). We see that not every element of \\(X\\) or \\(Y\\) have to be used in defining a function.\n\n\nExample 2 Now consider the set of natural numbers \\(\\mathbb{N}\\) and the set of all pairs \\(\\mathbb{N}\\times \\mathbb{N}\\). Illustrate examples of functions.\n\n\nSolution 2. The special subset \\(G\\coloneqq \\{(9,1),(4,4),(1,9)\\}\\) is a function where the domain and range are the same set \\(D_G=R_G=\\{1,4,9\\}\\). The sets \\(\\{(1,1),(2,1),(3,1)\\}\\) and \\(\\{(3,1),(5,1),(13,1)\\}\\) are also functions, but \\(\\{(1,1),(1,2),(3,1)\\}\\) is not.\n\n\nExample 3 Consider now the set \\(\\mathbb{R}\\times \\mathbb{R}\\). The truth set of the statement \\(y=x^2\\) is the subset of all pairs \\((x,y)\\) that make it true, formally we write \\(\\{(x,y)\\in \\mathbb{R}\\times \\mathbb{R}\\,|\\,\\,y=x^2\\}\\). This list of pairs is special since for every real \\(x\\) we choose, the calculation \\(x^2\\) yields a single number, which is always positive or zero. What is the domain, codomain and range?\n\n\nSolution 3. The pairs of this set come from the cartesian product \\(\\mathbb{R}\\times \\mathbb{R}\\), the proposition \\(y=x^2\\) does not put any restriction on the choice of \\(x\\), thus \\(x\\) can take any real value, meaning the domain is \\(\\mathbb{R}\\). On the other hand, this proposition demands that \\(y\\) be positive or zero, because \\(x^2\\) it is for any \\(x\\) in the domain, as a result the range is just the positive reals or zero \\(\\mathbb{R}^+_0\\).\n\n\nExercise 1 Consider the sets \\(A\\coloneqq \\{(x,y) \\in \\mathbb{R}\\times\\mathbb{R}\\,|\\,\\, y=1\\}\\) and \\(B\\coloneqq \\{(x,y) \\in \\mathbb{R}\\times \\mathbb{R}\\,|\\,\\, x=1\\}\\). Convince yourself whether it is or not a function using the Definition 1. For both specify the domain, range and codomain. What about \\(B'\\coloneqq \\{(x,y) \\in \\mathbb{R}\\times \\{2\\}\\,|\\,\\, x=1\\}\\)? Solution\n\n\nExercise 2 With \\(X\\coloneqq \\{\\alpha,\\beta,\\gamma,\\delta\\}\\) and \\(Y\\coloneqq \\{1,2,3,4\\}\\) create relations, two that are not functions and two that are. Make sure you create function, which use all \\(X\\) and others that do not. Solution"
  },
  {
    "objectID": "einstein_model_of_solid.html#classical-model",
    "href": "einstein_model_of_solid.html#classical-model",
    "title": "Einstein model for a solid",
    "section": "Classical Model",
    "text": "Classical Model\nWhat is the total energy of small oscillations?\nThe system is a box attached to a spring, the coordinate \\(x\\) measures the deviation from the equilibrium position and the coordinate \\(p\\) is the box’s linear momentum. These coordinates are real functions of time.\n\n\n\n\n\n\nFigure 1: One dimensional oscilator box always wants to return to the equilibrium position. For small amplitudes \\(A\\), the spring force on the box is given by Hook’s law.\n\n\n\nThe total energy of the system: We want to obtain a formula for the total energy in terms of the system coordinates using classical theory since its most familiar to us.\nRecall it is the sum of the kinetic energy of the box, which has the usual form \\(p^2/2m\\), and the potential energy \\(v(x)\\) that characterizes the spring force. What function \\(v(x)\\) can possibly describe well such a system?\nNote the assumptions! We want to describe small oscillations, i.e., the deviation \\(x\\) from equilibrium must be really small, much smaller than 1, mathematically \\(|x|\\ll 1\\) or if you prefer, the limit as \\(x \\longrightarrow 0\\).\nWith this information in our hands we can do something wonderful; we can break \\(v(x)\\) in two parts: the relevant part and negligible one. Using the power of Taylor series we find:\n\\[ v(x) = \\overbrace{v(0) + v'(0)x + \\frac{1}{2}v''(0)x^2}^{Relevant}+\\mathcal{O}(x^3).  \\tag{1}\\]\nTo simplify \\(v(x)\\): the constant term \\(v(0)\\) is set to zero to simplify notation and since \\(v(x)\\) has a minimum at \\(x=0\\), \\(v'(0)=0\\) aswell. Additionally, we identify \\(v''(0)\\) as Hook constant \\(k\\) which, by the way, can be rewritten as \\(m\\omega^2\\), the \\(\\omega\\) is the characteristic frequency \\(\\omega\\) of an oscillator for which this approximation is valid.\nThis makes this formula pleasantly simple\n\\[\nv(x) \\sim \\frac{1}{2}kx^2 =\\frac{1}{2}m \\omega^2x^2\n\\tag{2}\\]\nPictorially, the rhs describes the parabola (pink) that better fits the original potential energy (blue).\n\n\n\n\n\n\nFigure 2: The actual potential vs its approximation, valid when x is small.\n\n\n\nWhat have we achieved so far? We got the formula Equation 2 . Look at it as the functional form of any function that describes the potential energy as a function of \\(x\\) of an oscillator, oscillating :) with small amplitude. Small compared to what, small enough that the third order \\(\\mathcal{O}(x^3)\\) terms of the Taylor Equation 1 are negligible compared with \\(\\mathcal{O}(x^2)\\) term.\nThe total energy of the system is now one step away, its called the Hamiltonian \\(h\\) of the system as is (as usual) the sum of the kinetic energy plus potential energy which we express has:\n\\[\nh=\\frac{p^2}{2m}+\\frac{1}{2}m \\omega^2x^2.\n\\tag{3}\\]\nWith the classical mindset we could, if we wish, derive the laws of motion for this spring-box system using this Hamiltonian, these laws are:\n\\[\n\\begin{cases}\n\\dot{x} = \\frac{\\partial h}{\\partial p}\\\\\n\\dot{p} = - \\frac{\\partial h}{\\partial x}\n\\end{cases}\n\\]\nThis system of differential equations yields the functions \\(x(t)\\) and \\(p(t)\\). But to compute them is not our goal.\n\n\n\n\n\n\nComment\n\n\n\nIs it me, or did I pull the frequency \\(\\omega\\) out of my a#$. Right? Here is some insight, imagine a stiff spring, then we expect a high frequency of oscillation, this is what the formula\n\\[ \\omega =\\sqrt{\\frac{k}{m}},  \\]\ntells us, the higher the \\(k\\) for a fixed mass \\(m\\), the higher the frequency \\(\\omega\\). The formula is derived from classical theory, if you don’t remember how, don’t worry since we will not need it anyway, just take it as a fact of life that makes intuitive sense."
  },
  {
    "objectID": "example 2.html",
    "href": "example 2.html",
    "title": "Example 2",
    "section": "",
    "text": "Example 2:\nConsider a spin-1/2 particle whose energy eigenstates and eigenvalues are given by \\(|n\\rangle\\) and \\(E_n=n^2 E_1\\) for \\(n=1,2,..\\)\nAssume we have a system with two of these particles, which do not interact.\n\nWrite the ground state and first excited states when the total spin is zero \\(S=0\\). What are the corresponding eigenvalues?\nWhat is the ground state when the total spin is \\(1\\)?\n\nAnswer:\nThe single particle states are of the form \\(|\\text{spacial}\\rangle \\otimes|\\text{spin}\\rangle\\) where\n\\[\n\\begin{align}\n&|\\text{spacial}\\rangle \\in \\text{span}\\{\\text{Eigenstates of $h$ }\\}\\\\\n&|\\text{spin}\\rangle \\in \\text{span}\\{\\text{Eigenstates of $S^2$ and $S_z$}\\}\n\\end{align}\n\\]\nMore concretely we have \\(|n\\rangle\\otimes |m\\rangle\\) with \\(n=1,2,..\\) and \\(m=\\pm1/2\\); usually written as \\(|n,m\\rangle\\).\nWhy are the single particle eigenstates of this form? Answer: The observables \\(\\{h,S^2,S_z\\}\\) commutes (common eigenvectors exists) and the operators are independent (\\(h\\) is not a function of either \\(S^2\\) or \\(S_z\\)) and thus it suffices to specify the quantum numbers of each operator (the quantum number \\(s=1/2\\) is suppressed in the notation)\nThe possible quantum states of the total system are the eigenstates of the observables \\(\\{H,S_1^2,S_{1z}, S_2^2,S_{2z} \\}\\) with the total Hamiltonian as \\(H = h_1 +h_2+V_{12}\\) where \\(V_{12}\\) is zero since we are assuming non-interaction particles. We should look at these operators as extensions, for example the Hamiltonian is:\n\\[\nh_1 \\otimes 1_2 \\otimes 1_{s1} \\otimes 1_{s2}+ 1_1 \\otimes h_2 \\otimes 1_{s1} \\otimes 1_{s2}\n\\]\nwith\n\\[ h_i|i:n_i\\rangle =n_i^2E_1|i:n_i\\rangle \\]\nThe eigenstates of the system are of the form:\n\\[\n|1:n_1,m_1\\rangle \\otimes |2:n_2,m_2\\rangle\n\\]\nwhich can be written more compactly as:\n\\[\n|n_1,n_2\\rangle \\otimes |m_1,m_2\\rangle\n\\]\nHence for \\(i=1,2\\):\n\\[\n\\begin{align}\n&H|n_1,n_2\\rangle \\otimes |m_1,m_2\\rangle = \\frac{n_1^2+n_2^2}{2I}E_1|n_1,n_2\\rangle \\otimes |m_1,m_2\\rangle\\\\\n&S^2_i|n_1,n_2\\rangle \\otimes |m_1,m_2\\rangle = \\frac{3\\hbar^2}{4}|n_1,n_2\\rangle \\otimes |m_1,m_2\\rangle\\\\\n&S_{iz}|n_1,n_2\\rangle \\otimes |m_1,m_2\\rangle = m_i\\hbar |n_1,n_2\\rangle \\otimes |m_1,m_2\\rangle\n\\end{align}\n\\]\nThe two question posed refer to the total spin of the system, so it is a good idea to change the spin basis and write the spin part involving the total spin. We know from the previous theory that:\n\\[\n\\text{span}\\{|n_1,n_2\\rangle \\otimes |m_1,m_2\\rangle\\} = \\text{span}\\{|n_1,n_2\\rangle \\otimes |S,M\\rangle\\}\n\\]\nWhere again we suppress from the spin part this: \\(|\\overbrace{1/2,1/2},S,M\\rangle\\)\n\\[\n\\begin{align}\n&H|n_1,n_2\\rangle \\otimes |S,M\\rangle = \\frac{n_1^2+n_2^2}{2I}E_1|n_1,n_2\\rangle \\otimes |S,M\\rangle\\\\\n&S^2_i|n_1,n_2\\rangle \\otimes |S,M\\rangle = \\frac{3\\hbar^2}{4}|n_1,n_2\\rangle \\otimes |S,M\\rangle\\\\\n&S^2|n_1,n_2\\rangle \\otimes |S,M\\rangle = S(S+1)\\hbar^2|n_1,n_2\\rangle \\otimes |S,M\\rangle\\\\\n&S_{z}|n_1,n_2\\rangle \\otimes |S,M\\rangle = M\\hbar |n_1,n_2\\rangle \\otimes |S,M\\rangle\n\\end{align}\n\\]\n\nWe want the states of the form \\(|n_1,n_2\\rangle \\otimes |S,M\\rangle\\) such that the following statements are true:\n\\[\n\\begin{cases}\nP_{21}|n_1,n_2\\rangle \\otimes |S,M\\rangle =-1\\times|n_1,n_2\\rangle \\otimes |S,M\\rangle\\\\\n|S,M\\rangle = |S=0,M=0\\rangle\\\\\n\\frac{n_1^2+n_2^2}{2I}E_1\\quad \\text{is minimum}\n\\end{cases}\n\\] Since the spin part is antissimetric:\n\\[\n\\begin{align}\nP_{21} |0,0\\rangle &= P_{21} \\frac{1}{\\sqrt{2}}(|1:\\uparrow,2:\\downarrow\\rangle - |1:\\downarrow,2:\\uparrow\\rangle ) \\\\\n&= -1\\times \\frac{1}{\\sqrt{2}}(|1:\\uparrow,2:\\downarrow\\rangle - |1:\\downarrow,2:\\uparrow\\rangle )\\\\\n&-1\\times |0,0\\rangle\n\\end{align}\n\\] we seek a spacial ket that is symmetric and whose energy is minimum.\nThe spacial state \\(n_1=n_2=1\\) statisfies the criteria. Therefore the final answer is:\n\\[\n|1,1\\rangle\\otimes|0,0\\rangle\n\\]\nor\n\\[\n|1,1\\rangle \\otimes \\frac{1}{\\sqrt{2}}(|1:\\uparrow,2:\\downarrow\\rangle - |1:\\downarrow,2:\\uparrow\\rangle )\n\\]\nwhose eigenvalue is \\((1^2 + 1^2) E_1\\).\nFollowing a similar line of reasoning the first excited state with \\(S=0\\) must be:\n\\[\n\\frac{1}{\\sqrt{2}}(|2,1\\rangle + |1,2\\rangle) \\otimes |0,0\\rangle\n\\]\nObserve that the spacial spate is symmetric when we use the \\(+\\) sign!\nWhen \\(S=1\\) the spin state is one of the symetric triplet states:\n$$\n\\[\\begin{align}\n&\\ket{1,1}=\\ket{1/2,1/2}\\\\\n&\\ket{1,0}=\\frac{1}{\\sqrt{2}}\\left(\\ket{-\\frac{1}{2},\\frac{1}{2}}+\\ket{\\frac{1}{2},-\\frac{1}{2}}\\right)\\\\\n&\\ket{1,-1}=\\ket{-1/2,-1/2}\n\\end{align}\\] $$\nAs a result the spacial part must be antissymetric for the whole quantum state of the two particles to be antissymetric as PEP requires. Thus:\n\\[\n\\begin{align}\n&\\frac{1}{\\sqrt{2}}(|2,1\\rangle - |1,2\\rangle)\\otimes |1,1\\rangle\\\\\n&\\frac{1}{\\sqrt{2}}(|2,1\\rangle - |1,2\\rangle)\\otimes |1,0\\rangle\\\\\n&\\frac{1}{\\sqrt{2}}(|2,1\\rangle - |1,2\\rangle)\\otimes |1,-1\\rangle\\\\\n\\end{align}\n\\]\nNotice the minimum energy AND antissymetry of the spacial part only occurs at the FIRST excited state. The spacial ground state is the symmetric \\(|1,1\\rangle\\)."
  },
  {
    "objectID": "examples_of_pol.html",
    "href": "examples_of_pol.html",
    "title": "Examples of Polynomials",
    "section": "",
    "text": "In this section we analyse polynomials of degree one and two. These notes include the mediatrix equation for degree one and the focus-diretrix formulation for degree two."
  },
  {
    "objectID": "examples_of_pol.html#properties-of-the-atomic-polynomials",
    "href": "examples_of_pol.html#properties-of-the-atomic-polynomials",
    "title": "Examples of Polynomials",
    "section": "Properties of the atomic polynomials",
    "text": "Properties of the atomic polynomials\nRather than showing a picture of a small slice of the function as de did in Figure 1 we can instead zoom into some aspects of it.\n\nProperties of the connectivity between domain and codomain:\n\nRange: From inspection of Equation 1 (or Equation 2) we know the domain and codomain of these four functions is \\(\\mathbb{R}\\). The range of these functions is the image of the domain under the action of the function, we can infer what this image is by inspecting Figure 1, for example, the image of \\(\\mathbb{R}\\) under the action of \\(e_2\\) is the set of all positive reals as we check by the red curve hence we write \\(e_2(\\mathbb{R})=\\mathbb{R}^+_0\\). Similarly we find:\n\n\\[\ne_0(\\mathbb{R})=\\{1\\} \\qquad e_1(\\mathbb{R})=e_3(\\mathbb{R})=\\mathbb{R}\\qquad e_2(\\mathbb{R})=\\mathbb{R}^+_0\n\\]\n\nOnto: We also observe that for even powers of \\(n\\) such as \\(e_0\\) and \\(e_2\\), not all elements in the codomain \\(\\mathbb{R}\\) are hit by outputs, i.e., these are not onto function; as an example, [is \\(0\\) hit by some \\(e_0(x)\\) ? Is \\(-1\\) hit by \\(e_2(x)\\), for some \\(x\\) in its domain?] We can answer these question by trying to solve the equations \\(e_0(x)=0\\) and \\(e_2(x)=-1\\). To no avail! Neither have solutions \\(x\\) in the domain \\(\\mathbb{R}\\).\n1-1: Additionally nor are these even powers 1-1. Why? Because we can clearly see from Figure 1 that \\(e_0\\) is in fact the worst possible “labeller”, where all “fruits” \\(x\\) have exactly the same label \\(1\\), no other “label” in its codomain \\(\\mathbb{R}\\) is put into good use by this \\(e_0\\); the function \\(e_2\\) is not as bad since for each label we find just two elements in \\(\\mathbb{R}\\). The functions \\(e_1\\) and \\(e_3\\) are 1-1 since each element \\(x\\) in their domain have a unique element \\(y\\) in the codomain, hence if we choose a label \\(y\\) we uniquely know the \\(x\\). Summarizing:\n\\[\n\\begin{cases}e_0 \\qquad &\\text{worst possible labeler (one label for all inputs)}\\\\e_2 \\qquad  &\\text{bad labeller (one label for each pair of inputs)}\\\\e_1\\,\\, \\text{and}\\,\\, e_3 \\qquad &\\text{perfect (one label, one input)}\\end{cases}\n\\]\n\n\n\nProperties at interesting regions of the function:\n\nWho is bigger or smaller? The Figure 1 reveals something funny:\n\nfor points on the horizontal axis close to the origin, that is \\(-1&lt;x&lt;1\\), the outputs of the four functions have the following rank \\(x^3 &lt; x^2 &lt; x^1 &lt; x^0\\);\nprecisely at \\(x=1\\) we find they are all the same \\(x^0=x^1=x^2=x^3=1\\);\nmeanwhile for the remaining points, those in the region \\(x&lt;-1 \\lor 1&lt;x\\), the absolute value of these functions follow the reversed rank! \\(x^0&lt;x^1&lt;x^2&lt;x^3\\).\nFor \\(x\\) very far from \\(1\\) we find \\(x^0\\ll x^1\\ll x^2\\ll x^3\\), which means, for example, that \\(|x^3|\\) is much, much, much, much larger then \\(|x^2|\\), and so on.\n\nIn summary:\n\\[\n\\begin{cases}\nx^3 &lt; x^2 &lt; x^1 &lt; x^0 \\qquad &\\text{for $x$ close to 1}\\\\x^0=x^1=x^2=x^3 \\qquad &\\text{for $x=1$}\\\\x^0\\ll x^1\\ll x^2\\ll x^3 \\qquad &\\text{for $x$ far from $1$}\n\\end{cases}\n\\tag{3}\\]\nThis summary will be important when we study the limiting behavior of molecular polynomials.\nZeros: The zeros of a function are the coordinates of the points which intercept the axes. Observing Figure 1 we see polynomial \\(1\\) intercepts the y-axis at \\((0,1)\\) while all other polynomials intercept the x-axis and y-axis exactly at the origin \\((0,0)\\).\nMaxima and minima: By inspection of Figure 1, we observe the polynomials \\(1\\) has no maxima or minima, it is always flat; \\(x\\) and \\(x^3\\) on the other hand is a ramp, a never ending one, and thus also does not have a maximum or minimum. The only function exhibiting a minimum is \\(x^2\\), coincidentally, at \\((0,0)\\). Only when we study derivatives we’ll have the computational means to find these extrema.\n\n\n\n\n\n\n\nCommentary\n\n\n\nThe previous list of properties was already embedded in Figure 1 (or if you think about it, already at Equation 2), only by close observation we made it explicit. Of course, the list is not exhaustive, many other properties remain hidden in that picture, waiting to be dug out. This is why I said this list are a zoom into the picture, when we get closer we see more detail.\nAs additional examples we could also ask how fast a polynomial of degree one increases, i.e., what is its slope; for polynomials of degree two it would make sense to ask if it its graph is a cup or a cap, it all depends on \\(a_2\\) parameter.\nWe usually do not list the properties just as we did above, we just acknowledge in our mind that they exist and compute them as needed, since many times some of them are so bluntly obvious just by looking at the graph or the function formula, that no need is required for a calculation."
  },
  {
    "objectID": "examples_of_pol.html#polynomials-of-degree-one-are-lines",
    "href": "examples_of_pol.html#polynomials-of-degree-one-are-lines",
    "title": "Examples of Polynomials",
    "section": "Polynomials of degree one are lines",
    "text": "Polynomials of degree one are lines\nPolynomials of degree one are lists of ordered pairs of the form:\n\\[\n\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y=a_1 x+a_0\\}\n\\tag{4}\\]\nThe constants \\(a_1\\) and \\(a_0\\) are either given to us or we must choose them - we say they are parameter of the function - they are the weights of \\(x^1\\) and \\(x^0\\) in the linear combination \\(a_1x+a_0\\). Here are some examples of choices:\n\\[\n\\begin{align}\n&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = -2x \\}\\\\\n&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = 2x \\}\\\\\n&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = x+1 \\}\\\\\n&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = -2x-1 \\}\n\\end{align}\n\\]\nGraphically these corresponds to lines in a plane:\n\n\n\n\n\n\n\n\n\nIn maths, sometimes, changing just the aesthetic of the problem can bring great insight into what it means. Here is some changes into the aesthetic of the equation \\(y=a_1x+a_0\\):\n\\[\ny=a_1x+a_0 = a_1 x + a_0 + a_1 x_0 -a_1 x_0 = a_1(x-x_0)+(a_0+a_1x_0)\n\\tag{5}\\]\nNow define the following quantities:\n\\[\nm:=a_1 \\qquad y_0:=a_0+a_1x_0\n\\tag{6}\\]\nEven if you have no clue how this can be a good idea, just wait and appreciate the consequences - the equation \\(y=a_1x+a_0\\) shows up dressed differently:\n\\[\ny=m(x-x_0)+y_0\n\\tag{7}\\]\nthe new clothing is introduced in Equation 6 .\nIt look different, a new aesthetic, but it is still the same equation as \\(y=a_1x+a_0\\) because the parameters \\(a_1\\), \\(a_0\\), \\(m\\), \\(x_0\\) and \\(y_0\\) are related through the equations Equation 6. If values of some of these parameters are given, these equations can be used to compute the other.\nAs example: You give \\(a_0\\) and \\(a_1\\) and I can compute \\(x_0\\) and \\(y_0\\). How? If you give me \\(a_0=3\\) and \\(a_1=2\\), I pick an arbitrary \\(x_0\\), for example \\(x_0=4\\) and compute\n\\[\ny_0=3+2\\times 4=11\n\\]\nIn conclusion we have two equations aesthetically different but exactly the same:\n\\[\ny=2x+3 \\iff y=2(x-4)+11\n\\tag{8}\\]\nThe right hand side aesthetic is much better because it allow us to see easily how fast these polynomials of degree one increase.\n\n\nHow fast does the function increase?\nThe constant \\(m\\) (the \\(2\\) in Equation 8) is called the slope of the line, what does it mean the value of the slope? The meaning of this constant - in fact any constant in mathematics - can be deduced by analyzing the equation in which it appears. In this case, we understand \\(m\\) by understanding \\(y=m(x-x_0)+y_0\\), rearranging it we find:\n\\[\nm=\\frac{y-y_0}{x-x_0}\n\\tag{9}\\]\nwhich tells us that \\(m\\) is a ratio of two distances, the height of the triangle \\(y-y_0\\) and the length of the base of the triangle \\(x-x_0\\).\n\n\n\n\n\n\nFigure 2: A line and two point.\n\n\n\nSince \\(m\\) is the ratio of these two distances, its sign determines weather the function is increasing or not. Observing Figure 1 we see \\(x-x_0&gt;0\\) and \\(y-y_0&gt;0\\) therefore for this particular choice of two points we find \\(m&gt;0\\), the same conclusion would hold for any other. When \\(m\\) is positive, then the \\(y\\)’s in \\((x,y)\\) increase when the \\(x\\)’s increase as well.\nBy the same line of reasoning the \\(m&lt;0\\) would correspond to decreasing function, one where as \\(x\\) increases then the \\(y\\)’s decrease. And by the way, \\(m=0\\) correspond to horizontal lines but these are polynomials of degree zero and not of degree one. The larger \\(m\\) becomes the closer the line is to a vertical line, but it never truly becomes a vertical line.\nWe can actually arrive at these conclusions just looking at \\(y=m(x-x_0)+y_0\\), no rearranging required: when \\(m\\) is positive, then the greater \\(x-x_0\\) is, the more we add to the default value \\(y_0\\), thus the \\(y\\) must increase; if on the other hand \\(m\\) is negative, the term \\(m(x-x_0)\\) can only become more negative as \\(x-x_0\\) increases, thus lowering the value of \\(m(x-x_0)+y_0\\), that is, the value of \\(y\\), the function must be decreasing when \\(x-x_0\\) increases.\n\n\nZeros\nWhat are the zeros in \\(P_1\\)? Look at Figure 2, there are (in this case) two types of zeros: \\(x\\)-zeros and \\(y\\)-zeros. The \\(x\\)-zeros occur when the \\(x\\) coordinate is zero and the \\(y\\)-zeros when \\(y\\) is zero. In other words, the zeros of a polynomials of degree one are the ordered pairs (in \\(P_1\\)) that satisfy either one of these equations:\n\\[\n\\begin{equation}\\begin{cases}y=y_0+m(x-x_0)\\\\y=0\\end{cases}\\qquad\\qquad\\begin{cases}y=y_0+m(x-x_0)\\\\x=0\\end{cases}\\end{equation}\n\\]\n\\[\n\\begin{equation}\\begin{cases}y_0=a_1x_0+a_0\\\\y=0\\end{cases}\\qquad\\qquad\\begin{cases}y=a_1x+a_0\\\\x=0\\end{cases}\\end{equation}\n\\]\n\\[\n(x_0,0)\\in \\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = a_1x+a_0 \\}\n\\]\n\\[\n(0,y_0)\\in \\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = a_1x+a_0 \\}\n\\]\nSolving the first systems of equations we get \\(x=(0-y_0)/m+x_0\\) and solving the second \\(y=y_0+m(0-x_0)\\). Hence we know zeros of \\(P_1\\):\n\\[\n\\begin{align}\n&\\textit{x-zero:} \\qquad (0,y_0-mx_0)\\\\\n&\\textit{y-zero:} \\qquad (x_0-y_0/m,0)\n\\end{align}\n\\]\n\n\nWhen \\(x\\) is large, then what?\nWe can get to know qualitatively what happens with points coordinates of \\(P_1\\) at its extreme ends of the set by supposing \\(x\\) is a very, very large number; there is however an important key point: we are not particularly worried about a specific large value of \\(x\\), like \\(x=10^{100}\\) , and its corresponding \\(y\\); what we really want to know is - as \\(x\\) is getting larger and larger, how does \\(y\\) behave then? It is a dynamical process, not a specific evaluation of the function.\n\nParaphrased differently, we want to know what happens to \\((x,y)\\) during the process of making \\(x\\) bigger and bigger? To state that \\(x\\) is getting bigger and bigger, we write \\(x\\longrightarrow \\infty\\). In such a dynamic case, what happens with the value \\(y\\)? Meaning, what is the dynamic solution \\(y\\) of the system:\n\\[\n\\begin{equation}\\begin{cases}y=y_0+m(x-x_0)\\\\x\\longrightarrow +\\infty\\end{cases}\\end{equation}\n\\tag{10}\\]\nLets make an intuitive guess about the solution of this problem (later, when we study limits we’ll see a more systematic way to solve these problems), the Figure 3 helps a lot. To understand how the values of \\(y\\) behave when \\(x\\) increases, we have to understand how the calculation \\(y_0+m(x-x_0)\\) behaves, when we keep increasing \\(x\\). A key aspect of it, is that the term \\(m x\\) gets larger and larger while \\(y_0-mx_0\\) remains a constant, eventually, \\(mx\\) is so much larger than this constant (we are increasing \\(x\\) after all) that our original equation \\(y=y_0+m(x-x_0)\\) becomes approximately given by \\(y\\sim mx\\). The functions \\(y_0+m(x-x_0)\\) and \\(mx\\) are different! But when \\(x\\) is large they are very similar because the constant \\(y_0-mx_0\\) becomes irrelevant. Now we shift our focus to the approximate equation and try to solve it:\n\\[\n\\begin{equation}\\begin{cases}y\\sim mx\\\\x\\longrightarrow +\\infty\\end{cases}\\end{equation}\n\\tag{11}\\]\nThis is easy, because its just an atomic polynomial. The dynamical solution to this problem is [ \\(y\\) will increase when \\(x\\) increases], we write the answer as \\(y\\longrightarrow +\\infty\\) when \\(m\\) is positive. As a consequence the answer to Equation 10 is also \\(y\\longrightarrow +\\infty\\).\nObserve that the function \\(y\\sim mx\\) is not equal to \\(y=y_0+m(x-x_0)\\), and thus their graphs are not equal, but when \\(x\\) is very large, they are similar, see picture below. Solving Equation 11 we automatically get the answer to the problem Equation 10 . Both answers are \\(y\\longrightarrow+\\infty\\).\n\n\n\n\n\n\nFigure 3: Does y grow when x grows?\n\n\n\nIf on the other hand \\(m\\) is negative then, by the same reasoning, \\(y\\) also becomes very negative.\nAwful Exercise: What happens to \\(y\\) when \\(x\\longrightarrow-\\infty\\)?"
  },
  {
    "objectID": "examples_of_pol.html#mediatrix-equation",
    "href": "examples_of_pol.html#mediatrix-equation",
    "title": "Examples of Polynomials",
    "section": "Mediatrix Equation",
    "text": "Mediatrix Equation\nWith similar flavor as a line that passes through two point, our goal now is to find the equation of the line that is perpendicular to the line that passes through the point and is equidistant from the two points.\nThe word equidistant my cause some head scratching, here’s a picture\n\n\n\nA blue line that intercepts the two red points and the mediatrix in pink.\n\n\nFor each point \\((x,y)\\) on the pink line compute the distance from the red points, these two distances must be the same. This is what describes the mediatrix line (in pink).\n\n\n\n\n\n\nRecall\n\n\n\nThe distance, \\(d\\), between the point \\((x_0,y_0)\\) and the point \\((x,y)\\)\n\nis given by the Pythagoras theorem:\n\\[\nd^2 = (x-x_0)^2+(y-y_0)^2\n\\]\n\n\nFormally, lets say the red points have coordinates \\((x_0,y_0)\\) and \\((x_1,y_1)\\), our function is composed by the points which is equidistant from both, in mathematical notation, the mediatrix line is given by:\n\\[\n\\{(x,y)\\in\\mathbb{R}^2\\,\\,|\\,\\, (x-x_0)^2+(y-y_0)^2 = (x-x_1)^2+(y-y_1)^2\\}\n\\tag{14}\\]\nLets simplify the element-hood test:\n\\[\n\\begin{align}\n&(x-x_0)^2+(y-y_0)^2 = (x-x_1)^2+(y-y_1)^2\\\\\n\\iff &x^2-2x_0x+x_0^2 + y^2-2y_0y+y_0^2 = x^2-2x_1x+x_1^2 + y^2-2y_1y+y_1^2\\\\\n\\iff & 0= 2(y_0-y_1)y + 2(x_0-x_1)x+y_1^2-y_0^2+x_1^2-x_0^2\\\\\n\\iff & y = -\\frac{x_0-x_1}{y_0-y_1}x +\\frac{1}{2}(y_1^2+x_1^2-y_0^2-x_0^2)\\\\\n\\iff & y = -\\frac{1}{m} x + \\frac{1}{2}\\Delta^2\n\\end{align}\n\\]\nwhere in the last line I lumped terms into the parameters \\(m\\) and \\(\\Delta\\).\n\\[\nm:=\\frac{y_1-y_0}{x_1-x_0}\\qquad \\Delta^2:=y_1^2+x_1^2-y_0^2-x_0^2\n\\]\n\\(m\\) is the slope of the line that contains the red points, while \\(\\Delta^2\\) is the difference between the squared-distances of the red points with respect to the origin.\nAs we can see the second powers in the element-hood test cancel and what remain is the equation \\(y = -\\frac{1}{m} x + \\frac{1}{2}\\Delta^2\\), which is just a linear combination of \\(x\\) and \\(x^0\\) with weights \\(-1/m\\) and \\(\\Delta^2/2\\), the hallmark of a line equation of the line. We conclude, the set Equation 14 is equal to\n\\[\n\\{(x,y)\\in\\mathbb{R}^2\\,\\,|\\,\\, y = -\\frac{1}{m} x + \\frac{1}{2}\\Delta^2\\}\n\\]\nComparing with Equation 4 we identify \\(a_1 =-1/m\\) and \\(a_0=\\Delta^2/2\\)."
  },
  {
    "objectID": "examples_of_pol.html#zeros-of-these-polynomials",
    "href": "examples_of_pol.html#zeros-of-these-polynomials",
    "title": "Examples of Polynomials",
    "section": "Zeros of these polynomials:",
    "text": "Zeros of these polynomials:\nWe wish now to zoom into polynomials given as Equation 18 or Equation 19 and see the zeros of these function are expressed in term of the diretrix and focus properties.\nTo find vertical axis interception is to find the point \\((x,y)\\) in \\(P_2\\) whose \\(x=0\\):\n\\[\n\\begin{equation}\\begin{cases}y-y_0=\\frac{1}{4c}(x-x_0)^2\\\\x=0\\end{cases}\\implies y-y_0=\\frac{1}{4c}(0-x_0)^2\\implies y=\\frac{1}{4c}x_0^2+y_0\\end{equation}\n\\]\nwhich tells us the interception point coordinates \\((0,\\frac{1}{4c}x_0^2+y_0)\\) belonging to \\(P_2\\).\nWhen seeking for the points with \\(y=0\\) we must be more careful since they may not exist (the parabola might be completely above the x-axis). By substituting again \\(y=0\\) in the element-hood test we get:\n\\[\n\\begin{equation}\\begin{cases}y-y_0=\\frac{1}{4c}(x-x_0)^2\\\\y=0\\end{cases}\\implies(x-x_0)^2=-4cy_0\\end{equation}\n\\]\nWe must be careful when solving for \\(x\\) the later equation since we must have a positive right hand side, this is only possible if the parameter \\(y_0\\leq 0\\). This is the condition for the existence of zeros, i.e., it is the condition that ensures us the parabola is not completely above the x-axis.\n\n\n\nA parabola and the x-axis (diretrix and focus NOT shown).\n\n\nSaying \\(y_0\\leq 0\\) either means, \\(y_0=0\\), in which case we immediately know \\(x=x_0\\) and thus \\((x_0,0)\\) is the ONLY horizontal axis interception (the vertex sits exactly at the x-axis); or it means \\(y_0&lt;0\\), in this case we solve the equation for \\(x\\) as follows\n\\[\n\\begin{equation}\\begin{split}(x-x_0)^2&=4c|y_0|\\\\&\\implies x-x_0=\\pm\\sqrt{4c|y_0|}\\\\&\\implies x=x_0\\pm \\sqrt{4c|y_0|}\\end{split}\\end{equation}\n\\]\nThere are two interception, at \\((x_0+ \\sqrt{4c|y_0|},0)\\) and another at \\((x_0-\\sqrt{4c|y_0|},0)\\), which is expected when part of the parabola is below the x-axis."
  },
  {
    "objectID": "examples_of_pol.html#when-x-is-large",
    "href": "examples_of_pol.html#when-x-is-large",
    "title": "Examples of Polynomials",
    "section": "When \\(x\\) is large:",
    "text": "When \\(x\\) is large:\nTo understand the behavior when \\(x\\) is very large, the extreme ends of \\(P_2\\), we have to note the following:\n\nObserve the term \\((x-x_0)^2\\), it is a squared of a number \\(x-x_0\\), hence always positive. This implies the smallest value it takes is zero.\nWhen \\(c\\) is positive, then \\((x-x_0)^2/4c\\) is positive for any \\(x\\), the minimum is zero when we set \\(x-x_0=0\\).\n\nThe procedure \\((x-x_0)^2/4c+y_0\\) results from the addition of a strictly positive number \\((x-x_0)^2/4c\\) to \\(y_0\\), hence the minimum value it can take is \\(y_0\\) when \\(x=x_0\\).\nNow let us turn into what happens with the function when \\(x\\) becomes very large (\\(x\\longrightarrow \\pm\\infty\\)). Another glance at the procedure tell us that the quantity \\((x-x_0)^2\\) becomes very large either way, multiplying it now by \\(1/4c\\) we must be clear about the sign of \\(c\\). If it is positive the function will grow without end in either case, if \\(c\\) is negative, it becomes very small."
  },
  {
    "objectID": "examples_of_pol_part_2.5.html",
    "href": "examples_of_pol_part_2.5.html",
    "title": "Polynomials of degree one are lines",
    "section": "",
    "text": "Polynomials of degree one are lists of ordered pairs of the form:\n\\[\n\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y=a_1 x+a_0\\}\n\\tag{1}\\]\nThe constants \\(a_1\\) and \\(a_0\\) are either given to us or we must choose them - we say they are parameter of the function - they are the weights of \\(x^1\\) and \\(x^0\\) in the linear combination \\(a_1x+a_0\\). Here are some examples of choices:\n\\[\n\\begin{align}\n&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = -2x \\}\\\\\n&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = 2x \\}\\\\\n&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = x+1 \\}\\\\\n&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = -2x-1 \\}\n\\end{align}\n\\]\nGraphically these corresponds to lines in a plane:\n\n\n\n\n\n\n\n\n\nIn maths, sometimes, changing just the aesthetic of the problem can bring great insight into what it means. Here is some changes into the aesthetic of the equation \\(y=a_1x+a_0\\):\n\\[\ny=a_1x+a_0 = a_1 x + a_0 + a_1 x_0 -a_1 x_0 = a_1(x-x_0)+(a_0+a_1x_0)\n\\tag{2}\\]\nNow define the following quantities:\n\\[\nm:=a_1 \\qquad y_0:=a_0+a_1x_0\n\\tag{3}\\]\nEven if you have no clue how this can be a good idea, just wait and appreciate the consequences - the equation \\(y=a_1x+a_0\\) shows up dressed differently:\n\\[\ny=m(x-x_0)+y_0\n\\tag{4}\\]\nthe new clothing is introduced in Equation 3 .\nIt look different, a new aesthetic, but it is still the same equation as \\(y=a_1x+a_0\\) because the parameters \\(a_1\\), \\(a_0\\), \\(m\\), \\(x_0\\) and \\(y_0\\) are related through the equations Equation 3. If values of some of these parameters are given, these equations can be used to compute the other.\nAs example: You give \\(a_0\\) and \\(a_1\\) and I can compute \\(x_0\\) and \\(y_0\\). How? If you give me \\(a_0=3\\) and \\(a_1=2\\), I pick an arbitrary \\(x_0\\), for example \\(x_0=4\\) and compute\n\\[\ny_0=3+2\\times 4=11\n\\]\nIn conclusion we have two equations aesthetically different but exactly the same:\n\\[\ny=2x+3 \\iff y=2(x-4)+11\n\\tag{5}\\]\nThe right hand side aesthetic is much better because it allow us to see easily how fast these polynomials of degree one increase.\n\n\nHow fast does the function increase?\nThe constant \\(m\\) (the \\(2\\) in Equation 5) is called the slope of the line, what does it mean the value of the slope? The meaning of this constant - in fact any constant in mathematics - can be deduced by analyzing the equation in which it appears. In this case, we understand \\(m\\) by understanding \\(y=m(x-x_0)+y_0\\), rearranging it we find:\n\\[\nm=\\frac{y-y_0}{x-x_0}\n\\tag{6}\\]\nwhich tells us that \\(m\\) is a ratio of two distances, the height of the triangle \\(y-y_0\\) and the length of the base of the triangle \\(x-x_0\\).\n\n\n\n\n\n\nFigure 1: A line and two point.\n\n\n\nSince \\(m\\) is the ratio of these two distances, its sign determines weather the function is increasing or not. Observing ?@fig-window we see \\(x-x_0&gt;0\\) and \\(y-y_0&gt;0\\) therefore for this particular choice of two points we find \\(m&gt;0\\), the same conclusion would hold for any other. When \\(m\\) is positive, then the \\(y\\)’s in \\((x,y)\\) increase when the \\(x\\)’s increase as well.\nBy the same line of reasoning the \\(m&lt;0\\) would correspond to decreasing function, one where as \\(x\\) increases then the \\(y\\)’s decrease. And by the way, \\(m=0\\) correspond to horizontal lines but these are polynomials of degree zero and not of degree one. The larger \\(m\\) becomes the closer the line is to a vertical line, but it never truly becomes a vertical line.\nWe can actually arrive at these conclusions just looking at \\(y=m(x-x_0)+y_0\\), no rearranging required: when \\(m\\) is positive, then the greater \\(x-x_0\\) is, the more we add to the default value \\(y_0\\), thus the \\(y\\) must increase; if on the other hand \\(m\\) is negative, the term \\(m(x-x_0)\\) can only become more negative as \\(x-x_0\\) increases, thus lowering the value of \\(m(x-x_0)+y_0\\), that is, the value of \\(y\\), the function must be decreasing when \\(x-x_0\\) increases.\n\n\nZeros\nWhat are the zeros in \\(P_1\\)? Look at Figure 1, there are (in this case) two types of zeros: \\(x\\)-zeros and \\(y\\)-zeros. The \\(x\\)-zeros occur when the \\(x\\) coordinate is zero and the \\(y\\)-zeros when \\(y\\) is zero. In other words, the zeros of a polynomials of degree one are the ordered pairs (in \\(P_1\\)) that satisfy either one of these equations:\n\\[\n\\begin{equation}\\begin{cases}y=y_0+m(x-x_0)\\\\y=0\\end{cases}\\qquad\\qquad\\begin{cases}y=y_0+m(x-x_0)\\\\x=0\\end{cases}\\end{equation}\n\\]\n\\[\n\\begin{equation}\\begin{cases}y_0=a_1x_0+a_0\\\\y=0\\end{cases}\\qquad\\qquad\\begin{cases}y=a_1x+a_0\\\\x=0\\end{cases}\\end{equation}\n\\]\n\\[\n(x_0,0)\\in \\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = a_1x+a_0 \\}\n\\]\n\\[\n(0,y_0)\\in \\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = a_1x+a_0 \\}\n\\]\nSolving the first systems of equations we get \\(x=(0-y_0)/m+x_0\\) and solving the second \\(y=y_0+m(0-x_0)\\). Hence we know zeros of \\(P_1\\):\n\\[\n\\begin{align}\n&\\textit{x-zero:} \\qquad (0,y_0-mx_0)\\\\\n&\\textit{y-zero:} \\qquad (x_0-y_0/m,0)\n\\end{align}\n\\]\n\n\nWhen \\(x\\) is large, then what?\nWe can get to know qualitatively what happens with points coordinates of \\(P_1\\) at its extreme ends of the set by supposing \\(x\\) is a very, very large number; there is however an important key point: we are not particularly worried about a specific large value of \\(x\\), like \\(x=10^{100}\\) , and its corresponding \\(y\\); what we really want to know is - as \\(x\\) is getting larger and larger, how does \\(y\\) behave then? It is a dynamical process, not a specific evaluation of the function.\n\nParaphrased differently, we want to know what happens to \\((x,y)\\) during the process of making \\(x\\) bigger and bigger? To state that \\(x\\) is getting bigger and bigger, we write \\(x\\longrightarrow \\infty\\). In such a dynamic case, what happens with the value \\(y\\)? Meaning, what is the dynamic solution \\(y\\) of the system:\n\\[\n\\begin{equation}\\begin{cases}y=y_0+m(x-x_0)\\\\x\\longrightarrow +\\infty\\end{cases}\\end{equation}\n\\tag{7}\\]\nLets make an intuitive guess about the solution of this problem (later, when we study limits we’ll see a more systematic way to solve these problems), the Figure 2 helps a lot. To understand how the values of \\(y\\) behave when \\(x\\) increases, we have to understand how the calculation \\(y_0+m(x-x_0)\\) behaves, when we keep increasing \\(x\\). A key aspect of it, is that the term \\(m x\\) gets larger and larger while \\(y_0-mx_0\\) remains a constant, eventually, \\(mx\\) is so much larger than this constant (we are increasing \\(x\\) after all) that our original equation \\(y=y_0+m(x-x_0)\\) becomes approximately given by \\(y\\sim mx\\). The functions \\(y_0+m(x-x_0)\\) and \\(mx\\) are different! But when \\(x\\) is large they are very similar because the constant \\(y_0-mx_0\\) becomes irrelevant. Now we shift our focus to the approximate equation and try to solve it:\n\\[\n\\begin{equation}\\begin{cases}y\\sim mx\\\\x\\longrightarrow +\\infty\\end{cases}\\end{equation}\n\\tag{8}\\]\nThis is easy, because its just an atomic polynomial. The dynamical solution to this problem is [ \\(y\\) will increase when \\(x\\) increases], we write the answer as \\(y\\longrightarrow +\\infty\\) when \\(m\\) is positive. As a consequence the answer to Equation 7 is also \\(y\\longrightarrow +\\infty\\).\nObserve that the function \\(y\\sim mx\\) is not equal to \\(y=y_0+m(x-x_0)\\), and thus their graphs are not equal, but when \\(x\\) is very large, they are similar, see picture below. Solving Equation 8 we automatically get the answer to the problem Equation 7 . Both answers are \\(y\\longrightarrow+\\infty\\).\n\n\n\n\n\n\nFigure 2: Does y grow when x grows?\n\n\n\nIf on the other hand \\(m\\) is negative then, by the same reasoning, \\(y\\) also becomes very negative.\nAwful Exercise: What happens to \\(y\\) when \\(x\\longrightarrow-\\infty\\)?",
    "crumbs": [
      "Bried Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Polynomials of degree one"
    ]
  },
  {
    "objectID": "examples_of_pol_part_3.html",
    "href": "examples_of_pol_part_3.html",
    "title": "Line’s equation from two given points",
    "section": "",
    "text": "As we have seen a polynomial of degree one can either be specified by providing the parameters \\(a_0\\) and \\(a_1\\) or alternatively (as the author prefers by the way) by providing a point \\((x_0,y_0)\\) on the line and a slope value \\(m\\). This is what we know so far. In this section we see a third way of specifying one of these polynomials, namely, to provide the coordinates of any two distinct point on the line \\((x_0,y_0)\\) and \\((x_1, y_1)\\). We can already sense it, somehow the two point must determine \\(a_0\\) and \\(a_1\\) or \\((x_0,y_0)\\) and \\(m\\).\nHere is how to do it:\n\nHave the two point \\((-2,3)\\) and \\((3,-7)\\). These are a given. No action needed.\nAssume they both belong to the same line, i.e.,\n\\[\n\\begin{align}\n(-2,3)\\in \\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = a_1x+a_0 \\}\\\\\n(3,-7)\\in \\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y = a_1x+a_0 \\}\n\\end{align}\n\\tag{1}\\]\nfor some \\(a_1\\) and \\(a_0\\) still unknown but we wish to determine. These two statements characterize the parameters \\(a_1\\) and \\(a_0\\).\nWe want to compute the values of \\(a_0\\) and \\(a_1\\) that make true the statements Equation 1. The statements Equation 1 are equivalent to the following system of equations:\n\\[\n\\begin{cases}\n3=a_1\\times(-2)+a_0\\\\\n-7=a_1\\times 3 +a_0\n\\end{cases}\n\\]\nJust plug in the point coordinates into the element-hood test of the sets \\(y=a_1x+a_0\\).\nSolve the system of equations using substitution, the solution is:\n\\[\na_0=-1 \\qquad a_1=-2\n\\]\nThe polynomial’s formula that passes through both points is:\n\\[\ny=-2 x-1\n\\tag{2}\\]\n\nAn alternative method is to find \\(m\\) from the coordinates of the two point using the formula ?@eq-slope_formula and then set \\((x_0,y_0)\\) in \\(y=m(x-x_0)+y_0\\) as either one of the two point.\nHere are the steps:\n\nCompute \\(m = (-7-3)/(3-(-2)) = -2\\). Warning! When using ?@eq-slope_formula, make sure you order the two point by the \\(x\\)-coordinate. In this example I assigned \\((x_0,y_0)=(-2,3)\\) and \\((x_1,y_1)=(3,-7)\\) because \\(-2&lt;3\\). This guarantees the denominator of ?@eq-slope_formula is positive.\nChoose the \\(x_0\\) and \\(y_0\\) in \\(y=m(x-x_0)+y_0\\) as either \\((-2,3)\\) or \\((3,-7)\\). I choose\n\n\\[\n(x_0,y_0) = (3,-7)\n\\]\n\nWrite the final result:\n\\[\ny=-2(x-3)-7\n\\]\nwhich is equivalent to Equation 2.",
    "crumbs": [
      "Bried Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Line's equation from two points"
    ]
  },
  {
    "objectID": "examples_of_pol_part_5.html",
    "href": "examples_of_pol_part_5.html",
    "title": "Focus-diretrix formulation",
    "section": "",
    "text": "Observing Figure 1 we observe two points: the vertex with coordinates \\((x_0,y_0)\\) and the focus \\(c\\) units above at \\((x_0,y_0+c)\\). A parabola is by definition the set of points which:\nWhen \\(c&gt;0\\) the polynomials describes a cup whose minimum is the vertex; when \\(c&lt;0\\) it describes a cap whose maximum is the vertex.\nLets express condition 1. and 2. using mathematical notation.\nUsing the distance formula we seen above, we can express both distances in terms of the coordinates of the vertex and focus points as\n\\[\n\\begin{cases}d_1^2=(x-x_0)^2+(y-(y_0+c))^2\\\\d_2^2=(y-(y_0-c))^2\\end{cases}\n\\]\nCondition 2. becomes:\n\\[\n\\begin{align}\n&(x-x_0)^2+(y-(y_0+c))^2 =(y-(y_0-c))^2\\\\\n\\iff &(x-x_0)^2+y^2-2(y_0+c)y+(y_0+c)^2 = y^2-2(y_0-c)y+(y_0-c)^2\\\\\n\\iff &(x-x_0)^2+y^2-2(y_0+c)y+y_0^2+2y_0c+c^2 = y^2-2(y_0-c)y+y_0^2-2y_0c+c^2\\\\\n\\iff&(x-x_0)^2-4cy+4y_0c=0\\\\\n\\iff&y=y_0 + \\frac{1}{4c}(x-x_0)^2\n\\end{align}\n\\]\nThis is the element-hood test for the points in the parabola allowing us describe the set the parabola as the set of pairs\n\\[\n\\begin{equation}P_2:= \\{(x,y)\\,\\,|\\,\\,d_1=d_2\\}=\\{(x,y)\\,\\,|\\,\\,y-y_0=\\frac{1}{4c}(x-x_0)^2\\}\\end{equation}\n\\tag{1}\\]\nUsing the procedure point of view we write:\n\\[\n\\begin{equation}\\begin{split}p_2:\\,\\, &\\mathbb{R}\\longrightarrow \\mathbb{R}\\\\&x \\longrightarrow p_2(x):= \\frac{1}{4c}(x-x_0)^2+y_0\\end{split}\\end{equation}\n\\tag{2}\\]\nThese formulas are equivalent to ?@eq-poldeg2, the difference lies in what the parameters mean! While in ?@eq-poldeg2, the \\(a_0\\), \\(a_1\\) and \\(a_2\\) are just the weights of \\(1\\), \\(x\\) and \\(x^2\\); in Equation 1, the \\(x_0\\) and \\(y_0\\) are the vertex point of the function and \\(c\\) is distance from the focus. An analogous situation occurred in polynomials of degree one when we introduced the mediatrix equation.",
    "crumbs": [
      "Bried Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Focus-diretrix formulation"
    ]
  },
  {
    "objectID": "examples_of_pol_part_5.html#zeros-of-these-polynomials",
    "href": "examples_of_pol_part_5.html#zeros-of-these-polynomials",
    "title": "Focus-diretrix formulation",
    "section": "Zeros of these polynomials:",
    "text": "Zeros of these polynomials:\nWe wish now to zoom into polynomials given as Equation 1 or Equation 2 and see the zeros of these function are expressed in term of the diretrix and focus properties.\nTo find vertical axis interception is to find the point \\((x,y)\\) in \\(P_2\\) whose \\(x=0\\):\n\\[\n\\begin{equation}\\begin{cases}y-y_0=\\frac{1}{4c}(x-x_0)^2\\\\x=0\\end{cases}\\implies y-y_0=\\frac{1}{4c}(0-x_0)^2\\implies y=\\frac{1}{4c}x_0^2+y_0\\end{equation}\n\\]\nwhich tells us the interception point coordinates \\((0,\\frac{1}{4c}x_0^2+y_0)\\) belonging to \\(P_2\\).\nWhen seeking for the points with \\(y=0\\) we must be more careful since they may not exist (the parabola might be completely above the x-axis). By substituting again \\(y=0\\) in the element-hood test we get:\n\\[\n\\begin{equation}\\begin{cases}y-y_0=\\frac{1}{4c}(x-x_0)^2\\\\y=0\\end{cases}\\implies(x-x_0)^2=-4cy_0\\end{equation}\n\\]\nWe must be careful when solving for \\(x\\) the later equation since we must have a positive right hand side, this is only possible if the parameter \\(y_0\\leq 0\\). This is the condition for the existence of zeros, i.e., it is the condition that ensures us the parabola is not completely above the x-axis.\n\n\n\nA parabola and the x-axis (diretrix and focus NOT shown).\n\n\nSaying \\(y_0\\leq 0\\) either means, \\(y_0=0\\), in which case we immediately know \\(x=x_0\\) and thus \\((x_0,0)\\) is the ONLY horizontal axis interception (the vertex sits exactly at the x-axis); or it means \\(y_0&lt;0\\), in this case we solve the equation for \\(x\\) as follows\n\\[\n\\begin{equation}\\begin{split}(x-x_0)^2&=4c|y_0|\\\\&\\implies x-x_0=\\pm\\sqrt{4c|y_0|}\\\\&\\implies x=x_0\\pm \\sqrt{4c|y_0|}\\end{split}\\end{equation}\n\\]\nThere are two interception, at \\((x_0+ \\sqrt{4c|y_0|},0)\\) and another at \\((x_0-\\sqrt{4c|y_0|},0)\\), which is expected when part of the parabola is below the x-axis.",
    "crumbs": [
      "Bried Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Focus-diretrix formulation"
    ]
  },
  {
    "objectID": "examples_of_pol_part_5.html#when-x-is-large",
    "href": "examples_of_pol_part_5.html#when-x-is-large",
    "title": "Focus-diretrix formulation",
    "section": "When \\(x\\) is large:",
    "text": "When \\(x\\) is large:\nTo understand the behavior when \\(x\\) is very large, the extreme ends of \\(P_2\\), we have to note the following:\n\nObserve the term \\((x-x_0)^2\\), it is a squared of a number \\(x-x_0\\), hence always positive. This implies the smallest value it takes is zero.\nWhen \\(c\\) is positive, then \\((x-x_0)^2/4c\\) is positive for any \\(x\\), the minimum is zero when we set \\(x-x_0=0\\).\n\nThe procedure \\((x-x_0)^2/4c+y_0\\) results from the addition of a strictly positive number \\((x-x_0)^2/4c\\) to \\(y_0\\), hence the minimum value it can take is \\(y_0\\) when \\(x=x_0\\).\nNow let us turn into what happens with the function when \\(x\\) becomes very large (\\(x\\longrightarrow \\pm\\infty\\)). Another glance at the procedure tell us that the quantity \\((x-x_0)^2\\) becomes very large either way, multiplying it now by \\(1/4c\\) we must be clear about the sign of \\(c\\). If it is positive the function will grow without end in either case, if \\(c\\) is negative, it becomes very small.",
    "crumbs": [
      "Bried Notes",
      "Basic Maths",
      "Intro to Polynomials",
      "Focus-diretrix formulation"
    ]
  },
  {
    "objectID": "Example_of_pol_division_5.html",
    "href": "Example_of_pol_division_5.html",
    "title": "example_of_pol_division_5",
    "section": "",
    "text": "Another useful decomposition is:\n\\[ 1001=600+401 \\]\nHow would we argue in this case?\nGuess 1:\nObserving that \\(1001=600+401\\) and \\(5\\times 12 =60\\) we guess that actual solution \\(q\\) is close to \\(10\\times 5\\):\n\\[ 600+401 = \\overbrace{10 \\times 5}^{q} \\times 12 +r \\]\nwith the remainder \\(r=401\\). Since it is not between \\(0\\) and \\(12\\) we can do better. Lets break \\(401\\) by guessing what is \\(q'\\) and \\(r'\\) such that:\n\\[ 401 = q'\\times 12 + r' \\qquad 0\\leq r'&lt;12 \\]\nWe think \\(q'=3\\times10\\) is a good solution since \\(q'\\times 12 = 360\\), as a result the remainder is \\(r'=401-360=41\\)\nNot ideal, we want it at most \\(12\\). Therefore we again break into pieces:\n\\[ 41 = q''\\times 12 + r'' \\qquad 0\\leq r''&lt;12 \\]\nWith the guess \\(q''=3\\) and the current remainder is just \\(r''=41-36=5\\).\nNote, finally!! We have a remainder smaller than \\(12\\).\nCollecting out calculations we have:\n\\[ 1001 = \\overbrace{(10 \\times 5 + 3\\times 10 + 3)}^{q=83}\\times 12 + 5 \\]"
  },
  {
    "objectID": "Example_of_pol_division_5.html#example-5-100112-using-basic-principles-b",
    "href": "Example_of_pol_division_5.html#example-5-100112-using-basic-principles-b",
    "title": "example_of_pol_division_5",
    "section": "",
    "text": "Another useful decomposition is:\n\\[ 1001=600+401 \\]\nHow would we argue in this case?\nGuess 1:\nObserving that \\(1001=600+401\\) and \\(5\\times 12 =60\\) we guess that actual solution \\(q\\) is close to \\(10\\times 5\\):\n\\[ 600+401 = \\overbrace{10 \\times 5}^{q} \\times 12 +r \\]\nwith the remainder \\(r=401\\). Since it is not between \\(0\\) and \\(12\\) we can do better. Lets break \\(401\\) by guessing what is \\(q'\\) and \\(r'\\) such that:\n\\[ 401 = q'\\times 12 + r' \\qquad 0\\leq r'&lt;12 \\]\nWe think \\(q'=3\\times10\\) is a good solution since \\(q'\\times 12 = 360\\), as a result the remainder is \\(r'=401-360=41\\)\nNot ideal, we want it at most \\(12\\). Therefore we again break into pieces:\n\\[ 41 = q''\\times 12 + r'' \\qquad 0\\leq r''&lt;12 \\]\nWith the guess \\(q''=3\\) and the current remainder is just \\(r''=41-36=5\\).\nNote, finally!! We have a remainder smaller than \\(12\\).\nCollecting out calculations we have:\n\\[ 1001 = \\overbrace{(10 \\times 5 + 3\\times 10 + 3)}^{q=83}\\times 12 + 5 \\]"
  },
  {
    "objectID": "Example_of_pol_division_5.html#commentary",
    "href": "Example_of_pol_division_5.html#commentary",
    "title": "example_of_pol_division_5",
    "section": "Commentary",
    "text": "Commentary\nBreaking \\(p\\) into powers of then has proven to be useful to give us clues about what values of \\(q\\) to guess. Another strategy is to break \\(q\\) (in the example above \\(p=1001\\)) into a large and a smaller part.\n\\[ p=[\\text{large part}]+[\\text{small part}]  \\]\nAnd then guess the best you can the \\(q\\) that cancels the large part.\nFor example \\(1001=100\\cdot 10+1\\) where the large part is \\(100\\cdot10\\), additionally \\(100\\) is divisible by \\(12\\) since its larger.\nThere are many ways to break \\(p\\), experience will tell you what is ideal or not."
  },
  {
    "objectID": "favourites.html",
    "href": "favourites.html",
    "title": "Randomm",
    "section": "",
    "text": "beans\nseeds\ntofu\nlegumes\nmore"
  },
  {
    "objectID": "favourites.html#my-list-of-things",
    "href": "favourites.html#my-list-of-things",
    "title": "Randomm",
    "section": "",
    "text": "beans\nseeds\ntofu\nlegumes\nmore"
  },
  {
    "objectID": "favourites.html#fun-picture-1",
    "href": "favourites.html#fun-picture-1",
    "title": "Randomm",
    "section": "Fun Picture 1",
    "text": "Fun Picture 1\n\n\n\n\n\n\nFigure 1: When you keep using the same strategy, expecting …..\n\n\n\n… unless you are having fun; some music ."
  },
  {
    "objectID": "favourites.html#fun-picture-2",
    "href": "favourites.html#fun-picture-2",
    "title": "Randomm",
    "section": "Fun Picture 2",
    "text": "Fun Picture 2\n\n\n\n\n\n\nFigure 2: When you keep using the same strategy, expecting …..\n\n\n\nTrue, both pictures have the same caption"
  },
  {
    "objectID": "favourites.html#best-movies-of-all-time.",
    "href": "favourites.html#best-movies-of-all-time.",
    "title": "Randomm",
    "section": "Best Movies of ALL TIME.",
    "text": "Best Movies of ALL TIME.\n\nSucker Punch\nReturn of the King\nMatrix: Reloaded\nMatrix: Revolution\nMatrix\nAnything Rambo or Steven Seagel\nEtc.\n(no more questions)"
  },
  {
    "objectID": "favourites.html#useful-links",
    "href": "favourites.html#useful-links",
    "title": "Randomm",
    "section": "Useful links",
    "text": "Useful links\n\nPeace of Mind good!\nArt or whatever..\nFunFun Short"
  },
  {
    "objectID": "favourites.html#oracle-advice-for-2024",
    "href": "favourites.html#oracle-advice-for-2024",
    "title": "Randomm",
    "section": "Oracle advice for 2024",
    "text": "Oracle advice for 2024\n\n\\[ 2024 = 2^3 \\times 11 \\times 23 \\]\n\nI actually payed for this advice."
  },
  {
    "objectID": "favourites.html#the-secret-code",
    "href": "favourites.html#the-secret-code",
    "title": "Randomm",
    "section": "The Secret Code",
    "text": "The Secret Code\ndef Update(x):\n  return 10 * x\n…that solves all Earth’s problems according to the gospel of Marx."
  },
  {
    "objectID": "favourites.html#a-one-inch-long-equation",
    "href": "favourites.html#a-one-inch-long-equation",
    "title": "Randomm",
    "section": "A one inch-long Equation",
    "text": "A one inch-long Equation\n\\[ F = ma \\]\n\n\n\n\n\n\nTip\n\n\n\nThe equation is a differential vector equation. Aesthetically, it is a pleasing equation as written, but it is more meaningfully pleasing by switching the right and left hand sides."
  },
  {
    "objectID": "favourites.html#testing-the-dirac-notation",
    "href": "favourites.html#testing-the-dirac-notation",
    "title": "Randomm",
    "section": "Testing the Dirac Notation",
    "text": "Testing the Dirac Notation\n\\[ i\\hbar\\frac{\\partial |\\psi\\rangle}{\\partial t} = \\hat{H}|\\psi\\rangle  \\]\nIt works."
  },
  {
    "objectID": "favourites.html#the-song-for-2024",
    "href": "favourites.html#the-song-for-2024",
    "title": "Randomm",
    "section": "THE Song for 2024",
    "text": "THE Song for 2024\n\n\nReferences for this outstanding site:\nMaster-Cohen Tannoudji, Quantum Mechanics, f#ing-Wiley"
  },
  {
    "objectID": "functions_as_relations.html",
    "href": "functions_as_relations.html",
    "title": "asymptotic to MARS",
    "section": "",
    "text": "A relation is a general connection between elements of two sets, consider the sets\n\\[\\begin{equation}\\begin{split}&Friends\\coloneqq \\{\\text{Abby, Eve, John, Hugo}\\}\\\\&Heights\\coloneqq \\{\\dots,1.60,1.70,1.80,1.90,\\dots\\}\\end{split} \\end{equation}\\]\nA connection between elements of both sets can be established by making a list of ordered pairs of the form \\((friend,height)\\), this can be done in many ways, here are two examples:\n\\[ \\begin{align}\n&G\\coloneqq \\{(\\text{Abby,1.70}),(\\text{Eve,1.80}),(\\text{Eve,1.70}),(\\text{John,1.80}),(\\text{Hugo,1.60})\\}\\\\\n&F\\coloneqq \\{(\\text{Abby,1.70}),(\\text{Eve,1.70}),(\\text{John,1.80}),(\\text{Hugo,1.60})\\}\n\\end{align}  \\tag{1}\\]\nThe process of choosing elements of \\(Friends\\) and \\(Heigts\\) and pairing them to build \\(G\\) or \\(F\\) can be viewed another way, we start by constructing all possible pair and save them into large list we call the Cartesian product between Friends and Heights, denoted by\n\\[ \\begin{equation}Friends \\times Heights \\coloneqq \\{(\\text{friend,height})\\,\\,|\\,\\,\\text{friend} \\in \\text{Friends}\\,\\, \\textit{and}\\,\\, \\text{height} \\in \\text{Heights}\\}\\end{equation}  \\tag{2}\\]\nThen we choose from \\(Friends \\times Heights\\) then pairs we want for \\(G\\) or \\(F\\).\nFrom this point of view, every subset of \\(Friends \\times Heights\\) is a relation and in particular we have:\n\\[\n\\begin{equation}\\begin{split} G \\subset Friends \\times Heights\\\\ F \\subset Friends \\times Heights\\\\ \\end{split} \\end{equation}\n\\]\n\\(G\\) and \\(F\\) are relations but only the latter achieves the status of a function since \\(F\\) has a special property not share with \\(G\\). Close inspection of \\(F\\) reveals that if one chooses a friend, say Eve, the function \\(F\\) tells us immediately and unquestionably that her height is \\(1.70\\). On the other hand, \\(G\\) wont since we find two pairs, which assign Eve with \\(1.80\\) or \\(1.70\\). Observation of the remainder pairs in \\(F\\) shows that the pairing in is in such a way that for any \\(friend\\) we choose, we have an associated \\(height\\) that is unique, as a result each first entry of the pairs only occurs once in the list Equation 1, thus avoiding the ambiguity found in \\(G\\) of having two heights assigned to the same friend. This is the special property that a list of ordered pairs (i.e. relation) must have to be classified as a function; irrelevant to this is whether when we pick an height we have none, or one or more friends. For example the next subsets of Equation 2 assign the same height to two or all friends:\n\\[ \\begin{equation}\\begin{split}I\\coloneqq \\{(\\text{Abby,1.80}),(\\text{Eve,1.60}),(\\text{John,1.80}),(\\text{Hugo,1.60})\\}\\\\J\\coloneqq \\{(\\text{Abby,1.80}),(\\text{Eve,1.80}),(\\text{John,1.80}),(\\text{Hugo,1.80})\\}\\end{split}\\end{equation} \\]\nThe lists \\(I\\) and \\(J\\) are relation and also functions, as each friend only has one height, thus appearing only one time in the list.\nFrom this discussion we can distill the essence of what a function is in the following definition:\n\nDefinition 1  \n\nLet \\(X\\) and \\(Y\\) be two sets. A function \\(F\\) is a list of pairs \\((x,y)\\) with \\(x\\) belonging to \\(X\\) and \\(y\\) in \\(Y\\) with the special property that each \\(x\\) is associated uniquely with one \\(y\\) (the reverse may or may not be true). In other words, \\(F\\) is a special subset of \\(X\\times Y\\) that makes the following statement true\n\\[\n\\forall x : \\exists ! y : (x,y) \\in F\n\\tag{3}\\]\nThe domain of \\(F\\) is the set of all \\(x\\)’s appearing in the pairs of \\(F\\), while the range is the the set of \\(y\\)’s; these sets are named \\(D_F\\) and \\(R_F\\) respectively. The set \\(Y\\) is called the codomain of \\(F\\) and may contain or be equal to \\(R_F\\); \\(X\\) doesn’t have an official name, lets call it the starting set, and may contain or be equal to \\(D_F\\), many times \\(X=D_F\\).\n\n\nExercise\n\n\n\n\n\n\nCommentary\n\n\n\nThe statement in Equation 3 is neither true nor false until we specify what \\(F\\) is. It works like an “equation” where the unknown in \\(F\\). We read it as follows: For all \\(x\\) there is only one \\(y\\) that make true \\((x,y)\\in F\\). This can only occur provided we chose an \\(F\\) where all pairs have \\(x\\)’s assigned to unique \\(y\\)’s. The expression \\(\\forall x : \\exists ! y : (x,y) \\in\\) acts on \\(F\\), by checking its content.",
    "crumbs": [
      "Bried Notes",
      "Basic Maths",
      "Functions",
      "Functions as relations"
    ]
  },
  {
    "objectID": "functions_as_relations.html#functions-as-relations",
    "href": "functions_as_relations.html#functions-as-relations",
    "title": "asymptotic to MARS",
    "section": "",
    "text": "A relation is a general connection between elements of two sets, consider the sets\n\\[\\begin{equation}\\begin{split}&Friends\\coloneqq \\{\\text{Abby, Eve, John, Hugo}\\}\\\\&Heights\\coloneqq \\{\\dots,1.60,1.70,1.80,1.90,\\dots\\}\\end{split} \\end{equation}\\]\nA connection between elements of both sets can be established by making a list of ordered pairs of the form \\((friend,height)\\), this can be done in many ways, here are two examples:\n\\[ \\begin{align}\n&G\\coloneqq \\{(\\text{Abby,1.70}),(\\text{Eve,1.80}),(\\text{Eve,1.70}),(\\text{John,1.80}),(\\text{Hugo,1.60})\\}\\\\\n&F\\coloneqq \\{(\\text{Abby,1.70}),(\\text{Eve,1.70}),(\\text{John,1.80}),(\\text{Hugo,1.60})\\}\n\\end{align}  \\tag{1}\\]\nThe process of choosing elements of \\(Friends\\) and \\(Heigts\\) and pairing them to build \\(G\\) or \\(F\\) can be viewed another way, we start by constructing all possible pair and save them into large list we call the Cartesian product between Friends and Heights, denoted by\n\\[ \\begin{equation}Friends \\times Heights \\coloneqq \\{(\\text{friend,height})\\,\\,|\\,\\,\\text{friend} \\in \\text{Friends}\\,\\, \\textit{and}\\,\\, \\text{height} \\in \\text{Heights}\\}\\end{equation}  \\tag{2}\\]\nThen we choose from \\(Friends \\times Heights\\) then pairs we want for \\(G\\) or \\(F\\).\nFrom this point of view, every subset of \\(Friends \\times Heights\\) is a relation and in particular we have:\n\\[\n\\begin{equation}\\begin{split} G \\subset Friends \\times Heights\\\\ F \\subset Friends \\times Heights\\\\ \\end{split} \\end{equation}\n\\]\n\\(G\\) and \\(F\\) are relations but only the latter achieves the status of a function since \\(F\\) has a special property not share with \\(G\\). Close inspection of \\(F\\) reveals that if one chooses a friend, say Eve, the function \\(F\\) tells us immediately and unquestionably that her height is \\(1.70\\). On the other hand, \\(G\\) wont since we find two pairs, which assign Eve with \\(1.80\\) or \\(1.70\\). Observation of the remainder pairs in \\(F\\) shows that the pairing in is in such a way that for any \\(friend\\) we choose, we have an associated \\(height\\) that is unique, as a result each first entry of the pairs only occurs once in the list Equation 1, thus avoiding the ambiguity found in \\(G\\) of having two heights assigned to the same friend. This is the special property that a list of ordered pairs (i.e. relation) must have to be classified as a function; irrelevant to this is whether when we pick an height we have none, or one or more friends. For example the next subsets of Equation 2 assign the same height to two or all friends:\n\\[ \\begin{equation}\\begin{split}I\\coloneqq \\{(\\text{Abby,1.80}),(\\text{Eve,1.60}),(\\text{John,1.80}),(\\text{Hugo,1.60})\\}\\\\J\\coloneqq \\{(\\text{Abby,1.80}),(\\text{Eve,1.80}),(\\text{John,1.80}),(\\text{Hugo,1.80})\\}\\end{split}\\end{equation} \\]\nThe lists \\(I\\) and \\(J\\) are relation and also functions, as each friend only has one height, thus appearing only one time in the list.\nFrom this discussion we can distill the essence of what a function is in the following definition:\n\nDefinition 1  \n\nLet \\(X\\) and \\(Y\\) be two sets. A function \\(F\\) is a list of pairs \\((x,y)\\) with \\(x\\) belonging to \\(X\\) and \\(y\\) in \\(Y\\) with the special property that each \\(x\\) is associated uniquely with one \\(y\\) (the reverse may or may not be true). In other words, \\(F\\) is a special subset of \\(X\\times Y\\) that makes the following statement true\n\\[\n\\forall x : \\exists ! y : (x,y) \\in F\n\\tag{3}\\]\nThe domain of \\(F\\) is the set of all \\(x\\)’s appearing in the pairs of \\(F\\), while the range is the the set of \\(y\\)’s; these sets are named \\(D_F\\) and \\(R_F\\) respectively. The set \\(Y\\) is called the codomain of \\(F\\) and may contain or be equal to \\(R_F\\); \\(X\\) doesn’t have an official name, lets call it the starting set, and may contain or be equal to \\(D_F\\), many times \\(X=D_F\\).\n\n\nExercise\n\n\n\n\n\n\nCommentary\n\n\n\nThe statement in Equation 3 is neither true nor false until we specify what \\(F\\) is. It works like an “equation” where the unknown in \\(F\\). We read it as follows: For all \\(x\\) there is only one \\(y\\) that make true \\((x,y)\\in F\\). This can only occur provided we chose an \\(F\\) where all pairs have \\(x\\)’s assigned to unique \\(y\\)’s. The expression \\(\\forall x : \\exists ! y : (x,y) \\in\\) acts on \\(F\\), by checking its content.",
    "crumbs": [
      "Bried Notes",
      "Basic Maths",
      "Functions",
      "Functions as relations"
    ]
  },
  {
    "objectID": "implicit_derivative.html",
    "href": "implicit_derivative.html",
    "title": "Implicit derivatives",
    "section": "",
    "text": "A more suitable title for this section is “what can we say about the derivative of an a function defined implicitly?”\nWith that, the first step is to understand what is an implicit function, only then we bother with its derivative.",
    "crumbs": [
      "Bried Notes",
      "Calculus",
      "Implicit derivative"
    ]
  },
  {
    "objectID": "implicit_derivative.html#implicit-functions",
    "href": "implicit_derivative.html#implicit-functions",
    "title": "Implicit derivatives",
    "section": "Implicit functions",
    "text": "Implicit functions\nAn implicit function is a function defined by an equation. An example shows well the idea:\nConsider the equation \\(x^2 + y^2=1\\). The truthset of this statement is the circle of radius one:\n\\[\n\\{(x,y)\\in\\mathbb{R}^2\\,\\,|\\,\\,x^2+y^2=1\\}\n\\]\nWe can use it to define functions, after all functions are also sets of ordered pair albeit with the property that each \\(x\\) can only appear one time in the set (please note this is not the case for the circle, each \\(x\\) occurs twice.)\nWhat we are going to do is to slice, restrict, the set in such a way as a function comes out. There are many ways of doing so, but here is the more useful ones: Make the upper half of the circle one function and the bottom half another.\nIn mathematical notation, the upper half is defined as:\n\\[\n\\begin{align}\nf: &[-1,1] \\longrightarrow \\mathbb{R}\\\\\n&x\\longmapsto f(x):=[\\text{positive $y$ solution of $x^2+y^2=1$}]\n\\end{align}\n\\]\nthe bottom half is:\n\\[\n\\begin{align}\ng: &[-1,1] \\longrightarrow \\mathbb{R}\\\\\n&x\\longmapsto g(x):=[\\text{negative $y$ solution of $x^2+y^2=1$}]\n\\end{align}\n\\]\nThe functions \\(f\\) and \\(g\\) are defined implicitly, because the procedure that one must follow to assign \\(y\\) to the chosen \\(x\\) is implict in solving an equation, in this case \\(x^2 + y^2=1\\); by implicit I mean, the sequence of steps you must follow to solve it, is not given to you explicitly.\nA more practical way to look at the upper half (similarly for the bottom half) is to say that \\(y\\) is the positive function of \\(x\\) such that it satisfies the equation \\(x^2 +y^2=1\\), and to make that evident we can write:\n\\[\nx^2 +y(x)^2=1\n\\]This notation tell us: \\(y\\) is a function of \\(x\\), though how we get the value of \\(y(x)\\) from \\(x\\) requires to solve the equation with whatever means are necessary. It is precise this “with whatever means…” that promotes the \\(y(x)\\) function (equivalently the \\(f\\) function) to the class of implicit functions.",
    "crumbs": [
      "Bried Notes",
      "Calculus",
      "Implicit derivative"
    ]
  },
  {
    "objectID": "implicit_derivative.html#derivative-of-an-implicit-function",
    "href": "implicit_derivative.html#derivative-of-an-implicit-function",
    "title": "Implicit derivatives",
    "section": "Derivative of an implicit function",
    "text": "Derivative of an implicit function\nWe can almost antecipate, since the function is not given explicitly, chances are, its derivative is also not given explicitly.\nPlease note that when a function is given explicitly, we used the derivatives rules to obtain an explicit derivative function.\nWhen dealing with implicit function, i.e., functions on which we only know the statement - in the case above \\(x^2+y^2=1\\) - that it obeys, the same thing occurs with its derivative, we’ll only know a statement it obeys.\nThe way to obtain that statement is to use the chain rule.\nTake the derivative of the lhs and rhs of \\(x^2 +y^2=1\\), the result is:\n\\[\n(x^2+y^2)'=1'\n\\implies 2x+2y(x)y'(x) = 0\n\\]\nOn the rhs we find the statement: whatever the function \\(y'(x)\\) is, it obeys that equation, hence to find what is the value of \\(y'(x)\\) corresponding to the chosen \\(x\\), we have to solve the equation \\(2x+2y(x)y'(x) = 0\\). To do that, chose an \\(x\\), compute \\(y(x)\\) using whatever means necessary (the function \\(y(x)\\) is implicit!), plug in that result in the equation above and in turn solve it for \\(y'(x)\\).\nAs you can see the sequence of steps that one must follow to arrive at one evaluation is implicit!",
    "crumbs": [
      "Bried Notes",
      "Calculus",
      "Implicit derivative"
    ]
  },
  {
    "objectID": "inverse_matrix.html",
    "href": "inverse_matrix.html",
    "title": "Inverse Matrix",
    "section": "",
    "text": "Only full rank square matrices can have inverses. The inverse of \\(A\\) is the unique solution \\(A^{-1}\\) of the equation:\n\\[\nAA^{-1} = I = A^{-1}A\n\\]\nHow do we solve this equation?",
    "crumbs": [
      "Bried Notes",
      "Linear Algebra",
      "Inverse Matrix"
    ]
  },
  {
    "objectID": "inverse_matrix.html#how-to-compute-an-inverse",
    "href": "inverse_matrix.html#how-to-compute-an-inverse",
    "title": "Inverse Matrix",
    "section": "How to compute an inverse?",
    "text": "How to compute an inverse?\nConsider the matrix\n\\[\nA=\\left(\\begin{matrix}1 & 2 \\\\3 & 5  \\end{matrix}\\right)\n\\]\nThe goal is to solve:\n\\[\n\\left(\\begin{matrix}1 & 2 \\\\3 & 5  \\end{matrix}\\right)\\left(\\begin{matrix}a & c \\\\b & d  \\end{matrix}\\right)=\\left(\\begin{matrix}1 & 0 \\\\0 & 1  \\end{matrix}\\right)\n\\]\nClose inspection of the product between the matrices shows us that we can break this problem into two independent problems:\n\\[\n\\begin{align}\n&\\left(\\begin{matrix}1 & 2 \\\\3 & 5  \\end{matrix}\\right)\\left(\\begin{matrix}a\\\\b  \\end{matrix}\\right) = \\left(\\begin{matrix}1\\\\0  \\end{matrix}\\right) \\\\\n&\\left(\\begin{matrix}1 & 2 \\\\3 & 5  \\end{matrix}\\right)\\left(\\begin{matrix}c\\\\d  \\end{matrix}\\right) = \\left(\\begin{matrix}0\\\\1  \\end{matrix}\\right)\n\\end{align}\n\\]\nWith elimination we find:\n\\[\n\\begin{align}\n&\\left(\\begin{matrix}1 & 2 \\\\3 & 5  \\end{matrix}\\;\\middle|\\;\\begin{matrix}1\\\\0 \\end{matrix}\\right)\n\\overset{l_2'=l_2-3l_1}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 2 \\\\0 & -1  \\end{matrix}\\;\\middle|\\;\\begin{matrix}1\\\\-3 \\end{matrix}\\right)\n\\overset{l_2'=-l_2}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 2 \\\\0 & 1  \\end{matrix}\\;\\middle|\\;\\begin{matrix}1\\\\3 \\end{matrix}\\right)\n\\overset{l_1'=l_1-2l_2}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 0 \\\\0 & 1  \\end{matrix}\\;\\middle|\\;\\begin{matrix}-5\\\\3 \\end{matrix}\\right)\\\\\n&\\left(\\begin{matrix}1 & 2 \\\\3 & 5  \\end{matrix}\\;\\middle|\\;\\begin{matrix}0\\\\1 \\end{matrix}\\right)\n\\overset{l_2'=l_2-3l_1}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 2 \\\\0 & -1  \\end{matrix}\\;\\middle|\\;\\begin{matrix}0\\\\1 \\end{matrix}\\right)\n\\overset{l_2'=-l_2}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 2 \\\\0 & 1  \\end{matrix}\\;\\middle|\\;\\begin{matrix}0\\\\-1 \\end{matrix}\\right)\n\\overset{l_1'=l_1-2l_2}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 0 \\\\0 & 1  \\end{matrix}\\;\\middle|\\;\\begin{matrix}2\\\\-1 \\end{matrix}\\right)\n\\end{align}\n\\tag{1}\\]\nFrom the right systems we conclude that the inverse matrix is:\n\\[\nA^{-1}=\\left(\\begin{matrix}-5 & 3 \\\\2 & -1  \\end{matrix}\\right)\n\\]\nChecking the result:\n\\[\nAA^{-1}=\\left(\\begin{matrix}1 & 2 \\\\3 & 5  \\end{matrix}\\right)\\left(\\begin{matrix}-5 & 2 \\\\3 & -1  \\end{matrix}\\right)=\\left(\\begin{matrix}1 & 0 \\\\0 & 1  \\end{matrix}\\right)\n\\]\nIt works!\nObservations:\n\nObserving the calculations Equation 1 we notice the arrow operators are the same for both systems, we solved both systems with the intent (as elimination method prescribes) of simplifying the matrix as much as possible, since in this case we are dealing with a full rank matrix, the end point simplification must be an identity matrix. What is different is the right vectors, which in the end tell us the columns of the inverse.\n\n\n\n\n\n\n\nRecall\n\n\n\nIn case it is not clear why the columns \\((-5,3)\\) and \\((2,-1)\\) tell us the columns of \\(A^{-1}\\), remember the meaning of the extended matrix notation in Equation 1:\n\\[\n\\begin{align}\n&\\left(\\begin{matrix}1 & 0 \\\\0 & 1  \\end{matrix}\\;\\middle|\\;\\begin{matrix}-5\\\\3 \\end{matrix}\\right)\\leftrightsquigarrow\\left(\\begin{matrix}1 & 0 \\\\0 & 1  \\end{matrix}\\right)\\left(\\begin{matrix}a\\\\b \\end{matrix}\\right)=\\left(\\begin{matrix}-5\\\\3\\end{matrix}\\right) \\implies \\left(\\begin{matrix}a\\\\b \\end{matrix}\\right)=\\left(\\begin{matrix}-5\\\\3\\end{matrix}\\right)\\\\\n&\\left(\\begin{matrix}1 & 0 \\\\0 & 1  \\end{matrix}\\;\\middle|\\;\\begin{matrix}2\\\\-1 \\end{matrix}\\right)\\leftrightsquigarrow\\left(\\begin{matrix}1 & 0 \\\\0 & 1  \\end{matrix}\\right)\\left(\\begin{matrix}c\\\\d \\end{matrix}\\right)=\\left(\\begin{matrix}2\\\\-1\\end{matrix}\\right) \\implies \\left(\\begin{matrix}c\\\\d \\end{matrix}\\right)=\\left(\\begin{matrix}2\\\\-1\\end{matrix}\\right)\n\\end{align}\n\\]\n\n\n\nSince the calculations operations in Equation 1 are the same, the main difference being the right hand side vector on which they are applied, rather than doing the same thing twice, we do it one shot adopting the following notation for Equation 1:\n\\[\n\\left(\\begin{matrix}1 & 2 \\\\3 & 5  \\end{matrix}\\;\\middle|\\;\\begin{matrix}1 & 0\\\\0 & 1\\end{matrix}\\right)\n\\overset{l_2'=l_2-3l_1}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 2 \\\\0 & -1  \\end{matrix}\\;\\middle|\\;\\begin{matrix}1 & 0\\\\-3 &1 \\end{matrix}\\right)\n\\overset{l_2'=-l_2}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 2 \\\\0 & 1  \\end{matrix}\\;\\middle|\\;\\begin{matrix}1 & 0\\\\3 & -1\\end{matrix}\\right)\n\\overset{l_1'=l_1-2l_2}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 0 \\\\0 & 1  \\end{matrix}\\;\\middle|\\;\\begin{matrix}-5 & 2\\\\3 & -1 \\end{matrix}\\right)\\\\\n\\]\nWe turn the matrix \\(A\\) into an \\(I\\) with elimination operators, those applied to the columns of \\(I\\) yield the \\(A^{-1}\\).\n\nWe distill the essential strategy to compute the inverse of \\(A\\) from the previous example:\n\nStep 1: Write the extended matrix \\([A|I]\\)\nStep 2: Apply elimination operation on all entries bellow and above the diagonal so that the block \\(A\\) ends becoming an \\(I\\), in the process apply the same operation on the extended \\(I\\) block.\nStep 3: The final result has the form \\([I| A^{-1}]\\), read the inverse matrix entries from this notation and check your results by computing \\(AA^{-1}=I\\).\nStep 4: Congratulations!",
    "crumbs": [
      "Bried Notes",
      "Linear Algebra",
      "Inverse Matrix"
    ]
  },
  {
    "objectID": "inverse_matrix.html#matrix-with-no-inverse",
    "href": "inverse_matrix.html#matrix-with-no-inverse",
    "title": "Inverse Matrix",
    "section": "Matrix with no inverse",
    "text": "Matrix with no inverse\nWhen a matrix does not have a full rank, it has no inverse [comment: in Linear function section we’ll explain why in terms of maps, for now the determinant =0 suffices, the determinant argument is a practical argument, the conceptual argument requires the idea of linear function, but that will only come later]\nThe reason being the determinant is zero and as a result: the calculation Equation 2 cannot be performed.\nNot every square matrix will have an inverse. For example \\(E_1\\) is square, thus it may or not have an inverse \\(E_1^{-1}\\), on the other hand the matrix \\(A\\) is rectangular, thus no inverse exists.\nHow do we compute an inverse of a matrix?\nThe \\(E\\)’s matrices have inverses, always and thus:\n\\[\nAx=b \\iff EAx=Eb \\iff A'x=b'\n\\]\nare equivalent and and as a result their truth sets are equal! Finding the truth set of the later is easier than the former.",
    "crumbs": [
      "Bried Notes",
      "Linear Algebra",
      "Inverse Matrix"
    ]
  },
  {
    "objectID": "limits_of_sequences.html",
    "href": "limits_of_sequences.html",
    "title": "Limits of sequences",
    "section": "",
    "text": "Sequences are also functions, and as a result many of the concepts we introduced for functions apply to sequences as well. For instance, we may:\n\nmake a table out of them\nplot them in some relevant window\ndescribe in detail some slices of it; in particular the region when \\(n\\) is large.\n\nIn these notes we will focus on the later.\nConsider the sequence:\n\\[\nu_n=2+\\frac{1}{n}\n\\]whose graph for \\(n=1,.. ,20\\) is:\n\n\n\n\n\nA few things strikes us in the region when \\(n\\) is large:\n\nAs \\(n\\) gets larger and larger, then \\(2+1/n\\) gets closer and closer to \\(2\\).\nThe values \\(u_n\\) never actually reaches the value of \\(2\\) because \\(1/n\\) is never zero, no matter how large \\(n\\) is.\n\nIn other word: the sequence \\(u_n\\) eventually lies within any interval centered at \\(2\\).\nLets make this wording rigorous:\n\nTo say that an interval centered at \\(2\\) contains a sequence means: \\(|2-u_n|&lt;\\varepsilon\\). The radius of the interval is \\(\\varepsilon\\) and is positive, because we are speaking about an interval, not a point like thing!\nThe eventually words means that after some value \\(N\\), any \\(n\\), guarantees \\(|2-u_n|&lt;\\varepsilon\\)\nThe any word means, that the above is true for any positive \\(\\varepsilon\\)\n\nPutting it all together:\n\n(1) For any \\(\\varepsilon&gt;0\\), the following is true:\n\n(2) That there exists some \\(N\\) that guarantees that:\n\n(3) for any \\(n\\geq N\\) the following statement is true\n\n(4) the statement that \\(|2-u_n|&lt;\\varepsilon\\)\n\n\n\n\nWe write this description using complicated mathematical notation as:\n\\[ \\overbrace{\\forall \\varepsilon &gt;0 }^{(1)}:\\overbrace{\\exists N}^{(2)}:\\overbrace{\\forall  n\\geq N}^{(3)}: (\\overbrace{ |2-u_n|&lt;\\varepsilon)}^{(4)}  \\tag{1}\\]\nSince it is just complicated to write Equation 1, the following abbreviation is introduced:\n\\[\n\\lim_{n\\longrightarrow \\infty}u_n = 2\n\\]\nThe lhs should be view as a single symbol that says: “the limit value of \\(2+1/n\\) when \\(n\\) gets larger and larger (infinity is not a number)”. The rhs tell us that \\(2\\) is the limit value; or better said, the point around which the sequence eventually always lurks.\n\n\n\n\n\n\nCommentary\n\n\n\nSequences of the form \\(x_0\\pm1/n\\) are important since they describe a step by step approach to the number \\(x_0\\) from values above \\(+\\) or below \\(-\\); without never actually getting there. They will be relevant when computing.",
    "crumbs": [
      "Bried Notes",
      "Calculus",
      "Limits of sequences"
    ]
  },
  {
    "objectID": "limits_of_sequences.html#an-example",
    "href": "limits_of_sequences.html#an-example",
    "title": "Limits of sequences",
    "section": "An example",
    "text": "An example\nWe will prove that:\n\\[\n\\lim \\frac{4n+1}{2n}=2\n\\]\nMeaning we want to prove that:\n\\[\n\\forall \\varepsilon &gt;0:\\exists N:\\forall n\\geq N : |2-(4n+1)/2n|&lt;\\varepsilon\n\\]\nWe proceed by unpacking the meaning with indented assumptions:\n\nAssume we have some value \\(\\varepsilon\\), though it is arbitrary:\n\nWe seek to find an \\(N\\) that makes \\(\\forall n\\geq N : |2-(4n+1)/2n|&lt;\\varepsilon\\) true, i.e., we seek a lower bound on \\(n\\) that makes this true:\n\nTo do that, we assume that we have some value \\(n^*\\) which is larger than the \\(N\\) and which makes the statement \\(|2-(4n^*+1)/2n^*|&lt;\\varepsilon\\) true. This much we know about \\(n^*\\). No special conclusion was taken, yet.\nNow simplify this statement and see if we find something new about \\(n^*\\):\n\\[\n\\begin{align}\n& |2-\\frac{4n^*+1}{2n^*}|&lt;\\varepsilon\\\\\n&\\implies |\\frac{4n^*-4n^*+1}{2n^*}|&lt;\\varepsilon\\\\\n&\\implies |\\frac{1}{2n^*}|&lt;\\varepsilon\\\\\n&\\implies n^*&gt;\\frac{1}{2\\varepsilon}\n\\end{align}\n\\]\n\n\n\nNow, this is significant! Lets recollect what we found in the three indentations: Given a radius \\(\\varepsilon\\), a \\(n^*\\)bounded below by \\(N\\) and which obeys our statement is a \\(n^*\\) larger than \\(1/2\\varepsilon\\). The consequences of this are inescapable. If you choose smaller and smaller \\(\\varepsilon\\) the larger and larger \\(n^*\\) must be. \\(\\varepsilon\\) can be make as small as we wish and we can always find, through the inequality \\(n^*&gt;1/2\\varepsilon\\), a \\(n^*\\) that makes \\(|2-(4n^*+1)/2n^*|&lt;\\varepsilon\\) true. And since \\(N\\) is smaller than \\(n^*\\) we can, in conclusion, always say it exist a lower bound \\(N\\) for this \\(n^*\\).\nTo say \\(\\lim \\frac{4n+1}{2n}=2\\) is just an abbreviation for the whole reasoning made above!",
    "crumbs": [
      "Bried Notes",
      "Calculus",
      "Limits of sequences"
    ]
  },
  {
    "objectID": "limits_of_sequences.html#basic-sequences-limits",
    "href": "limits_of_sequences.html#basic-sequences-limits",
    "title": "Limits of sequences",
    "section": "Basic sequences limits",
    "text": "Basic sequences limits\nUsing arguments such as the one shown above we can establish a myriad of limits for basic sequences, we are going ino the details of the proofs and just list the results:\n\n\\(\\lim \\frac{a}{b}=\\frac{a}{b}\\) provided \\(b\\not = 0\\)\n\\(\\lim \\frac{an+b}{cn+d}=\\frac{a}{c}\\) provided \\(c\\not = 0\\)\n\\(\\lim \\frac{1}{n^p} =0\\), if \\(p&gt;0\\)\nso on …\n\nWe can also prove basic rules for limits, an important one is:\n\nIf \\(a_n\\) and \\(b_n\\) are convergent to limits \\(a\\) and \\(b\\), then:\n\nthe sequence \\(a_n+b_n\\) converges to \\(a+b\\).\nthe sequence \\(a_nb_n\\) converges to \\(ab\\)\nthe sequence \\(a_n/b_n\\) converges to \\(a/b\\) if \\(b\\not=0\\)\nso on…\n\n\nThe idea behind these pre-made limits and the rules is similar to the one from derivatives: If you want to compute a limit of complicated sequence, then your goal is to rearrange that sequence and the limit in such a way that the basic pre-made limits can be used. Giving us the limit value of the complicated sequence.",
    "crumbs": [
      "Bried Notes",
      "Calculus",
      "Limits of sequences"
    ]
  },
  {
    "objectID": "linear_combinations.html",
    "href": "linear_combinations.html",
    "title": "Linear combinations of vectors",
    "section": "",
    "text": "We’ll consider a vector an array of numbers:\n\\[ \\begin{pmatrix}1\\\\5\\end{pmatrix}, \\begin{pmatrix}1\\\\-5\\\\0.1\\end{pmatrix},\\begin{pmatrix}x\\\\y\\\\z\\end{pmatrix} \\]\nWhat can you do with vectors?\nAnswer: linear combinations! Which can be viewed analytically or geometrically.\nAnalytical l.c. : A linear combination is a computation that looks like this:\n\\[ 2 \\mathbf{u}+4\\mathbf{v} = 2\\begin{pmatrix} 1\\\\3\\end{pmatrix}+4\\begin{pmatrix} 1\\\\-1\\end{pmatrix} \\]\nFirst multiply the vectors by the scalars, then add the vectors by adding the entries, the result is:\n\\[ \\begin{pmatrix} 2\\\\6\\end{pmatrix}+\\begin{pmatrix} 4\\\\-4\\end{pmatrix} = \\begin{pmatrix} 6\\\\2 \\end{pmatrix} \\]\nA generic linear combination looks like this:\n\\[ a \\mathbf{u}+b\\mathbf{v} = a\\begin{pmatrix} 1\\\\5\\end{pmatrix}+b\\begin{pmatrix} 2\\\\10\\end{pmatrix} = \\begin{pmatrix}a +2b\\\\5a +10b \\end{pmatrix} \\]\nwhere \\(a\\) and \\(b\\) are scalars.\nIf you know how to combine two vectors, you know how to combine three and so on.\n\nDefinition 1 The list of vectors \\(\\mathbf{u}_1,\\dots,\\mathbf{u}_k\\) from \\(\\mathbb{R}^n\\) (i.e. \\(\\mathbf{u}_i=(u_{i1},\\dots,u_{ik})\\)), can be linearly combined with numbers \\(c_1,\\dots,c_k\\) as:\n\\[\nc_1\\mathbf{u}_1+\\dots c_k\\mathbf{u}_k =: \\sum_{j=1}^kc_j\\mathbf{u}_j=:\\mathbf{v}\n\\]\nThis l.c. is a new vector of \\(\\mathbb{R}^n\\), call it \\(\\mathbf{v}\\).\n\nGeometrical l.c. : A l.c. is performed geometrically using the parallelogram rule:\n\n\nExercise 1 Solve 1.1 &gt; 2 &gt; c",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Linear Combinations"
    ]
  },
  {
    "objectID": "linear_functions.html",
    "href": "linear_functions.html",
    "title": "Linear functions",
    "section": "",
    "text": "Up to now, we learned a more pratical part of the course:\nIt always how to compute. Now we enter a more conceptual part of the course and organize ideas.",
    "crumbs": [
      "Bried Notes",
      "Linear Algebra",
      "Linear functions"
    ]
  },
  {
    "objectID": "linear_functions.html#is-it-linear-or-not",
    "href": "linear_functions.html#is-it-linear-or-not",
    "title": "Linear functions",
    "section": "Is it linear or not?",
    "text": "Is it linear or not?\n\nExample 1:\nConsider the function:\n\\[\n\\begin{align}f:\\mathbb{R}&\\longrightarrow \\mathbb{R}\\\\ x&\\longmapsto f(x):=ax\\end{align}\n\\]\nwhere \\(a\\) is some real constant. This is a typical high-school function also known as proportionality function aka polynomial of degree one.\nThe domain and codomain are both \\(\\mathbb{R}\\).\nTo check its linear we choose two generic elements of the domain \\(x_1\\) and \\(x_2\\) and l.c. them using a generic coefficients \\(c_1\\) and \\(c_2\\) :\n\\[\nf(c_1 x_1+ c_2 x_2) = a ( c_1 x_1+ c_2 x_2) = ac_1 x_1+ ac_2 x_2 = c_1f(x_1)+c_2f(x_2)\n\\]\nIndeed it is.\n\n\nExample 2:\nNow a function from \\(\\mathbb{R}\\) into \\(\\mathbb{R}^2\\)\n\\[\n\\begin{align}g:\\mathbb{R}&\\longrightarrow \\mathbb{R}^2\\\\ x&\\longmapsto g(x):=(2x,x)\\end{align}\n\\]\nIs this function linear?\n\\[\ng(c_1 x_1+ c_2 x_2)=(2(c_1 x_1+ c_2 x_2),c_1 x_1+ c_2 x_2) = c_1(2x_1+,x_1)+c_2(2x_2,x_2)=c_1g(x_1)+c_2g(x_2)\n\\]\nBecause our choices where arbitrary, yes, this \\(g\\) is linear.\n\n\nExample 3:\nThe projection function, picks out the \\(x\\) component of the vector \\((x,y)\\):\n\\[\n\\begin{align}\\pi:\\mathbb{R}^2&\\longrightarrow \\mathbb{R}\\\\ (x,y)&\\longmapsto \\pi(x,y):=x\\end{align}\n\\]\nIs it linear?\n\\[\n\\pi(c_1(x_1,y_1)+c_2(x_2,y_2)) = \\pi (c_1x_1+c_2x_2,c_1y_1+c_2y_2)=c_1x_1+c_2x_2 =c_1 \\pi(x_1,y_1) + c_2\\pi(x_2,y_2)\n\\]\nA l.c. of inputs - \\((x_1,y_1)\\) and \\((x_2,y_2)\\) - yields the same l.c. of outputs - \\(\\pi(x_1,y_1)\\) and \\(\\pi(x_2,y_2)\\).\n\n\nExample 4:\nFrom \\(\\mathbb{R}^2\\) to \\(\\mathbb{R}^2\\):\n\\[\n\\begin{align}\\Phi:\\mathbb{R}^2&\\longrightarrow \\mathbb{R}^2\\\\ (x,y)&\\longmapsto \\Phi(x,y):=(2x-y,x+y)\\end{align}\n\\]\nIs it?\n\\[\n\\begin{align}\n\\Phi(c_1(x_1,y_1)+c_2(x_2,y_2)) &= \\Phi(c_1x_1+c_2x_2,c_1y_1+c_2y_2)\\\\\n&=(2(c_1x_1+c_2x_2)-(c_1y_1+c_2y_2),c_1x_1+c_2x_2+c_1y_1+c_2y_2)\\\\\n&=c_1(2x_1-y_1,x_1+y_1)+c_2(2x_2-y_2,x_2+y_2)\\\\\n&=c_1\\Phi(x_1,y_1)+c_2\\Phi(x_2,y_2)\n\\end{align}\n\\]\nYes.\n\n\nExample 5:\nFrom \\(\\mathbb{R}^2\\) to \\(\\mathbb{R}^2\\) once more:\n\\[ \\begin{align}\\Xi:\\mathbb{R}^2&\\longrightarrow \\mathbb{R}^2\\\\ (x,y)&\\longmapsto \\Xi(x,y):=(2x-2y,x-y)\\end{align} \\]\n?\n\\[\n\\begin{align}\n\\Xi(c_1(x_1,y_1)+c_2(x_2,y_2)) &= \\Xi(c_1x_1+c_2x_2,c_1y_1+c_2y_2)\\\\\n&=(2(c_1x_1+c_2x_2)-2(c_1y_1+c_2y_2),c_1x_1+c_2x_2-c_1y_1-c_2y_2)\\\\\n&=c_1(2x_1-2y_1,x_1-y_1)+c_2(2x_2-2y_2,x_2-y_2)\\\\\n&=c_1\\Xi(x_1,y_1)+c_2\\Xi(x_2,y_2)\n\\end{align}\n\\]\nIt is.\n\n\nExample 6: (NON-linear)\nAgain, from \\(\\mathbb{R}^2\\) to \\(\\mathbb{R}^2\\), but this time we have a square involved, hence the name square function:\n\\[\n\\begin{align}\\square:\\mathbb{R}^2&\\longrightarrow \\mathbb{R}^2\\\\ (x,y)&\\longmapsto \\square(x,y):=(2x^2,y)\\end{align}\n\\]\nChecking as usual:\n\\[\n\\begin{align}\n\\square(c_1(x_1,y_1)+c_2(x_2,y_2))&=\\square(c_1x_1+c_2x_2,c_1y_1+c_2y_2)\\\\\n&=(2(c_1x_1+c_2x_2)^2,c_1y_1+c_2y_2)\\\\\n&=(2((c_1x_1)^2+2c_1c_2x_1x_2+(c_2x_2)^2),c_1y_1+c_2y_2)\\\\\n&\\not=c_1\\square(x_1,y_1)+c_2\\square(x_2,y_2)\n\\end{align}\n\\]\nThe range is \\(\\square(\\mathbb{R}^2)=[0,\\infty)\\times\\mathbb{R}\\).\n\n\n\n\n\n\nCommentary\n\n\n\n\nIt is usual to give a more break down \\(f(\\nu \\bf{u}+\\lambda \\bf{v}) = \\nu f (\\bf{u}) + \\lambda f(\\bf{v})\\) into two rules:\n\n\\(f(\\bf{u}+\\bf{v})=f(\\bf{u})+f(\\bf{v})\\)\n\\(f(\\lambda \\bf{u})=\\lambda f(\\bf{u})\\)\n\nRather than checking one more complex rule we can check two simpler rules.\nNotice the summation and multiplication are defined in distinct vector spaces.",
    "crumbs": [
      "Bried Notes",
      "Linear Algebra",
      "Linear functions"
    ]
  },
  {
    "objectID": "linear_functions.html#example-7-matrices-are-procedures-that-define-linear-functions",
    "href": "linear_functions.html#example-7-matrices-are-procedures-that-define-linear-functions",
    "title": "Linear functions",
    "section": "Example 7: Matrices are procedures that define linear functions",
    "text": "Example 7: Matrices are procedures that define linear functions\nIdea: Consider two vector spaces \\(\\mathbb{R}^4\\) and \\(\\mathbb{R}^3\\). The vectors that live in \\(\\mathbb{R}^4\\) have the form \\(\\mathbf{x}=(x,y,z,w)\\) while those in \\(\\mathbb{R}^2\\) are \\(\\mathbf{u}=(u,v,t)\\).\nA function \\(f\\) that maps vectors from \\(\\mathbb{R}^4\\) and \\(\\mathbb{R}^3\\) can be constructed from the procedure \\(A\\mathbf{x}\\).\nWe define this function as:\n\\[ \\begin{align} f:\\mathbb{R}^4&\\longrightarrow \\mathbb{R}^3\\\\ \\mathbf{x}&\\longmapsto f(\\mathbf{x}):=A\\mathbf{x}  \\end{align} \\]\nwhere \\(A\\) is for example (an already familiar matrix):\n\\[ A=\\begin{pmatrix}1 & 2 & 2 & 2 \\\\2 & 4 & 6 & 8 \\\\3 & 6 & 8 & 10 \\end{pmatrix} \\]\nIts shape tell us all, \\(4\\) columns ready to multiply the \\(4\\) coefficients in \\((x,y,z,w)\\) returning a \\(3\\) entry vector \\(A\\mathbf{x}\\).\nNotice how the property \\(A(\\alpha \\mathbb{x} +\\beta \\mathbf{y})=\\alpha A\\mathbf{x} +\\beta A \\mathbf{y}\\) leads to \\(f\\) being linear as well\\(f(\\alpha \\mathbb{x} +\\beta \\mathbf{y})=\\alpha f(\\mathbf{x}) +\\beta f (\\mathbf{y})\\).\nEssentially, what we want with the introduction of this \\(f\\) is to name an idea already present in \\(A\\mathbf{x}=\\mathbf{b}\\), the idea that the \\(A\\mathbf{x}\\) part of the equation is a function that maps \\(\\mathbf{x}\\) into the vector \\(A\\mathbf{x}\\). With this idea in mind, the system of equations encoded \\(A\\mathbf{x}=\\mathbf{b}\\) is like asking what \\(\\mathbf{x}\\) in the domain of \\(f\\) is mapped into the fixed \\(\\mathbf{b}\\) in the codomain.\n\\[ A\\mathbf{x}=\\mathbf{b} \\iff f(\\mathbf{x})=\\mathbf{b} \\]",
    "crumbs": [
      "Bried Notes",
      "Linear Algebra",
      "Linear functions"
    ]
  },
  {
    "objectID": "linear_functions.html#mathematical-motivation",
    "href": "linear_functions.html#mathematical-motivation",
    "title": "Linear functions",
    "section": "(Mathematical) Motivation",
    "text": "(Mathematical) Motivation\nWhy are these linear functions important?\nWe will answer this question in two ways: a zoom in version and a zoom out version (later)\nZoom in version: The rule says something important about the map \\(f\\), on the lhs we see an l.c. of elements of \\(\\mathbb{R}^n\\), i.e., \\(\\nu\\bf{u}\\) and \\(\\lambda \\bf{v}\\) are mapped into \\(\\nu\\bf{u}+\\lambda\\bf{v}\\in \\mathbb{R}^n\\). This vector in turn is mapped, now under \\(f\\), into the element \\(f(\\nu\\bf{u}+\\lambda\\bf{v})\\in\\mathbb{R}^m\\). On the rhs we see a l.c. of elements of \\(\\mathbb{R}^m\\): \\(f (\\bf{u})\\) is being combined with \\(f (\\bf{v})\\) with the coefficients \\(\\nu\\) and \\(\\lambda\\).\n\nOn the picture we see a l.c. diagram-\\((\\nu,\\lambda)\\) on the left and another on the right whose coefficients are the same. These diagrams and all the other are what we can the connectivity structure of the vector space. Notice, the elements being combined are different - on the left we have \\(\\bf{u}\\) with \\(n\\) dimensions while \\(\\bf{u}'\\) has \\(m\\) dimensions.\nThe equality \\(f(\\nu \\bf{u}+\\lambda \\bf{v}) = \\nu f (\\bf{u}) + \\lambda f(\\bf{v})\\) says the three points of diagram on the left are “connected” to a diagram on the right involving the same coefficients. As a result, the connection on the left is preserved because it is reproduced on the right. Its image under \\(f\\) does not change.\n\n\n\n\n\n\nCommentary\n\n\n\nAn example of a non-preserving function was the \\(\\square\\) function whose behavior is diagrammatically akin to this:\n\nThe image of the l.c. diagram on the left does not have a corresponding l.c. diagram (with the same coefficients) on the right. Thus we say \\(\\square\\) does not preserve the diagram during is mapping action.",
    "crumbs": [
      "Bried Notes",
      "Linear Algebra",
      "Linear functions"
    ]
  },
  {
    "objectID": "linear_functions.html#kernel-of-a-linear-function",
    "href": "linear_functions.html#kernel-of-a-linear-function",
    "title": "Linear functions",
    "section": "Kernel of a linear function",
    "text": "Kernel of a linear function\n\nDefinition 2 Let \\(f:\\mathbb{R}^n\\overset{\\sim}{\\longrightarrow} \\mathbb{R}^m\\), then the kernel or nullspace of \\(f\\) is the subset of the domain \\(\\mathbb{R}^n\\):\n\\[\n\\ker f = \\{\\bf{u}\\in\\mathbb{R}^n\\,\\,|\\,\\,f(\\bf{u})=\\bf{0}\\}\n\\]\nMoreover, it is in fact a subspace.\n\n\nExample 1 (cont):\nThe kernel of \\(f(x)=ax\\) is the zeros of the function:\n\\[\n\\ker f = \\{x\\in \\mathbb{R}\\,\\,|\\,\\,ax =0\\}=\\{0\\}\n\\]\nIt is just one point in the domain, geometrically where the line intercept the x-axis.\n\n\nExample 2 (cont):\nReturning to \\(g(x)=(2x,x)\\), its nullspace is the subspace:\n\\[\n\\ker g = \\{x\\in \\mathbb{R}\\,\\,|\\,\\,(2x,x) =(0,0)\\}=\\{0\\}\n\\]\nAgain, just one vector.\n\n\nExample 3 (cont):\n\\[\n\\ker \\pi = \\{(x,y)\\in \\mathbb{R}\\,\\,|\\,\\,\\pi(x,y)=x=0\\}=\\{(0,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y\\in \\mathbb{R}\\}\n\\]\n\nAny vector along the subspace [0y axis] of the domain \\(\\mathbb{R}^2\\) is always projected in the \\(0\\) of the codomain. This vertical axis is the kernel of \\(\\pi\\) (determined by \\(\\pi\\) itself)\n\n\nExample 4 (cont):\nThe kernel of \\(\\Phi(x,y)=(2x-y,x+y)\\) is:\n\\[\n\\ker \\Phi = \\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\,(2x-y,x+y)=(0,0)\\}\n\\]\nWe have to solve the system of equations:\n\\[\n\\begin{cases}\n2x-y=0\\\\\nx+y=0\n\\end{cases}\n\\leftrightsquigarrow\n\\begin{pmatrix}\n2 & -1\\\\\n1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx\\\\y\n\\end{pmatrix}\n=\\begin{pmatrix}\n0\\\\0\n\\end{pmatrix}\n\\leftrightsquigarrow\n\\begin{pmatrix}2 & -1 &\\bigm|0\\\\1 & 1 &\\bigm| 0\\end{pmatrix}\n\\]\nBy inspection, the columns are independent and thus the solution is \\((0,0)\\).\n\n\nExample 5 (cont):\n\\[\n\\ker \\Xi = \\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\,(2x-2y,x-y)=(0,0)\\}\n\\]\nThe corresponding system to solve is\n\\[\n\\begin{pmatrix}2 & -2 &\\bigm|0\\\\1 & -1 &\\bigm| 0\\end{pmatrix}\n\\]\nwhose solution is \\(c(1,1)\\). The kernel is the whole line along \\((1,1)\\).",
    "crumbs": [
      "Bried Notes",
      "Linear Algebra",
      "Linear functions"
    ]
  },
  {
    "objectID": "linear_functions.html#image-of-the-domain-range-of-linear-functions",
    "href": "linear_functions.html#image-of-the-domain-range-of-linear-functions",
    "title": "Linear functions",
    "section": "Image of the domain (range of linear functions)",
    "text": "Image of the domain (range of linear functions)\n\nDefinition 3 Let \\(f:\\mathbb{R}^n\\overset{\\sim}{\\longrightarrow} \\mathbb{R}^m\\), then the range of \\(f\\) is the subset of the domain \\(\\mathbb{R}^m\\):\n\\[\nf(\\mathbb{R}^n) = \\{f(\\bf{u})\\in\\mathbb{R}^m\\,\\,|\\,\\,\\bf{u}\\in\\mathbb{R}^n\\}\n\\]\nMoreover, it is also a subspace.\n\nThe range of \\(f\\) can is hard to compute, the concept is what is most important here.\nHow do we compute the image of the domain?\nExample 1 (cont):\nIs it possible to indentify any element \\(y\\) of the codomain \\(\\mathbb{R}\\) by some label \\(x\\) of the domain \\(\\mathbb{R}\\)? In other words, can we solve for \\(x\\), the equation:\n\\[\nax = y\n\\]\ngiven an arbitrary \\(y\\)?\nAnswer is yes, the form of the element of the domain assigned to \\(y\\) is \\(y/a\\).\nWe conclude \\(f(\\mathbb{R})=\\mathbb{R}\\).\nExample 2 (cont):\nIs there for every \\((u,v)\\in \\mathbb{R}^2\\) a corresponding element \\(x\\) in the domain \\(\\mathbb{R}\\)? We answer by solving for \\(x\\):\n\\[\n(2x,x)=(u,v) \\implies \\begin{cases}2x=u\\\\x=v\\end{cases}\\implies \\begin{cases}x=u/2\\\\x=v\\end{cases}\n\\]\nThe answer is no. We cannot choose \\(u,v\\) arbitrarily and find an \\(x\\), the system only has solution \\(x\\) for specific pairs of \\(u,v\\), namely those that lie along the line \\(y=x/2\\).\nComputing the image of the domain is \\(f(\\mathbb{R})=[\\text{line with equation}\\,\\, y=x/2]\\) we can see that it is a subset of the codomain \\(\\mathbb{R}^2\\) and thus not everyone of its elements (those outside the line in question) are not assigned to some real \\(x\\). The function is therefore not surjective.\nExample 3 (cont):\nThe codomain is \\(\\mathbb{R}\\) is it true or false that every one of its elements has a corresponding \\((x,y)\\) living in the domain \\(\\mathbb{R}^2\\).\nLets solve for \\((x,y)\\) the equation:\n\\[\n\\pi(x,y) = u\\implies x=u\n\\]\nThere is indeed a solution for this equation, in fact many of them, of the form \\((u,y)\\) where \\(y\\) is any real number.\nThe image of the domain under \\(\\pi\\) is the same set as the codomain - \\(f(\\mathbb{R}^2)=\\mathbb{R}\\) - and thus the function is surjective.\nExample 4 (cont):\nWe seek, given any \\((u,y)\\) of the codomain, the corresponding \\((x,y)\\) in the domain:\n\\[\n\\Phi(x,y)=(u,v)\\implies \\begin{cases}2x-y=u\\\\x+y=v\\end{cases}\\implies\\begin{cases}x=(u+v)/3\\\\y=(2v-u)/3\\end{cases}\n\\]\nAnother way to check this is to compute the image of the domain: \\(\\Phi(\\mathbb{R}^2)=\\mathbb{R}^2\\), because the column of the matrix are independent. The \\(\\Phi\\) function is surjective.\nExample 5 (cont):\nThe function \\(\\Xi\\) maps \\((x,y)\\in\\mathbb{R}^2\\) into \\((u,v)\\in\\mathbb{R}^2\\). Is it true that every \\((u,y)\\) comes from some \\((x,y)\\)? Lets solve the adequate equation:\n\\[\n\\Xi(x,y)=(u,v) \\implies \\begin{cases}2x-2y=u\\\\x-y=v\\end{cases}\\implies \\begin{cases}x=u/2+y\\\\v=2u\\end{cases}\n\\]\nThe equation \\(l_2\\) tells us that we cannot choose any pair \\(u,v\\) hence not every element of the codomain \\(\\mathbb{R}^2\\) is assigned to some \\((x,y)\\).\nAnother way is to compute the image of the domain, notice as \\(x,y\\) ranges in \\(\\mathbb{R}\\), then \\(x-y\\) ranges in \\(\\mathbb{R}\\) as well, and thus the \\(\\Xi\\) function is just like \\(\\Xi(\\spadesuit) = (2\\spadesuit,\\spadesuit)\\), see example 2.\nThus\n\\(\\Xi(\\mathbb{R}^2) = [\\text{line with equation}\\,\\, y=x/2]\\) again. And the function is not surjective.",
    "crumbs": [
      "Bried Notes",
      "Linear Algebra",
      "Linear functions"
    ]
  },
  {
    "objectID": "matrices.html",
    "href": "matrices.html",
    "title": "Matrices",
    "section": "",
    "text": "A matrix is simply put an array of numbers with \\(n\\) rows and \\(m\\) columns. Here are some examples:\n\\[\n\\left(\\begin{matrix}1 \\\\2\\\\ 100  \\end{matrix}\\right)\n\\qquad\n\\left(\\begin{matrix}1 & 1 & -1\\\\2 & -1 & 2  \\end{matrix}\\right)\n\\qquad\n\\left(\\begin{matrix}1 & 0 & 0\\\\0 & 1 & 0\\\\0 & 0 & 1  \\end{matrix}\\right)\n\\qquad\n\\left(\\begin{matrix}1 & 1 \\\\2 & -1\\\\0 & 0  \\end{matrix}\\right)\n\\qquad\n\\left[(-1)^{i+j}\\right]\n\\]\nWhat can we do with matrices?\nAnswer: linear combinations! [Comment: provided their shape is compatible]\nBut this time we’ll introduce more: matrix multiplication, transpose, inverses, determinants.",
    "crumbs": [
      "Bried Notes",
      "Linear Algebra",
      "Matrices"
    ]
  },
  {
    "objectID": "matrices.html#maps-involved-in-matrix-multiplication-composition",
    "href": "matrices.html#maps-involved-in-matrix-multiplication-composition",
    "title": "Matrices",
    "section": "Maps involved in matrix multiplication (composition)",
    "text": "Maps involved in matrix multiplication (composition)",
    "crumbs": [
      "Bried Notes",
      "Linear Algebra",
      "Matrices"
    ]
  },
  {
    "objectID": "matrices.html#cramer",
    "href": "matrices.html#cramer",
    "title": "Matrices",
    "section": "Cramer",
    "text": "Cramer",
    "crumbs": [
      "Bried Notes",
      "Linear Algebra",
      "Matrices"
    ]
  },
  {
    "objectID": "polynomials.html",
    "href": "polynomials.html",
    "title": "Getting the feet wet with polynomials",
    "section": "",
    "text": "Polynomials are the simplest functions we can study and are an ideal arena to put into practice the two key views on functions:\n\nRelation Point of View\nProcedure Point of View\n\n\n\nUnder the relation perspective, we specify a function by listing all ordered pairs that make up its graph. Here are four examples of polynomial functions:\n\\[\n\\begin{align}&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y=100\\}\\\\&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y=3x+1\\}\\\\&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y=x^2+3\\}\\\\&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y=2x^3-x^2+0x+1\\}\\end{align}\n\\tag{1}\\]\nHow should we interpret this notation?\n\nThese are sets because they appear between \\(\\{\\) and \\(\\}\\).\nStating that \\((x,y)\\in \\mathbb{R}^2\\), just says two things at once:\n\nthe elements of the set are ordered pairs \\((x,y)\\)\nthat \\(x\\in\\mathbb{R}\\) and \\(y\\in \\mathbb{R}\\), in other words, \\(x\\) and \\(y\\) are reals.\n\nthe bar \\(|\\) we see in Equation 1, reads as “such that”, after which we write some rule/property/equation that we wish these ordered pairs of reals to have. For example the equation \\(y=100\\) is the rule we want for the first polynomial in Equation 1, therefore this function is composed by all ordered pairs \\((x,y)\\) with \\(y\\) equal to \\(100\\), while \\(x\\) ranges on all reals.\nTo interpret the notation as a whole, take \\(\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y=3x+1\\}\\). We read is as a “set that contains ordered reals, such that they obey the equation \\(y=3x+1\\)”. As an example, the pair \\((1.3,2.9)\\) obeys the equation because \\(2.9 = 3\\times 1.3 +1\\) is true, therefore it belong to the set, but \\((1,3)\\) does not since \\(3=3\\times 1 +1\\) is false.\n\nThe equations we wrote after \\(|\\) allow us to compute from a given \\(x\\) a unique value \\(y\\). As an example of this fact, choose the equation of the second polynomial, \\(y=3x+1\\), plug in \\(x=0\\), what do we get? We get \\(y=3\\cdot 0 +1\\), that is, just the number \\(y=1\\). Notice we do not get other possible value for \\(y\\), we just get the \\(y=1\\), no other possibility; in fact the same happens for any other \\(x\\) we choose. This is such an obvious fact we actually skip by it. But it is important, because the fact that the equations in Equation 1 have that property ensures us that each possible value of \\(x\\) does not appear two or more times within these set. In such situation we have in our hands more than a set, we have a function!\n\n\n\n\n\n\nCommentary\n\n\n\nThe set \\(\\{(1,2),(1,3),(2,4)\\}\\) have \\(1\\) connected to \\(2\\) and \\(3\\), thus it is not a function. The set\\(\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, x\\in \\mathbb{R} \\land y\\in[2,3]\\}\\) have each real \\(x\\) connected to every real between \\(2\\) and \\(3\\), its also not a function. The set \\(\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y^2=x^2\\}\\) is not a function, if I specify \\(x=1\\) then we get \\(y^2=1\\) whose solution is either \\(y=\\pm 1\\). Therefore we have two y-numbers \\(\\pm1\\) attached to \\(x=1\\).\nThese sets are not functions, but still, they make connections between the values of \\(x\\) and \\(y\\) . Because they relate the values \\(x\\) and \\(y\\), we call these sets relations. Note that functions are also relations.\n\n\nObservation of the right hand side of the rules in Equation 1 show they are all some combination of a power of \\(x\\) times some number (positive or negative), the jargon for this is to say we have a linear combination of powers of \\(x\\). From Equation 1 we conclude the general form of these rules (equations) is:\n\\[\ny=a_nx^n+a_{n-1}x^{n-1}+\\dots+a_1 x+a_0\n\\tag{2}\\]\nwhere the coefficients are labeled by \\(a_0\\) through \\(a_n\\) are reals.\n\n\n\n\n\n\nRecall\n\n\n\nA Cartesian product of the sets \\(X=\\{1,2,3\\}\\) and \\(Y=\\{4,5\\}\\) is defined as:\n\\[\nX\\times Y =\\{(1,4),(1,5),(2,4),(2,5),(3,4),(3,5)\\}\n\\]\n\n\nThe truth set of the statement Equation 2 is the function:\n\\[\nP_n := \\{(x,y)\\in X\\times Y\\,\\,|\\,\\, y=a_nx^n+a_{n-1}x^{n-1}+\\dots+a_1 x+a_0\\}\n\\]\nWhich reads: from the pairs in the Cartesian product of the set \\(X\\) with the set \\(Y\\) we choose only the elements such that the rule \\(y=a_nx^n+a_{n-1}x^{n-1}+\\dots+a_1 x+a_0\\) is obeyed; it is useful to think about it as if we were choosing one by one the elements of \\(X\\times Y\\) which are of the form \\((x,y)\\) and then verifying whether that rules applies or not (True or False), see Figure 1 .\n\n\n\n\n\n\nFigure 1: Step by step construction of a polynomial function.\n\n\n\nThe highest power of \\(x\\) is \\(n\\) and is called the degree of the polynomial. We can and will use it to classify the polynomial functions, that is the reason why we decided to label the set by the symbol \\(P_n\\).\n\n\n\nRather then the view of Figure 1 on how to build the function \\(P_n\\) we adopt a slightly different strategy:\n\n\n\n\n\n\nFigure 2: An alternative way to construct a polynomial function.\n\n\n\nRather than to choose, test and verify each element of the Cartesian product we can instead choose from \\(X\\) the elements one by one and compute the corresponding \\(y\\) by the calculation \\(a_nx^n+a_{n-1}x^{n-1}+\\dots+a_1 x+a_0\\); then we just include the so obtained ordered pair in the set \\(P_n\\). The perceptive in Figure 2 is expressed in mathematical notation as:\n\\[\n\\begin{align}\np_n:X&\\longrightarrow Y\\\\x &\\longmapsto p_n(x):= a_nx^n+a_{n-1}x^{n-1}+\\dots+a_1 x+a_0\n\\end{align}\n\\tag{3}\\]\nLets unpack what does this mean:\n\nOn the first row we find symbols that stand for:\nThe procedure labeled by the symbol \\(p_n\\) maps elements of \\(X\\) into elements in \\(Y\\)\nThis row of information immediately tells us what sets are involved, we can picture it as\n\n\n\n\n\n\n\nFigure 3: Sets involved in the construction of the function, as well as the name of procedure that maps elements of the first into the second. From the first row we still do not know how the map works, this is specified in the second row.\n\n\n\n\\(X\\) is called the domain of the function, while \\(Y\\) is the codomain.\n\nThe second row explains many aspects of the procedure between these sets: the symbol \\(x\\) stands for a generic element of the set \\(X\\) while the symbol \\(p_n(x)\\) represents the element in \\(Y\\), which is assigned to \\(x\\) (notice the \\(\\mapsto\\) arrow), when the procedure \\(p_n\\) acts on it. This row also tell us what is the element \\(p_n(x)\\), by telling what \\(p_n\\) does with the number \\(x\\)! The string of symbols:\n\\[\np_n(x):= a_nx^n+a_{n-1}x^{n-1}+\\dots+a_1 x+a_0\n\\]\nmust now be well understood:\ni) \\(a_nx^n+a_{n-1}x^{n-1}+\\dots+a_1 x+a_0\\) is the LONG symbol for the element of \\(Y\\) which is assigned (connected) to \\(x\\);\nii) it is LONG because it explains how the number comes about. We usually abbreviate the LONG symbol by a short symbol such as \\(p_n(x)\\). The symbol \\(:=\\) means “by definition is equal to”.\n\n\nThe entire definition Equation 3 is the definition of a function, out of convince we name the function after the procedure, and refer to the function just as the “\\(p_n\\) function”. Observe that the symbol \\(p_n\\) is our choice, we can use any symbol as we wish, traditionally we use just one letter \\(f\\), \\(g\\), \\(h\\), etc.\n\n\n\n\n\n\nComment\n\n\n\nThe largest domain supported by the procedure \\(p_n\\) is \\(X=\\mathbb{R}\\) irrespective of the degree \\(n\\), because with any real \\(x\\) we can do the calculation \\(p_n(x)\\); as usual the \\(Y\\) is the codomain and could either be chosen as \\(\\mathbb{R}\\) or the range of the function. The range of a function depends on the degree \\(n\\), being \\(\\mathbb{R}^+_0\\) if \\(n\\) is even and \\(\\mathbb{R}\\) is \\(n\\) is odd integer.\n\n\n\n\n\nConsider the functions:\n\\[\n\\begin{align}\n&F:=\\{(x,y)\\in\\mathbb{R}\\times\\mathbb{R}\\,\\,|\\,\\,y=3x+1\\}\\\\\n&G:=\\{(x,y)\\in\\mathbb{R}^+\\times\\mathbb{R}\\,\\,|\\,\\,y=2x^3-x^2+0x+1\\}\n\\end{align}\n\\]\nThe domain of \\(F\\) is \\(X=\\mathbb{R}\\) while that of \\(G\\) is \\(X=\\mathbb{R}^+\\), the codomains are both \\(Y=\\mathbb{R}\\).\nWe can rewrite the functions \\(F\\) and \\(G\\) using the procedure notation as\n\\[ \\begin{align} f:\\mathbb{R}&\\longrightarrow \\mathbb{R}\\\\ x &\\longmapsto f(x):= 3x+1 \\end{align}  \\tag{4}\\]\n\\[ \\begin{align} g:\\mathbb{R}^+&\\longrightarrow \\mathbb{R}\\\\ x &\\longmapsto g(x):= 2x^3-x^2+1 \\end{align} \\]\nThe range is the set of all outputs of a function, conveniently named by the symbols \\(f(\\mathbb{R})\\) and \\(g(\\mathbb{R})\\). What are these sets? Unfortunately, this requires tools not yet introduced in these notes, to circumvent that, we will simply plot the functions and guess what we think the range is:\n\n\n\n\n\n\n\n\n\nWe can see that the outputs of the \\(F\\) function increase/decrease without bound as \\(x\\) increases, thus the range is \\(\\mathbb{R}\\) in both functions. For the function \\(G\\), as \\(x\\) increase, the outputs do as well; observe \\(x\\) is never zero! Thus the range is \\(\\mathbb{R}^+\\).\n\n\n\n\n\n\nCommentary\n\n\n\nWhen one wishes to refer to functions defined as Equation 3 we simply say “the function \\(p_n(x)\\)”. For example if I want to say Equation 4 has property \\(A\\), then we should say “the function \\(f(x)\\) has property \\(A\\)” or more simply “the function \\(f\\) has property \\(A\\)” or “the function \\(3x+1\\) has property \\(A\\)” or even “the function \\(y=3x+1\\) has property \\(A\\)”. Even though the later are not accurate ( \\(f(x)\\) is a variable symbol, \\(f\\) labels the procedure and \\(y=3x+1\\) is an equation) we will still use it. We sacrifice a bit of accuracy for the sake of brevity.\n\n\n\n\n\nDefine Equation 3 for the first and third function sets in Equation 1. Be careful in specifying what is the range of either function.\n\n\n\nFrom the set:\n\\[\n\\{(x,y)\\in \\mathbb{R}\\times\\mathbb{R}\\,\\,|\\,\\, y=100\\}\n\\]\nwe identify the domain and codomain as the first and second sets in \\(\\mathbb{R}\\times\\mathbb{R}\\), thus the domain is \\(\\mathbb{R}\\) and the codomain is \\(\\mathbb{R}\\) aswell. Meanwhile the range is just is \\(l(\\mathbb{R})=\\{100\\}\\). We can write this function using the procedure notation as:\n\\[\n\\begin{align}\nl:\\mathbb{R}&\\longrightarrow \\{1\\}\\subset \\mathbb{R}\\\\\nx &\\longmapsto l(x):= 100\n\\end{align}\n\\]\nThe second function-set:\n\\[\n\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y=x^2+3\\}\n\\]\nIs translated into:\n\\[\n\\begin{align}\nc:\\mathbb{R}&\\longrightarrow [3,+\\infty[\\subset \\mathbb{R}\\\\\nx &\\longmapsto c(x):= x^2+3\n\\end{align}\n\\]\nThe domain and range are both \\(\\mathbb{R}\\) again but the range is trickier. We want to compute the set \\(c(\\mathbb{R})\\), to do it, we note by inspecting \\(x^2+3\\) that its minimum value occurs when \\(x=0\\) and the larger the \\(x\\), the larger is \\(x^2+3\\), thus \\(c(\\mathbb{R})=[3,+\\infty[\\), which is a subset of the codomain \\(\\mathbb{R}\\)."
  },
  {
    "objectID": "polynomials.html#relation-definition-of-polynomial-functions",
    "href": "polynomials.html#relation-definition-of-polynomial-functions",
    "title": "Getting the feet wet with polynomials",
    "section": "",
    "text": "Under the relation perspective, we specify a function by listing all ordered pairs that make up its graph. Here are four examples of polynomial functions:\n\\[\n\\begin{align}&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y=100\\}\\\\&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y=3x+1\\}\\\\&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y=x^2+3\\}\\\\&\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y=2x^3-x^2+0x+1\\}\\end{align}\n\\tag{1}\\]\nHow should we interpret this notation?\n\nThese are sets because they appear between \\(\\{\\) and \\(\\}\\).\nStating that \\((x,y)\\in \\mathbb{R}^2\\), just says two things at once:\n\nthe elements of the set are ordered pairs \\((x,y)\\)\nthat \\(x\\in\\mathbb{R}\\) and \\(y\\in \\mathbb{R}\\), in other words, \\(x\\) and \\(y\\) are reals.\n\nthe bar \\(|\\) we see in Equation 1, reads as “such that”, after which we write some rule/property/equation that we wish these ordered pairs of reals to have. For example the equation \\(y=100\\) is the rule we want for the first polynomial in Equation 1, therefore this function is composed by all ordered pairs \\((x,y)\\) with \\(y\\) equal to \\(100\\), while \\(x\\) ranges on all reals.\nTo interpret the notation as a whole, take \\(\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y=3x+1\\}\\). We read is as a “set that contains ordered reals, such that they obey the equation \\(y=3x+1\\)”. As an example, the pair \\((1.3,2.9)\\) obeys the equation because \\(2.9 = 3\\times 1.3 +1\\) is true, therefore it belong to the set, but \\((1,3)\\) does not since \\(3=3\\times 1 +1\\) is false.\n\nThe equations we wrote after \\(|\\) allow us to compute from a given \\(x\\) a unique value \\(y\\). As an example of this fact, choose the equation of the second polynomial, \\(y=3x+1\\), plug in \\(x=0\\), what do we get? We get \\(y=3\\cdot 0 +1\\), that is, just the number \\(y=1\\). Notice we do not get other possible value for \\(y\\), we just get the \\(y=1\\), no other possibility; in fact the same happens for any other \\(x\\) we choose. This is such an obvious fact we actually skip by it. But it is important, because the fact that the equations in Equation 1 have that property ensures us that each possible value of \\(x\\) does not appear two or more times within these set. In such situation we have in our hands more than a set, we have a function!\n\n\n\n\n\n\nCommentary\n\n\n\nThe set \\(\\{(1,2),(1,3),(2,4)\\}\\) have \\(1\\) connected to \\(2\\) and \\(3\\), thus it is not a function. The set\\(\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, x\\in \\mathbb{R} \\land y\\in[2,3]\\}\\) have each real \\(x\\) connected to every real between \\(2\\) and \\(3\\), its also not a function. The set \\(\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y^2=x^2\\}\\) is not a function, if I specify \\(x=1\\) then we get \\(y^2=1\\) whose solution is either \\(y=\\pm 1\\). Therefore we have two y-numbers \\(\\pm1\\) attached to \\(x=1\\).\nThese sets are not functions, but still, they make connections between the values of \\(x\\) and \\(y\\) . Because they relate the values \\(x\\) and \\(y\\), we call these sets relations. Note that functions are also relations.\n\n\nObservation of the right hand side of the rules in Equation 1 show they are all some combination of a power of \\(x\\) times some number (positive or negative), the jargon for this is to say we have a linear combination of powers of \\(x\\). From Equation 1 we conclude the general form of these rules (equations) is:\n\\[\ny=a_nx^n+a_{n-1}x^{n-1}+\\dots+a_1 x+a_0\n\\tag{2}\\]\nwhere the coefficients are labeled by \\(a_0\\) through \\(a_n\\) are reals.\n\n\n\n\n\n\nRecall\n\n\n\nA Cartesian product of the sets \\(X=\\{1,2,3\\}\\) and \\(Y=\\{4,5\\}\\) is defined as:\n\\[\nX\\times Y =\\{(1,4),(1,5),(2,4),(2,5),(3,4),(3,5)\\}\n\\]\n\n\nThe truth set of the statement Equation 2 is the function:\n\\[\nP_n := \\{(x,y)\\in X\\times Y\\,\\,|\\,\\, y=a_nx^n+a_{n-1}x^{n-1}+\\dots+a_1 x+a_0\\}\n\\]\nWhich reads: from the pairs in the Cartesian product of the set \\(X\\) with the set \\(Y\\) we choose only the elements such that the rule \\(y=a_nx^n+a_{n-1}x^{n-1}+\\dots+a_1 x+a_0\\) is obeyed; it is useful to think about it as if we were choosing one by one the elements of \\(X\\times Y\\) which are of the form \\((x,y)\\) and then verifying whether that rules applies or not (True or False), see Figure 1 .\n\n\n\n\n\n\nFigure 1: Step by step construction of a polynomial function.\n\n\n\nThe highest power of \\(x\\) is \\(n\\) and is called the degree of the polynomial. We can and will use it to classify the polynomial functions, that is the reason why we decided to label the set by the symbol \\(P_n\\)."
  },
  {
    "objectID": "polynomials.html#procedure-definition-of-polynomials-functions",
    "href": "polynomials.html#procedure-definition-of-polynomials-functions",
    "title": "Getting the feet wet with polynomials",
    "section": "",
    "text": "Rather then the view of Figure 1 on how to build the function \\(P_n\\) we adopt a slightly different strategy:\n\n\n\n\n\n\nFigure 2: An alternative way to construct a polynomial function.\n\n\n\nRather than to choose, test and verify each element of the Cartesian product we can instead choose from \\(X\\) the elements one by one and compute the corresponding \\(y\\) by the calculation \\(a_nx^n+a_{n-1}x^{n-1}+\\dots+a_1 x+a_0\\); then we just include the so obtained ordered pair in the set \\(P_n\\). The perceptive in Figure 2 is expressed in mathematical notation as:\n\\[\n\\begin{align}\np_n:X&\\longrightarrow Y\\\\x &\\longmapsto p_n(x):= a_nx^n+a_{n-1}x^{n-1}+\\dots+a_1 x+a_0\n\\end{align}\n\\tag{3}\\]\nLets unpack what does this mean:\n\nOn the first row we find symbols that stand for:\nThe procedure labeled by the symbol \\(p_n\\) maps elements of \\(X\\) into elements in \\(Y\\)\nThis row of information immediately tells us what sets are involved, we can picture it as\n\n\n\n\n\n\n\nFigure 3: Sets involved in the construction of the function, as well as the name of procedure that maps elements of the first into the second. From the first row we still do not know how the map works, this is specified in the second row.\n\n\n\n\\(X\\) is called the domain of the function, while \\(Y\\) is the codomain.\n\nThe second row explains many aspects of the procedure between these sets: the symbol \\(x\\) stands for a generic element of the set \\(X\\) while the symbol \\(p_n(x)\\) represents the element in \\(Y\\), which is assigned to \\(x\\) (notice the \\(\\mapsto\\) arrow), when the procedure \\(p_n\\) acts on it. This row also tell us what is the element \\(p_n(x)\\), by telling what \\(p_n\\) does with the number \\(x\\)! The string of symbols:\n\\[\np_n(x):= a_nx^n+a_{n-1}x^{n-1}+\\dots+a_1 x+a_0\n\\]\nmust now be well understood:\ni) \\(a_nx^n+a_{n-1}x^{n-1}+\\dots+a_1 x+a_0\\) is the LONG symbol for the element of \\(Y\\) which is assigned (connected) to \\(x\\);\nii) it is LONG because it explains how the number comes about. We usually abbreviate the LONG symbol by a short symbol such as \\(p_n(x)\\). The symbol \\(:=\\) means “by definition is equal to”.\n\n\nThe entire definition Equation 3 is the definition of a function, out of convince we name the function after the procedure, and refer to the function just as the “\\(p_n\\) function”. Observe that the symbol \\(p_n\\) is our choice, we can use any symbol as we wish, traditionally we use just one letter \\(f\\), \\(g\\), \\(h\\), etc.\n\n\n\n\n\n\nComment\n\n\n\nThe largest domain supported by the procedure \\(p_n\\) is \\(X=\\mathbb{R}\\) irrespective of the degree \\(n\\), because with any real \\(x\\) we can do the calculation \\(p_n(x)\\); as usual the \\(Y\\) is the codomain and could either be chosen as \\(\\mathbb{R}\\) or the range of the function. The range of a function depends on the degree \\(n\\), being \\(\\mathbb{R}^+_0\\) if \\(n\\) is even and \\(\\mathbb{R}\\) is \\(n\\) is odd integer."
  },
  {
    "objectID": "polynomials.html#examples",
    "href": "polynomials.html#examples",
    "title": "Getting the feet wet with polynomials",
    "section": "",
    "text": "Consider the functions:\n\\[\n\\begin{align}\n&F:=\\{(x,y)\\in\\mathbb{R}\\times\\mathbb{R}\\,\\,|\\,\\,y=3x+1\\}\\\\\n&G:=\\{(x,y)\\in\\mathbb{R}^+\\times\\mathbb{R}\\,\\,|\\,\\,y=2x^3-x^2+0x+1\\}\n\\end{align}\n\\]\nThe domain of \\(F\\) is \\(X=\\mathbb{R}\\) while that of \\(G\\) is \\(X=\\mathbb{R}^+\\), the codomains are both \\(Y=\\mathbb{R}\\).\nWe can rewrite the functions \\(F\\) and \\(G\\) using the procedure notation as\n\\[ \\begin{align} f:\\mathbb{R}&\\longrightarrow \\mathbb{R}\\\\ x &\\longmapsto f(x):= 3x+1 \\end{align}  \\tag{4}\\]\n\\[ \\begin{align} g:\\mathbb{R}^+&\\longrightarrow \\mathbb{R}\\\\ x &\\longmapsto g(x):= 2x^3-x^2+1 \\end{align} \\]\nThe range is the set of all outputs of a function, conveniently named by the symbols \\(f(\\mathbb{R})\\) and \\(g(\\mathbb{R})\\). What are these sets? Unfortunately, this requires tools not yet introduced in these notes, to circumvent that, we will simply plot the functions and guess what we think the range is:\n\n\n\n\n\n\n\n\n\nWe can see that the outputs of the \\(F\\) function increase/decrease without bound as \\(x\\) increases, thus the range is \\(\\mathbb{R}\\) in both functions. For the function \\(G\\), as \\(x\\) increase, the outputs do as well; observe \\(x\\) is never zero! Thus the range is \\(\\mathbb{R}^+\\).\n\n\n\n\n\n\nCommentary\n\n\n\nWhen one wishes to refer to functions defined as Equation 3 we simply say “the function \\(p_n(x)\\)”. For example if I want to say Equation 4 has property \\(A\\), then we should say “the function \\(f(x)\\) has property \\(A\\)” or more simply “the function \\(f\\) has property \\(A\\)” or “the function \\(3x+1\\) has property \\(A\\)” or even “the function \\(y=3x+1\\) has property \\(A\\)”. Even though the later are not accurate ( \\(f(x)\\) is a variable symbol, \\(f\\) labels the procedure and \\(y=3x+1\\) is an equation) we will still use it. We sacrifice a bit of accuracy for the sake of brevity."
  },
  {
    "objectID": "polynomials.html#exercise",
    "href": "polynomials.html#exercise",
    "title": "Getting the feet wet with polynomials",
    "section": "",
    "text": "Define Equation 3 for the first and third function sets in Equation 1. Be careful in specifying what is the range of either function."
  },
  {
    "objectID": "polynomials.html#answer",
    "href": "polynomials.html#answer",
    "title": "Getting the feet wet with polynomials",
    "section": "",
    "text": "From the set:\n\\[\n\\{(x,y)\\in \\mathbb{R}\\times\\mathbb{R}\\,\\,|\\,\\, y=100\\}\n\\]\nwe identify the domain and codomain as the first and second sets in \\(\\mathbb{R}\\times\\mathbb{R}\\), thus the domain is \\(\\mathbb{R}\\) and the codomain is \\(\\mathbb{R}\\) aswell. Meanwhile the range is just is \\(l(\\mathbb{R})=\\{100\\}\\). We can write this function using the procedure notation as:\n\\[\n\\begin{align}\nl:\\mathbb{R}&\\longrightarrow \\{1\\}\\subset \\mathbb{R}\\\\\nx &\\longmapsto l(x):= 100\n\\end{align}\n\\]\nThe second function-set:\n\\[\n\\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y=x^2+3\\}\n\\]\nIs translated into:\n\\[\n\\begin{align}\nc:\\mathbb{R}&\\longrightarrow [3,+\\infty[\\subset \\mathbb{R}\\\\\nx &\\longmapsto c(x):= x^2+3\n\\end{align}\n\\]\nThe domain and range are both \\(\\mathbb{R}\\) again but the range is trickier. We want to compute the set \\(c(\\mathbb{R})\\), to do it, we note by inspecting \\(x^2+3\\) that its minimum value occurs when \\(x=0\\) and the larger the \\(x\\), the larger is \\(x^2+3\\), thus \\(c(\\mathbb{R})=[3,+\\infty[\\), which is a subset of the codomain \\(\\mathbb{R}\\)."
  },
  {
    "objectID": "pol_division_PT.html",
    "href": "pol_division_PT.html",
    "title": "pol_division",
    "section": "",
    "text": "The goal of polynomial division is relate two polynomials.\nFor example:\n\nThe equation that relates and is\nThe equation that relates and is\n\nWhy compute relations of this form? This relation breaks the polynomial into simpler polynomials. It is easier to analyse the simpler parts.\n\nThe algorithm for division of polynomials is analogous to the algorithm of long division learned in elementary school. We shall start by understanding the basic ideas behind long division and then use them in the division of polynomials."
  },
  {
    "objectID": "pol_division_PT.html#exemplo-1-223",
    "href": "pol_division_PT.html#exemplo-1-223",
    "title": "pol_division",
    "section": "Exemplo 1: \\(22/3\\)",
    "text": "Exemplo 1: \\(22/3\\)\n\n\nO múltiplo de \\(3\\) mais perto de \\(22\\) é \\(7\\).\nComo \\(7\\times 3 = 21\\), a subtracção \\(22-21\\) dá-nos \\(1\\).\nDado que \\(1\\) não é divisível por \\(3\\), este é considerado o resto da divisão.\nConcluímos que: \\(22 = 7\\times 3 +1\\).\n\n\nUma forma alternativa de calcular \\(22/3\\)\nO quociente de inteiros \\(p/d\\) é o número que nos diz quantas vezes \\(d\\) cabe em \\(p\\), esta é a ideia chave!\nPor sua vez este número diz-nos quem é maior, \\(p\\) ou \\(d\\)?\n\nQuando \\(p/d&lt;1\\) então \\(d\\) é maior.\nQuando \\(p/d=1\\) são o mesmo número.\nQuando o quociente é maior do que \\(1\\), significa que \\(p\\) é maior.\n\nFoque-mo-nos no último caso.\nConsidera \\(22\\) e \\(3\\), repara que, \\(3\\), cabe \\(7\\) vezes em \\(22\\) pois \\(3\\times 7=21\\). Concluímos então que \\(22=7\\cdot 3+1\\), o resto \\(1\\) é a pequena correcção que falta a \\(3\\times 7\\) para chegar a \\(22\\). A formula\n\\[\n22=7\\cdot 3 +1\n\\]\nrelacciona os números \\(22\\) e \\(3\\). Observa, no lado esquerdo um único número \\(22\\) e no lado direito \\(7\\times 3 +1\\) as peças que o compõem.\nUsando em geral esta decomposição tem as seguintes características:\nSejam \\(p\\) e \\(d\\) dois interiros. Então existe um inteiro \\(r\\) tal que \\(0\\leq r&lt;d\\) e um inteiro \\(q\\geq 0\\) tal que:\n\\[\np=qd+r\n\\]\nA ideia chave para realizar o calculo \\(22/3\\) é ver quantas vezes \\(3\\) cabe dentro de \\(22\\), por outras palavras, a ideia chave é encontrar o número \\(q\\) que garante a correcção \\(r\\) mais pequena (neste caso inferior a \\(3\\)), ou seja:\n\\[\n22=q\\cdot 3+r \\qquad 0\\leq r&lt;3\n\\]\nComo é que encontramos este número \\(q\\)? Resposta: Adivinhamos a resposta. Eis alguns exemplos:\n\nE se \\(q=6\\)?\nSubstituindo na equação obtemos \\(22=6\\cdot 3+r\\) o que é equivalente a\\(r=22-18=4\\). Da do que o resto não está entre \\(0\\) e \\(3\\), temos de parti-lo em partes mais pequenas:\n\\[\n4 = q'\\cdot 3 + r' \\qquad 0\\leq r' &lt; 3\n\\]\nAdivinhando que a resposta é \\(q'=1\\), obtemos \\(r'=1\\).\nEm conclusão:\n\\[\n22 = 6\\times 3 + 1\\times 3 + +1\n\\]\nOu seja:\n\\[\n22 = 7\\times 3 +1\n\\]\nE se \\(q=7\\)?\nSubstituindo obtemos\n\\[\n22 = 7\\times 3 + r\n\\]\no que é equivalente a\n\\[\nr = 22-21 = 1\n\\]\nConcluímos que\n\\[\n22 = 7\\times 3 +1\n\\]\nE se \\(q=8\\) ?\n\\[\n22 = 8\\times 3 + r \\iff r=22-24 =-2\n\\]\nEntão\n\\[\n22 = 8\\times 3 -2\n\\]\nComo o resto é negativo e precisamos dele positivo, vamos corrigir esta expressão adicionado \\(0=3-3\\):\n\\[\n22 = 8\\times 3 -2 +3-3 = (8-1)\\times 3 +(3-2) = 7\\times 3 +1\n\\]"
  },
  {
    "objectID": "pol_division_PT.html#example-2-100112",
    "href": "pol_division_PT.html#example-2-100112",
    "title": "pol_division",
    "section": "Example 2: \\(1001/12\\)",
    "text": "Example 2: \\(1001/12\\)\nLet us consider now a more sophisticated division \\(1001/12\\), how do we argue as in elementary school?\nWe would first ask:\n\n\n\n\n\nImplicit in this question is the observation that \\(1001 = 100\\times 10 +1\\).\nThe answer is obviously \\(8\\):\n\n\n\n\n\nLook the position of the \\(8\\) is at the \\(10\\)’s place (under the \\(1\\) in \\(12\\)).\nMultiplying now \\(8\\times 12 = 96\\) we place it under the \\(10\\)’s place of \\(1001\\):\n\n\n\n\n\nNow we remove from \\(1001\\) the value \\(96 \\times 10\\) since \\(96\\) sits under the \\(10\\)’s place, the result is a \\(4\\) under the \\(10\\)’s place:\n\n\n\n\n\nRemoving \\(96\\times 10\\) from \\(1000\\) yields the \\(4 \\times 10\\), but removing \\(96\\times 10\\) from \\(1001\\) gives \\(4\\times 10 +1\\), thus we drop the \\(1\\) in \\(1001\\) and ask:\n\n\n\n\n\nThe answer is \\(3\\), we place it under the \\(1\\)’s place (under the \\(2\\) in \\(12\\)):\n\n\n\n\n\nMultiplying \\(3\\times 12\\) we obtain \\(36\\), we write it under the \\(1\\)’s place of \\(41\\):\n\n\n\n\n\nRemoving from the current remainder \\(41\\) this \\(36\\) gives:\n\n\n\n\n\nCombining all these removals from \\(1001\\) we conclude:\n\n\n\n\n\nWhat we have been computing in elementary school with this dividion algorithm is the solution \\(q\\) and \\(r\\) of the equation:\n\\[\n1001 = q \\times 12 + r \\qquad 0\\leq r&lt;12\n\\]\nThere are many paths and we follow then in stages.\nThe extreme of simplicity is to immediately guess the answer as \\(q=83\\) and \\(r=5\\), you do this if you are a genius, a path of a single step.\nDepending on the quality of our guesses we increase the number of steps.\nGuess 1:\nObserving that \\(1001=600+401\\) and \\(5\\times 12 =60\\) we guess that actual solution \\(q\\) is close to \\(10\\times 5\\):\n\\[\n600+401 = \\overbrace{10 \\times 5}^{q} \\times 12 +r\n\\]\nwith the remainder \\(r=401\\). Since it is not between \\(0\\) and \\(12\\) we can do better. Lets break \\(401\\) by guessing what is \\(q'\\) and \\(r'\\) such that:\n\\[\n401 = q'\\times 12 + r' \\qquad 0\\leq r'&lt;12\n\\]\nWe think \\(q'=3\\times10\\) is a good solution since \\(q'\\times 12 = 360\\), as a result the remainder is \\(r'=401-360=41\\)\nNot ideal, we want it at most \\(12\\). Therefore we again break into pieces:\n\\[\n41 = q''\\times 12 + r'' \\qquad 0\\leq r''&lt;12\n\\]\nWith the guess \\(q''=3\\) and the current remainder is just \\(r''=41-36=5\\).\nNote, finally!! We have a remainder smaller than \\(12\\).\nCollecting out calculations we have:\n\\[\n1001 = \\overbrace{(10 \\times 5 + 3\\times 10 + 3)}^{q=83}\\times 12 + 5\n\\]\n\n\n\n\n\n\nCommentary\n\n\n\nWhen guessing \\(q\\) it is useful to break \\(p\\) (in the example above \\(p=1001\\)) into a large and a smaller part.\n\\[\np=[\\text{large part}]+[\\text{small part}]\n\\]\nAnd then guess the best you can the \\(q\\) that cancels the large part.\nThere are many ways to break \\(p\\), experience will tell you what is ideal or not."
  },
  {
    "objectID": "question_q_13.html",
    "href": "question_q_13.html",
    "title": "asymptotic to MARS",
    "section": "",
    "text": "Exercise 1 Consider the sets \\(A\\coloneqq \\{(x,y) \\in \\mathbb{R}\\times\\mathbb{R}\\,|\\,\\, y=1\\}\\) and \\(B\\coloneqq \\{(x,y) \\in \\mathbb{R}\\times \\mathbb{R}\\,|\\,\\, x=1\\}\\). Convince yourself whether it is or not a function using the Definition 1. For both specify the domain, range and codomain. What about \\(B'\\coloneqq \\{(x,y) \\in \\mathbb{R}\\times \\{2\\}\\,|\\,\\, x=1\\}\\)?\n\n\nSolution 1. \\(A\\) is a function because all real \\(x\\) is assigned to \\(1\\), so for all \\(x\\) is paired uniquely with \\(1\\). \\(B\\) is not a function as \\(x=1\\) is paired with every real \\(y\\), so the pairs where \\(x\\) occur are not unique. \\(D_A=\\mathbb{R}\\), \\(R_A=\\{1\\}\\), \\(D_B=\\{1\\}\\), \\(R_B=\\mathbb{R}\\), the codomain is equal in both cases to \\(\\mathbb{R}\\). The set \\(B'=\\{(1,2)\\}\\) is a function, although an weird one, because it is just one ordered pair. \\(D_{B'}=\\{1\\}\\), \\(R_B'=\\{2\\}\\). The starting and codomain can be red from \\(\\mathbb{R}\\times \\{2\\}\\)."
  },
  {
    "objectID": "rutherfords atom.html",
    "href": "rutherfords atom.html",
    "title": "rutherfords atom",
    "section": "",
    "text": "Átomo era indivisível (Maxwell 1872)\n1899 - Thomson nos seus estudos da condução de electricidade observa que átomo é divisível, a corrente eléctrica evolve movimento livre de partes (electrões) de um átomo\nExperiências com radioactividade realizadas por Marie Curie (1867- 1934), Ernest Rutherford (1871-1937) e Frederick Soddy apontam também para divisibilidade do átomo. Descobriu-se que a transformação radioactiva dos átomos envolve a emissão de partículas pelos átomos (partículas subatómicas)\nEm 1910 as experiências de Lenard mostraram que electrões conseguem atravessar folhas de metal finas, mostrando assim que átomos são porosos.\nDescoberta que cada elemento tem-lhe associado uma serie espectral, i.e., um conjunto de comprimentos de onda de radiação emitida. Tal sugere que deve haver movimento de cargas no interior dos átomos.\n\nTodas estas descobertas levaram ao desenvolvimento de modelos do átomo:\n\nModelo de Thomson (1898) - Átomos são esferas de cargas positivas e negativas distribuídas uniformemente (passas no pudim). Este modelo evita o colapso do átomo, mas não explica as series espectrais dos elementos.\nModelo de Rutherford, Hans Geiger (1882-1945), Enerst Marsden (1899-1970) através de experiências realizadas entre 1909 e 1914 conduzem à descoberta que átomos têm estrutura interna, um núcleo onde a maior parte da massa está concentrada e onde está também localizada toda a carga positiva do átomo. A sua experiência consiste em:\n\nO seu modelo de átomo:\n\n\nThe theoretical framework that fits his experimental observations lead to the famous Rutherford formula."
  },
  {
    "objectID": "special relativity.html",
    "href": "special relativity.html",
    "title": "Special Relativity",
    "section": "",
    "text": "Key Ideas of Taylor Chap. 15"
  },
  {
    "objectID": "special relativity.html#galilean-transformation",
    "href": "special relativity.html#galilean-transformation",
    "title": "Special Relativity",
    "section": "Galilean Transformation",
    "text": "Galilean Transformation\nConsider a set of reference frames that move at relative constant velocity. Let us deduce a particular and simple case of Galilean transformations between two frames, this simple case is a consequence of the following assumption on the two chosen frames:\n\nThe frames S and S’ are oriented in such a way that the x,y,z-axes are parallel and they move with velocity \\(V\\) along the x axis, see Fig. 15.1\nS and S’ are equipped with a clock, to measure time and both clocks measure \\(t=t'=0\\) when initially both frames’s origin coincide.\n\nAs a result we have the following relation between measurements \\((x,y,z,t)\\) and \\((x',y',z',t')\\):\n\\[\n\\begin{cases}\nx' =x-Vt\\\\\ny'=y\\\\\nz'=z\\\\\nt'=t\n\\end{cases}\n\\]\nA generalization of these formulas for two frames where \\(V\\) has any direction \\(\\mathbf{V}\\) is\n\\[\n\\mathbf{r}'=\\mathbf{r}-\\mathbf{V}t \\qquad t'=t\n\\tag{1}\\]\nFurther generalizations are possible if we allow the two frame’s axes to be non-parallel but rotated, but the formula above suffices.\nDifferentiating the relation we find:\n\\[\n\\mathbf{v}'=\\mathbf{v}-\\mathbf{V}\n\\tag{2}\\]"
  },
  {
    "objectID": "special relativity.html#measurement-of-time-in-a-single-frame",
    "href": "special relativity.html#measurement-of-time-in-a-single-frame",
    "title": "Special Relativity",
    "section": "Measurement of Time in a Single Frame",
    "text": "Measurement of Time in a Single Frame\nAttached to any frame S is a square grid, at each vertex there is someone ready measure \\((x,y,z,t)\\) of a nearby event."
  },
  {
    "objectID": "special relativity.html#time-dilation",
    "href": "special relativity.html#time-dilation",
    "title": "Special Relativity",
    "section": "Time Dilation",
    "text": "Time Dilation\nConsider the train situation with the two frames S and S’.\nS is inertial and thus S’ is inertial, therefore the law distance = velocity \\(\\times\\) time holds in both. In particular for S’\n\\[\n\\Delta t' =\\frac{2h}{c}\n\\]\nUsing again the rule distance = velocity \\(\\times\\) time in S we find the lengths of the sides of the triangle, which in turn are related by\n\\[\n(\\frac{c\\Delta t}{2})^2=h^2+(\\frac{V\\Delta t}{2})^2\n\\]\nwhich is equivalent to:\n\\[\n\\Delta t = \\frac{2h}{c}\\frac{1}{\\sqrt{1-\\beta^2}} \\qquad \\beta = \\frac{V}{c}\n\\]\nWe conclude:\n\\[\n\\Delta t = \\frac{\\Delta t'}{\\sqrt{1-\\beta^2}}\n\\]\nTwo events occurring at the same place in S’ separated by \\(\\Delta t'\\) seconds are seen in S as being separated by \\(\\Delta t\\) seconds.\nWe want to reformulate this expression, define\n\\[\n\\gamma = \\frac{1}{\\sqrt{1-\\beta^2}}\n\\]\nand \\(\\Delta t_0:= \\Delta t'\\) the time elapsed in the reference frame (in this case S’) where the event occurred at the same place.\nThus:\n\\[\n\\Delta t = \\gamma \\Delta t_0\\geq \\Delta t_0\n\\]\nsince \\(\\gamma \\geq 1\\).\nWe should think about this formula as:\n\\[\n[\\text{time elaspsed measured in two places}] = \\gamma\\times [\\text{time elapsed measured at same place}]\n\\tag{3}\\]"
  },
  {
    "objectID": "systems_of_equations.html",
    "href": "systems_of_equations.html",
    "title": "Systems of equations",
    "section": "",
    "text": "Lets review the method you use in high-school to solve systems of equations - the elimination method - and then rewrite it under a new notation - the matrix and vector notation.\nA few examples will be given.\nKey concepts: systems of equations in matrix-vector notation, elimination, pivot, rank, conditions for 1,0 or infinite solutions.",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Solving systems using elimination"
    ]
  },
  {
    "objectID": "systems_of_equations.html#matrix-vector-notation-for-a-system",
    "href": "systems_of_equations.html#matrix-vector-notation-for-a-system",
    "title": "Systems of equations",
    "section": "Matrix-vector notation for a system",
    "text": "Matrix-vector notation for a system\nConsider the system with two equations, called \\(l_1\\) and \\(l_2\\):\n\\[\n\\begin{cases}\nx-4y =2\\\\\n2x-6y = 5\n\\end{cases}\n\\tag{1}\\]\nIn Equation 1 we find two equations and two unknowns \\(x\\) and \\(y\\). We want their values such that both equations are satisfied. (this system represents the interception of two lines)\nUsing the matrix-vector notation we can write Equation 1 as:\n\\[\n\\begin{pmatrix} 1 & -4\\\\ 2 & -6 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}2\\\\5\\end{pmatrix}\n\\tag{2}\\]\nLets read Equation 2 in words: the \\(2\\) by \\(2\\) matrix of coefficients is multiplied by the column vector \\(\\begin{pmatrix}x\\\\y\\end{pmatrix}\\) , the result is \\(\\begin{pmatrix}2\\\\5\\end{pmatrix}\\). This is one equation with one unknown, the column vector \\(\\begin{pmatrix}x\\\\y\\end{pmatrix}\\). Traditionally we write Equation 2 as \\(A \\mathbf{x} =\\mathbf{b}\\).\nHow do we multiply a vector by a matrix?\nAnswer:\n\\[\n\\overbrace{\\begin{pmatrix} 1 & -4\\\\ 2 & -6 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}}^\\text{matrix-vector mult.} =\\overbrace{\\begin{pmatrix}1\\cdot x-4\\cdot y \\\\ 2\\cdot x-6\\cdot y\\end{pmatrix}}^\\text{scalar mult.}\n\\]\nMatrix times a vector on the lhs is just a super-compact way of writing the vector on the rhs. Moreover, notice the shapes of the matrix and vectors, this is very, very important. A \\(2\\) by \\(2\\) matrix times a \\(2\\) by \\(1\\) column vector yields a \\(2\\) by \\(1\\) column vector! If you understand this it should not be a problem to see what shapes are or not compatible, check this:\n\n[\\(2\\times2\\)][ \\(3\\times 1\\)] \\(=\\) Nonsense\n[\\(3\\times2\\)][ \\(2\\times 1\\)] \\(=\\) [\\(3\\times 1\\)]\n[\\(2\\times3\\)][ \\(3\\times 1\\)] \\(=\\) [\\(2\\times 1\\)]\n[\\(3\\times2\\)][ \\(3\\times 1\\)] \\(=\\) Nonsense\n[\\(1\\times3\\)][ \\(3\\times 1\\)] \\(=\\) [\\(1\\times 1\\)]",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Solving systems using elimination"
    ]
  },
  {
    "objectID": "systems_of_equations.html#solving-the-system-using-the-elimination-method",
    "href": "systems_of_equations.html#solving-the-system-using-the-elimination-method",
    "title": "Systems of equations",
    "section": "Solving the system using the Elimination method",
    "text": "Solving the system using the Elimination method\nLets solve the system Equation 1 using the traditional rules we already know from high-school, recall:\n\nyou can replace an equation by itself times some constant.\nyou can replace any one equation by the sum both equations.\nyou can isolate \\(x\\) or \\(y\\) in one equation and substitute in the other equation.\n\nIn other words, 1. and 2., just say this: you can replace \\(l_1\\) or \\(l_2\\) by some convenient combination \\(al_1+bl_2\\). Rule 3. is known as back-substitution.\nApplying any one of these operations yields another and equivalent system of equations.\nThe central idea of the Elimination method is use linear combination of equations (1. and 2.) to eliminate variables and thus giving us an equivalent and easier to solve system. To eliminate variables we make clever use of rules \\(1\\) and \\(2\\). Once the system is simple enough we can use rule \\(3\\). How do you know what is or not a good combination? We’ll see that with examples. But the guiding principle is to use the pivots.\nThis recaps what you know, now lets use these rules to solve the Equation 1 and in parallel see the corresponding matrix-vector version.\n\nstep 1: Replace equation \\(l_2\\) by, \\(l_2\\) minus twice the equation \\(l_1\\), i.e., make the new second equation \\(l_2'\\) into \\(l_2-2l_1\\). This gives us:\n\\[\n\\begin{cases}x-4y =2\\\\2x-6y = 5\\end{cases} \\overset{l_2'=l_2-2l_1}{\\longrightarrow}\\begin{cases} x-4y =2\\\\ 2y = 1\\end{cases}\n\\]\nCorrespondingly we subtract from row \\(l_2\\) twice the row \\(l_1\\) in Equation 2, giving us\n\\[\n\\begin{pmatrix} 1 & -4\\\\ 2 & -6 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}2\\\\5\\end{pmatrix}\\overset{l_2'=l_2-2l_1}{\\longrightarrow} \\begin{pmatrix} 1 & -4\\\\ 0 & 2 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}2\\\\1\\end{pmatrix}\n\\]\nA good way to look at this is to focus on using the \\(1\\), to eliminate the \\(2\\). This entry of the matrix we focus on is called a pivot entry!\nstep 2: Multiply equation \\(l_2\\) by \\(1/2\\):\n\\[\n\\begin{cases} x-4y =2\\\\ 2y = 1\\end{cases}\\overset{l_2'=1/2l_2}{\\longrightarrow} \\begin{cases} x-4y =2\\\\ y = 1/2\\end{cases}\n\\]\nIn matrix-vector notation we find:\n\\[\n\\begin{pmatrix} 1 & -4\\\\ 0 & 2 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}2\\\\1\\end{pmatrix}\\overset{l_2'=1/2l_2}{\\longrightarrow} \\begin{pmatrix} 1 & -4\\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}2\\\\1/2\\end{pmatrix}\n\\]\nstep 3: Focusing on the second pivot entry, we eliminate the entry, \\(-4\\), by replacing \\(l_1\\) by \\(l_1\\) plus four times \\(l_2\\):\n\\[\n\\begin{cases} x -4y =2\\\\ y = 1/2\\end{cases}\\overset{l_1'=l_1+4l_2}{\\longrightarrow}\\begin{cases} x =4\\\\ y = 1/2\\end{cases}\n\\]\nIn matrix-vector notation we find:\n\\[\n\\begin{pmatrix} 1 & -4\\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}2\\\\1/2\\end{pmatrix}\\overset{l_1'=l_1+4l_2}{\\longrightarrow} \\begin{pmatrix} 1 & 0\\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}4\\\\1/2\\end{pmatrix}\n\\]\nFrom which we can read the final result \\(x=4\\) and \\(y=1/2\\).\n\nBetter notation: Going through the three steps again we notice we can improve our matrix-vector notation by suppressing from it the column \\(\\begin{pmatrix}x\\\\y\\end{pmatrix}\\) and writing instead the steps as:\n\\[\n\\left(\\begin{matrix} 1 & -4 \\\\ 2 & -6 \\end{matrix}\\;\\middle|\\; \\begin{matrix} 2\\\\5\\end{matrix}\\right)\n\\overset{l_2'=l_2-2l_1}{\\longrightarrow}\n\\left(\\begin{matrix} 1 & -4 \\\\ 0 & 2 \\end{matrix}\\;\\middle|\\; \\begin{matrix} 2\\\\1\\end{matrix}\\right)\n\\overset{l_2'=1/2l_2}{\\longrightarrow}\n\\left(\\begin{matrix} 1 & -4 \\\\ 0 & 1 \\end{matrix}\\;\\middle|\\; \\begin{matrix} 2\\\\1/2\\end{matrix}\\right)\n\\overset{l_1'=l_1+4l_2}{\\longrightarrow}\n\\left(\\begin{matrix} 1 & 0 \\\\ 0 & 1 \\end{matrix}\\;\\middle|\\; \\begin{matrix} 4\\\\1/2\\end{matrix}\\right)\n\\]\nFrom now on we’ll adopt this way of writing systems of equations, its called the extended matrix notation, because we appended a new column to right side of the \\(2\\times2\\) matrix. From the extended matrix we read the solution as follows\n\\[\n\\left(\\begin{matrix} 1 & 0 \\\\ 0 & 1 \\end{matrix}\\;\\middle|\\; \\begin{matrix} 4\\\\1/2\\end{matrix}\\right)\\longrightarrow \\begin{pmatrix} 1 & 0\\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}4\\\\1/2\\end{pmatrix}\\longrightarrow \\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}4\\\\1/2\\end{pmatrix}\n\\]\nwhere in the last step we multiplied the vector by the matrix.\nThe vector\n\\[\n\\begin{pmatrix}4\\\\1/2\\end{pmatrix}\n\\]\nis the solution of\n\\[\n\\begin{pmatrix} 1 & -4\\\\ 2 & -6 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}2\\\\5\\end{pmatrix}\n\\]\nMeaning, the solution of\n\\[\n\\begin{cases}x-4y =2\\\\2x-6y = 5\\end{cases}\n\\]\nis:\n\\[\n\\begin{cases}x =4\\\\y=1/2\\end{cases}\n\\]",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Solving systems using elimination"
    ]
  },
  {
    "objectID": "systems_of_equations.html#more-examples-of-the-elimination-method",
    "href": "systems_of_equations.html#more-examples-of-the-elimination-method",
    "title": "Systems of equations",
    "section": "More examples of the elimination method",
    "text": "More examples of the elimination method\nThe following examples will be given:\n\nA system with one solution\nA system with no solution\nA system with many solutions\nAnother system with many solutions\n\n\nExample 1: A system with one solution\nConsider the system, which we write in three different notations.\n\\[\n\\begin{cases}\nx+y-z=1\\\\\n2x-y+2z = 9\\\\\n2y=-x+z\n\\end{cases}\n\\leftrightsquigarrow\n\\begin{pmatrix}\n1 & 1 & -1\\\\\n2 & -1 & 2\\\\\n1 & 2 &-1\n\\end{pmatrix}\n\\begin{pmatrix}\nx\\\\\ny\\\\\nz\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1\\\\\n9\\\\\n0\n\\end{pmatrix}\n\\leftrightsquigarrow\n\\left(\n\\begin{matrix}\n1 & 1 & -1\\\\\n2 & -1 & 2\\\\\n1 & 2 & -1\n\\end{matrix}\n\\;\\middle|\\;\n\\begin{matrix}\n1\\\\\n9\\\\\n0\n\\end{matrix}\n\\right)\n\\tag{3}\\]\n(This system represents the interception of three planes at one point, each row of the matrix is a vector perpendicular to a plane)\nAgain we’ll use the extended notation during the elimination algorithm, because we don’t want to carry around the \\(x\\), \\(y\\) and \\(z\\) at each step.\n\\[\n\\begin{align}\n&\\left(\\begin{matrix}1 & 1 & -1\\\\2 & -1 & 2 \\\\1 & 2 & -1 \\end{matrix}\\;\\middle|\\;\\begin{matrix}1\\\\9\\\\0 \\end{matrix}\\right)\n\\overset{l_2'=l_2-2l_1\\\\l_3'=l_3-l_1}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 1 & -1 \\\\0 & -3 & 4 \\\\0 & 1 & 0 \\end{matrix}\\;\\middle|\\;\\begin{matrix}1\\\\7\\\\-1\\end{matrix}\\right)\n\\overset{l_2 \\leftrightarrow l_3}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 1 & -1 \\\\0 & 1 & 0\\\\0 & -3 & 4\\end{matrix}\\;\\middle|\\;\\begin{matrix}1\\\\-1\\\\7\\end{matrix}\\right)\\\\\n\\overset{l_3'=l_3+3l_2}{\\longrightarrow}\n&\\left(\\begin{matrix}1 & 1 & -1 \\\\0 & 1 & 0 \\\\0 & 0 & 4 \\end{matrix}\\;\\middle|\\;\\begin{matrix}1\\\\-1\\\\4\\end{matrix}\\right)\n\\overset{l_3'=1/4l_3}{\\longrightarrow}\\left(\\begin{matrix}1 & 1 & -1 \\\\0 & 1 & 0 \\\\0 & 0 & 1 \\end{matrix}\\;\\middle|\\;\\begin{matrix}1\\\\-1\\\\1\\end{matrix}\\right)\n\\overset{l_1'=l_1+l_3}{\\longrightarrow}\\left(\\begin{matrix}1 & 1 & 0 \\\\0 & 1 & 0 \\\\0 & 0 & 1 \\end{matrix}\\;\\middle|\\;\\begin{matrix}2\\\\-1\\\\1\\end{matrix}\\right) \\\\\n\\overset{l_1'=l_1-l_2}{\\longrightarrow}\n&\\left(\\begin{matrix}1 & 0 & 0 \\\\0 & 1 & 0 \\\\0 & 0 & 1 \\end{matrix} \\;\\middle|\\;\\begin{matrix}3\\\\-1\\\\1\\end{matrix}\\right)\n\\end{align}\n\\]\nEach step has its own commentary:\n\nOn the first step we use the pivot \\(A_{11}=1\\) to eliminate the entries \\(2\\) and \\(1\\) bellow.\nThe second step we switched equations, so as to bring a pivot \\(1\\) at position \\(A_{22}\\).\nThe third step consists in using this pivot to eliminate the entry \\(-3\\) below.\nIn step four we divided \\(l_3\\) by \\(4\\) so that that the pivot \\(A_{33}\\) is \\(1\\) instead of \\(4\\).\nAt step five we use \\(A_{33}\\) to eliminate the entry \\(-1\\) above it.\nStep six we just used the pivot \\(A_{22}\\), to eliminate the entry \\(1\\) at \\(A_{12}\\).\n\nAfter simplification, it is time to go back to the original notation - we find:\n\\[\n\\begin{cases}\nx = 3\\\\y=-1\\\\z=1\n\\end{cases}\n\\]\nwhich is the solution of the system of equations! Correspondingly, the solution for the matrix-vector notation is the following vector\n\\[\n\\begin{pmatrix}\n2\\\\\n-1\\\\\n1\n\\end{pmatrix}\n\\]\nImportant observations (that we’ll come very useful later): the form of the matrix after all these l.c. of rows, it has the form:\n\\[\n\\begin{pmatrix}I\\end{pmatrix}\n\\]\nand its rank is \\(3\\), meaning, the number of pivots if \\(3\\). Note as well that if we consider each column of the matrix as a vector, then we find \\(3\\) independent vectors. We write \\(r=3\\).\n\nDefinition 1 [rank \\(r\\) of a matrix \\(A\\)] = \\(r\\) = [# of pivots] =[# of indep columns of \\(A\\)] = [# of indep rows of \\(A\\)]\n\nMoreover, notice that the rank of the extended matrix \\(r^*\\) is also \\(3\\). And that the number of columns \\(n=3\\) as well.\nFrom this example we see something that happens in general:\nA system of equations \\(A\\mathbf{x}=\\mathbf{b}\\) have one solution provided \\(r=r^*=n\\).\n\n\nExample 2: A system with no solution\nThe system this time is:\n\\[\n\\begin{cases}\n2x-y=8\\\\\ny+2x = 4\\\\\nx=-y-1\n\\end{cases}\n\\leftrightsquigarrow\n\\begin{pmatrix}\n2 & -1\\\\\n2 & 1\\\\\n1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx\\\\\ny\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n8\\\\\n4\\\\\n-1\n\\end{pmatrix}\n\\leftrightsquigarrow\n\\left(\n\\begin{matrix}\n2 & -1 \\\\\n2 & 1  \\\\\n1 & 1\n\\end{matrix}\n\\;\\middle|\\;\n\\begin{matrix}\n8\\\\\n4\\\\\n-1\n\\end{matrix}\n\\right)\n\\tag{4}\\]\nTo find how many solutions \\(\\begin{pmatrix}x\\\\y\\end{pmatrix}\\) are there, we try to compute them using the Elimination algorithm:\n\\[\n\\begin{align}\n&\\left(\\begin{matrix}2 & -1 \\\\2 & 1 \\\\1 & 1  \\end{matrix}\\;\\middle|\\;\\begin{matrix}8\\\\4\\\\-1\\end{matrix}\\right)\n\\overset{l_1\\leftrightarrow l_3}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 1\\\\ 2 & 1 \\\\2 & -1 \\end{matrix} \\;\\middle|\\;\\begin{matrix}-1\\\\4\\\\8\\end{matrix}\\right)\n\\overset{l_2'=l_2-2l_1\\\\l_3'=l_3-2l_1}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 1 \\\\0 & -1 \\\\0 & -3 \\end{matrix}\\;\\middle|\\;\\begin{matrix}-1\\\\6\\\\10\\end{matrix}\\right)\\\\\n\\overset{l_2'=-l_2\\\\l_3'=l_3-3l_2}{\\longrightarrow}\n&\\left(\\begin{matrix}1 & 1 \\\\0 & 1 \\\\0 & 0 \\end{matrix} \\;\\middle|\\; \\begin{matrix}-1\\\\-6\\\\-8\\end{matrix}\\right)\n\\overset{l_1'=l_1-l_2}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 0 \\\\0 & 1 \\\\0 & 0 \\end{matrix} \\;\\middle|\\;\\begin{matrix}5\\\\-6\\\\-8\\end{matrix}\\right)\n\\end{align}\n\\]\nThis means:\n\\[\n\\left(\\begin{matrix}1 & 0 \\\\0 & 1 \\\\0 & 0 \\end{matrix} \\;\\middle|\\;\\begin{matrix}5\\\\-6\\\\-8\\end{matrix}\\right)\n\\leftrightsquigarrow\n\\begin{pmatrix}1 & 0 \\\\0 & 1 \\\\0 & 0 \\end{pmatrix} \\begin{pmatrix}x\\\\y\\end{pmatrix}=\\begin{pmatrix}5\\\\-6\\\\-8\\end{pmatrix}\n\\]\nNow that most simplification is done, lets convert back to the system’s notation:\n\\[\n\\begin{cases}\nx=5\\\\\ny=-6\\\\\n0=-8\n\\end{cases}\n\\tag{5}\\]\nIt is clearly impossible. No choice of \\(x\\) or \\(y\\) makes this three statements true simultaneously! Since Equation 4 is equivalent to Equation 5, therefore our original system Equation 4 has no solution as well.\nObservations: Notice the form of the matrix is\n\\[\n\\begin{pmatrix}I\\\\\\mathbf{0}\\end{pmatrix}\n\\]\nthe fact that \\(r=2\\), \\(r^*=3\\) and \\(n=2\\). We see in this example something which happens in general and which we’ll justify later:\nA system of equations \\(A\\mathbf{x}=\\mathbf{b}\\) have no solution provided \\(r&lt;r^*\\).\n\n\nExample 3: A system with many solutions\nThe system is:\n\\[\n\\begin{cases}\nx+y-z=0\\\\\n2x-y+2z = 0\\\\\n\\end{cases}\n\\leftrightsquigarrow\n\\begin{pmatrix}\n1 & 1 & -1\\\\\n2 & -1 & 2\\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nx\\\\\ny\\\\\nz\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0\\\\\n0\n\\end{pmatrix}\n\\leftrightsquigarrow\n\\left(\n\\begin{matrix}\n1 & 1 & -1\\\\\n2 & -1 & 2 \\\\\n\\end{matrix}\n\\;\\middle|\\;\n\\begin{matrix}\n0\\\\0\n\\end{matrix}\n\\right)\n\\tag{6}\\]\nSolving:\n\\[\n\\begin{align}\n&\\begin{pmatrix}1 & 1 & -1 &\\bigm| & 0\\\\2 & -1 & 2 &\\bigm| & 0\\\\\\end{pmatrix}\n\\overset{l_2'=l_2-2l_1}{\\longrightarrow}\n\\begin{pmatrix}1 & 1 & -1 &\\bigm| & 0\\\\0 & -3 & 4 &\\bigm| & 0\\end{pmatrix}\n\\overset{l_2'=-1/3l_2}{\\longrightarrow}\n\\begin{pmatrix}1 & 1 & -1 &\\bigm| & 0\\\\0 & 1 & -4/3 &\\bigm| & 0\\end{pmatrix}\\\\\n\\overset{l_1'=l_1-l_2}{\\longrightarrow}\n&\\begin{pmatrix}1 & 0 & 1/3 &\\bigm| & 0\\\\0 & 1 & -4/3 &\\bigm| & 0\\end{pmatrix}\n\\end{align}\n\\]\nOnce again, this notation means:\n\\[\n\\begin{pmatrix}1 & 0 & 1/3 &\\bigm| & 0\\\\0 & 1 & -4/3 &\\bigm| & 0\\end{pmatrix}\n\\leftrightsquigarrow\n\\begin{pmatrix}1 & 0 & 1/3\\\\0 & 1 & -4/3 \\end{pmatrix}\n\\begin{pmatrix}x\\\\y\\\\z\\end{pmatrix}\n=\n\\begin{pmatrix}0\\\\0\\end{pmatrix}\n\\tag{7}\\]\nConverting back to the system notation have:\n\\[\n\\begin{cases}\nx+1/3z =0\\\\\ny-4/3z=0\n\\end{cases}\n\\iff\n\\begin{cases}\nx=-1/3z\\\\\ny=4/3z\n\\end{cases}\n\\tag{8}\\]\nAsking what \\((x,y,z)\\in \\mathbb{R}^3\\) that satisfy the equation Equation 6 is equivalent to ask, what is \\(x,y,z\\in \\mathbb{R}\\) that satisfy Equation 8 . Each real \\(z\\) we choose gives us the corresponding \\(x\\) and \\(y\\); as a consequence we have many solution. In other words, the solution is\n\\[\n\\begin{pmatrix}\nx\\\\\ny\\\\\nz\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-1/3z\\\\\n4/3z\\\\\nz\n\\end{pmatrix}\n\\tag{9}\\]\nObservations: The aspect of the final matrix is:\n\\[\n\\begin{pmatrix}I \\,\\,F\\end{pmatrix}\n\\]\nwhere the \\(F\\) block is the third column \\((1/3, -4/3)^\\intercal\\), while \\(I\\) is the \\(2\\) by \\(2\\) identity.\nAdditionally, \\(r=2\\), \\(r^*=2\\) and \\(n=3\\). A general rule (to be explained later) is:\nA system of equations \\(A\\mathbf{x}=\\mathbf{b}\\) have infinite solutions provided \\(r=r^*&lt;n\\).\n\n\nExample 4: Another system with many solutions\nWhat is the solution \\((x,y, z, w)\\) for the following system:\n\\[\n\\begin{cases}\n&x&+ &2 y&+&2 z &+&2w &= 1\\\\\n&2x&+&4y&+&6z&+&8w &= 2\\\\\n&3x&+&6y&+&8z&+&10w &=3\n\\end{cases}\n\\]\nUsing elimination algorithm we make linear combinations of the equation with the goal of eliminating variables, here is one way to go\n\\[\n\\begin{align}\n&\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm|1\\\\\n2 & 4 & 6 & 8 &\\bigm|2\\\\\n3 & 6 & 8 & 10 &\\bigm| 3\n\\end{pmatrix}\n\\overset{l_2' = l_2-2l_1}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| 1\\\\\n0 & 0 & 2 & 4 &\\bigm| 0\\\\\n3 & 6 & 8 & 10 &\\bigm| 3\n\\end{pmatrix}\n\\overset{l_3' = l_3-3l_1}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| 1\\\\\n0 & 0 & 2 & 4 &\\bigm| 0\\\\\n0 & 0 & 2 & 4 &\\bigm| 0\n\\end{pmatrix}\\\\\n&\\overset{l_3'=l_3-l_2}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| 1\\\\\n0 & 0 & 2 & 4 &\\bigm| 0\\\\\n0 & 0 & 0 & 0 &\\bigm| 0\n\\end{pmatrix}\n\\overset{l_2'=1/2l_2}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| 1\\\\\n0 & 0 & 1 & 2 &\\bigm| 0\\\\\n0 & 0 & 0 & 0 &\\bigm| 0\n\\end{pmatrix}\n\\overset{l_1'=l_1-2l_2}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 0 & -2 &\\bigm| 1\\\\\n0 & 0 & 1 & 2 &\\bigm| 0\\\\\n0 & 0 & 0 & 0 &\\bigm| 0\n\\end{pmatrix}\n\\end{align}\n\\tag{10}\\]\nWe simplified it as much as we can, going back to the system’s notation we have:\n\\[\n\\begin{cases}\nx + 2 y & &- &2w &=1\\\\\n&z &+ &2w &=0\n\\end{cases}\n\\]\nNow, promote \\(y\\) and \\(w\\) into parameters and express \\(x\\) and \\(z\\) in term of them (if why we take this step is not natural, it will be soon)\n\\[\n\\begin{cases}\nx + 2 y & &- &2w &=1\\\\\n&z &+ &2w &=0\n\\end{cases}\n\\longrightarrow\n\\begin{cases}\nx &=1-2y&-&2w\\\\\nz &=&-&2w\n\\end{cases}\n\\tag{11}\\]\nFor each values we assign to \\(y\\) and \\(w\\) we get one solution! There’s an infinite number of them.\nThis example is given so that you can see, that the final aesthetic of the simplified \\(A\\) may have the \\(I\\) and \\(F\\) blocks mixed!\nThis is also a matrix with form:\n\\[\n\\begin{pmatrix}I \\,\\,F\\end{pmatrix}\n\\]\nThe rank is \\(r=2\\), the rank of the extended matrix is \\(r^*=2\\), meanwhile \\(n=4\\). Since \\(r=r^*&lt;n\\) we have an infinite number of solutions.\n\n\nExample 5: A system that may have one, none or many solutions\nWhat is the solution of:\n\\[\n\\begin{cases}\n&x&+ &k y&+&2 z &= 1\\\\\n&x&+&y&+&(k+1)z &= k\\\\\n&-x&-&y&-&z &=k+1\n\\end{cases}\n\\]\nUsing the extended matrix formulation we use the pivots to eliminate entries:\n\\[ \\begin{align} &\\begin{pmatrix} 1 & k & 2  &\\bigm|1\\\\ 1 & 1 & k+1 &\\bigm|k\\\\ -1 & -1 & -1 &\\bigm| k+1 \\end{pmatrix} \\overset{l_2' = l_2-l_1\\\\l_3'=l_3+l_1}{\\longrightarrow} \\begin{pmatrix} 1 & k & 2 &\\bigm| 1\\\\ 0 & 1-k & k-1 &\\bigm| k-1\\\\ 0 & k-1 & 1 &\\bigm| k+2 \\end{pmatrix} \\overset{l_3' = l_3+l_2}{\\longrightarrow} \\begin{pmatrix} 1 & k & 2  &\\bigm| 1\\\\ 0 & 1-k & k-1 &\\bigm| k-1\\\\ 0 & 0 & k &\\bigm| 2k+1 \\end{pmatrix} \\end{align} \\]\nNow, depending upon on the value \\(k\\):\nFor \\(k=1\\) then\n\\[\n\\begin{pmatrix} 1 & 1 & 2  &\\bigm|1\\\\ 0 & 0 & 0 &\\bigm|0\\\\ 0 & 0 & 1 &\\bigm| 3 \\end{pmatrix} \\overset{l_2 \\leftrightarrow l_3}{\\longrightarrow} \\begin{pmatrix} 1 & 1 & 2  &\\bigm|1\\\\ 0 & 0 & 1 &\\bigm|3\\\\ 0 & 0 & 0 &\\bigm| 0 \\end{pmatrix}\\overset{l_1'=l_1-2l_2 }{\\longrightarrow}\\begin{pmatrix} 1 & 1 & 0  &\\bigm|-5\\\\ 0 & 0 & 1 &\\bigm|3\\\\ 0 & 0 & 0 &\\bigm| 0 \\end{pmatrix}\n\\tag{12}\\]\nwhich has the form:\n\\[\n\\begin{pmatrix}I\\,\\,F \\end{pmatrix}\n\\]\nalso: \\(r=2\\), \\(r^*=2\\) and \\(n=3\\).\nSince \\(r=r^*&lt;n\\) we conclude we have an infinite number of solutions as we readily check by actually computing them. From Equation 12 we know:\n\\[\n\\begin{cases}\nx + y = -5\\\\\nz=3\n\\end{cases}\n\\]\nSince we have two equations and three unknowns, we promote one of them, let it be \\(x\\), to the status of a parameter \\(x_0\\), hence:\n\\[\n\\begin{pmatrix}x\\\\y\\\\z\\end{pmatrix}=\\begin{pmatrix}x_0\\\\-5-x_0\\\\3\\end{pmatrix}=\\begin{pmatrix}0\\\\-5\\\\3\\end{pmatrix}+x_0\\begin{pmatrix}1\\\\-1\\\\0\\end{pmatrix}\n\\]\nFor each \\(x_0\\) of our choice we have a distinct solution. There is infinite of them.\nFor \\(k=0\\)\n\\[\n\\begin{pmatrix} 1 & 0 & 2  &\\bigm|1\\\\ 0 & 1 & -1 &\\bigm|-1\\\\ 0 & 0 & 0 &\\bigm| 1 \\end{pmatrix}\n\\]\nThe system has the form:\n\\[\n\\begin{pmatrix}\nI & F\\\\\n\\mathbf{0} &\\mathbf{0}\n\\end{pmatrix}\n\\]\nAnd note: \\(r=2\\), \\(r^*=3\\) and \\(n=3\\), thus \\(r&lt;r^*=n\\) which tells us there is no solution. In fact we can see why by rolling back to:\n\\[\n\\begin{cases}\nx+2z=1\\\\\ny-z =-1\\\\\n0=1\n\\end{cases}\n\\]\nThere are no \\(x\\), \\(y\\) and \\(z\\) that satisfies these three equations simultaneously.\nIf \\(k\\) is not \\(0,1\\) then we always have \\(r=r^*=n\\), thus, there is always a unique solution.\nIn summary:\n\nA system of equations \\(A\\mathbf{x}=\\mathbf{b}\\) have infinite solutions provided \\(r=r^*&lt;n\\).\nA system of equations \\(A\\mathbf{x}=\\mathbf{b}\\) have one solution provided \\(r=r^*=n\\).\nA system of equations \\(A\\mathbf{x}=\\mathbf{b}\\) have no solutions provided \\(r&lt;r^*\\).\n\n\n\n\n\n\n\nCommentaries\n\n\n\n\nFor the moment assume this summary as a fact of life, later we’ll justify in detail the meaning of this triple equality, for know just focus on learning elimination and testing the triple equality\nHow do we know how many pivots are there? Using l.c. of rows, simplify the matrix by eliminating as many entries as possible using the pivots.\nA matrix where all entries below the pivots were killed is in triangular form. Back substitution can already be used at this stage. (we did not do this in the examples, but we could) If we proceed and also kill all entries above the pivots, we get the best matrix (this is what we did in the examples). A matrix is then said to be in reduced row echelon form. Back substitution can also used at this final stage.\nWith these three statements we can decide whether a system has one, none or infinite solution. But thinking about how did we get \\(r\\) and \\(r^*\\)? We did use elimination, which is a labor intensive process, to bring \\(A\\mathbf{x}=\\mathbf{b}\\) into a simplified form. So much work was done, one might as well solve the entire thing we from the solution itself evaluate whether there is one, none or infinite solutions. Here comes a very important point, these three statements are worthless as a practical point of view to decide in which case are we. These three are like a tip of an iceberg, and what’s underneath is beautiful way to understand why some systems have one, none or infinite solution. For the moment, we use them as facts of life, later we uncover what is going on. These statements are only useful in situations where the matrix depend on some parameter \\(k\\) and we which, after putting the matrix in triangular form, we want to check for which values there is one, none or infinite solutions.",
    "crumbs": [
      "Brief Notes",
      "Linear Algebra",
      "Solving systems using elimination"
    ]
  },
  {
    "objectID": "vector_spaces.html",
    "href": "vector_spaces.html",
    "title": "Vector spaces",
    "section": "",
    "text": "Instead of telling you what a vector is lets see what can you do with them. So, for the moment we’ll consider a vector, or better said - a column vector - an array of numbers which we write as:\n\\[\n\\begin{pmatrix}1\\\\5\\end{pmatrix}, \\begin{pmatrix}1\\\\-5\\\\0.1\\end{pmatrix},\\begin{pmatrix}x\\\\y\\\\z\\end{pmatrix}\n\\]\nWhat can you do with vectors? Answer: linear combinations! A linear combination is a computation that looks like this:\n\\[\n3 \\mathbf{u}+2\\mathbf{v} = 2\\begin{pmatrix} 1\\\\3\\end{pmatrix}+4\\begin{pmatrix} 1\\\\-1\\end{pmatrix}\n\\]\nFirst multiply the vectors by the scalars, then add the vectors by adding the entries, the result is:\n\\[\n\\begin{pmatrix} 2\\\\6\\end{pmatrix}+\\begin{pmatrix} 4\\\\-4\\end{pmatrix} = \\begin{pmatrix} 6\\\\2 \\end{pmatrix}\n\\]\nA generic linear combination looks like this:\n\\[\na \\mathbf{u}+b\\mathbf{v} = a\\begin{pmatrix} 1\\\\5\\end{pmatrix}+b\\begin{pmatrix} 2\\\\10\\end{pmatrix} = \\begin{pmatrix}a +2b\\\\5a +10b \\end{pmatrix}\n\\]\nwhere \\(a\\) and \\(b\\) are scalars.\nIf you know how to combine two vectors, you know how to combine three and so on.\nThe geometrical addition of vectors is performed using the paralelogram rule:"
  },
  {
    "objectID": "vector_spaces.html#matrix-vector-notation-for-a-system",
    "href": "vector_spaces.html#matrix-vector-notation-for-a-system",
    "title": "Vector spaces",
    "section": "Matrix-vector notation for a system",
    "text": "Matrix-vector notation for a system\nConsider the system with two equations, called \\(l_1\\) and \\(l_2\\):\n\\[\n\\begin{cases}\nx-4y =2\\\\\n2x-6y = 5\n\\end{cases}\n\\tag{1}\\]\nIn Equation 1 we find two equations and two unknowns \\(x\\) and \\(y\\). We want their values such that both equations are satisfied.\nUsing the matrix-vector notation we can write Equation 1 as:\n\\[\n\\begin{pmatrix} 1 & -4\\\\ 2 & -6 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}2\\\\5\\end{pmatrix}\n\\tag{2}\\]\nLets read Equation 2 in words: the \\(2\\) by \\(2\\) matrix of coefficients is multiplied by the column vector \\(\\begin{pmatrix}x\\\\y\\end{pmatrix}\\) , the result is \\(\\begin{pmatrix}2\\\\5\\end{pmatrix}\\). This is one equation, and the unknown is the column vector \\(\\begin{pmatrix}x\\\\y\\end{pmatrix}\\).\nHow do we multiply a vector by a matrix?\nAnswer:\n\\[\n\\overbrace{\\begin{pmatrix} 1 & -4\\\\ 2 & -6 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}}^\\text{matrix-vector mult.} =\\overbrace{\\begin{pmatrix}1\\cdot x-4\\cdot y \\\\ 2\\cdot x-6\\cdot y\\end{pmatrix}}^\\text{scalar mult.}\n\\]\nMatrix times a vector on the lhs is just a super-compact way of writing the vector on the rhs. Moreover, notice the shapes of the matrix and vectors, this is very, very important. A \\(2\\) by \\(2\\) matrix times a \\(2\\) by \\(1\\) column vector yields a \\(2\\) by \\(1\\) column vector! If you understand this it should not be a problem to see what shapes are or not compatible, check this:\n\n[\\(2\\times2\\)][ \\(3\\times 1\\)] \\(=\\) Nonsense\n[\\(3\\times2\\)][ \\(2\\times 1\\)] \\(=\\) [\\(3\\times 1\\)]\n[\\(2\\times3\\)][ \\(3\\times 1\\)] \\(=\\) [\\(2\\times 1\\)]\n[\\(3\\times2\\)][ \\(3\\times 1\\)] \\(=\\) Nonsense\n[\\(1\\times3\\)][ \\(3\\times 1\\)] \\(=\\) [\\(1\\times 1\\)]"
  },
  {
    "objectID": "vector_spaces.html#solving-the-system-using-the-elimination-method",
    "href": "vector_spaces.html#solving-the-system-using-the-elimination-method",
    "title": "Vector spaces",
    "section": "Solving the system using the Elimination method",
    "text": "Solving the system using the Elimination method\nLets solve the system Equation 1 using the traditional rules we already know from high-school, recall:\n\nyou can replace an equation by itself times some constant.\nyou can replace any one equation by the sum both equations.\nyou can isolate \\(x\\) or \\(y\\) in one equation and substitute in the other equation.\n\nIn other words, 1. and 2., just say this: you can replace \\(l_1\\) or \\(l_2\\) by some convenient combination \\(al_1+bl_2\\).\nThe central idea of the Elimination method is use combination of equations to eliminate variables and thus giving us an equivalent and easier to solve system. To eliminate variables we use clever use of rules \\(1\\) and \\(2\\). Once the system is simple enough we can use rule \\(3\\). How do you know what is or not a good combination? Just practice and see.\nThis recaps what you know, now lets use these rules to solve the Equation 1 and in parallel see the corresponding matrix-vector version.\n\nstep 1: Replace equation \\(l_2\\) by, \\(l_2\\) minus twice the equation \\(l_1\\), i.e., make the new second equation \\(l_2'\\) into \\(l_2-2l_1\\). This gives us:\n\\[\n\\begin{cases}x-4y =2\\\\2x-6y = 5\\end{cases} \\overset{l_2'=l_2-2l_1}{\\longrightarrow}\\begin{cases} x-4y =2\\\\ 2y = 1\\end{cases}\n\\]\nCorrespondingly we subtract from row \\(l_2\\) twice the row \\(l_1\\) in Equation 2, giving us\n\\[\n\\begin{pmatrix} 1 & -4\\\\ 2 & -6 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}2\\\\5\\end{pmatrix}\\overset{l_2'=l_2-2l_1}{\\longrightarrow} \\begin{pmatrix} 1 & -4\\\\ 0 & 2 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}2\\\\1\\end{pmatrix}\n\\]\nstep 2: Multiply equation \\(l_2\\) by \\(1/2\\):\n\\[\n\\begin{cases} x-4y =2\\\\ 2y = 1\\end{cases}\\overset{l_2'=1/2l_2}{\\longrightarrow} \\begin{cases} x-4y =2\\\\ y = 1/2\\end{cases}\n\\]\nIn matrix-vector notation we find:\n\\[\n\\begin{pmatrix} 1 & -4\\\\ 0 & 2 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}2\\\\1\\end{pmatrix}\\overset{l_2'=1/2l_2}{\\longrightarrow} \\begin{pmatrix} 1 & -4\\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}2\\\\1/2\\end{pmatrix}\n\\]\nstep 3: Since the system is simple enough, there are two way to go at this stage. You either substitute the equation \\(y=1/2\\) into the first equation \\(x-4y=2\\) and solve for \\(x\\), this gives us the answer \\(x=4\\) and \\(y=1/2\\). The second way to to go about it, is to simplify even further the system of equations, we do that by replacing \\(l_1\\) by, \\(l_1\\) plus four times \\(l_2\\):\n\\[\n\\begin{cases} x-4y =2\\\\ y = 1/2\\end{cases}\\overset{l_1'=l_1+4l_2}{\\longrightarrow}\\begin{cases} x =4\\\\ y = 1/2\\end{cases}\n\\]\nIn matrix-vector notation we find:\n\\[\n\\begin{pmatrix} 1 & -4\\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}2\\\\1/2\\end{pmatrix}\\overset{l_1'=l_1+4l_2}{\\longrightarrow} \\begin{pmatrix} 1 & 0\\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}4\\\\1/2\\end{pmatrix}\n\\]\nFrom which we can read the final result \\(x=4\\) and \\(y=1/2\\).\n\nGoing through the three steps again we notice we can improve our matrix-vector notation by suppressing from it the column \\(\\begin{pmatrix}x\\\\y\\end{pmatrix}\\) and writing instead the steps as:\n\\[\n\\begin{pmatrix} 1 & -4 &\\bigm| & 2 \\\\ 2 & -6 &| & 5 \\end{pmatrix}\\overset{l_2'=l_2-2l_1}{\\longrightarrow} \\begin{pmatrix} 1 & -4 &\\bigm| & 2\\\\ 0 & 2 &\\bigm| & 1\\end{pmatrix} \\overset{l_2'=1/2l_2}{\\longrightarrow} \\begin{pmatrix} 1 & -4 &\\bigm| & 2\\\\ 0 & 1 &\\bigm| & 1/2\\end{pmatrix}\\overset{l_1'=l_1+4l_2}{\\longrightarrow} \\begin{pmatrix} 1 & 0 &\\bigm| & 4\\\\ 0 & 1 &\\bigm| & 1/2\\end{pmatrix}\n\\]\nFrom now on we’ll adopt this way of writing systems of equations, its called the extended matrix notation, because we appended a new column to right side of the \\(2\\times2\\) matrix. From the extended matrix we read the solution as follows\n\\[\n\\begin{pmatrix} 1 & 0 &\\bigm| & 4\\\\ 0 & 1 &\\bigm| & 1/2\\end{pmatrix}\\longrightarrow \\begin{pmatrix} 1 & 0\\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}4\\\\1/2\\end{pmatrix}\\longrightarrow \\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}4\\\\1/2\\end{pmatrix}\n\\]\nwhere in the last step we multiplied the vector by the matrix.\nThe vector\n\\[\n\\begin{pmatrix}4\\\\1/2\\end{pmatrix}\n\\]\nis the solution of\n\\[\n\\begin{pmatrix} 1 & -4\\\\ 2 & -6 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix}= \\begin{pmatrix}2\\\\5\\end{pmatrix}\n\\]\nMeaning, the solution of\n\\[\n\\begin{cases}x-4y =2\\\\2x-6y = 5\\end{cases}\n\\]\nis:\n\\[\n\\begin{cases}x =4\\\\y=1/2\\end{cases}\n\\]"
  },
  {
    "objectID": "vector_spaces.html#more-examples-of-the-elimination-method",
    "href": "vector_spaces.html#more-examples-of-the-elimination-method",
    "title": "Vector spaces",
    "section": "More examples of the elimination method",
    "text": "More examples of the elimination method\nThe following examples will be given:\n\nA system with one solution\nA system with no solution\nA system with many solutions\nAnother system with many solutions\n\n\nExample 1: A system with one solution\nConsider the system, which we write in three different notations.\n\\[\n\\begin{cases}\nx+y-z=1\\\\\n2x-y+2z = 9\\\\\n2y=-x+z\n\\end{cases}\n\\leftrightsquigarrow\n\\begin{pmatrix}\n1 & 1 & -1\\\\\n2 & -1 & 2\\\\\n1 & 2 &-1\n\\end{pmatrix}\n\\begin{pmatrix}\nx\\\\\ny\\\\\nz\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1\\\\\n9\\\\\n0\n\\end{pmatrix}\n\\leftrightsquigarrow\n\\begin{pmatrix}\n1 & 1 & -1 &\\bigm| & 1\\\\\n2 & -1 & 2 &\\bigm| & 9\\\\\n1 & 2 & -1 &\\bigm| & 0\n\\end{pmatrix}\n\\tag{3}\\]\nAgain we’ll use the later notation during the elimination algorithm, because we don’t have to carry around the \\(x\\), \\(y\\) and \\(z\\) at each step.\n\\[\n\\begin{align}\n&\\begin{pmatrix}1 & 1 & -1 &\\bigm| & 1\\\\2 & -1 & 2 &\\bigm| & 9\\\\1 & 2 & -1 &\\bigm| & 0\\end{pmatrix}\n\\overset{l_2'=l_2-2l_1\\\\l_3'=l_3-l_1}{\\longrightarrow}\n\\begin{pmatrix}1 & 1 & -1 &\\bigm| & 1\\\\0 & -3 & 4 &\\bigm| & 7\\\\0 & 1 & 0 &\\bigm| & -1\\end{pmatrix}\n\\overset{l_2 \\leftrightarrow l_3}{\\longrightarrow}\n\\begin{pmatrix}1 & 1 & -1 &\\bigm| & 1\\\\0 & 1 & 0 &\\bigm| & -1\\\\0 & -3 & 4 &\\bigm| & 7\\end{pmatrix}\\\\\n\\overset{l_3'=l_3+3l_2}{\\longrightarrow}\n&\\begin{pmatrix}1 & 1 & -1 &\\bigm| & 1\\\\0 & 1 & 0 &\\bigm| & -1\\\\0 & 0 & 4 &\\bigm| & 4\\end{pmatrix}\n\\overset{l_3'=1/4l_3}{\\longrightarrow}\n\\begin{pmatrix}1 & 1 & -1 &\\bigm| & 1\\\\0 & 1 & 0 &\\bigm| & -1\\\\0 & 0 & 1 &\\bigm| & 1\\end{pmatrix}\n\\overset{l_1'=l_1+l_3}{\\longrightarrow}\n\\begin{pmatrix}1 & 1 & 0 &\\bigm| & 2\\\\0 & 1 & 0 &\\bigm| & -1\\\\0 & 0 & 1 &\\bigm| & 1\\end{pmatrix} \\\\\n\\overset{l_1'=l_1-l_2}{\\longrightarrow}\n&\\begin{pmatrix}1 & 0 & 0 &\\bigm| & 2\\\\0 & 1 & 0 &\\bigm| & -1\\\\0 & 0 & 1 &\\bigm| & 1\\end{pmatrix}\n\\end{align}\n\\]\nMost of the system simplification (by elimination of variables) is done in this matrix-vector notation, looking at we find we went as far as we can, thus it is time to go back to the original notation - we find:\n\\[\n\\begin{cases}\nx = 3\\\\y=-1\\\\z=1\n\\end{cases}\n\\]\nwhich is the solution of the system of equations! Correspondingly, the solution for the matrix-vector notation is the following vector\n\\[\n\\begin{pmatrix}\n2\\\\\n-1\\\\\n1\n\\end{pmatrix}\n\\]\n\n\nExample 2: A system with no solution\nThe system this time is:\n\\[\n\\begin{cases}\n2x-y=8\\\\\ny+2x = 4\\\\\nx=-y-1\n\\end{cases}\n\\leftrightsquigarrow\n\\begin{pmatrix}\n2 & -1\\\\\n2 & 1\\\\\n1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx\\\\\ny\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n8\\\\\n4\\\\\n-1\n\\end{pmatrix}\n\\leftrightsquigarrow\n\\begin{pmatrix}\n2 & -1 &\\bigm| & 8\\\\\n2 & 1  &\\bigm| & 4\\\\\n1 & 1  &\\bigm| & -1\n\\end{pmatrix}\n\\]\nTo find how many solutions \\(\\begin{pmatrix}x\\\\y\\end{pmatrix}\\) are there, we try to compute them using the Elimination algorithm:\n\\[\n\\begin{align}\n&\\begin{pmatrix}2 & -1 &\\bigm| & 8\\\\2 & 1  &\\bigm| & 4\\\\1 & 1  &\\bigm| & -1\\end{pmatrix}\n\\overset{l_1\\leftrightarrow l_3}{\\longrightarrow}\n\\begin{pmatrix}1 & 1 &\\bigm| & -1\\\\2 & 1 &\\bigm| & 4\\\\2 & -1 &\\bigm| & 8\\end{pmatrix}\n\\overset{l_2'=l_2-2l_1\\\\l_3'=l_3-2l_1}{\\longrightarrow}\n\\begin{pmatrix}1 & 1 &\\bigm| & -1\\\\0 & -1 &\\bigm| & 6\\\\0 & -3 &\\bigm| & 10\\end{pmatrix}\\\\\n\\overset{l_2'=-l_2\\\\l_3'=l_3-3l_2}{\\longrightarrow}\n&\\begin{pmatrix}1 & 1 &\\bigm| & -1\\\\0 & 1 &\\bigm| & -6\\\\0 & 0 &\\bigm| & 8\\end{pmatrix}\n\\overset{l_1'=l_1-l_2}{\\longrightarrow}\n\\begin{pmatrix}1 & 0 &\\bigm| & 5\\\\0 & 1 &\\bigm| & -6\\\\0 & 0 &\\bigm| & -8\\end{pmatrix}\n\\end{align}\n\\]\nThis means:\n\\[\n\\begin{pmatrix}1 & 0 &\\bigm| & 5\\\\0 & 1 &\\bigm| & -6\\\\0 & 0 &\\bigm| & -8\\end{pmatrix}\n\\leftrightsquigarrow\n\\begin{pmatrix}1 & 0 \\\\0 & 1 \\\\0 & 0 \\end{pmatrix} \\begin{pmatrix}x\\\\y\\end{pmatrix}=\\begin{pmatrix}5\\\\-6\\\\-8\\end{pmatrix}\n\\]\nNow that most simplification is done, lets convert back to the system’s notation:\n\\[\n\\begin{cases}\nx=5\\\\\ny=-6\\\\\n0=-8\n\\end{cases}\n\\tag{4}\\]\nIt is clearly impossible. No choice of \\(x\\) or \\(y\\) makes this three statements true simultaneously! Since Equation 3 is equivalent to Equation 4, therefore our original system Equation 3 has no solution as well.\n\n\nExample 3: A system with many solutions\nThe system is:\n\\[\n\\begin{cases}\nx+y-z=0\\\\\n2x-y+2z = 0\\\\\n\\end{cases}\n\\leftrightsquigarrow\n\\begin{pmatrix}\n1 & 1 & -1\\\\\n2 & -1 & 2\\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nx\\\\\ny\\\\\nz\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0\\\\\n0\n\\end{pmatrix}\n\\leftrightsquigarrow\n\\begin{pmatrix}\n1 & 1 & -1 &\\bigm| & 0\\\\\n2 & -1 & 2 &\\bigm| & 0\\\\\n\\end{pmatrix}\n\\tag{5}\\]\nSolving:\n\\[\n\\begin{align}\n&\\begin{pmatrix}1 & 1 & -1 &\\bigm| & 0\\\\2 & -1 & 2 &\\bigm| & 0\\\\\\end{pmatrix}\n\\overset{l_2'=l_2-2l_1}{\\longrightarrow}\n\\begin{pmatrix}1 & 1 & -1 &\\bigm| & 0\\\\0 & -3 & 4 &\\bigm| & 0\\end{pmatrix}\n\\overset{l_2'=-1/3l_2}{\\longrightarrow}\n\\begin{pmatrix}1 & 1 & -1 &\\bigm| & 0\\\\0 & 1 & -4/3 &\\bigm| & 0\\end{pmatrix}\\\\\n\\overset{l_1'=l_1-l_2}{\\longrightarrow}\n&\\begin{pmatrix}1 & 0 & 1/3 &\\bigm| & 0\\\\0 & 1 & -4/3 &\\bigm| & 0\\end{pmatrix}\n\\end{align}\n\\]\nOnce again, this notation means:\n\\[\n\\begin{pmatrix}1 & 0 & 1/3 &\\bigm| & 0\\\\0 & 1 & -4/3 &\\bigm| & 0\\end{pmatrix}\n\\leftrightsquigarrow\n\\begin{pmatrix}1 & 0 & 1/3\\\\0 & 1 & -4/3 \\end{pmatrix}\n\\begin{pmatrix}x\\\\y\\\\z\\end{pmatrix}\n=\n\\begin{pmatrix}0\\\\0\\end{pmatrix}\n\\tag{6}\\]\nConverting back to the system notation have: \\[\n\\begin{cases}\nx+1/3z =0\\\\\ny-4/3z=0\n\\end{cases}\n\\]\nSubstituting the \\(z\\) of the second equation into the \\(z\\) of the first we find:\n\\[\n\\begin{cases}\nx+1/3z =0\\\\\ny-4/3z=0\n\\end{cases}\n\\iff\n\\begin{cases}\nx=-1/3z\\\\\ny=4/3z\n\\end{cases}\n\\tag{7}\\]\nAsking what \\(x,y,z\\in \\mathbb{R}\\) that satisfy the equation Equation 5 is equivalent to ask, what is \\(x,y,z\\in \\mathbb{R}\\) that satisfy Equation 7 . Each real \\(z\\) we choose gives us the corresponding \\(x\\) and \\(y\\); as a consequence we have many solution. In other words, the solution is\n\\[\n\\begin{pmatrix}\nx\\\\\ny\\\\\nz\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-1/3z\\\\\n4/3z\\\\\nz\n\\end{pmatrix}\n\\tag{8}\\]\n\n\nExample 4: Another system with many solutions\nWhat is the solution \\((x,y, z, w)\\) for the following system:\n\\[\n\\begin{cases}\n&x&+ &2 y&+&2 z &+&2w &= 1\\\\\n&2x&+&4y&+&6z&+&8w &= 2\\\\\n&3x&+&6y&+&8z&+&10w &=3\n\\end{cases}\n\\]\nUsing elimination algorithm we make linear combinations of the equation with the goal of eliminating variables, here is one way to go\n\\[\n\\begin{align}\n&\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm|1\\\\\n2 & 4 & 6 & 8 &\\bigm|2\\\\\n3 & 6 & 8 & 10 &\\bigm| 3\n\\end{pmatrix}\n\\overset{l_2' = l_2-2l_1}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| 1\\\\\n0 & 0 & 2 & 4 &\\bigm| 0\\\\\n3 & 6 & 8 & 10 &\\bigm| 3\n\\end{pmatrix}\n\\overset{l_3' = l_3-3l_1}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| 1\\\\\n0 & 0 & 2 & 4 &\\bigm| 0\\\\\n0 & 0 & 2 & 4 &\\bigm| 0\n\\end{pmatrix}\\\\\n&\\overset{l_3'=l_3-l_2}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| 1\\\\\n0 & 0 & 2 & 4 &\\bigm| 0\\\\\n0 & 0 & 0 & 0 &\\bigm| 0\n\\end{pmatrix}\n\\overset{l_2'=1/2l_2}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| 1\\\\\n0 & 0 & 1 & 2 &\\bigm| 0\\\\\n0 & 0 & 0 & 0 &\\bigm| 0\n\\end{pmatrix}\n\\overset{l_1'=l_1-2l_2}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 0 & -2 &\\bigm| 1\\\\\n0 & 0 & 1 & 2 &\\bigm| 0\\\\\n0 & 0 & 0 & 0 &\\bigm| 0\n\\end{pmatrix}\n\\end{align}\n\\tag{9}\\]\nWe simplified it as much as we can, going back to the system’s notation we have:\n\\[\n\\begin{cases}\nx + 2 y & &- &2w &=1\\\\\n&z &+ &2w &=0\n\\end{cases}\n\\]\nNow, promote \\(y\\) and \\(w\\) into parameters and express \\(x\\) and \\(z\\) in term of them (if why we take this step is not natural, it will be soon)\n\\[\n\\begin{cases}\nx + 2 y & &- &2w &=1\\\\\n&z &+ &2w &=0\n\\end{cases}\n\\longrightarrow\n\\begin{cases}\nx &=1-2y&-&2w\\\\\nz &=&-&2w\n\\end{cases}\n\\tag{10}\\]\nFor each values we assign to \\(y\\) and \\(w\\) we get one solution! There’s an infinite number of them."
  },
  {
    "objectID": "vector_spaces.html#matrix-multiplication-and-linear-combination-of-rows",
    "href": "vector_spaces.html#matrix-multiplication-and-linear-combination-of-rows",
    "title": "Vector spaces",
    "section": "Matrix multiplication and linear combination of rows",
    "text": "Matrix multiplication and linear combination of rows\nThe first step in Equation 9 is\n\\[\n\\begin{pmatrix}1 & 2 & 2 & 2\\\\2 & 4 & 6 & 8\\\\3 & 6 & 8 & 10 \\end{pmatrix}\\overset{l_2' = l_2-2l_1}{\\longrightarrow}\\begin{pmatrix}1 & 2 & 2 & 2 \\\\0 & 0 & 2 & 4 \\\\3 & 6 & 8 & 10 \\end{pmatrix}\n\\]\nand\n\\[\n\\begin{pmatrix}\n1\\\\2\\\\3\n\\end{pmatrix}\n\\overset{l_2' = l_2-2l_1}{\\longrightarrow}\n\\begin{pmatrix}\n1\\\\0\\\\3\n\\end{pmatrix}\n\\]\nImplicit was the following matrix multiplication:\n\\[\n\\begin{pmatrix}1 & 0 & 0 \\\\-2 & 1 & 0 \\\\0 & 0 & 1 \\end{pmatrix}\\begin{pmatrix}1 & 2 & 2 & 2 \\\\2 & 4 & 6 & 8 \\\\3 & 6 & 8 & 10\\end{pmatrix} = \\begin{pmatrix}1 & 2 & 2 & 2 \\\\0 & 0 & 2 & 4 \\\\3 & 6 & 8 & 10 \\end{pmatrix}\n\\]\nand\n\\[\n\\begin{pmatrix}1 & 0 & 0 \\\\-2 & 1 & 0 \\\\0 & 0 & 1 \\end{pmatrix}\\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix}=\\begin{pmatrix}1\\\\0\\\\3\\end{pmatrix}\n\\]\nHow do we do these calculations?\nThe general rule is:\n\n\n\n\n\n\nFigure 1: Matrix multiplication of a 3x4 matrix by a 3x3 matrix.\n\n\n\nwhere, for example:\n\\[\n\\begin{align}\na_{22}'= e_{21}a_{12} + e_{22}a_{22} +e_{23}a_{32}\\\\\na_{23}'= e_{21}a_{13} + e_{22}a_{23} +e_{23}a_{33}\\\\\n\\end{align}\n\\tag{11}\\]\nLets name the matrices involved, the \\(e\\)’s matrix is called \\(E_1\\) and the \\(a\\)’s matrix is called \\(A\\), the result of the multiplication is called \\(A'\\).\nObserving Figure 1 and Equation 11 we see two important aspects about \\(E_1 A\\)\n\nThe shape of \\(E_1\\) is \\(3\\times3\\),the shape of \\(A\\) is \\(3\\times 4\\) and the shape of \\(A'\\) is \\(3\\times4\\).\nThe matrix \\(E_1\\) needs to have as many columns as there are rows in \\(A\\) (in this case \\(3\\)), otherwise it is not possible to do matrix multiplication. In other words, the compatible shapes for matrix multiplication are of the form \\([m\\times r][r\\times n] = [m\\times n]\\), for any integers \\(m,n,r\\). For example, the case above is \\([3\\times 3][3\\times 4]\\) and thus \\(m=3\\), \\(r=3\\) and \\(n=4\\).\nThe general formula for the entries of \\(A'\\) is the complicated formula:\n\\[\na_{ij}'=\\sum_{k=1}^re_{ik}a_{kj}\n\\]"
  },
  {
    "objectID": "vector_spaces.html#inverse-matrix",
    "href": "vector_spaces.html#inverse-matrix",
    "title": "Vector spaces",
    "section": "Inverse Matrix",
    "text": "Inverse Matrix\nOnly square matrices can have inverses.\nNot every square matrix will have an inverse. For example \\(E_1\\) is square, thus it may or not have an inverse \\(E_1^{-1}\\), on the other hand the matrix \\(A\\) is rectangular, thus no inverse exists.\nHow do we compute an inverse of a matrix?\nThe \\(E\\)’s matrices have inverses, always and thus:\n\\[\nAx=b \\iff EAx=Eb \\iff A'x=b'\n\\]\nare equivalent and and as a result their truth sets are equal! Finding the truth set of the later is easier than the former."
  },
  {
    "objectID": "vector_spaces.html#two-special-subspaces",
    "href": "vector_spaces.html#two-special-subspaces",
    "title": "Vector spaces",
    "section": "Two special subspaces",
    "text": "Two special subspaces\nHere we focus on the column space and nullspace of a matrix.\nThe column space is the vector space that is generated by taking all linear combinations of the columns of a matrix. For example, the column space of the matrix\n\\[\nA=\\begin{pmatrix}1 & 2 & 2 & 2 \\\\2 & 4 & 6 & 8 \\\\3 & 6 & 8 & 10 \\end{pmatrix}\n\\tag{12}\\]\nfrom example 4 called \\(C(A)\\) and is generated by\n\\[\na\\begin{pmatrix}\n1\\\\2\\\\3\n\\end{pmatrix}\n+\nb\\begin{pmatrix}2\\\\4\\\\6\\end{pmatrix}\n+\nc\\begin{pmatrix}2\\\\6\\\\8\\end{pmatrix}\n+\nd\\begin{pmatrix}2\\\\8\\\\10\\end{pmatrix}\n\\]\nIt may happen, and it is the case as we shall see, that we need not all the columns of \\(A\\) to generate \\(C(A)\\), the reason being, some of the column vectors may depend on other columns, thus not providing no additional information. At any rate, \\(C(A)\\) always comes as a result of combining all the columns, it is just that some columns are redundant and we can generate the same with less.\nThe nullspace of a matrix, take again Equation 12, is the vector space generated by all solutions \\(\\mathbf{x}_N\\) of the equation:\n\\[\nA\\mathbf{x}_N=\\mathbf{0}\n\\]\nThe concept of column space and nullspace will be key from now on."
  },
  {
    "objectID": "vector_spaces.html#systems-of-equations-from-a-new-view",
    "href": "vector_spaces.html#systems-of-equations-from-a-new-view",
    "title": "Vector spaces",
    "section": "Systems of equations from a new view",
    "text": "Systems of equations from a new view\nThe system of equations used in example 4 is:\n\\[\nA\\mathbf{x}=\\mathbf{b}\n\\leftrightsquigarrow\n\\begin{pmatrix}1 & 2 & 2 & 2 \\\\2 & 4 & 6 & 8 \\\\3 & 6 & 8 & 10 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\\\z\\\\w\\end{pmatrix}=\\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix}\n\\leftrightsquigarrow\n\\begin{pmatrix}1 & 2 & 2 & 2 &\\bigm|1\\\\2 & 4 & 6 & 8 &\\bigm|2\\\\3 & 6 & 8 & 10 &\\bigm| 3\\end{pmatrix}\n\\tag{13}\\]\nWe can rewrite the system of equation in, yet, another manner:\n\\[\n\\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix}x+\\begin{pmatrix}2\\\\4\\\\6\\end{pmatrix}y+\\begin{pmatrix}2\\\\6\\\\8\\end{pmatrix}z+\\begin{pmatrix}2\\\\8\\\\10\\end{pmatrix}w = \\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix}\n\\tag{14}\\]\nIn other words, a matrix times a column vector is a linear combination of the columns of the matrix. Under the Equation 13 we would ask, what is the column vector \\(\\mathbf{x}\\) which when multiplied by the matrix \\(A\\) yields the column vector \\(\\mathbf{b}\\).\nUnder the Equation 14 we would ask: what is the linear combination of the columns that leads to the vector \\(\\mathbf{b}\\) on the rhs. But now note: this combination is only possible, provided the \\(\\mathbf{b}\\) vector is in the column space!\nThis crucial observation can be paraphrased differently; going back to Equation 13, focus on the extended matrix version of the system of the equations and note, that, when we extend the matrix, i.e., when we append the \\(\\mathbf{b}\\) vector on the rhs, the column space must not change, for otherwise the \\(\\mathbf{b}\\) was not in the column space.\nMoreover, one more way, to paraphrase the paragraph above, the \\(\\mathbf{b}\\) vector must be dependent on the columns of \\(A\\)."
  },
  {
    "objectID": "vector_spaces.html#putting-all-together",
    "href": "vector_spaces.html#putting-all-together",
    "title": "Vector spaces",
    "section": "Putting all together",
    "text": "Putting all together\nBecause the \\(E_k\\) matrices are invertible we have the following equivalence:\n\\(A\\mathbf{x}=\\mathbf{b} \\iff A'\\mathbf{x}=\\mathbf{b}'\\)\nAnd thus the solutions of both equations are the same, in particular, following example 4 we have (using the extended-matrix notation) \\[\n\\begin{align}\n&\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm|1\\\\\n2 & 4 & 6 & 8 &\\bigm|2\\\\\n3 & 6 & 8 & 10 &\\bigm| 3\n\\end{pmatrix}\n\\iff\n\\begin{pmatrix}\n1 & 2 & 0 & -2 &\\bigm| 1\\\\\n0 & 0 & 1 & 2 &\\bigm| 0\\\\\n0 & 0 & 0 & 0 &\\bigm| 0\n\\end{pmatrix}\n\\end{align}\n\\]\nWe can rewrite both sides of the equivalence as the follows:\n\\[\n\\begin{align}\n&\\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix}x+\\begin{pmatrix}2\\\\4\\\\6\\end{pmatrix}y+\\begin{pmatrix}2\\\\6\\\\8\\end{pmatrix}z+\\begin{pmatrix}2\\\\8\\\\10\\end{pmatrix}w = \\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix}\\\\\n&\\iff\n\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}x+\\begin{pmatrix}2\\\\0\\\\0\\end{pmatrix}y+\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}z+\\begin{pmatrix}-2\\\\2\\\\0\\end{pmatrix}w = \\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}\n\\end{align}\n\\tag{15}\\]\nWhat this says, is that the coefficients that combine the columns of \\(A\\) to give us the \\(\\mathbf{b}\\) is exactly the same as the coefficients that, when combine the columns of \\(A'\\) give us the vector \\(\\mathbf{b}'\\).\nWe already know how to find the solution for this problem, see Section, what we want to do now to find the solution using the matrix-vector notation. The way you do it in this notation, provides you with deeper insight about when there is one, none or many solutions.\nHow would we find \\(x,y,z,w\\) that satisfy the equations above?\n\nSimplify the system as much as you can, we already did this. The lhs of Equation 15 is complicated, but the rhs (full of zeros and many ones) is simple.\nIdentify the dependent and independent columns of the simple matrix. We do this by visual inspection:\nThe first and third columns can be linearly combined to generate the second and fourth columns:\n\\[\n\\text{col}_2 =2\\text{col}_1\\qquad \\text{col}_4=2\\text{col}_3-2\\text{col}_1\n\\]\nNotice the independent columns have an entry \\(1\\) and the remaining entries are zeros, the independent columns are also called the pivot columns. The dependent columns are called free columns.\nThe \\(y\\) and \\(w\\) unknowns are the free unknowns.\nTo find a particular solution of the system, set the free unknowns to zero:\n\\[\ny=0\\qquad w=0\n\\]\n\\[\n\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}x\n+\\begin{pmatrix}2\\\\0\\\\0\\end{pmatrix}0\n+\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}z\n+\\begin{pmatrix}-2\\\\2\\\\0\\end{pmatrix}0 = \\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}\n\\]\nNow find \\(x\\) and \\(z\\), by inspection:\n\\[\nx=1 \\qquad z=0\n\\]\nThe particular solution is\n\\[ \\begin{pmatrix} 1\\\\0\\\\0\\\\0 \\end{pmatrix} \\]\nTo find the general solution, compute the nullspace by solving \\(A'\\mathbf{x}_N=\\mathbf{0}\\) and add it to the particular solution\n\\[\n\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}x_N+\\begin{pmatrix}2\\\\0\\\\0\\end{pmatrix}y_N\n+\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}z_N+\\begin{pmatrix}-2\\\\2\\\\0\\end{pmatrix}w_N = \\begin{pmatrix}0\\\\0\\\\0\\end{pmatrix}\n\\tag{16}\\]\nReminding ourselves which columns are independent and which ones are dependent tells us how to find the solution. The column two and four can be built out from approriate linear combinations of column one and three.\nTo solve the equation we choose the free variables \\(y_N\\) and \\(w_N\\) as we wish and then solve for the \\(x_N\\) and \\(z_N\\). When choosing freely, at least choose something that simplify you calculations, for example set \\(y_N=1\\) and \\(w_N=0\\) in Equation 16 and guess what is \\(x_N\\) and \\(z;_N\\) we get:\n\\[\n\\mathbf{x}_N =\\begin{pmatrix}-2\\\\1\\\\0\\\\0 \\end{pmatrix}\n\\]\nNow substitute \\(y_N=0\\) and \\(w_N=1\\) into Equation 16 and guess the corresponding \\(x_N\\) and \\(z_N\\); the answer gives us:\n\\[\n\\mathbf{x}_N=\\begin{pmatrix}2\\\\0\\\\-2\\\\1\\end{pmatrix}\n\\]\nThe nullspace of the matrix \\(A\\) is composed by all linear combinations:\n\\[\na\\begin{pmatrix}-2\\\\1\\\\0\\\\0 \\end{pmatrix} +b\\begin{pmatrix}2\\\\0\\\\-2\\\\1 \\end{pmatrix}\n\\]\nSo far we know a particular solution for the problem \\(A\\mathbf{x}_p=\\mathbf{b}\\) and we know the nullspace of \\(A\\), the key observations is this:\nIf \\(\\mathbf{x}_P\\) is a particular solution of \\(A\\mathbf{x}=\\mathbf{b}\\) then adding to it any \\(\\mathbf{x}_N\\) will not make a difference.\nJustification: \\(A(\\mathbf{x}_P+\\mathbf{x}_N)=A\\mathbf{x}_P+A\\mathbf{x}_N=\\mathbf{b}+\\mathbf{0}=\\mathbf{b}\\)\nThus, the system \\(A\\mathbf{x} =\\mathbf{b}\\) has an infinite number of solutions, all of the form \\(\\mathbf{x}=\\mathbf{x}_P+\\mathbf{x}_N\\).\nFor the matrix in question, the solutions are these:\n\\[\n\\mathbf{x}=\\begin{pmatrix}1\\\\0\\\\0\\\\0\\end{pmatrix}+a\\begin{pmatrix}-2\\\\1\\\\0\\\\0 \\end{pmatrix} +b\\begin{pmatrix}2\\\\0\\\\-2\\\\1 \\end{pmatrix}\n\\tag{17}\\]\n\nIts important to compare the solutions Equation 10 and Equation 17.\nRearranging Equation 10 we have:\n\\[\n\\begin{cases}x &=1-2y&-&2w\\\\z &=&-&2w\\end{cases}\n\\iff\n\\begin{cases}\nx &= &1&+& &-&2a&+& &-&2b\\\\\ny &= & &+& &+&a&+& &+& \\\\\nz &= & &+& &+& &+& &-&2b\\\\\nw &= & &+& &+& &+& &+&b\n\\end{cases}\n\\]\nThe rhs of this equivalence is exactly what Equation 17 means!"
  },
  {
    "objectID": "vector_spaces.html#one-none-or-many-solutions",
    "href": "vector_spaces.html#one-none-or-many-solutions",
    "title": "Vector spaces",
    "section": "One, none or many solutions?",
    "text": "One, none or many solutions?\nWith the above discussion understood we have the following cases:\n\nCase: One solution\nWhen the matrix is square and when its rref is the identity, then there is only one solution!\nExample:\n\nThe solution of this example is \\(\\mathbf{x} =(3,-1,1)^T\\). The nullspace of the matrix is just the zero vector.\n\n\nCase: One or none solution\nWhen the matrix is rectangular \\(r=n&lt;m\\) and its rref is like:\n\nthen, there is no solution! The \\(\\mathbf{b}=(5,-6,-8)^T\\) does not belong to the column space.\nIf the system is instead like\n\nthen, there is one solution, \\(\\mathbf{x}=(5,-6)^T\\). By making the last entry of \\(\\mathbf{b}\\) zero, now it belongs to the columns space, \\(5\\) times the first column minus \\(6\\) times the second column gives us this \\(\\mathbf{b}\\).\n\n\nCase: Infinite solutions\nConsider a matrix that looks like this, identity columns and free column separated\n\nor like this, with the identity and free blocks mixed:\n\nIn this situation there is an infinite number of solutions. The first system has solutions of the form\n\\[\n\\begin{pmatrix}\nx\\\\\ny\\\\\nz\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-1/3z_0\\\\\n4/3z_0\\\\\nz_0\n\\end{pmatrix}\n\\]\nwhile the second has them of the form\n\\[\n\\begin{pmatrix}\nx\\\\\ny\\\\\nz\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-1/3y_0\\\\\ny_0\\\\\n4/3y_0\n\\end{pmatrix}\n\\]\nThese solutions also constitute the nullspaces.\n\n\nCase zero or infinite solutions\nLets look again Equation 6, we identify three unknown and two equations, if we imagine we have one more equation provided by ourselves the system would have a solution, assume what equation is just \\(z=z_0\\) for some value of \\(z_0\\) of our liking then the system would have been\n\\[\n\\begin{pmatrix}1 & 0 & 1/3\\\\0 & 1 & -4/3\\\\0 & 0 & 1\\end{pmatrix}\\begin{pmatrix}x\\\\y\\\\z\\end{pmatrix}=\\begin{pmatrix}0\\\\0\\\\z_0\\end{pmatrix}\n\\]\nThe solution is:\n\\[\n\\begin{pmatrix}\nx\\\\\ny\\\\\nz\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-1/3z_0\\\\\n4/3z_0\\\\\nz_0\n\\end{pmatrix}\n\\]\nJust like we computed above. I’m showing this example for you to see that we have to have as many independent equations as unknowns, only then we have a unique solution. The key take away is that a parameter dependent solution like Equation 8 is equivalent to adding an equation ourselves."
  },
  {
    "objectID": "vector_spaces.html#subspaces",
    "href": "vector_spaces.html#subspaces",
    "title": "Vector spaces",
    "section": "Subspaces",
    "text": "Subspaces"
  },
  {
    "objectID": "vector_spaces.html#dot-product",
    "href": "vector_spaces.html#dot-product",
    "title": "Vector spaces",
    "section": "Dot product",
    "text": "Dot product\nFalar novamente dos subespaços do secundário, que já mensionei acima na sec dos subespaços, mas desta vez dar usar a equação com produto interno tal como fazes nos teus resumos de GA do 11o ano."
  },
  {
    "objectID": "vector_spaces.html#transpose-of-a-matrix",
    "href": "vector_spaces.html#transpose-of-a-matrix",
    "title": "Vector spaces",
    "section": "Transpose of a matrix",
    "text": "Transpose of a matrix"
  },
  {
    "objectID": "vector_spaces.html#rank-of-a-matrix",
    "href": "vector_spaces.html#rank-of-a-matrix",
    "title": "Vector spaces",
    "section": "Rank of a matrix",
    "text": "Rank of a matrix"
  },
  {
    "objectID": "vector_spaces.html#basis",
    "href": "vector_spaces.html#basis",
    "title": "Vector spaces",
    "section": "Basis?",
    "text": "Basis?"
  },
  {
    "objectID": "vector_spaces.html#matrices-as-linear-operators",
    "href": "vector_spaces.html#matrices-as-linear-operators",
    "title": "Vector spaces",
    "section": "Matrices as linear Operators?",
    "text": "Matrices as linear Operators?"
  },
  {
    "objectID": "vector_spaces.html#four-subspaces-of-a-matrix",
    "href": "vector_spaces.html#four-subspaces-of-a-matrix",
    "title": "Vector spaces",
    "section": "Four subspaces of a matrix",
    "text": "Four subspaces of a matrix\n\\[\n\\begin{align}\n&\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| &b_1\\\\\n2 & 4 & 6 & 8 &\\bigm| &b_2\\\\\n3 & 6 & 8 & 10 &\\bigm| &b_3\n\\end{pmatrix}\n\\overset{l_2' = l_2-2l_1}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| &b_1\\\\\n0 & 0 & 2 & 4 &\\bigm| &b_2-2b_1\\\\\n3 & 6 & 8 & 10 &\\bigm| &b_3\n\\end{pmatrix}\\\\\n&\\overset{l_3' = l_3-3l_1}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| &b_1\\\\\n0 & 0 & 2 & 4 &\\bigm| &b_2-2b_1\\\\\n0 & 0 & 2 & 4 &\\bigm| &b_3-3b_1\n\\end{pmatrix}\n\\overset{l_3'=l_3-l_2}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| &b_1\\\\\n0 & 0 & 2 & 4 &\\bigm| &b_2-2b_1\\\\\n0 & 0 & 0 & 0 &\\bigm| &b_3-b_2-b_1\n\\end{pmatrix}\\\\\n&\\overset{l_2'=1/2l_2}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 2 & 2 &\\bigm| &b_1\\\\\n0 & 0 & 1 & 2 &\\bigm| &b_2/2-b_1\\\\\n0 & 0 & 0 & 0 &\\bigm| &b_3-b_2-b_1\n\\end{pmatrix}\n\\overset{l_1'=l_1-2l_2}{\\longrightarrow}\n\\begin{pmatrix}\n1 & 2 & 0 & -2 &\\bigm| &3b_1-b_2\\\\\n0 & 0 & 1 & 2 &\\bigm| &b_2/2-b_1\\\\\n0 & 0 & 0 & 0 &\\bigm| &b_3-b_2-b_1\n\\end{pmatrix}\n\\end{align}\n\\]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Brief Notes",
    "section": "",
    "text": "Getting the feet wet with Polynomials\nAngular momentum in quantum mechanics\nEinstein Model of Solid"
  },
  {
    "objectID": "index.html#teh-notes",
    "href": "index.html#teh-notes",
    "title": "Brief Notes",
    "section": "",
    "text": "Getting the feet wet with Polynomials\nAngular momentum in quantum mechanics\nEinstein Model of Solid"
  },
  {
    "objectID": "index.html#notes",
    "href": "index.html#notes",
    "title": "Brief Notes",
    "section": "",
    "text": "Getting the feet wet with Polynomials\nAngular momentum in quantum mechanics\nEinstein Model of Solid"
  }
]