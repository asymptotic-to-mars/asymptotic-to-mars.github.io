{"title":"Linear dependence and independence","markdown":{"yaml":{"title":"Linear dependence and independence","execute":{"echo":false}},"headingText":"Video","containsRefs":false,"markdown":"\n\n::: {.callout-tip appearance=\"simple\"}\n\n[Testing linear independence](https://youtu.be/Lu1zby_EykI){target=\"_blank\"}\n:::\n\nFirst lets see what is linear (in)dependence, then how to check it. As we will see throughout the course there are many ways of doing it.\n\nLinear (in)dependence is seen best with examples.\n\n-   **Example 1:** Just consider two parallel vectors\n\n    ![](figs/two_parallel_vectors.png){width=\"169\"}\n\n    One is just twice the other (i.e. proportional). They are dependent because you can create one from the other:\n\n    $$ \\begin{pmatrix} 2\\\\4\\end{pmatrix} = 2 \\begin{pmatrix} 1\\\\2\\end{pmatrix} \\qquad\\text{or}\\qquad \\begin{pmatrix} 1\\\\2\\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 2\\\\4\\end{pmatrix} $$\n\n    ::: {.callout-caution appearance=\"simple\"}\n    ## Exercise\n\n    Are the following vectors dependent or independent?\n\n    -   $\\begin{pmatrix} 1\\\\3\\end{pmatrix}$ and $\\begin{pmatrix} -1\\\\-3\\end{pmatrix}$\n\n    -   $\\begin{pmatrix} 2\\\\3\\end{pmatrix}$ and $\\begin{pmatrix} 1\\\\3\\end{pmatrix}$\n    :::\n\n-   **Example 2:** More difficult. Consider again the vectors $\\begin{pmatrix} 1\\\\2\\end{pmatrix}$, $\\begin{pmatrix} 1\\\\0\\end{pmatrix}$ and $\\begin{pmatrix} 0\\\\1\\end{pmatrix}$. If you discard the first vector, nothing is lost because through the linear combination of the second and third vectors we can recover it. Here is how:\n\n    $$ \\begin{pmatrix} 1\\\\0\\end{pmatrix}+2\\begin{pmatrix} 0\\\\1\\end{pmatrix} = \\begin{pmatrix} 1\\\\2\\end{pmatrix} $$\n\n    In other words the first vector depends on the second and third vector through the linear combination above, it can be fully build from these pieces and thus it does not bring anything new.\n\n    ![](figs/three_vectors_R2.png){width=\"235\"}\n\n    What may not be so obvious is the fact that any vector from the three could be discarded, for example drop the second one! No problem the first and third vectors can construct it:\n\n    $$ \\begin{pmatrix} 1\\\\2\\end{pmatrix}-2\\begin{pmatrix} 0\\\\1\\end{pmatrix} = \\begin{pmatrix} 1\\\\0\\end{pmatrix} $$\n\nFrom these examples we conclude that the following statements are equivalent:\n\n1.  *Vectors from a list are linearly independent if you cannot construct any one of them from the remaining ones. (*they point in distinct directions):\n2.  *Vectors from a list are linearly independent when there exist scalars that combine them, yielding the zero vector. Otherwise they are dependent;*\n\nThere two statements in English translates to Mathematical language:\n\n::: {#def-dep_or_indep style=\"color:gray\"}\n\\[$\\mathbf{u},\\mathbf{v},\\mathbf{w}$ are independent\\] := \\[the only solution of $\\alpha \\mathbf{u} +\\beta \\mathbf{v} + \\gamma \\mathbf{w} = \\mathbf{0}$ is $\\alpha=\\beta=\\gamma=0$\\]\n\n\\[$\\mathbf{u},\\mathbf{v},\\mathbf{w}$ are dependent\\] := \\[there are solutions of $\\alpha \\mathbf{u} +\\beta \\mathbf{v} + \\gamma \\mathbf{w} = \\mathbf{0}$ where $\\alpha,\\beta,\\gamma$ need not be $0$\\]\n\n\\[$\\mathbf{w}$ is dependent on $\\mathbf{u}$ and $\\mathbf{v}$\\] := \\[$\\mathbf{w} = \\alpha \\mathbf{u} + \\beta\\mathbf{w}$ for some $\\alpha,\\beta$\\] = \\[$\\mathbf{w} \\in \\text{span}\\{\\mathbf{u},\\mathbf{v}\\}$\\]\n:::\n\n::: callout-note\n## Commentaries\n\n-   What the dependence definition is really telling us, is that there is at least one vector that can be written in terms of the others. And that \"writing in terms of the others\" is accomplished just by rearranging $\\alpha \\mathbf{u} +\\beta \\mathbf{v} + \\gamma \\mathbf{w} = \\mathbf{0}$.\n\n    For example, if $\\mathbf{u} +2\\mathbf{v} - \\mathbf{w} = \\mathbf{0}$, then isolating $\\mathbf{w}$ we obtain $\\mathbf{w}=\\mathbf{u} + 2\\mathbf{v}$, telling $\\mathbf{w}$ depends on $\\mathbf{u}$ and $\\mathbf{v}$; similarly isolating $\\mathbf{u}$ or $\\mathbf{v}$ tells us they are also dependent on the remaining. Geometrically the vectors lie in the same plane. (see example 2)\n\n-   In a later section we'll see the definition above is in fact solving $A\\bf{x}=\\bf{0}$. To check whether one vector is dependent or not on other vectors is a matter of whether $A\\mathbf{x}=\\mathbf{b}$ has or not a solution.\n:::\n\n## How to check whether a set of vectors is or not independent?\n\nConsider again example 2 above. We were given the vectors: $\\begin{pmatrix} 1\\\\2\\end{pmatrix}$ , $\\begin{pmatrix} 1\\\\0\\end{pmatrix}$ and $\\begin{pmatrix} 0\\\\1\\end{pmatrix}$ . Are they dependent or independent?\n\nIn other words, using the definition: what is the solution $\\alpha$, $\\beta$, $\\gamma$ of the equation:\n\n$$\n\\alpha \\begin{pmatrix} 1\\\\2\\end{pmatrix} +\\beta \\begin{pmatrix} 1\\\\0\\end{pmatrix}+\\gamma\\begin{pmatrix} 0\\\\1\\end{pmatrix}=\\begin{pmatrix} 0\\\\0\\end{pmatrix}\n$$\n\nWell, rewriting it, we obtain a system of equations to solve:\n\n$$\n\\begin{cases}\n\\alpha + \\beta =0\\\\\n2\\alpha +\\gamma =0\n\\end{cases}\n\\implies\n\\begin{cases}\n\\alpha = -\\beta\\\\\n\\alpha =-\\gamma/2\n\\end{cases}\n$$\n\nObserving the systems of equations we note that there are solutions for this system, for example $\\gamma=2$, $\\beta= -1$ and $\\alpha=1$. This result shows there are non-zero numbers $\\alpha$, $\\beta$ and $\\gamma$ that combine the three vectors to give the zero vector, that is to say at least one of the three vectors can be constructed from the remaining.\n\n::: callout-note\n## Commentary\n\nLater we will introduce the concept of *nullspace* of a matrix and we will learn about a more systematic way of checking the (in)dependence of a list of vectors called *the elimination method*. In this section what is important is the concept.\n:::\n\n## How to check whether a vector depends on two other vectors?\n\nIs $\\begin{pmatrix} 1\\\\2\\end{pmatrix}$ dependent on $\\begin{pmatrix} 1\\\\0\\end{pmatrix}$ and $\\begin{pmatrix} 0\\\\1\\end{pmatrix}$ ? In other words, is it true or false that $\\begin{pmatrix} 1\\\\2\\end{pmatrix} \\in \\text{span} \\{\\begin{pmatrix} 1\\\\0\\end{pmatrix},\\begin{pmatrix} 0\\\\1\\end{pmatrix}\\}$\n\nUsing the definition above we seek for solutions of\n\n$$ \\begin{pmatrix} 1\\\\2\\end{pmatrix}=\\beta\\begin{pmatrix} 1\\\\0\\end{pmatrix}+\\gamma\\begin{pmatrix} 0\\\\1\\end{pmatrix} $$\n\n(we set $\\alpha=-1$)\n\nThe solutions are obtained by solving the system:\n\n$$ \\begin{cases} 1=\\beta\\\\ 2=\\gamma \\end{cases} $$\n\nwhich is immediate! So yes, the vector is dependent.\n\n::: {.callout-caution appearance=\"simple\"}\n## Exercise\n\nFind the numbers $\\alpha_1$, $\\beta_1$, $\\alpha_2$, $\\beta_2$, $\\alpha_3$, $\\beta_3$ that satisfy:\n\n-   $\\begin{pmatrix} 1\\\\2\\end{pmatrix} =\\alpha_1 \\begin{pmatrix} 1\\\\0\\end{pmatrix}+\\beta_2\\begin{pmatrix} 0\\\\1\\end{pmatrix}$\n\n-   $\\begin{pmatrix} 1\\\\0\\end{pmatrix}= \\alpha_1 \\begin{pmatrix} 1\\\\2\\end{pmatrix}+\\beta_2\\begin{pmatrix} 0\\\\1\\end{pmatrix}$\n\n-   $\\begin{pmatrix} 0\\\\1\\end{pmatrix}= \\alpha_3 \\begin{pmatrix} 1\\\\2\\end{pmatrix}+\\beta_3\\begin{pmatrix} 1\\\\0\\end{pmatrix}$\n\nThus confirming that any one of the three vectors can be built from the others. The vectors are thus dependent.\n:::\n\n::: {.callout-caution appearance=\"simple\"}\n## Exercise\n\nCheck whether $\\begin{pmatrix} 1\\\\2\\end{pmatrix}$, $\\begin{pmatrix} 1\\\\0\\end{pmatrix}$ and $\\begin{pmatrix} 2\\\\4\\end{pmatrix}$ are dependent and independent.\n\nTry to express each in terms of the remaining.\n\nDraw a picture of the three vectors. What can you conclude?\n:::\n\n::: callout-note\n## Commentary\n\nWhen the number of vectors in $\\mathbb{R}^n$ being spanned is larger than the dimension $n$ of the space, then some are dependent vectors. In the example above, $n=2$ and there are $3$ vectors being spanned.\n:::\n","srcMarkdownNoYaml":"\n\n::: {.callout-tip appearance=\"simple\"}\n## Video\n\n[Testing linear independence](https://youtu.be/Lu1zby_EykI){target=\"_blank\"}\n:::\n\nFirst lets see what is linear (in)dependence, then how to check it. As we will see throughout the course there are many ways of doing it.\n\nLinear (in)dependence is seen best with examples.\n\n-   **Example 1:** Just consider two parallel vectors\n\n    ![](figs/two_parallel_vectors.png){width=\"169\"}\n\n    One is just twice the other (i.e. proportional). They are dependent because you can create one from the other:\n\n    $$ \\begin{pmatrix} 2\\\\4\\end{pmatrix} = 2 \\begin{pmatrix} 1\\\\2\\end{pmatrix} \\qquad\\text{or}\\qquad \\begin{pmatrix} 1\\\\2\\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 2\\\\4\\end{pmatrix} $$\n\n    ::: {.callout-caution appearance=\"simple\"}\n    ## Exercise\n\n    Are the following vectors dependent or independent?\n\n    -   $\\begin{pmatrix} 1\\\\3\\end{pmatrix}$ and $\\begin{pmatrix} -1\\\\-3\\end{pmatrix}$\n\n    -   $\\begin{pmatrix} 2\\\\3\\end{pmatrix}$ and $\\begin{pmatrix} 1\\\\3\\end{pmatrix}$\n    :::\n\n-   **Example 2:** More difficult. Consider again the vectors $\\begin{pmatrix} 1\\\\2\\end{pmatrix}$, $\\begin{pmatrix} 1\\\\0\\end{pmatrix}$ and $\\begin{pmatrix} 0\\\\1\\end{pmatrix}$. If you discard the first vector, nothing is lost because through the linear combination of the second and third vectors we can recover it. Here is how:\n\n    $$ \\begin{pmatrix} 1\\\\0\\end{pmatrix}+2\\begin{pmatrix} 0\\\\1\\end{pmatrix} = \\begin{pmatrix} 1\\\\2\\end{pmatrix} $$\n\n    In other words the first vector depends on the second and third vector through the linear combination above, it can be fully build from these pieces and thus it does not bring anything new.\n\n    ![](figs/three_vectors_R2.png){width=\"235\"}\n\n    What may not be so obvious is the fact that any vector from the three could be discarded, for example drop the second one! No problem the first and third vectors can construct it:\n\n    $$ \\begin{pmatrix} 1\\\\2\\end{pmatrix}-2\\begin{pmatrix} 0\\\\1\\end{pmatrix} = \\begin{pmatrix} 1\\\\0\\end{pmatrix} $$\n\nFrom these examples we conclude that the following statements are equivalent:\n\n1.  *Vectors from a list are linearly independent if you cannot construct any one of them from the remaining ones. (*they point in distinct directions):\n2.  *Vectors from a list are linearly independent when there exist scalars that combine them, yielding the zero vector. Otherwise they are dependent;*\n\nThere two statements in English translates to Mathematical language:\n\n::: {#def-dep_or_indep style=\"color:gray\"}\n\\[$\\mathbf{u},\\mathbf{v},\\mathbf{w}$ are independent\\] := \\[the only solution of $\\alpha \\mathbf{u} +\\beta \\mathbf{v} + \\gamma \\mathbf{w} = \\mathbf{0}$ is $\\alpha=\\beta=\\gamma=0$\\]\n\n\\[$\\mathbf{u},\\mathbf{v},\\mathbf{w}$ are dependent\\] := \\[there are solutions of $\\alpha \\mathbf{u} +\\beta \\mathbf{v} + \\gamma \\mathbf{w} = \\mathbf{0}$ where $\\alpha,\\beta,\\gamma$ need not be $0$\\]\n\n\\[$\\mathbf{w}$ is dependent on $\\mathbf{u}$ and $\\mathbf{v}$\\] := \\[$\\mathbf{w} = \\alpha \\mathbf{u} + \\beta\\mathbf{w}$ for some $\\alpha,\\beta$\\] = \\[$\\mathbf{w} \\in \\text{span}\\{\\mathbf{u},\\mathbf{v}\\}$\\]\n:::\n\n::: callout-note\n## Commentaries\n\n-   What the dependence definition is really telling us, is that there is at least one vector that can be written in terms of the others. And that \"writing in terms of the others\" is accomplished just by rearranging $\\alpha \\mathbf{u} +\\beta \\mathbf{v} + \\gamma \\mathbf{w} = \\mathbf{0}$.\n\n    For example, if $\\mathbf{u} +2\\mathbf{v} - \\mathbf{w} = \\mathbf{0}$, then isolating $\\mathbf{w}$ we obtain $\\mathbf{w}=\\mathbf{u} + 2\\mathbf{v}$, telling $\\mathbf{w}$ depends on $\\mathbf{u}$ and $\\mathbf{v}$; similarly isolating $\\mathbf{u}$ or $\\mathbf{v}$ tells us they are also dependent on the remaining. Geometrically the vectors lie in the same plane. (see example 2)\n\n-   In a later section we'll see the definition above is in fact solving $A\\bf{x}=\\bf{0}$. To check whether one vector is dependent or not on other vectors is a matter of whether $A\\mathbf{x}=\\mathbf{b}$ has or not a solution.\n:::\n\n## How to check whether a set of vectors is or not independent?\n\nConsider again example 2 above. We were given the vectors: $\\begin{pmatrix} 1\\\\2\\end{pmatrix}$ , $\\begin{pmatrix} 1\\\\0\\end{pmatrix}$ and $\\begin{pmatrix} 0\\\\1\\end{pmatrix}$ . Are they dependent or independent?\n\nIn other words, using the definition: what is the solution $\\alpha$, $\\beta$, $\\gamma$ of the equation:\n\n$$\n\\alpha \\begin{pmatrix} 1\\\\2\\end{pmatrix} +\\beta \\begin{pmatrix} 1\\\\0\\end{pmatrix}+\\gamma\\begin{pmatrix} 0\\\\1\\end{pmatrix}=\\begin{pmatrix} 0\\\\0\\end{pmatrix}\n$$\n\nWell, rewriting it, we obtain a system of equations to solve:\n\n$$\n\\begin{cases}\n\\alpha + \\beta =0\\\\\n2\\alpha +\\gamma =0\n\\end{cases}\n\\implies\n\\begin{cases}\n\\alpha = -\\beta\\\\\n\\alpha =-\\gamma/2\n\\end{cases}\n$$\n\nObserving the systems of equations we note that there are solutions for this system, for example $\\gamma=2$, $\\beta= -1$ and $\\alpha=1$. This result shows there are non-zero numbers $\\alpha$, $\\beta$ and $\\gamma$ that combine the three vectors to give the zero vector, that is to say at least one of the three vectors can be constructed from the remaining.\n\n::: callout-note\n## Commentary\n\nLater we will introduce the concept of *nullspace* of a matrix and we will learn about a more systematic way of checking the (in)dependence of a list of vectors called *the elimination method*. In this section what is important is the concept.\n:::\n\n## How to check whether a vector depends on two other vectors?\n\nIs $\\begin{pmatrix} 1\\\\2\\end{pmatrix}$ dependent on $\\begin{pmatrix} 1\\\\0\\end{pmatrix}$ and $\\begin{pmatrix} 0\\\\1\\end{pmatrix}$ ? In other words, is it true or false that $\\begin{pmatrix} 1\\\\2\\end{pmatrix} \\in \\text{span} \\{\\begin{pmatrix} 1\\\\0\\end{pmatrix},\\begin{pmatrix} 0\\\\1\\end{pmatrix}\\}$\n\nUsing the definition above we seek for solutions of\n\n$$ \\begin{pmatrix} 1\\\\2\\end{pmatrix}=\\beta\\begin{pmatrix} 1\\\\0\\end{pmatrix}+\\gamma\\begin{pmatrix} 0\\\\1\\end{pmatrix} $$\n\n(we set $\\alpha=-1$)\n\nThe solutions are obtained by solving the system:\n\n$$ \\begin{cases} 1=\\beta\\\\ 2=\\gamma \\end{cases} $$\n\nwhich is immediate! So yes, the vector is dependent.\n\n::: {.callout-caution appearance=\"simple\"}\n## Exercise\n\nFind the numbers $\\alpha_1$, $\\beta_1$, $\\alpha_2$, $\\beta_2$, $\\alpha_3$, $\\beta_3$ that satisfy:\n\n-   $\\begin{pmatrix} 1\\\\2\\end{pmatrix} =\\alpha_1 \\begin{pmatrix} 1\\\\0\\end{pmatrix}+\\beta_2\\begin{pmatrix} 0\\\\1\\end{pmatrix}$\n\n-   $\\begin{pmatrix} 1\\\\0\\end{pmatrix}= \\alpha_1 \\begin{pmatrix} 1\\\\2\\end{pmatrix}+\\beta_2\\begin{pmatrix} 0\\\\1\\end{pmatrix}$\n\n-   $\\begin{pmatrix} 0\\\\1\\end{pmatrix}= \\alpha_3 \\begin{pmatrix} 1\\\\2\\end{pmatrix}+\\beta_3\\begin{pmatrix} 1\\\\0\\end{pmatrix}$\n\nThus confirming that any one of the three vectors can be built from the others. The vectors are thus dependent.\n:::\n\n::: {.callout-caution appearance=\"simple\"}\n## Exercise\n\nCheck whether $\\begin{pmatrix} 1\\\\2\\end{pmatrix}$, $\\begin{pmatrix} 1\\\\0\\end{pmatrix}$ and $\\begin{pmatrix} 2\\\\4\\end{pmatrix}$ are dependent and independent.\n\nTry to express each in terms of the remaining.\n\nDraw a picture of the three vectors. What can you conclude?\n:::\n\n::: callout-note\n## Commentary\n\nWhen the number of vectors in $\\mathbb{R}^n$ being spanned is larger than the dimension $n$ of the space, then some are dependent vectors. In the example above, $n=2$ and there are $3$ vectors being spanned.\n:::\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"linear_dependence_and_independence.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.32","editor":"visual","comments":{"hypothesis":true},"preview":{"browser":"chrome"},"theme":{"light":"minty"},"fontcolor":"rgb(190,190,190)","backgroundcolor":"rgb(32,31,30)","margin-left":"rgb(32,31,30)","navbar-color":"rgba(216, 6, 33 .65)","title":"Linear dependence and independence"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}