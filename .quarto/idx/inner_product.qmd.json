{"title":"Definition of inner product and its properties","markdown":{"yaml":{"title":"Definition of inner product and its properties","execute":{"echo":false}},"headingText":"Video","containsRefs":false,"markdown":"\n\n::: {.callout-tip appearance=\"simple\"}\n\n[inner product definition](https://youtu.be/Fua3wTeAjjs)\n:::\n\nThe inner product operation $\\cdot$ is an operation between vectors that yields a number, it is defined as:\n\n::: {#def-inner_product style=\"color: gray; border: 3px solid rgb(230, 230, 230);\"}\nLet $\\mathbf{u}$ and $\\mathbf{v}$ be two vectors from the same vector space $\\mathbb{R}^n$ (i.e. $\\mathbf{u}=(u_1,\\dots,u_n)$ and $\\mathbf{v}=(v_1,\\dots,v_n)$). The inner product of these two vectors is the real number of the form:\n\n$$\n[\\text{inner product between}\\,\\, \\mathbf{u}\\,\\, \\text{and} \\,\\,\\mathbf{v}]:=\\mathbf{u}\\cdot\\mathbf{v} = u_1v_1+\\dots+u_nv_n =: \\sum_{i=1}^nu_iv_i\n$$\n\nAnother notation for this number/operation is $\\mathbf{u}^\\intercal\\mathbf{v}$.\n\nThe inner product also results from the calculation $\\mathbf{u}\\cdot\\mathbf{v} = uv\\cos\\theta$.\n:::\n\n**Properties:**\n\n-   it obeys the property of linearity\n\n    $$ \\mathbf{u} \\cdot (\\mathbf{v} + \\lambda \\cdot \\mathbf{w}) = \\mathbf{u}\\cdot \\mathbf{v} + \\lambda \\mathbf{u}\\cdot \\mathbf{w} $$ where $\\lambda$ is any real number. In other words, the $\\cdot$ operation obeys a distributive rule and the \"we can pull out numbers\" rule.\n\n-   The order of the vectors being dotted is irrelevant $\\mathbf{u}\\cdot\\mathbf{v} = \\mathbf{v}\\cdot\\mathbf{u}$ and $\\mathbf{u}^\\intercal\\mathbf{v} = \\mathbf{v}^\\intercal \\mathbf{u}$.\n\n-   Since $-1\\leq\\cos\\theta\\leq 1$, then $-uv\\leq\\mathbf{u}\\cdot\\mathbf{v} \\leq uv$.\n\n-   (**Pythagoras theorem**) If two vectors are orthogonal, then $|\\mathbf{u}+\\mathbf{v}|^2=u^2+v^2$. \\[*Commentary:* Two orthogonal vectors can come about when decomposing a single vector $\\mathbf{w}\\in\\mathbb{R}^n$ in two complementary spaces $\\mathbb{R}^n=U\\oplus U^\\perp$.\\]\n\n-   (**Cauchy-Schwarz inequality** aka triangular inequality) For any vectors $\\mathbf{u}$ and $\\mathbf{v}$ we have the following: $|\\mathbf{u}+\\mathbf{v}|\\leq u+v$. Equality holding when the vectors are parallel and pointing in the same sense.\n\n**Exercises:** 1.6.3,5,6,7\n\n**Usefulness:** The inner product operation is a tool to gather geometrical information about vectors and to describe planes and hyperplanes. I.e. it allows answering the following questions:\n\n## What is the length of a vector?\n\n::: {.callout-tip appearance=\"simple\"}\n## Video\n\n[Magnitude of a vector](https://youtu.be/DOW-QTRLqiQ)\n:::\n\n*Answer:* Compute the inner product of a vector with itself.\n\n$$\n\\mathbf{u}^\\intercal\\mathbf{u} = u_1^2+\\dots+u_n^2\n$$\n\nWhose rhs we interpret, using the Pythagoras theorem, as the length square of $\\mathbf{u}$. The norm $|\\mathbf{u}|$ of the vector is the square-root of this value:\n\n$$\n|\\mathbf{u}|:=\\sqrt{u_1^2+\\dots+u_n^2}\n$$\n\n*Notation:* we can call the norm $|\\mathbf{u}|$ just by $u$.\n\n### Example\n\nThe length squared of the vector $\\mathbf{u}=(1,2)$ is:\n\n$$\n|\\mathbf{u}|^2 = \\mathbf{u}^\\intercal\\mathbf{u} = u_1^2+u_2^2 = 1^2+2^2=1+4=5\n$$\n\nThe norm of the vector is obtained by taking the square root of $5$:\n\n$$\nu=\\sqrt{5}\n$$\n\n![](figs/pythagoras.png){width=\"266\"}\n\nAnother way to compute the norm-squared of a vector is to consider $\\mathbb{R}^2=[\\textbf{x axis}]\\oplus[\\textbf{y axis}]$ and write $\\mathbf{u} = (0,1)+(2,0)$ and then use the Pythagoras theorem $|\\mathbf{w}+\\mathbf{v}|^2=w^2+v^2$ to compute:\n\n$$\n|\\mathbf{u}|^2=|(0,1)+(2,0)|^2=(0^2+1^2)^2+(2^2+0^2)^2=1+4=5\n$$\n\nThen take the square-root.\n\n**Exercises:** 1.6.1a\n\n## How to build a unit vector?\n\nStarting with the vector $\\mathbf{u}=(u_x,u_y)$ we can define a parallel unit vector by dividing it by it length:\n\n$$\n\\mathbf{e}_u = \\frac{\\mathbf{u}}{|\\mathbf{u}|}\n$$\n\n*Justification:* Compute its norm $|\\mathbf{e}_u|^2 = \\frac{\\mathbf{u}}{|\\mathbf{u}|}\\cdot \\frac{\\mathbf{u}}{|\\mathbf{u}|}=\\frac{|\\mathbf{u}|^2}{|\\mathbf{u}|^2}=1$\n\n### Example\n\nThe unit vector along $\\mathbf{u}=-(1,2)$ is:\n\n$$\n\\mathbf{e}_u = \\frac{-(1,2)}{\\sqrt{1+2^2}}=-\\frac{1}{\\sqrt{5}}(1,2)\n$$\n\nClearly: $|\\mathbf{e}_u|^2 = -\\frac{1}{\\sqrt{5}}(1,2)\\cdot (-\\frac{1}{\\sqrt{5}}(1,2))=\\frac{1}{5}(1+2^2)=1$\n\n**Exercise:** 1.6.2.c,e\n\n## How to build a vector perpendicular to another vector?\n\n### Example\n\nGiven again the vector $\\mathbf{n}=(1,2)$, a vector $\\mathbf{r}$ perpendicular to it, obeys $\\mathbf{n}^\\intercal \\mathbf{r}=0$. Solving this equation for $\\mathbf{r}$ we find:\n\n$$\n(1,2)\\cdot (x,y)=0 \\implies x+2y=0\n$$\n\nOne equation with two unknowns, we must promote one unknown to a parameter, let $x=-2$. Then $y=1$. Thus $(-2,1)$ is perpendicular to $(1,2)$. Notice this is not the only solution!\n\n\\[*Comment:* This is a very simple $A\\mathbf{x}=\\mathbf{0}$ type of problem with a matrix $A=(1,2)$ with one pivot $1$ and one dependent column $2$.\\]\n\n**Exercises:** 1.6.2.b,c,f\n\n## How to define a circles, lines, planes and hyperplane?\n\n::: {.callout-tip appearance=\"simple\"}\n## Video\n\n[A line in 2D and 3D](https://youtu.be/04eGSCYETkQ)\n:::\n\n**Circle and spheres:** A circle in $\\mathbb{R}^2$ or a sphere in $\\mathbb{R}^3$ centered at $\\mathbf{0}$ satisfy the equation: $|\\mathbf{r}|^2=R^2$ where $R$ is the radius. Centered at $\\mathbf{r}_0$ they obey $|\\mathbf{r}-\\mathbf{r}_0|^2=R^2$. The geometry of the situation should clearly justify the equations.\n\n**Exercise:** 1.6.8\n\n**Line in** $\\mathbb{R}^2$**:** A line perpendicular to $\\mathbf{n}=(1,2)$ is the set of *all* solutions to the equation $\\mathbf{n}^\\intercal \\mathbf{r}=0$, the answer is:\n\n$$\n[\\textbf{line}\\,\\perp\\,\\textbf{to}\\,\\,(1,2)]=\\{(-2c,c)\\,\\,|\\,\\,c\\in \\mathbb{R}\\}\n$$\n\n**Line in** $\\mathbb{R}^3$**:** A line (which crosses the origin) is the interception of two non-parallel planes (which cross the origin as well); a plane is the subspace perpendicular to a vector. Let $\\mathbf{n}_1=(1,0,1)$ and $\\mathbf{n}_2=(1,2,0)$ define the two planes $\\mathbf{n}_{1,2}^\\intercal \\mathbf{r}=0$. The interception is the set of vectors common to both planes, this means we have to find the $\\mathbf{r}=(x,y,z)\\in\\mathbb{R}^3$ such that both plane equations are satisfied, i.e.\n\n$$\n\\begin{cases}\n\\mathbf{n}_{1}^\\intercal \\mathbf{r}=0\\\\\n\\mathbf{n}_{2}^\\intercal \\mathbf{r}=0\n\\end{cases}\n\\leftrightsquigarrow\n\\left(\\begin{matrix}1 & 0 & 1\\\\1 & 2 & 0 \\end{matrix}\\;\\middle|\\;\\begin{matrix}0\\\\0 \\end{matrix}\\right) \n\\overset{l_2'=l_2-l_1}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 0 & 1\\\\0 & 2 & -1 \\end{matrix}\\;\\middle|\\;\\begin{matrix}0\\\\0 \\end{matrix}\\right) \n\\overset{l_2'=1/2l_2}{\\longrightarrow}\\left(\\begin{matrix}1 & 0 & 1\\\\0 & 1 & -1/2 \\end{matrix}\\;\\middle|\\;\\begin{matrix}0\\\\0 \\end{matrix}\\right)\n$$ {#eq-the_problem}\n\nThere are two pivots in the first two columns and a dependent column, one solution is $(-1,1/2,1)$, in general we have:\n\n$$\n\\mathbf{x}_N = c\\left(\\begin{matrix}-1\\\\1/2\\\\1\\end{matrix}\\right)\n$$\n\nwhich gives us the form of the elements along a line!\n\n$$\n[\\textbf{line}\\,\\perp\\, \\textbf{to}\\,\\, \\mathbf{n}_{1,2}] = \\{(-c,c/2,c)\\,\\,|,\\, c\\in \\mathbb{R}\\}\n$$\n\n*A Captain obvious observation:* If we know the process of finding all vector (the line) perpendicular to these two vector, I can find just one vector perpendicular to both.\n\n**A plane in** $\\mathbb{R}^3$**:** The subspace perpendicular to $(1,0,1)$ is the solution of $(1,0,1)\\cdot (x,y,z)=0$, because there are two free columns and one pivot we expect null space $N(1,0,1)$ with two dimensions. Setting the free variable $y=1$ and $z=0$ we find $x=-1$, hence $\\mathbf{x}_{N,1}=(-1,1,0)$; setting $y=0$ and $z=1$ we arrive at $\\mathbf{x}_{N,2}=(-1,0,1)$. Any l.c. of these solutions is a solution of the equation:\n\n$$\n\\mathbf{x}_N=\\alpha\\left(\\begin{matrix}-1\\\\1\\\\0 \\end{matrix}\\right)+\\beta\\left(\\begin{matrix}-1\\\\0\\\\1 \\end{matrix}\\right)\n$$\n\nwhich defines a plane containing $\\mathbf{0}$.\n\n**Hyperplane:** The idea is similar, just define the appropriate interception of subspaces perpendicular to the given vectors, then arrange the equations as an $A\\mathbf{r}=\\mathbf{0}$ problem.\n\n**Exercise:** 1.6.9\n\n## How much a vector looks like another vector?\n\n::: {.callout-tip appearance=\"simple\"}\n## Video\n\n[Comparing vectors](https://youtu.be/F8M_ItYLIkQ)\n:::\n\n### Example 1\n\nThe formula $\\mathbf{u}\\cdot\\mathbf{v} = uv\\cos\\theta$ is of great importance because it allow us to compute the angle between $\\mathbf{u}$ and $\\mathbf{v}$. Let $\\mathbf{u} = (1,2)$ and $\\mathbf{v}=(-2,3)$:\n\n![](figs/angle_between_two_vectors.png){width=\"240\"}\n\nThen the angle is obtained by solving the trigonometric equation:\n\n$$\n\\cos \\theta = \\frac{\\mathbf{u}^\\intercal\\mathbf{v}}{uv}=\\frac{1\\cdot (-2)+2\\cdot 3}{\\sqrt{1+2^2}\\sqrt{2^2+3^2}}=\\frac{4}{\\sqrt{65}}\\implies \\theta \\approx 1\\,\\, rad\n$$\n\nThe angle of $1\\,\\,rad$ is quiet large, hence the two vectors directions are not close to each other.\n\nAnother way to evaluate similarity is through the value of the cosine rather than the angle itself, in this case $\\cos \\theta$ is $4/\\sqrt{85}$ which is about $1/2$.\n\n![](figs/cos_plot.png){width=\"292\"}\n\nSince the $\\theta$ is smaller than $\\pi/2$, then the angle acute which means the vectors are not very similar, but at least point roughly in the same direction.\n\n**Exercises:** 1.6.4\n\n### Example 2\n\nIf we now let $\\mathbf{u} = (1,-2)$ and $\\mathbf{v}=(-2,3)$, then the angle is: $\\theta \\approx 0.95 \\pi \\,\\,rad$. Almost $180^\\circ$, very different vectors. The inner product by being negative already tells us that the vectors point in almost opposite directions.\n\n*Comment:* In example 1 and 2 above the calculation of the angle or the cosine is rather useless because we can easily draw both vectors and check whether they are or not similar. A harder task is when we try to do this in three, four, etc dimensions.\n\n### Example 3:\n\nLet $\\mathbf{u} = (1,2)$ and the unitary vector $\\mathbf{e}_v=(1,-1)/\\sqrt{2}$:\n\n![](figs/projection_vx.png){width=\"296\"}\n\nComputing the inner product we find:\n\n$$\n\\mathbf{e}_v\\cdot \\mathbf{u} = -\\frac{1}{\\sqrt{2}}=5\\cos \\theta\n$$\n\nOn the rhs we see using trigonometry, the projection (shadow) of the vector $\\mathbf{u}$ along the vector $\\mathbf{e}_v$, which is this case is negative, implying the vectors point in different directions.\n\n\\[*Comment:* The matrix $\\mathbf{v}\\mathbf{v}^\\intercal/v^2$ is a projector along $\\mathbf{v}$.\\]\n\n### Example 4\n\nThis time we start with $\\mathbf{e}_x$ and $\\mathbf{e}_x$, the unitary vector $(1,0)$ or $(0,1)$, and keep the usual $\\mathbf{u}=(1,2)$. The inner products yields:\n\n$$\n\\begin{cases}\n\\mathbf{e}_x^\\intercal \\mathbf{u}=1\\\\\n\\mathbf{e}_y^\\intercal \\mathbf{u}=2\\\\\n\\end{cases}\n$$\n\nMoreover:\n\n$$\n\\begin{cases}\n\\mathbf{e}_x^\\intercal \\mathbf{u}=5\\cos\\theta_x\\\\\n\\mathbf{e}_y^\\intercal \\mathbf{u}=5\\cos\\theta_y=5 \\sin\\theta_x\n\\end{cases}\n$$\n\nWhich means geometrically:\n\n![](figs/projection.png){width=\"220\"}\n\nDotting a vector with an unitary vector yields $5\\cos\\theta_x$ and $5\\cos\\theta_y$ which using trigonometry are the projections of $\\mathbf{u}$ along those directions, which in turn is are the entries $1$ and $2$ of the vector along the unitary vectors.\n\nWe conclude that, given a basis $B=\\{\\mathbf{e}_x, \\mathbf{e}_y\\}$ we can write the vector $\\mathbf{u}=(1,2)$ as:\n\n$$\n\\mathbf{u} = (\\mathbf{e}_x\\cdot\\mathbf{u})\\mathbf{e}_x+(\\mathbf{e}_y\\cdot\\mathbf{u})\\mathbf{e}_y= 1\\mathbf{e}_x+2 \\mathbf{e}_y\n$$ {#eq-partition_of_a_vector}\n\nWhich means: $[\\mathbf{u}]_B=(\\mathbf{e}_x\\cdot\\mathbf{u},\\mathbf{e}_y\\cdot\\mathbf{u})$\n\n::: callout-note\n## Commentary\n\nWe identify in @eq-partition_of_a_vector, the partition of the identity:\n\n$$ I=\\mathbf{e}_x\\mathbf{e}_x^\\intercal+\\mathbf{e}_y\\mathbf{e}_y^\\intercal $$\n\nwhich when it act on a vector $\\mathbf{u}$ in $\\mathbb{R}^2$ gives us:\n\n$$\n\\mathbf{u}=I\\mathbf{u} = \\mathbf{e}_x\\mathbf{e}_x^\\intercal\\mathbf{u}+\\mathbf{e}_y\\mathbf{e}_y^\\intercal\\mathbf{u}=1\\mathbf{e}_x+2 \\mathbf{e}_y\n$$\n:::\n\n**Exercises:** 1.7.5\n\n# Orthogonal spaces\n\n::: {.callout-tip appearance=\"simple\"}\n## Video\n\n[Definition and the 4 spaces](https://youtu.be/WgBGz6VKthE)\n:::\n\nTwo subspaces are orthogonal if every vector in one is orthogonal every vector in another. Take the following examples:\n\n![](figs/orthogonal_spaces_ex.png){width=\"559\"}\n\n**Far left:** The two subspaces of $\\mathbb{R}^2$ on the left are orthogonal. Only the origin $\\mathbf{0}$ is common. If we introduce a basis for both $U=span\\{\\mathbf{e}_u\\}$ and $V=span\\{\\mathbf{e}_v\\}$ we can say $\\mathbb{R}^2 = U\\oplus V$ and that $U$ is the orthogonal complement of $V$. Notice the dimension $n=2$ of the space is equal to the sum of the dimensions of the two subspaces: $\\dim \\mathbb{R}^2 = \\dim U +\\dim V$.\n\n**Middle Left:** The two subspaces (shown as rectangles) of $\\mathbb{R}^3$ are not orthogonal because at the interception we find vectors which are parallel and thus not orthogonal. More than the origin is common to both, the **y-axis** is as well.\n\n**Middle right:** We have again two orthogonal subspaces. Only the origin is common. Introducing basis to both subspaces $U=span\\{\\mathbf{e}_x,\\mathbf{e}_y\\}$ and $V=span\\{\\mathbf{e}_z\\}$, we see that $\\mathbb{R}^3=U\\oplus V$. Again we see the dimension $3$ of $\\mathbb{R}^3$ is equal to the sum of the dimensions of the orthogonal complements.\n\n**Far right:** Finally we have two orthogonal subspaces, but we cannot write $\\mathbb{R}^3 = U\\oplus V$. At most we can only say: $[\\textbf{xy plane}] = U\\oplus V$.\n\nThese examples, motivate us to introduce the following definitions:\n\n::: {#def-orthogonal_spaces style=\"color: gray; border: 3px solid rgb(230, 230, 230);\"}\n\\[$U$ is orthogonal to $V$\\] $:=$ \\[Every vector $\\mathbf{u}\\in U$ is orthogonal to every $\\mathbf{v}\\in V$\\]\n\n\\[orthogonal complement of $U$ in $\\mathbb{R}^n$\\] $:=U^\\perp:=\\{\\mathbf{v}\\in\\mathbb{R}^n\\,\\,|\\,\\, \\forall \\mathbf{u}:\\mathbf{u}^\\intercal\\mathbf{v}=0\\}$\n\n*Note:* orthogonal complement of $U$ in $\\mathbb{R}^n$ is the subspace that complements $U$ to gives us $\\mathbb{R}^n$, i.e., it is the solution $U^\\perp$ of $\\mathbb{R}^n = U\\oplus U^\\perp$. Clearly the sum of dimensions of both spaces must be $n$.\n:::\n\n## How to define a basis for a subspace orthogonal to another given subspace?\n\nEssentially what is being asked is to solve $U\\oplus U^\\perp=\\mathbb{R}^n$ given $U$. And since $U^\\perp$ has all its vectors perpendicular to $U$, it is the nullspace of a matrix whose rows are the basis of $U$.\n\n### Example\n\nConsider the subspace of $\\mathbb{R}^3$ given by $U=span\\{(1,0,1),(1,2,0)\\}$. *Question:* Define a basis for $U^\\perp$.\n\n*Answer:* If we find the set of vectors perpendicular to the basis vectors of $U$, we found $U^\\perp$. The basis of $U$ is $\\{(1,0,1),(1,2,0)\\}$, a set of vectors perpendicular to it is the solution of:\n\n$$\n\\left(\\begin{matrix}1 & 0 & 1\\\\1 & 2 & 0 \\end{matrix}\\;\\middle|\\;\\begin{matrix}0\\\\0 \\end{matrix}\\right)\n$$\n\nThis system is solved the usual manner, see @eq-the_problem. Thus:\n\n$$ U^\\perp=[\\textbf{line}\\,\\perp\\, \\textbf{to}\\,\\, \\mathbf{n}_{1,2}] = \\{(-c,c/2,c)\\,\\,|,\\, c\\in \\mathbb{R}\\} $$\n\nNow that we have the set $U^\\perp$ we can pick a basis for it, for example $(-1,1/2,1)$. One basis of $U^\\perp$ is $(-1,1/2,1)$.\n\nSometimes we might want a normalized basis in which case, we pick from $U^\\perp$ the vector $(-1,1/2,1)/\\sqrt{(-1)^2+(1/2)^2+1^2}$.\n\n**Exercise:** 1.7.2,3\n\n## Four very important subspaces\n\nFor a given matrix $A$ with shape $n\\times m$ we already defined the column space $C(A)$ which is a subspace of $\\mathbb{R}^n$ and the nullspace $N(A)$ which is a subspace of $\\mathbb{R}^m$. Recall the matrix before and after elimination:\n\n$$\nA=\\begin{pmatrix}1 & 2 & 2 & 2 \\\\2 & 4 & 6 & 8 \\\\3 & 6 & 8 & 10 \\end{pmatrix}\\longrightarrow\\longrightarrow\\longrightarrow  \\begin{pmatrix}\\mathbf{1} & 2 & 0 & -2 \\\\0 & 0 & \\mathbf{1} & 2 \\\\0 & 0 & 0 & 0 \\end{pmatrix}=A' \\equiv\\text{rref} \\,A \n$$\n\nThe column space spanned by the first and third column (because they are independent), its dimension is the number of pivots, in this case $2$; it is a subspace of $\\mathbb{R}^3$:\n\n$$\nC(A)=span\\{(1,2,3),(2,6,8)\\}\\subset_{\\text{vec}}\\mathbb{R}^3\n$$\n\nWhile the nullspace was obtained by solving $A\\mathbf{x}_N=\\mathbf{0}$, its dimension is the number of dependent columns, which is the number of columns $m=4$ minus the number of pivots $r=2$; it is a subspace of $\\mathbb{R}^4$.\n\n$$\nN(A) = span\\{(-2,1,0,0),(2,0,-2,1)\\}\\subset_{\\text{vec}}\\mathbb{R}^4\n$$\n\nNow we'll introduce their orthogonal complements in the $\\mathbb{R}^3$ and $\\mathbb{R}^4$ respectively.\n\n#### Row space\n\n$N(A)^\\perp$ is the set of all vectors perpendicular to every vector in $N(A)$.\n\n![It is the column space of $A^\\intercal$, aka, row space. Since row 1 and 2 have the pivots, these form a basis:](figs/row_space_perp_to_nullspace.png){width=\"329\"}\n\n$$\nC(A^\\intercal)=span\\{(1,2,2,2),(2,4,6,8)\\}\\qquad \\dim C(A^\\intercal) = 2\n$$\n\n#### Left Nullspace\n\n$C(A)^\\perp$ is the set of all vectors perpendicular to every vector in $C(A)$.\n\n![](figs/leftnullspace_perp_colspace.png){width=\"317\"}\n\nIt is the nullspace of $A^\\intercal$, aka, left nullspace $N(A^\\intercal)$. For example:\n\n$$\nA^\\intercal=\\begin{pmatrix}1 & 2 & 3\\\\2 & 4 & 6 \\\\2 & 6 & 8\\\\ 2 & 8 & 10 \\end{pmatrix}\\longrightarrow\\longrightarrow\\longrightarrow  \\begin{pmatrix}1 & 0 & 1\\\\0 & 1 & 1 \\\\0 & 0 & 0\\\\ 0 & 0 & 0 \\end{pmatrix}=(A^\\intercal)' \\equiv\\text{rref} \\,A^\\intercal \n$$\n\nFrom the reduced form we deduce the solutions set:\n\n$$\nN(A^\\intercal)=span\\{(-1,-1,1)\\} \\qquad \\dim N(A^\\intercal) = 1\n$$\n\n#### Observations:\n\nWe know ($n=3$ and $m=4$):\n\n$$\n\\mathbb{R}^n = C(A) \\oplus N(A^\\intercal)\\qquad \\mathbb{R}^m=C(A^\\intercal)\\oplus N(A)\n$$\n\nSince $\\dim C(A) = \\dim C(A^\\intercal) =r$, $\\dim N(A) =n-r$ and $\\dim N(A^\\intercal) =m-r$ we can verify that:\n\n$$\nn = \\dim C(A) + \\dim N(A^\\intercal) \\qquad m = \\dim C(A^\\intercal)+\\dim N(A)\n$$\n\nPictorially we find:\n\n![](figs/4_subspaces_new.png)\n\nWith this subspaces in hand we can look again at $A\\mathbf{x}=\\mathbf{b}$:\n\n-   Any vector $\\mathbf{x}$ lives in $\\mathbb{R}^m$, it has as many entries as there are columns in the matrix it multiplies. Since this space is broken down into two complements $\\mathbb{R}^m=C(A^\\intercal)\\oplus N(A)$ we can also break $\\mathbf{x}$ into $\\mathbf{x}=\\mathbf{x}_\\text{row}+\\mathbf{x}_\\text{null}$.\n\n-   The vector belong to the space $\\mathbf{b}\\in\\mathbb{R}^n$ and can be broken in $\\mathbf{b} = \\mathbf{b}_{col}+\\mathbf{b}_{\\textit{left null}}$.\n\n::: callout-note\n## Commentary\n\nWhen solving $A\\mathbf{x}=\\mathbf{b}$ we found $\\mathbf{x}=\\mathbf{x}_P+\\mathbf{x}_N$. So it seem as if $x_P$ and $x_{row}$ must be the same thing. But need not be the same. The particular solution may have some component in the nullspace of $A$. As an example, our favorite system of equation has general solution:\n\n$$\n\\begin{pmatrix} 1\\\\0\\\\0\\\\0 \\end{pmatrix} +span\\{\\begin{pmatrix}-2\\\\1\\\\0\\\\0 \\end{pmatrix} ,\\begin{pmatrix}2\\\\0\\\\-2\\\\1 \\end{pmatrix}\\}\n$$\n\nNotice $(1,0,0,0)$ has non zero projection along the $(-2,1,0,0)$ and $(2,0,-2,1)$. However this solution can be rewritten written as $\\mathbf{x}_\\text{row}+\\mathbf{x}_\\text{null}$. To achieve that some they strategies already used in Gram-Schmidt might be useful.\n:::\n\n**Exercises:** 1.5.11,12\n\n# Gram-Schmidt orthogonalization\n\n**Goal:** From a set of independent vectors $\\{\\mathbf{q}_1,\\mathbf{q}_2,\\mathbf{q}_3\\}$ to obtain a set of orthogonal vectors.\n\n**Idea:** First consider one of the vectors of the list, say $\\mathbf{q}_1$, as being fixed. Choose a second vector of the list, for example $\\mathbf{q}_2$; to make it orthogonal to $\\mathbf{q}_1$, I just need to project $\\mathbf{q}_2$ on $\\mathbf{q}_1$ and then remove from $\\mathbf{q}_2$ this projection; what is left in $\\mathbf{q}_2$ is orthogonal to $\\mathbf{q}_1$, call this remainder as $\\mathbf{q}_2^\\perp$. Choose a third vector $\\mathbf{q}_3$ from the list, we want to make it perpendicular to $\\mathbf{q}_1$ and $\\mathbf{q}_2^\\perp$; to do that we have to project it on the plane formed by $\\mathbf{q}_1$ and $\\mathbf{q}_2^\\perp$, then we remove from $\\mathbf{q}_3$ this projection, once again what is left is the perpendicular part $\\mathbf{q}_3^\\perp$ to the plane. And so on...\n\nTo orthogonalize the following list of vectors $\\{\\mathbf{q}_1,\\mathbf{q}_2,\\mathbf{q}_3\\}$:\n\n1.  Fix $\\mathbf{q}_1$.\n\n2.  Remove from $\\mathbf{q}_2$ the projection part on $\\mathbf{q}_1$:\n\n    $$\n    \\mathbf{q}_2^\\perp = \\mathbf{q}_2-\\frac{\\mathbf{q}_1\\mathbf{q}_1^\\intercal}{\\mathbf{q}_1^\\intercal\\mathbf{q}_1}\\mathbf{q}_2\n    $$\n\n3.  Remove from $\\mathbf{q}_3$ the projection on $\\mathbf{q}_1$ and $\\mathbf{q}_2^\\perp$ (i.e. the projection on the plane formed by both):\n\n    $$\n    \\mathbf{q}_3^\\perp = \\mathbf{q}_3-\\frac{\\mathbf{q}_1\\mathbf{q}_1^\\intercal}{\\mathbf{q}_1^\\intercal\\mathbf{q}_1}\\mathbf{q}_3-\\frac{\\mathbf{q}_2^\\perp(\\mathbf{q}_2^\\perp)^\\intercal}{(\\mathbf{q}_2^\\perp)^\\intercal\\mathbf{q}_2^\\perp}\\mathbf{q}_3\n    $$\n\n::: callout-note\n## Commentary\n\nThe expression $\\mathbf{q}^\\intercal \\mathbf{q}$ found in the denominator in these expression is the norm square $|\\mathbf{q}|^2$ of the vectors in the numerator. The terms\n\n$$\n\\frac{\\mathbf{q}\\mathbf{q}^\\intercal}{|\\mathbf{q}|^2}=\\frac{\\mathbf{q}}{|\\mathbf{q}|}\\frac{\\mathbf{q}^\\intercal}{|\\mathbf{q}|}=\\hat{\\mathbf{q}}\\hat{\\mathbf{q}}^\\intercal\n$$\n\nare projectors along the unitary vector $\\hat{\\mathbf{q}}$.\n:::\n\n**Exercise:** 1.7.6,7,8\n","srcMarkdownNoYaml":"\n\n::: {.callout-tip appearance=\"simple\"}\n## Video\n\n[inner product definition](https://youtu.be/Fua3wTeAjjs)\n:::\n\nThe inner product operation $\\cdot$ is an operation between vectors that yields a number, it is defined as:\n\n::: {#def-inner_product style=\"color: gray; border: 3px solid rgb(230, 230, 230);\"}\nLet $\\mathbf{u}$ and $\\mathbf{v}$ be two vectors from the same vector space $\\mathbb{R}^n$ (i.e. $\\mathbf{u}=(u_1,\\dots,u_n)$ and $\\mathbf{v}=(v_1,\\dots,v_n)$). The inner product of these two vectors is the real number of the form:\n\n$$\n[\\text{inner product between}\\,\\, \\mathbf{u}\\,\\, \\text{and} \\,\\,\\mathbf{v}]:=\\mathbf{u}\\cdot\\mathbf{v} = u_1v_1+\\dots+u_nv_n =: \\sum_{i=1}^nu_iv_i\n$$\n\nAnother notation for this number/operation is $\\mathbf{u}^\\intercal\\mathbf{v}$.\n\nThe inner product also results from the calculation $\\mathbf{u}\\cdot\\mathbf{v} = uv\\cos\\theta$.\n:::\n\n**Properties:**\n\n-   it obeys the property of linearity\n\n    $$ \\mathbf{u} \\cdot (\\mathbf{v} + \\lambda \\cdot \\mathbf{w}) = \\mathbf{u}\\cdot \\mathbf{v} + \\lambda \\mathbf{u}\\cdot \\mathbf{w} $$ where $\\lambda$ is any real number. In other words, the $\\cdot$ operation obeys a distributive rule and the \"we can pull out numbers\" rule.\n\n-   The order of the vectors being dotted is irrelevant $\\mathbf{u}\\cdot\\mathbf{v} = \\mathbf{v}\\cdot\\mathbf{u}$ and $\\mathbf{u}^\\intercal\\mathbf{v} = \\mathbf{v}^\\intercal \\mathbf{u}$.\n\n-   Since $-1\\leq\\cos\\theta\\leq 1$, then $-uv\\leq\\mathbf{u}\\cdot\\mathbf{v} \\leq uv$.\n\n-   (**Pythagoras theorem**) If two vectors are orthogonal, then $|\\mathbf{u}+\\mathbf{v}|^2=u^2+v^2$. \\[*Commentary:* Two orthogonal vectors can come about when decomposing a single vector $\\mathbf{w}\\in\\mathbb{R}^n$ in two complementary spaces $\\mathbb{R}^n=U\\oplus U^\\perp$.\\]\n\n-   (**Cauchy-Schwarz inequality** aka triangular inequality) For any vectors $\\mathbf{u}$ and $\\mathbf{v}$ we have the following: $|\\mathbf{u}+\\mathbf{v}|\\leq u+v$. Equality holding when the vectors are parallel and pointing in the same sense.\n\n**Exercises:** 1.6.3,5,6,7\n\n**Usefulness:** The inner product operation is a tool to gather geometrical information about vectors and to describe planes and hyperplanes. I.e. it allows answering the following questions:\n\n## What is the length of a vector?\n\n::: {.callout-tip appearance=\"simple\"}\n## Video\n\n[Magnitude of a vector](https://youtu.be/DOW-QTRLqiQ)\n:::\n\n*Answer:* Compute the inner product of a vector with itself.\n\n$$\n\\mathbf{u}^\\intercal\\mathbf{u} = u_1^2+\\dots+u_n^2\n$$\n\nWhose rhs we interpret, using the Pythagoras theorem, as the length square of $\\mathbf{u}$. The norm $|\\mathbf{u}|$ of the vector is the square-root of this value:\n\n$$\n|\\mathbf{u}|:=\\sqrt{u_1^2+\\dots+u_n^2}\n$$\n\n*Notation:* we can call the norm $|\\mathbf{u}|$ just by $u$.\n\n### Example\n\nThe length squared of the vector $\\mathbf{u}=(1,2)$ is:\n\n$$\n|\\mathbf{u}|^2 = \\mathbf{u}^\\intercal\\mathbf{u} = u_1^2+u_2^2 = 1^2+2^2=1+4=5\n$$\n\nThe norm of the vector is obtained by taking the square root of $5$:\n\n$$\nu=\\sqrt{5}\n$$\n\n![](figs/pythagoras.png){width=\"266\"}\n\nAnother way to compute the norm-squared of a vector is to consider $\\mathbb{R}^2=[\\textbf{x axis}]\\oplus[\\textbf{y axis}]$ and write $\\mathbf{u} = (0,1)+(2,0)$ and then use the Pythagoras theorem $|\\mathbf{w}+\\mathbf{v}|^2=w^2+v^2$ to compute:\n\n$$\n|\\mathbf{u}|^2=|(0,1)+(2,0)|^2=(0^2+1^2)^2+(2^2+0^2)^2=1+4=5\n$$\n\nThen take the square-root.\n\n**Exercises:** 1.6.1a\n\n## How to build a unit vector?\n\nStarting with the vector $\\mathbf{u}=(u_x,u_y)$ we can define a parallel unit vector by dividing it by it length:\n\n$$\n\\mathbf{e}_u = \\frac{\\mathbf{u}}{|\\mathbf{u}|}\n$$\n\n*Justification:* Compute its norm $|\\mathbf{e}_u|^2 = \\frac{\\mathbf{u}}{|\\mathbf{u}|}\\cdot \\frac{\\mathbf{u}}{|\\mathbf{u}|}=\\frac{|\\mathbf{u}|^2}{|\\mathbf{u}|^2}=1$\n\n### Example\n\nThe unit vector along $\\mathbf{u}=-(1,2)$ is:\n\n$$\n\\mathbf{e}_u = \\frac{-(1,2)}{\\sqrt{1+2^2}}=-\\frac{1}{\\sqrt{5}}(1,2)\n$$\n\nClearly: $|\\mathbf{e}_u|^2 = -\\frac{1}{\\sqrt{5}}(1,2)\\cdot (-\\frac{1}{\\sqrt{5}}(1,2))=\\frac{1}{5}(1+2^2)=1$\n\n**Exercise:** 1.6.2.c,e\n\n## How to build a vector perpendicular to another vector?\n\n### Example\n\nGiven again the vector $\\mathbf{n}=(1,2)$, a vector $\\mathbf{r}$ perpendicular to it, obeys $\\mathbf{n}^\\intercal \\mathbf{r}=0$. Solving this equation for $\\mathbf{r}$ we find:\n\n$$\n(1,2)\\cdot (x,y)=0 \\implies x+2y=0\n$$\n\nOne equation with two unknowns, we must promote one unknown to a parameter, let $x=-2$. Then $y=1$. Thus $(-2,1)$ is perpendicular to $(1,2)$. Notice this is not the only solution!\n\n\\[*Comment:* This is a very simple $A\\mathbf{x}=\\mathbf{0}$ type of problem with a matrix $A=(1,2)$ with one pivot $1$ and one dependent column $2$.\\]\n\n**Exercises:** 1.6.2.b,c,f\n\n## How to define a circles, lines, planes and hyperplane?\n\n::: {.callout-tip appearance=\"simple\"}\n## Video\n\n[A line in 2D and 3D](https://youtu.be/04eGSCYETkQ)\n:::\n\n**Circle and spheres:** A circle in $\\mathbb{R}^2$ or a sphere in $\\mathbb{R}^3$ centered at $\\mathbf{0}$ satisfy the equation: $|\\mathbf{r}|^2=R^2$ where $R$ is the radius. Centered at $\\mathbf{r}_0$ they obey $|\\mathbf{r}-\\mathbf{r}_0|^2=R^2$. The geometry of the situation should clearly justify the equations.\n\n**Exercise:** 1.6.8\n\n**Line in** $\\mathbb{R}^2$**:** A line perpendicular to $\\mathbf{n}=(1,2)$ is the set of *all* solutions to the equation $\\mathbf{n}^\\intercal \\mathbf{r}=0$, the answer is:\n\n$$\n[\\textbf{line}\\,\\perp\\,\\textbf{to}\\,\\,(1,2)]=\\{(-2c,c)\\,\\,|\\,\\,c\\in \\mathbb{R}\\}\n$$\n\n**Line in** $\\mathbb{R}^3$**:** A line (which crosses the origin) is the interception of two non-parallel planes (which cross the origin as well); a plane is the subspace perpendicular to a vector. Let $\\mathbf{n}_1=(1,0,1)$ and $\\mathbf{n}_2=(1,2,0)$ define the two planes $\\mathbf{n}_{1,2}^\\intercal \\mathbf{r}=0$. The interception is the set of vectors common to both planes, this means we have to find the $\\mathbf{r}=(x,y,z)\\in\\mathbb{R}^3$ such that both plane equations are satisfied, i.e.\n\n$$\n\\begin{cases}\n\\mathbf{n}_{1}^\\intercal \\mathbf{r}=0\\\\\n\\mathbf{n}_{2}^\\intercal \\mathbf{r}=0\n\\end{cases}\n\\leftrightsquigarrow\n\\left(\\begin{matrix}1 & 0 & 1\\\\1 & 2 & 0 \\end{matrix}\\;\\middle|\\;\\begin{matrix}0\\\\0 \\end{matrix}\\right) \n\\overset{l_2'=l_2-l_1}{\\longrightarrow}\n\\left(\\begin{matrix}1 & 0 & 1\\\\0 & 2 & -1 \\end{matrix}\\;\\middle|\\;\\begin{matrix}0\\\\0 \\end{matrix}\\right) \n\\overset{l_2'=1/2l_2}{\\longrightarrow}\\left(\\begin{matrix}1 & 0 & 1\\\\0 & 1 & -1/2 \\end{matrix}\\;\\middle|\\;\\begin{matrix}0\\\\0 \\end{matrix}\\right)\n$$ {#eq-the_problem}\n\nThere are two pivots in the first two columns and a dependent column, one solution is $(-1,1/2,1)$, in general we have:\n\n$$\n\\mathbf{x}_N = c\\left(\\begin{matrix}-1\\\\1/2\\\\1\\end{matrix}\\right)\n$$\n\nwhich gives us the form of the elements along a line!\n\n$$\n[\\textbf{line}\\,\\perp\\, \\textbf{to}\\,\\, \\mathbf{n}_{1,2}] = \\{(-c,c/2,c)\\,\\,|,\\, c\\in \\mathbb{R}\\}\n$$\n\n*A Captain obvious observation:* If we know the process of finding all vector (the line) perpendicular to these two vector, I can find just one vector perpendicular to both.\n\n**A plane in** $\\mathbb{R}^3$**:** The subspace perpendicular to $(1,0,1)$ is the solution of $(1,0,1)\\cdot (x,y,z)=0$, because there are two free columns and one pivot we expect null space $N(1,0,1)$ with two dimensions. Setting the free variable $y=1$ and $z=0$ we find $x=-1$, hence $\\mathbf{x}_{N,1}=(-1,1,0)$; setting $y=0$ and $z=1$ we arrive at $\\mathbf{x}_{N,2}=(-1,0,1)$. Any l.c. of these solutions is a solution of the equation:\n\n$$\n\\mathbf{x}_N=\\alpha\\left(\\begin{matrix}-1\\\\1\\\\0 \\end{matrix}\\right)+\\beta\\left(\\begin{matrix}-1\\\\0\\\\1 \\end{matrix}\\right)\n$$\n\nwhich defines a plane containing $\\mathbf{0}$.\n\n**Hyperplane:** The idea is similar, just define the appropriate interception of subspaces perpendicular to the given vectors, then arrange the equations as an $A\\mathbf{r}=\\mathbf{0}$ problem.\n\n**Exercise:** 1.6.9\n\n## How much a vector looks like another vector?\n\n::: {.callout-tip appearance=\"simple\"}\n## Video\n\n[Comparing vectors](https://youtu.be/F8M_ItYLIkQ)\n:::\n\n### Example 1\n\nThe formula $\\mathbf{u}\\cdot\\mathbf{v} = uv\\cos\\theta$ is of great importance because it allow us to compute the angle between $\\mathbf{u}$ and $\\mathbf{v}$. Let $\\mathbf{u} = (1,2)$ and $\\mathbf{v}=(-2,3)$:\n\n![](figs/angle_between_two_vectors.png){width=\"240\"}\n\nThen the angle is obtained by solving the trigonometric equation:\n\n$$\n\\cos \\theta = \\frac{\\mathbf{u}^\\intercal\\mathbf{v}}{uv}=\\frac{1\\cdot (-2)+2\\cdot 3}{\\sqrt{1+2^2}\\sqrt{2^2+3^2}}=\\frac{4}{\\sqrt{65}}\\implies \\theta \\approx 1\\,\\, rad\n$$\n\nThe angle of $1\\,\\,rad$ is quiet large, hence the two vectors directions are not close to each other.\n\nAnother way to evaluate similarity is through the value of the cosine rather than the angle itself, in this case $\\cos \\theta$ is $4/\\sqrt{85}$ which is about $1/2$.\n\n![](figs/cos_plot.png){width=\"292\"}\n\nSince the $\\theta$ is smaller than $\\pi/2$, then the angle acute which means the vectors are not very similar, but at least point roughly in the same direction.\n\n**Exercises:** 1.6.4\n\n### Example 2\n\nIf we now let $\\mathbf{u} = (1,-2)$ and $\\mathbf{v}=(-2,3)$, then the angle is: $\\theta \\approx 0.95 \\pi \\,\\,rad$. Almost $180^\\circ$, very different vectors. The inner product by being negative already tells us that the vectors point in almost opposite directions.\n\n*Comment:* In example 1 and 2 above the calculation of the angle or the cosine is rather useless because we can easily draw both vectors and check whether they are or not similar. A harder task is when we try to do this in three, four, etc dimensions.\n\n### Example 3:\n\nLet $\\mathbf{u} = (1,2)$ and the unitary vector $\\mathbf{e}_v=(1,-1)/\\sqrt{2}$:\n\n![](figs/projection_vx.png){width=\"296\"}\n\nComputing the inner product we find:\n\n$$\n\\mathbf{e}_v\\cdot \\mathbf{u} = -\\frac{1}{\\sqrt{2}}=5\\cos \\theta\n$$\n\nOn the rhs we see using trigonometry, the projection (shadow) of the vector $\\mathbf{u}$ along the vector $\\mathbf{e}_v$, which is this case is negative, implying the vectors point in different directions.\n\n\\[*Comment:* The matrix $\\mathbf{v}\\mathbf{v}^\\intercal/v^2$ is a projector along $\\mathbf{v}$.\\]\n\n### Example 4\n\nThis time we start with $\\mathbf{e}_x$ and $\\mathbf{e}_x$, the unitary vector $(1,0)$ or $(0,1)$, and keep the usual $\\mathbf{u}=(1,2)$. The inner products yields:\n\n$$\n\\begin{cases}\n\\mathbf{e}_x^\\intercal \\mathbf{u}=1\\\\\n\\mathbf{e}_y^\\intercal \\mathbf{u}=2\\\\\n\\end{cases}\n$$\n\nMoreover:\n\n$$\n\\begin{cases}\n\\mathbf{e}_x^\\intercal \\mathbf{u}=5\\cos\\theta_x\\\\\n\\mathbf{e}_y^\\intercal \\mathbf{u}=5\\cos\\theta_y=5 \\sin\\theta_x\n\\end{cases}\n$$\n\nWhich means geometrically:\n\n![](figs/projection.png){width=\"220\"}\n\nDotting a vector with an unitary vector yields $5\\cos\\theta_x$ and $5\\cos\\theta_y$ which using trigonometry are the projections of $\\mathbf{u}$ along those directions, which in turn is are the entries $1$ and $2$ of the vector along the unitary vectors.\n\nWe conclude that, given a basis $B=\\{\\mathbf{e}_x, \\mathbf{e}_y\\}$ we can write the vector $\\mathbf{u}=(1,2)$ as:\n\n$$\n\\mathbf{u} = (\\mathbf{e}_x\\cdot\\mathbf{u})\\mathbf{e}_x+(\\mathbf{e}_y\\cdot\\mathbf{u})\\mathbf{e}_y= 1\\mathbf{e}_x+2 \\mathbf{e}_y\n$$ {#eq-partition_of_a_vector}\n\nWhich means: $[\\mathbf{u}]_B=(\\mathbf{e}_x\\cdot\\mathbf{u},\\mathbf{e}_y\\cdot\\mathbf{u})$\n\n::: callout-note\n## Commentary\n\nWe identify in @eq-partition_of_a_vector, the partition of the identity:\n\n$$ I=\\mathbf{e}_x\\mathbf{e}_x^\\intercal+\\mathbf{e}_y\\mathbf{e}_y^\\intercal $$\n\nwhich when it act on a vector $\\mathbf{u}$ in $\\mathbb{R}^2$ gives us:\n\n$$\n\\mathbf{u}=I\\mathbf{u} = \\mathbf{e}_x\\mathbf{e}_x^\\intercal\\mathbf{u}+\\mathbf{e}_y\\mathbf{e}_y^\\intercal\\mathbf{u}=1\\mathbf{e}_x+2 \\mathbf{e}_y\n$$\n:::\n\n**Exercises:** 1.7.5\n\n# Orthogonal spaces\n\n::: {.callout-tip appearance=\"simple\"}\n## Video\n\n[Definition and the 4 spaces](https://youtu.be/WgBGz6VKthE)\n:::\n\nTwo subspaces are orthogonal if every vector in one is orthogonal every vector in another. Take the following examples:\n\n![](figs/orthogonal_spaces_ex.png){width=\"559\"}\n\n**Far left:** The two subspaces of $\\mathbb{R}^2$ on the left are orthogonal. Only the origin $\\mathbf{0}$ is common. If we introduce a basis for both $U=span\\{\\mathbf{e}_u\\}$ and $V=span\\{\\mathbf{e}_v\\}$ we can say $\\mathbb{R}^2 = U\\oplus V$ and that $U$ is the orthogonal complement of $V$. Notice the dimension $n=2$ of the space is equal to the sum of the dimensions of the two subspaces: $\\dim \\mathbb{R}^2 = \\dim U +\\dim V$.\n\n**Middle Left:** The two subspaces (shown as rectangles) of $\\mathbb{R}^3$ are not orthogonal because at the interception we find vectors which are parallel and thus not orthogonal. More than the origin is common to both, the **y-axis** is as well.\n\n**Middle right:** We have again two orthogonal subspaces. Only the origin is common. Introducing basis to both subspaces $U=span\\{\\mathbf{e}_x,\\mathbf{e}_y\\}$ and $V=span\\{\\mathbf{e}_z\\}$, we see that $\\mathbb{R}^3=U\\oplus V$. Again we see the dimension $3$ of $\\mathbb{R}^3$ is equal to the sum of the dimensions of the orthogonal complements.\n\n**Far right:** Finally we have two orthogonal subspaces, but we cannot write $\\mathbb{R}^3 = U\\oplus V$. At most we can only say: $[\\textbf{xy plane}] = U\\oplus V$.\n\nThese examples, motivate us to introduce the following definitions:\n\n::: {#def-orthogonal_spaces style=\"color: gray; border: 3px solid rgb(230, 230, 230);\"}\n\\[$U$ is orthogonal to $V$\\] $:=$ \\[Every vector $\\mathbf{u}\\in U$ is orthogonal to every $\\mathbf{v}\\in V$\\]\n\n\\[orthogonal complement of $U$ in $\\mathbb{R}^n$\\] $:=U^\\perp:=\\{\\mathbf{v}\\in\\mathbb{R}^n\\,\\,|\\,\\, \\forall \\mathbf{u}:\\mathbf{u}^\\intercal\\mathbf{v}=0\\}$\n\n*Note:* orthogonal complement of $U$ in $\\mathbb{R}^n$ is the subspace that complements $U$ to gives us $\\mathbb{R}^n$, i.e., it is the solution $U^\\perp$ of $\\mathbb{R}^n = U\\oplus U^\\perp$. Clearly the sum of dimensions of both spaces must be $n$.\n:::\n\n## How to define a basis for a subspace orthogonal to another given subspace?\n\nEssentially what is being asked is to solve $U\\oplus U^\\perp=\\mathbb{R}^n$ given $U$. And since $U^\\perp$ has all its vectors perpendicular to $U$, it is the nullspace of a matrix whose rows are the basis of $U$.\n\n### Example\n\nConsider the subspace of $\\mathbb{R}^3$ given by $U=span\\{(1,0,1),(1,2,0)\\}$. *Question:* Define a basis for $U^\\perp$.\n\n*Answer:* If we find the set of vectors perpendicular to the basis vectors of $U$, we found $U^\\perp$. The basis of $U$ is $\\{(1,0,1),(1,2,0)\\}$, a set of vectors perpendicular to it is the solution of:\n\n$$\n\\left(\\begin{matrix}1 & 0 & 1\\\\1 & 2 & 0 \\end{matrix}\\;\\middle|\\;\\begin{matrix}0\\\\0 \\end{matrix}\\right)\n$$\n\nThis system is solved the usual manner, see @eq-the_problem. Thus:\n\n$$ U^\\perp=[\\textbf{line}\\,\\perp\\, \\textbf{to}\\,\\, \\mathbf{n}_{1,2}] = \\{(-c,c/2,c)\\,\\,|,\\, c\\in \\mathbb{R}\\} $$\n\nNow that we have the set $U^\\perp$ we can pick a basis for it, for example $(-1,1/2,1)$. One basis of $U^\\perp$ is $(-1,1/2,1)$.\n\nSometimes we might want a normalized basis in which case, we pick from $U^\\perp$ the vector $(-1,1/2,1)/\\sqrt{(-1)^2+(1/2)^2+1^2}$.\n\n**Exercise:** 1.7.2,3\n\n## Four very important subspaces\n\nFor a given matrix $A$ with shape $n\\times m$ we already defined the column space $C(A)$ which is a subspace of $\\mathbb{R}^n$ and the nullspace $N(A)$ which is a subspace of $\\mathbb{R}^m$. Recall the matrix before and after elimination:\n\n$$\nA=\\begin{pmatrix}1 & 2 & 2 & 2 \\\\2 & 4 & 6 & 8 \\\\3 & 6 & 8 & 10 \\end{pmatrix}\\longrightarrow\\longrightarrow\\longrightarrow  \\begin{pmatrix}\\mathbf{1} & 2 & 0 & -2 \\\\0 & 0 & \\mathbf{1} & 2 \\\\0 & 0 & 0 & 0 \\end{pmatrix}=A' \\equiv\\text{rref} \\,A \n$$\n\nThe column space spanned by the first and third column (because they are independent), its dimension is the number of pivots, in this case $2$; it is a subspace of $\\mathbb{R}^3$:\n\n$$\nC(A)=span\\{(1,2,3),(2,6,8)\\}\\subset_{\\text{vec}}\\mathbb{R}^3\n$$\n\nWhile the nullspace was obtained by solving $A\\mathbf{x}_N=\\mathbf{0}$, its dimension is the number of dependent columns, which is the number of columns $m=4$ minus the number of pivots $r=2$; it is a subspace of $\\mathbb{R}^4$.\n\n$$\nN(A) = span\\{(-2,1,0,0),(2,0,-2,1)\\}\\subset_{\\text{vec}}\\mathbb{R}^4\n$$\n\nNow we'll introduce their orthogonal complements in the $\\mathbb{R}^3$ and $\\mathbb{R}^4$ respectively.\n\n#### Row space\n\n$N(A)^\\perp$ is the set of all vectors perpendicular to every vector in $N(A)$.\n\n![It is the column space of $A^\\intercal$, aka, row space. Since row 1 and 2 have the pivots, these form a basis:](figs/row_space_perp_to_nullspace.png){width=\"329\"}\n\n$$\nC(A^\\intercal)=span\\{(1,2,2,2),(2,4,6,8)\\}\\qquad \\dim C(A^\\intercal) = 2\n$$\n\n#### Left Nullspace\n\n$C(A)^\\perp$ is the set of all vectors perpendicular to every vector in $C(A)$.\n\n![](figs/leftnullspace_perp_colspace.png){width=\"317\"}\n\nIt is the nullspace of $A^\\intercal$, aka, left nullspace $N(A^\\intercal)$. For example:\n\n$$\nA^\\intercal=\\begin{pmatrix}1 & 2 & 3\\\\2 & 4 & 6 \\\\2 & 6 & 8\\\\ 2 & 8 & 10 \\end{pmatrix}\\longrightarrow\\longrightarrow\\longrightarrow  \\begin{pmatrix}1 & 0 & 1\\\\0 & 1 & 1 \\\\0 & 0 & 0\\\\ 0 & 0 & 0 \\end{pmatrix}=(A^\\intercal)' \\equiv\\text{rref} \\,A^\\intercal \n$$\n\nFrom the reduced form we deduce the solutions set:\n\n$$\nN(A^\\intercal)=span\\{(-1,-1,1)\\} \\qquad \\dim N(A^\\intercal) = 1\n$$\n\n#### Observations:\n\nWe know ($n=3$ and $m=4$):\n\n$$\n\\mathbb{R}^n = C(A) \\oplus N(A^\\intercal)\\qquad \\mathbb{R}^m=C(A^\\intercal)\\oplus N(A)\n$$\n\nSince $\\dim C(A) = \\dim C(A^\\intercal) =r$, $\\dim N(A) =n-r$ and $\\dim N(A^\\intercal) =m-r$ we can verify that:\n\n$$\nn = \\dim C(A) + \\dim N(A^\\intercal) \\qquad m = \\dim C(A^\\intercal)+\\dim N(A)\n$$\n\nPictorially we find:\n\n![](figs/4_subspaces_new.png)\n\nWith this subspaces in hand we can look again at $A\\mathbf{x}=\\mathbf{b}$:\n\n-   Any vector $\\mathbf{x}$ lives in $\\mathbb{R}^m$, it has as many entries as there are columns in the matrix it multiplies. Since this space is broken down into two complements $\\mathbb{R}^m=C(A^\\intercal)\\oplus N(A)$ we can also break $\\mathbf{x}$ into $\\mathbf{x}=\\mathbf{x}_\\text{row}+\\mathbf{x}_\\text{null}$.\n\n-   The vector belong to the space $\\mathbf{b}\\in\\mathbb{R}^n$ and can be broken in $\\mathbf{b} = \\mathbf{b}_{col}+\\mathbf{b}_{\\textit{left null}}$.\n\n::: callout-note\n## Commentary\n\nWhen solving $A\\mathbf{x}=\\mathbf{b}$ we found $\\mathbf{x}=\\mathbf{x}_P+\\mathbf{x}_N$. So it seem as if $x_P$ and $x_{row}$ must be the same thing. But need not be the same. The particular solution may have some component in the nullspace of $A$. As an example, our favorite system of equation has general solution:\n\n$$\n\\begin{pmatrix} 1\\\\0\\\\0\\\\0 \\end{pmatrix} +span\\{\\begin{pmatrix}-2\\\\1\\\\0\\\\0 \\end{pmatrix} ,\\begin{pmatrix}2\\\\0\\\\-2\\\\1 \\end{pmatrix}\\}\n$$\n\nNotice $(1,0,0,0)$ has non zero projection along the $(-2,1,0,0)$ and $(2,0,-2,1)$. However this solution can be rewritten written as $\\mathbf{x}_\\text{row}+\\mathbf{x}_\\text{null}$. To achieve that some they strategies already used in Gram-Schmidt might be useful.\n:::\n\n**Exercises:** 1.5.11,12\n\n# Gram-Schmidt orthogonalization\n\n**Goal:** From a set of independent vectors $\\{\\mathbf{q}_1,\\mathbf{q}_2,\\mathbf{q}_3\\}$ to obtain a set of orthogonal vectors.\n\n**Idea:** First consider one of the vectors of the list, say $\\mathbf{q}_1$, as being fixed. Choose a second vector of the list, for example $\\mathbf{q}_2$; to make it orthogonal to $\\mathbf{q}_1$, I just need to project $\\mathbf{q}_2$ on $\\mathbf{q}_1$ and then remove from $\\mathbf{q}_2$ this projection; what is left in $\\mathbf{q}_2$ is orthogonal to $\\mathbf{q}_1$, call this remainder as $\\mathbf{q}_2^\\perp$. Choose a third vector $\\mathbf{q}_3$ from the list, we want to make it perpendicular to $\\mathbf{q}_1$ and $\\mathbf{q}_2^\\perp$; to do that we have to project it on the plane formed by $\\mathbf{q}_1$ and $\\mathbf{q}_2^\\perp$, then we remove from $\\mathbf{q}_3$ this projection, once again what is left is the perpendicular part $\\mathbf{q}_3^\\perp$ to the plane. And so on...\n\nTo orthogonalize the following list of vectors $\\{\\mathbf{q}_1,\\mathbf{q}_2,\\mathbf{q}_3\\}$:\n\n1.  Fix $\\mathbf{q}_1$.\n\n2.  Remove from $\\mathbf{q}_2$ the projection part on $\\mathbf{q}_1$:\n\n    $$\n    \\mathbf{q}_2^\\perp = \\mathbf{q}_2-\\frac{\\mathbf{q}_1\\mathbf{q}_1^\\intercal}{\\mathbf{q}_1^\\intercal\\mathbf{q}_1}\\mathbf{q}_2\n    $$\n\n3.  Remove from $\\mathbf{q}_3$ the projection on $\\mathbf{q}_1$ and $\\mathbf{q}_2^\\perp$ (i.e. the projection on the plane formed by both):\n\n    $$\n    \\mathbf{q}_3^\\perp = \\mathbf{q}_3-\\frac{\\mathbf{q}_1\\mathbf{q}_1^\\intercal}{\\mathbf{q}_1^\\intercal\\mathbf{q}_1}\\mathbf{q}_3-\\frac{\\mathbf{q}_2^\\perp(\\mathbf{q}_2^\\perp)^\\intercal}{(\\mathbf{q}_2^\\perp)^\\intercal\\mathbf{q}_2^\\perp}\\mathbf{q}_3\n    $$\n\n::: callout-note\n## Commentary\n\nThe expression $\\mathbf{q}^\\intercal \\mathbf{q}$ found in the denominator in these expression is the norm square $|\\mathbf{q}|^2$ of the vectors in the numerator. The terms\n\n$$\n\\frac{\\mathbf{q}\\mathbf{q}^\\intercal}{|\\mathbf{q}|^2}=\\frac{\\mathbf{q}}{|\\mathbf{q}|}\\frac{\\mathbf{q}^\\intercal}{|\\mathbf{q}|}=\\hat{\\mathbf{q}}\\hat{\\mathbf{q}}^\\intercal\n$$\n\nare projectors along the unitary vector $\\hat{\\mathbf{q}}$.\n:::\n\n**Exercise:** 1.7.6,7,8\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"inner_product.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.32","editor":"visual","comments":{"hypothesis":true},"preview":{"browser":"chrome"},"theme":{"light":"minty"},"fontcolor":"rgb(190,190,190)","backgroundcolor":"rgb(32,31,30)","margin-left":"rgb(32,31,30)","navbar-color":"rgba(216, 6, 33 .65)","title":"Definition of inner product and its properties"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}