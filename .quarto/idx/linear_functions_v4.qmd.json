{"title":"Linear functions","markdown":{"yaml":{"title":"Linear functions","execute":{"echo":false}},"headingText":"The concept of a vector space","containsRefs":false,"markdown":"\n\nUp to now, we learned a more practical part of the course:\n\n-   How to compute the solution of $A\\mathbf{x}=\\mathbf{b}$?\n\n-   How to compute a basis for the $4$ subspaces associated with $A$?\n\n-   How to compute the inverse $A^{-1}$?\n\n-   How to compute the determinant?\n\nIt always *how to compute*.\n\nNow we enter a more conceptual part of the course and organize ideas.\n\n\nWe have been speaking about a vector space as being closed under linear combinations. However if we stop to think about it, we can only compute a linear combination provided we know how to multiply vectors by numbers and how to add vectors, see the linear combination below:\n\n$$\n\\overbrace{\\alpha u}^\\text{number times a vector} + \\overbrace{\\beta v}^\\text{number times a vector}=\\underbrace{u'+v'}_\\text{sum of vectors}\n$$\n\nLets look more carefully at two questions:\n\n-   What is a sum of two vectors?\n\n-   What is a vector times a number?\n\nAnswering these questions is important because the concept of linear combination and of vector space we adopted so far - a set of things closed under linear combinations - hinges on answering these two questions.\n\nStart with a set of $n$ things with labels $v_1$, $v_2$, ..., $v_n$:\n\n$$\n\\mathbb{V}=\\{v_1,v_2,\\dots,v_n\\}\n$$\n\nWhat can we do with these things in this set?\n\nWe can define a map, i.e., an assignment or a function, from the set onto itself. There are two specific kinds of maps that are relevant to those questions:\n\n-   A function which to each pair of elements of $\\mathbb{V}$ assigns another element of $\\mathbb{V}$, in math language we write:$$\n    \\begin{align}\n    + : \\mathbb{V}\\times\\mathbb{V} &\\longrightarrow\\mathbb{V}\\\\\n    (u,v)&\\longmapsto u+v\n    \\end{align}\n    $$ {#eq-plus}\n\n    This is relevant for the first question.\n\n-   A function which to each $\\mathbb{V}$ and a real number assigns another element of $\\mathbb{V}$, in math language:\n\n    $$\n    \\begin{align}\\cdot : \\mathbb{R}\\times\\mathbb{V} &\\longrightarrow\\mathbb{V}\\\\(\\alpha,v)&\\longmapsto \\alpha\\cdot v\\end{align}\n    $$ {#eq-times} is relevant for the second question.\n\nThe $u+v$ is the symbol for the element in $\\mathbb{V}$ that corresponds to the pair $(u,v)$. And $\\alpha\\cdot v$ is the symbol for the element of $\\mathbb{V}$ assigned to $(\\alpha,v)$.\n\nWe wish additionally that those maps to have the properties:\n\n-   Commutative: $u+v=v+u$. \\[*You can flip the order*\\]\n\n-   Associative: $(u+v)+w=u+(v+w)$. \\[*You can shift the brackets*\\]\n\n-   Neutral element (wrt to $+$): There is an element in $\\mathbb{V}$, call it $o$ so that for all $u\\in \\mathbb{V}$ we find $u+o=u$. \\[*This is not* $0$ *from the real numbers, it is the zero of* $\\mathbb{V}$*. To avoid confusion you can call* $o$ *as circle rather than zero.*\\]\n\n-   Inverse element (wrt to $+$): For every element $u$ there an element, which we denote by $-u$, that guarantees $u+(-u)=o$. \\[ $u-v$ *is just a short notation for* $u+(-v)$.\n\nThe next list of rules describe how the operations $+$ and $\\cdot$ interact when both present in a calculation, two of this rules are distributive rules:\n\n-   Associative: $\\alpha(\\beta u)=(\\alpha\\beta)v$\n\n-   Distributive I: $(\\alpha+\\beta)u = \\alpha u +\\beta u$\n\n-   Distributive II: $\\alpha (u +v) = \\alpha u + \\alpha v$\n\n-   Unitary: $1 u = u$\n\nWhen looking at @eq-plus, @eq-times and the CANI-ADDU rules note we did not mention what are the elements of $\\mathbb{V}$ and neither did we mention how to compute the assignment, the only thing stated is that $+$, whatever it ends up being in practice, it maps two elements of $\\mathbb{V}$ into another element of $\\mathbb{V}$ and that, whatever the dot $\\cdot$ ends up meaning in practice, it must assign to a real number and an element $\\mathbb{V}$ another element of $\\mathbb{V}$. All this while, simultaneously all the above rules are obeyed. So, in practice, the $+$ and $\\cdot$ are really special operation, such is the number of constraints imposed.\n\nA vector space is then a triplet $(\\mathbb{V},+,\\cdot)$\n\n::: callout-note\n## Commentaries\n\n-   Why these two maps with these rules? Because on them fit many useful maps in practice. The list of properties is like describing someone: you can list all its characteristics, or just list the essential ones. Here essential depends for what the description is useful for. For example, your CV works for the purpose of getting a job.\n\n-   There are many constraints (CANI and ADDU) on the operation $+$ and $\\cdot$, but all these boil down to make $\\mathbb{V}$ being closed under l.c., where the l.c. is realized in practice the natural and expected way. The constraints above are the rules of common sense for l.c.\n\n-   The set of assignments made by $+$ and $\\cdot$ which we can write as $\\{(u,v,u+v)\\}$ and $\\{(c,u,cu)\\}$ is what we call the structure of the vector space.\n:::\n\nWhat specific sets and $+$ and $\\cdot$ operations fit the profile above? Here are some examples:\n\n## Example 1:\n\nThe set of all polynomials of degree $2$:\n\n$$\n\\mathbb{P}=\\{p:\\mathbb{R}\\rightarrow\\mathbb{R}; x\\mapsto a_2 x^2 +a_1 x +a_0\\,\\,|\\,\\, a_i \\text{'s are real}\\}\n$$\n\nTogether with:\n\n$$\n\\begin{align}+ : \\mathbb{P}\\times\\mathbb{P} &\\longrightarrow\\mathbb{V}\\\\(p,q)&\\longmapsto (p+q)(x)=p(x)+q(x)\\end{align}\n$$\n\nHere, the $+$ function of functions $p$ and $q$ is defined from the summation $+$ of real numbers $p(x)$ and $q(x)$. While before $+$ did not have meaning now it has in terms of the $+$ between numbers.\n\n$$\n\\begin{align}\\cdot : \\mathbb{R}\\times\\mathbb{P} &\\longrightarrow\\mathbb{V}\\\\(\\alpha,p)&\\longmapsto (\\alpha\\cdot p)(x):=\\alpha\\cdot p(x)\\end{align}\n$$\n\nThe multiplication between a real number and the function $p$ is understood in therms of the multiplication between the real numbers $\\alpha$ and $p(x)$.\n\nAll the rules above can be checked for this example.\n\nThe natural basis for this vector space is: $\\{1,x,x^2\\}$\n\n::: callout-note\n## Commentary\n\nThis example is provided to show that the elements of vectors spaces need not be column vectors!\n:::\n\n## Example 2 of a vector space\n\nConsider the set ordered pairs:\n\n$\\mathbb{V} = \\{(x,y)\\,\\,|\\,\\, x,y\\in \\mathbb{R}\\}$\n\nAnd the maps:\n\n$$\n\\begin{align}\n+ : \\mathbb{V}\\times\\mathbb{V} &\\longrightarrow\\mathbb{V}\\\\\n(u,v)&\\longmapsto u+v := (u_x,u_y)+(v_x,v_y)=(u_x+v_x,u_y+v_y)\n\\end{align}\n$$\n\nand\n\n$$\n\\begin{align}\\cdot : \\mathbb{R}\\times\\mathbb{V} &\\longrightarrow\\mathbb{V}\\\\(\\alpha,u)&\\longmapsto \\alpha\\cdot u:=(\\alpha u_x,\\alpha u_y)\\end{align}\n$$\n\nOnce again the summation of vectors manifests as a entry-wise summation of numbers and the multiplication of a vector by a real number means the multiplication of its entries (real numbers) by the real number $\\alpha$. While before $\\cdot$ procedure was not specified, now it is, through the multiplication $\\cdot$ between numbers.\n\nNotice that when I write $(u_x,u_y)+(v_x,v_y)$ one may wonder what it means, we know it is another element of $\\mathbb{V}$, but which one? To answer that we have to tell how the operation $+$ between two elements of $\\mathbb{V}$ actually maps. Whatever the way it maps, it must be consistent with the rules defined above. It turn out that to define this $+$ as doing $(u_x+v_x,u_y+v_y)$ (a plus between reals) is a good and useful choice.\n\n# Linear functions\n\n::: {.callout-tip appearance=\"simple\"}\n## Video\n\n[How to check if a function is linear?](https://youtu.be/RuVKcV56CbM)\n:::\n\nConsider two generic vector spaces $\\mathbb{V}$ and $\\mathbb{W}$.\n\nA linear function are the set of links between elements of both vectors spaces, these links (i.e. pairs $(v,w)$) are constructed as follows:\n\n::: {#def-linear_func style=\"color:gray\"}\n$$\n\\begin{align}\nf:\\mathbb{V}&\\overset{\\sim}{\\longrightarrow} \\mathbb{W}\\\\\nv&\\longmapsto f(v)\n\\end{align}\n$$\n\nthe word linear, describes a property that $f$ must have: $f(a u+b v) = a f (u) + b f(v)$ for all $a,b \\in \\mathbb{R}$ and $u,v\\in \\mathbb{V}$.\n\n*Terminology:* The vector space $\\mathbb{V}$ is the domain of the function $f$, while $\\mathbb{W}$ is the codomain (of $f$)\n\nWhen referring to a linear function we will use $\\sim$ and write: $f:\\mathbb{V}\\overset{\\sim}{\\longrightarrow} \\mathbb{W}$.\n:::\n\n\\[*Commentary:* Many function which are useful in practice have this property. Hence the definition.\\] Here are some examples of $f$'s and the tests for linearity property.\n\n## Example 1\n\nStart with the vectors spaces of polynomials of degree $2$ and $1$. The procedure $\\frac{d}{dx}$ defines the following map:\n\n$$\n\\begin{align}f:\\mathbb{P}_2&\\longrightarrow \\mathbb{P}_1,\\qquad  a_2x^2+a_1x+a_0\\longmapsto f(a_2x^2+a_1x+a_0):=\\frac{d}{dx}(a_2x^2+a_1x+a_0)=2a_2x+a_1\\end{align}\n$$\n\nwhich is clearly linear.\n\n## Example 2\n\nStill the same vector spaces but this time the map is:\n\n$$\n\\begin{align}f:\\mathbb{P}_2&\\longrightarrow \\mathbb{P}_1,\\qquad  a_2x^2+a_1x+a_0\\longmapsto f(a_2x^2+a_1x+a_0):=(a_2-a_1)x+(a_1+a_0)\\end{align}\n$$\n\nIt is linear as well!\n\n## Example 4\n\nNow a function from $\\mathbb{R}$ into $\\mathbb{R}^2$, which maps as follows:\n\n$$ \\begin{align}g:\\mathbb{R}&\\longrightarrow \\mathbb{R}^2\\\\ x&\\longmapsto g(x):=(2x,x)\\end{align} $$\n\nThe procedure on which the map is based is to each $x$ compute $2x$ and then write $(2x,x)$. Is this procedure linear?\n\n$$ g(c_1 x_1+ c_2 x_2)=(2(c_1 x_1+ c_2 x_2),c_1 x_1+ c_2 x_2) = c_1(2x_1+,x_1)+c_2(2x_2,x_2)=c_1g(x_1)+c_2g(x_2) $$\n\nBecause our choices where arbitrary, yes, it is linear.\n\n## Example 5:\n\nThe projection function procedure consists in picking out the $x$ component of the vector $(x,y)$:\n\n$$ \\begin{align}\\pi:\\mathbb{R}^2&\\longrightarrow \\mathbb{R}\\\\ (x,y)&\\longmapsto \\pi(x,y):=x\\end{align} $$\n\nIs it linear?\n\n$$ \\pi(c_1(x_1,y_1)+c_2(x_2,y_2)) = \\pi (c_1x_1+c_2x_2,c_1y_1+c_2y_2)=c_1x_1+c_2x_2 =c_1 \\pi(x_1,y_1) + c_2\\pi(x_2,y_2) $$\n\nA l.c. of inputs $(x_1,y_1)$ and $(x_2,y_2)$ yields the same l.c. of outputs $\\pi(x_1,y_1)$ and $\\pi(x_2,y_2)$.\n\n## Example 6:\n\nFrom $\\mathbb{R}^2$ to $\\mathbb{R}^2$:\n\n$$ \\begin{align}\\Phi:\\mathbb{R}^2&\\longrightarrow \\mathbb{R}^2\\\\ (x,y)&\\longmapsto \\Phi(x,y):=(2x-y,x+y)\\end{align} $$\n\nIs it?\n\n$$ \\begin{align} \\Phi(c_1(x_1,y_1)+c_2(x_2,y_2)) &= \\Phi(c_1x_1+c_2x_2,c_1y_1+c_2y_2)\\\\  &=(2(c_1x_1+c_2x_2)-(c_1y_1+c_2y_2),c_1x_1+c_2x_2+c_1y_1+c_2y_2)\\\\ &=c_1(2x_1-y_1,x_1+y_1)+c_2(2x_2-y_2,x_2+y_2)\\\\ &=c_1\\Phi(x_1,y_1)+c_2\\Phi(x_2,y_2) \\end{align} $$\n\nYes.\n\n::: callout-note\n## Commentary\n\nIt is usual to give a more break down version of the rule $f(a u+b v) = a f (u) + b f(v)$:\n\n-   $f(u+v)=f(u)+f(v)$\n\n-   $f(b u)=b f(v)$\n\nBecause, if you know these then $f(a u+b v) = a f (u) + b f(v)$ comes as natural consequence. So just checking for these two suffices.\n:::\n\n# The Matrix representation of a linear function\n\n::: {.callout-tip appearance=\"simple\"}\n## Video\n\n[From linear function a matrix is born](https://youtu.be/ZLeRc4rjUZ0)\n:::\n\nGiven a function $f$, for example $f(x,y)=(2x-y,x+y)$, what is its matrix representation?\n\n**How:**\n\n1.  Choose a basis for the domain and a basis for the codomain;\n2.  Compute the output of $f$ when it act on the basis of the domain and then write the output in terms of the basis of the codomain.\n3.  With 2. we can now act with the matrix on any vector of the domain, which is written wrt to that basis.\n\nLets follow these steps.\n\n## Step 1: Choose a basis for the domain and codomain\n\nAs said above we have to chose a basis for $\\mathbb{V}$, let it be:\n\n$$\n\\mathbf{B}=(v_1, v_2, .. , v_n)\n$$ {#eq-basis_of_domain}\n\nThe next step is to choose a basis for the codomain $\\mathbb{W}$.\n\n$$\n\\mathbf{C}=(w_1,w_1,..,w_m)\n$$ {#eq-basis_of_codomain}\n\nChoosing a basis allow us to express any element of these spaces in terms of the basis, therefore:\n\n-   If $v\\in\\mathbb{V}$ it has the form:\n\n$$ v=\\sum_{j=1}^n x_j v_j = \\mathbf{B}X,\\qquad X=\\begin{pmatrix}x_1\\\\\\vdots\\\\x_n\\end{pmatrix} $$ {#eq-v_repre}\n\n-   If $w\\in\\mathbb{W}$ it has the form:\n\n$$ w=\\sum_{i=1}^m y_i w_i = \\mathbf{C}Y,\\qquad Y=\\begin{pmatrix}x_1\\\\\\vdots\\\\x_m\\end{pmatrix} $$ {#eq-w_repre}\n\n### Example\n\nFor the function $f:\\mathbb{R}^2\\longrightarrow \\mathbb{R}^2$ given by $f(x,y)=(2x-y,x+y)$ we have $\\mathbb{V}=\\mathbb{W}=\\mathbb{R}^2$ . Choose the basis:\n\n$$\nv_1:=(1,-4),v_2:=(0,1)\\qquad w_1:=(1,0),w_2:=(1,1)\n$$ {#eq-chosen_bases}\n\nI would like to stress the fact that this is my choice, we can make other choices (as we shall see) of bases for $\\mathbb{V}$ and $\\mathbb{W}$.\n\nThus any vector $v$ of the domain is expressed as:\n\n$$\nv=\\begin{pmatrix}v_1 & v_2 \\end{pmatrix}\\begin{pmatrix}x_1\\\\x_2\\end{pmatrix}\n$$ {#eq-repre_in_V}\n\nand any $w$ from the codomain can be written as:\n\n$$\nw=\\begin{pmatrix}w_1 & w_2 \\end{pmatrix}\\begin{pmatrix}y_1\\\\y_2\\end{pmatrix}\n$$ {#eq-repre_in_W}\n\nHow to compute $x_1, x_2, y_1, y_2$ for given $v,w\\in\\mathbb{R}^2$ ?\n\nSubstitute into the lhs of @eq-repre_in_V our $v=(\\alpha,\\beta)$ and on the rhs our choice of $v_1$ and $v_2$ from @eq-chosen_bases:\n\n$$\n\\begin{pmatrix}\\alpha\\\\\\beta\\end{pmatrix}=\\begin{pmatrix}1 & 0\\\\-4 & 1 \\end{pmatrix}\\begin{pmatrix}x_1\\\\x_2\\end{pmatrix}\n$$ {#eq-repre_in_V_II}\n\nthen solve for $x_1$ and $x_2$.\n\nSimilarly, given $w=(\\mu,\\nu)^\\intercal$, @eq-repre_in_W becomes:\n\n$$\n\\begin{pmatrix}\\mu\\\\\\nu\\end{pmatrix}=\\begin{pmatrix}1 & 1\\\\0 & 1 \\end{pmatrix}\\begin{pmatrix}y_1\\\\y_2\\end{pmatrix}\n$$ {#eq-repre_in_W_II}\n\n## Step 2: Compute how $f$ acts on the domain's basis\n\nAct with $f$ on the chosen basis vectors $v_j$, obtaining $f(v_j)$.\n\nFor the particular example we are considering we find:\n\n$$\nf(v_1)=f(1,-4)=(2\\cdot 1-(-4),1-4)=(6,-3)\\qquad f(v_2)=f(0,1)=(2\\cdot 0-1,0+1)=(-1,1)\n$$\n\nBut since these outputs live in $\\mathbb{W}=\\mathbb{R}^2$ they can be written as an appropriate l.c. of the basis $w_1$ and $w_2$, see @eq-repre_in_W_II:\n\n$$\n\\begin{align}\nf(v_1)&=(6,-3)=9(1,0)-3(1,1)=9w_1-3w_2\\\\\nf(v_2)&=(-1,1)=-2(1,0)+1(1,1)=-2w_1+w_2\n\\end{align}\n$$ {#eq-f_onbasis}\n\nThe rhs of these formulas have the form:\n\n$$\nf(v_j) = \\sum_{i=1}^m A_{ij}\\,\\,w_i\n$$ {#eq-repre_of_outputs}\n\nthe coefficients are conveniently named as $A_{ij}$. See @eq-repre_of_outputs as a special case of the second equation in @eq-repre_in_W for the basis vectors.\n\nComparing @eq-repre_of_outputs and @eq-f_onbasis we see that:\n\n$$\n\\begin{matrix}\nA_{11} = 9 & A_{12}=-2\\\\\nA_{21} = -3 & A_{22} = 1\n\\end{matrix}\n$$\n\nThe numbers $A_{ij}$ tell us how much $f(v_j)$ looks like $w_i$ just as the $y_i$'s did in @eq-repre_in_W. Of course, unlike the $y_i$'s which were specific for the $w$ vector, the $A_{ij}$'s have an additional index $j$ to remind us that we are currently expressing $f(v_j)$. As a result $i=1,2,..,m$ and $j=1,2,..,n$, where $m=\\dim \\mathbb{W}$ is the dimension of the codomain and $n=\\dim \\mathbb{V}$ is that of the domain. In the present example both $n=m=2$.\n\nWe can write the $n$ results in @eq-repre_of_outputs in a compact manner as:\n\n$$ (f(v_1),..,f(v_n))=:f(\\mathbf{B})=\\mathbf{C}A $$ {#eq-fv}\n\nwhere the $A$ is a matrix with entries $A_{ij}$.\n\n@eq-fv for our particular case turns out to be:\n\n$$\n(f(v_1),f(v_2))=:f(\\mathbf{B})=\\begin{pmatrix}w_1 & w_2 \\end{pmatrix}\\begin{pmatrix}9 & -2\\\\-3 & 1\\end{pmatrix}=\\begin{pmatrix} 9w_1-3w_2 & -2w_1+w_2\\end{pmatrix}\n$$ {#eq-map_of_basis}\n\n## Step 3: Mapping a generic vector\n\nWe can act with $f(x,y)=(2x-y,x+y)$ on any vector of $\\mathbb{V}=\\mathbb{R}^2$, no problem, for example, acting with $f$ on $v=(1,3)$ gives us:\n\n$$\nf(1,3)=(2\\cdot 1-3,1+3)=(-1,4)\n$$ {#eq-eg_of_calc}\n\nHowever, what we want is to do something quiet different from the calculation @eq-eg_of_calc, which is to represent how $f$ acts on any element of $\\mathbb{V}$ expressed in terms of $\\mathbf{B}$ and return the output expressed in terms of the chosen $\\mathbf{C}$. Step 1 and 2 allow us to do that.\n\nWe express $v$ in terms of $\\mathbf{B}$ which is:\n\n$$\nv=\\begin{pmatrix}v_1 & v_2 \\end{pmatrix} \\begin{pmatrix}x_1\\\\x_2\\end{pmatrix}\\qquad\\text{with}\\qquad x_1=1\\,\\,\\, x_2=7\n$$\n\nAct with $f$ on it:\n\n$$  f\\left(\\begin{pmatrix}v_1 & v_2 \\end{pmatrix}\\begin{pmatrix}1\\\\7\\end{pmatrix}\\right)=f\\begin{pmatrix}v_1 & v_2 \\end{pmatrix}\\begin{pmatrix}1\\\\7\\end{pmatrix}=\\begin{pmatrix}w_1 & w_2 \\end{pmatrix}\\begin{pmatrix}9 & -2\\\\-3 & 1\\end{pmatrix}\\begin{pmatrix}1\\\\7\\end{pmatrix} = \\begin{pmatrix}w_1 & w_2 \\end{pmatrix}\\begin{pmatrix}-5\\\\4\\end{pmatrix} $$ {#eq-action_of_f}\n\nwhere in the first step we used linearity and in the second step we used how $f$ acts on the basis, see @eq-map_of_basis.\n\nThe vector $\\mathbf{B}\\begin{pmatrix}1\\\\7\\end{pmatrix}$ is mapped into the vector $\\mathbf{C}\\begin{pmatrix}-5\\\\4\\end{pmatrix}$ by the function $f$. As we can check, the output is the $(-1,4)$ we had in @eq-eg_og_calc is just the rhs of @eq-action_of_f:\n\n$$\n\\begin{pmatrix}w_1 & w_2 \\end{pmatrix}\\begin{pmatrix}-5\\\\4\\end{pmatrix} =-5w_1+4w_2 =  -5\\begin{pmatrix}1\\\\0 \\end{pmatrix}+4 \\begin{pmatrix}1\\\\1 \\end{pmatrix}=\\begin{pmatrix}-1\\\\4 \\end{pmatrix}\n$$\n\nIn general formula @eq-action_of_f can be compactly written as:\n\n$$\nf(v)=f(\\mathbf{B}X)=f(\\mathbf{B})X=\\mathbf{C}AX\n$$ {#eq-CAX}\n\n::: {.callout-important collapse=\"True\" appearance=\"minimal\"}\n## Details\n\nThe @eq-CAX is an abbreviation for:\n\n$$\n\\begin{align}f(v)&=f(\\sum_{j=1}^m x_jv_j)\\\\&=\\sum_{j=1}^m x_jf(v_j)\\\\&=\\sum_{j=1}^m x_j\\sum_{i=1}^n A_{ij}\\,\\,w_i\\\\&=\\sum_{i=1}^n\\left(\\sum_{j=1}^m A_{ij}x_j\\right)w_i\\end{align}\n$$\n\nwhere in the last line is the expanded version of $\\mathbf{C} AX$.\n:::\n\n# What does steps 1. 2. 3. above have to do with $Ax=b$?\n\nStarting from @eq-CAX, with further calculation, we can see the connection.\n\nSince $f(v)$ is in the codomain $\\mathbb{W}$, then it is some l.c. of the $w$'s, recall @eq-w_repre. Therefore:\n\n$$\nf(v)=\\sum_{i=1}^n y_i w_i =\\mathbf{C}Y, \\qquad Y=\\begin{pmatrix}y_1\\\\\\vdots\\\\y_n\\end{pmatrix}\n$$ {#eq-general_repre}\n\nComparing @eq-general_map with @eq-CAX:\n\n$$\n\\mathbf{C}Y = \\mathbf{C}A X\n$$ {#eq-almost_Axequaly}\n\nWhich implies:\n\n$$\nY=AX\n$$\n\nWhich is our $A\\mathbf{x} =\\mathbf{b}$ we have dealing with.\n\n::: {.callout-important collapse=\"True\" appearance=\"minimal\"}\n## Details\n\n@eq-almost_Axequaly is:\n\n$$\n\\begin{align}\n\\sum_{i=1}^m y_iw_i=\\sum_{i=1}^m\\left(\\sum_{j=1}^n A_{ij}x_j\\right)w_i\\\\\n\\implies y_i=\\sum_{j=1}^n A_{ij}x_j\n\\end{align}\n$$\n:::\n\nThis calculation tell us something important: from the coefficients of $v$ on the chosen basis of the domain of $f$, they give us the coefficients of $f(u)$ in the chosen basis of the codomain. Lets read better the meaning of this formula, the $x$'s tell us how much the input vector $v$ looks like to each one of the basis vectors $v$. The numbers $A_{ij}$ quantify how much a basis vector $f(v_j)$ look like the basis vector $w_j$. This means that the calculation $\\sum_{j=1}^n A_{ij}x_j$ can be understood like:\n\n**\\[**how much $f(v)$ looks like $w_i$**\\]** = **\\[**how much $f(v_1)$ looks like $w_i$**\\]** $\\times$ **\\[**how much $v$ looks like $v_1$**\\]** $+$ \\[how much $f(v_2)$ looks like $w_i$**\\]** $\\times$ **\\[**how much $\\mathbf{u}$ looks like $v_2$**\\]** $+\\dots +$ **\\[**how much $f(v_n)$ looks like $w_i$**\\]** $\\times$ **\\[**how much $v$ looks like $v_n$**\\]**\n\nIts a weighted sum of the representation of $v$!\n\nThis understanding is clear because we know exactly what basis we are speaking off, after all, the first step was to make the choice of bases.\n\n::: callout-note\n## Commentary\n\nI these sections we have started with a specific function $f$, specific sets $\\mathbb{V}$ and $\\mathbb{W}$ and bases. Notice however that the only essential ingredients for the whole derivation is to know how $f$ acting on the basis of the domain is expressed in the basis of the codomain! Knowing this together with the property $f(au+\\mu bv) = a f (u) + b f(v)$ allowed us to recast the procedures $f$ in a matrix form. These three steps are based on the idea that if we know how a linear function $f$ maps the chosen basis of the domain, we know how it maps any vector of the domain. And this is possible because $f$ has the linear property.\n:::\n\n# Isomorphism and stuff\n\nThe hypervector $\\mathbf{B}$ (and $\\mathbf{C}$) allow us to make a bijective correspondence between vectors in $\\mathbb{R}^n$ (and $\\mathbb{R}^m$) and vectors in $\\mathbb{V}$ (and $\\mathbb{W}$). A vector $X\\in\\mathbb{R}^n$ is transformed into $v\\in\\mathbb{V}$ by dotting the column vector $X$ with the hypervector $\\mathbf{B}$:\n\n$$\nv=\\mathbf{B}X\n$$\n\nsame thing with $Y$ and $w$:\n\n$$\nw=\\mathbf{C}Y\n$$\n\nWe picture this correspondence by the vertical arrows in the following picture:\n\n![](images/clipboard-690894529.png){width=\"402\"}\n\nSubspaces in $\\mathbb{R}^n$ are translated into subspaces of $\\mathbb{V}$, this has important consequences. If $\\mathbb{R}^n=\\alpha\\oplus\\beta$ is decomposed by a direct sum of subspaces $\\alpha$ and $\\beta$, then multiplying the vectors in the them by the hypervector $\\mathbf{B}$ yields vector spaces in $\\mathbb{V}=(\\mathbf{B}\\alpha) \\oplus(\\mathbf{B}\\beta)$. \\[Warning: if $\\beta=\\alpha^\\perp$ it may happen that it is *not* the case that$(\\mathbf{B}\\beta)=(\\mathbf{B}\\alpha)^\\perp$.\\]\n\nNow, introduce a map $f$ between elements of $\\mathbb{V}$ and $\\mathbb{W}$, as was our $f(x,y)=(2x-y,x+y)$, this is represented by the bottom arrow. Since the elements $v$ are being mapped into $w$ and both can be translated uniquely into $X$ and $Y$, we naturally expect that there is also map, called $A$, that directly connects $X$ with $Y$.\n\nIf $f$ links $\\mathbf{B}X$ with $\\mathbf{C}Y$, then $A$ links $X$ with $Y$, how? Answer: $Y=AX$.\n\nSee the diagram below:\n\n![](images/clipboard-3851920532.png){width=\"321\"}\n\nWe have been talking about maps of individual vectors, but we can also do it for subspaces. A subspace $V\\subset_{vec}\\mathbb{V}$ is mapped by $f$ into the subspace$f(V)\\subset_{vec} \\mathbb{W}$. Here is an intuitive justification: if $e_1$ and $e_2$ are a basis for $V$, then any l.c. of them gives us a vector in $V$, this is expected, after all $V$ is a vector space. But is it true that any l.c. of $f(e_1)$ and $f(e_2)$ is also a vector of $f(V)$? Well, a generic vector in $f(V)$ must have the form $f(ae_1+be_2)$, but since $f$ is linear this in turn means $af(e_1)+bf(e_2)$, but this just tells us that any element of $f(V)$ is a l.c. of $f(e_1)$ and $f(e_2)$.\n\n**Thus:** *a linear function* $f$ *maps subspaces of* $\\mathbb{V}$ *into subspaces of* $\\mathbb{W}$*.*\n\nAnd since subspaces of $\\mathbb{V}$ and $\\mathbb{W}$ in turn have a version in $\\mathbb{R}^n$ and $\\mathbb{R}^m$, the matrix $A$ maps those as well.\n\n**Thus:** *a linear function* $f$ *maps subspaces of* $\\mathbb{V}$ *into subspaces of* $\\mathbb{W}$*. And correspondingly, its matrix representation* $A$ *maps subspaces of* $\\mathbb{R}^n$ *into the corresponding subspaces of* $\\mathbb{R}^m$*.*\n\n## Revisiting Two important subspaces\n\nIn the domain and codomain of a map $f:\\mathbb{V}\\overset{\\sim}{\\longrightarrow}\\mathbb{W}$ we find two important subspaces.\n\n### Kernel of a linear function\n\n::: {#def-kernel style=\"color:gray\"}\nLet $f:\\mathbb{V}\\overset{\\sim}{\\longrightarrow} \\mathbb{W}$, then the kernel of $f$ is the subset of the domain $\\mathbb{V}$:\n\n$$ \\ker f = \\{v_N\\in\\mathbb{V}\\,\\,|\\,\\,f(v_N)=0\\} $$\n\nMoreover, it is in fact a subspace.\n:::\n\nWith basis introduced in both spaces we can write this set as:\n\n$$ \\ker f = \\{\\mathbf{B}X_N\\in\\mathbb{V}\\,\\,|\\,\\,\\mathbf{C}AX_N=\\mathbf{C}0\\} $$\n\nAfter solving $AX_N=0$ we multiply the solutions by $\\mathbf{B}$, giving us $\\ker f$.\n\nNotice that the solutions of $AX_N=0$ constitute the nullspace of the matrix $A$. The difference between $N(A)$ and $\\ker f$ is a multiplication by $\\mathbf{B}$\n\n$$ \\ker f = \\mathbf{B} N(A) $$\n\nSee the diagram above.\n\n### Image of the domain (range of linear functions)\n\n::: {#def-range style=\"color:gray\"}\nLet $f:\\mathbb{V}\\overset{\\sim}{\\longrightarrow} \\mathbb{W}$, then the range (image) of $f$ is the following subset of the domain $\\mathbb{W}$:\n\n$$ f(\\mathbb{V}) = \\{f(u)\\in\\mathbb{W}\\,\\,|\\,\\,u\\in\\mathbb{V}\\} $$\n\nMoreover, it is also a subspace.\n:::\n\nUpon introducing a basis, this problem can be reformulated as:\n\n$$ f(\\mathbb{V}) = \\{\\mathbf{C}AX\\in\\mathbb{W}\\,\\,|\\,\\,X\\in\\mathbb{R}^{\\dim \\mathbb{V}}\\} $$\n\nTo compute this vector space we find first the column space of $A$ and then multiply it by $\\mathbf{C}$ get us the image!\n\n$$ f(\\mathbb{V})=\\mathbf{C} C(A) $$\n\nSee the diagram above.\n\n::: callout-note\n## Commentary\n\nWe know that:\n\n-   $\\mathbb{R}^n=C(A^\\intercal) \\oplus N(A)$ which translate into $\\mathbb{V}=(\\mathbf{B}C(A^\\intercal)) \\oplus (\\mathbf{B}N(A))$\n\n-   $\\mathbb{R}^m=C(A) \\oplus N(A^\\intercal)$ which translates into $\\mathbb{W}=(\\mathbf{C}C(A)) \\oplus (\\mathbf{C}N(A^\\intercal))$\n\n-   $C(A^\\intercal) \\perp N(A)$ and that $C(A) \\perp N(A)^\\intercal$, always! But the same is not true after multiplying by $\\mathbf{B}$ and $\\mathbf{C}$, unless the chosen basis $\\mathbf{B}$ and $\\mathbf{C}$ are orthogonal (eg. canonical basis).\n:::\n","srcMarkdownNoYaml":"\n\nUp to now, we learned a more practical part of the course:\n\n-   How to compute the solution of $A\\mathbf{x}=\\mathbf{b}$?\n\n-   How to compute a basis for the $4$ subspaces associated with $A$?\n\n-   How to compute the inverse $A^{-1}$?\n\n-   How to compute the determinant?\n\nIt always *how to compute*.\n\nNow we enter a more conceptual part of the course and organize ideas.\n\n# The concept of a vector space\n\nWe have been speaking about a vector space as being closed under linear combinations. However if we stop to think about it, we can only compute a linear combination provided we know how to multiply vectors by numbers and how to add vectors, see the linear combination below:\n\n$$\n\\overbrace{\\alpha u}^\\text{number times a vector} + \\overbrace{\\beta v}^\\text{number times a vector}=\\underbrace{u'+v'}_\\text{sum of vectors}\n$$\n\nLets look more carefully at two questions:\n\n-   What is a sum of two vectors?\n\n-   What is a vector times a number?\n\nAnswering these questions is important because the concept of linear combination and of vector space we adopted so far - a set of things closed under linear combinations - hinges on answering these two questions.\n\nStart with a set of $n$ things with labels $v_1$, $v_2$, ..., $v_n$:\n\n$$\n\\mathbb{V}=\\{v_1,v_2,\\dots,v_n\\}\n$$\n\nWhat can we do with these things in this set?\n\nWe can define a map, i.e., an assignment or a function, from the set onto itself. There are two specific kinds of maps that are relevant to those questions:\n\n-   A function which to each pair of elements of $\\mathbb{V}$ assigns another element of $\\mathbb{V}$, in math language we write:$$\n    \\begin{align}\n    + : \\mathbb{V}\\times\\mathbb{V} &\\longrightarrow\\mathbb{V}\\\\\n    (u,v)&\\longmapsto u+v\n    \\end{align}\n    $$ {#eq-plus}\n\n    This is relevant for the first question.\n\n-   A function which to each $\\mathbb{V}$ and a real number assigns another element of $\\mathbb{V}$, in math language:\n\n    $$\n    \\begin{align}\\cdot : \\mathbb{R}\\times\\mathbb{V} &\\longrightarrow\\mathbb{V}\\\\(\\alpha,v)&\\longmapsto \\alpha\\cdot v\\end{align}\n    $$ {#eq-times} is relevant for the second question.\n\nThe $u+v$ is the symbol for the element in $\\mathbb{V}$ that corresponds to the pair $(u,v)$. And $\\alpha\\cdot v$ is the symbol for the element of $\\mathbb{V}$ assigned to $(\\alpha,v)$.\n\nWe wish additionally that those maps to have the properties:\n\n-   Commutative: $u+v=v+u$. \\[*You can flip the order*\\]\n\n-   Associative: $(u+v)+w=u+(v+w)$. \\[*You can shift the brackets*\\]\n\n-   Neutral element (wrt to $+$): There is an element in $\\mathbb{V}$, call it $o$ so that for all $u\\in \\mathbb{V}$ we find $u+o=u$. \\[*This is not* $0$ *from the real numbers, it is the zero of* $\\mathbb{V}$*. To avoid confusion you can call* $o$ *as circle rather than zero.*\\]\n\n-   Inverse element (wrt to $+$): For every element $u$ there an element, which we denote by $-u$, that guarantees $u+(-u)=o$. \\[ $u-v$ *is just a short notation for* $u+(-v)$.\n\nThe next list of rules describe how the operations $+$ and $\\cdot$ interact when both present in a calculation, two of this rules are distributive rules:\n\n-   Associative: $\\alpha(\\beta u)=(\\alpha\\beta)v$\n\n-   Distributive I: $(\\alpha+\\beta)u = \\alpha u +\\beta u$\n\n-   Distributive II: $\\alpha (u +v) = \\alpha u + \\alpha v$\n\n-   Unitary: $1 u = u$\n\nWhen looking at @eq-plus, @eq-times and the CANI-ADDU rules note we did not mention what are the elements of $\\mathbb{V}$ and neither did we mention how to compute the assignment, the only thing stated is that $+$, whatever it ends up being in practice, it maps two elements of $\\mathbb{V}$ into another element of $\\mathbb{V}$ and that, whatever the dot $\\cdot$ ends up meaning in practice, it must assign to a real number and an element $\\mathbb{V}$ another element of $\\mathbb{V}$. All this while, simultaneously all the above rules are obeyed. So, in practice, the $+$ and $\\cdot$ are really special operation, such is the number of constraints imposed.\n\nA vector space is then a triplet $(\\mathbb{V},+,\\cdot)$\n\n::: callout-note\n## Commentaries\n\n-   Why these two maps with these rules? Because on them fit many useful maps in practice. The list of properties is like describing someone: you can list all its characteristics, or just list the essential ones. Here essential depends for what the description is useful for. For example, your CV works for the purpose of getting a job.\n\n-   There are many constraints (CANI and ADDU) on the operation $+$ and $\\cdot$, but all these boil down to make $\\mathbb{V}$ being closed under l.c., where the l.c. is realized in practice the natural and expected way. The constraints above are the rules of common sense for l.c.\n\n-   The set of assignments made by $+$ and $\\cdot$ which we can write as $\\{(u,v,u+v)\\}$ and $\\{(c,u,cu)\\}$ is what we call the structure of the vector space.\n:::\n\nWhat specific sets and $+$ and $\\cdot$ operations fit the profile above? Here are some examples:\n\n## Example 1:\n\nThe set of all polynomials of degree $2$:\n\n$$\n\\mathbb{P}=\\{p:\\mathbb{R}\\rightarrow\\mathbb{R}; x\\mapsto a_2 x^2 +a_1 x +a_0\\,\\,|\\,\\, a_i \\text{'s are real}\\}\n$$\n\nTogether with:\n\n$$\n\\begin{align}+ : \\mathbb{P}\\times\\mathbb{P} &\\longrightarrow\\mathbb{V}\\\\(p,q)&\\longmapsto (p+q)(x)=p(x)+q(x)\\end{align}\n$$\n\nHere, the $+$ function of functions $p$ and $q$ is defined from the summation $+$ of real numbers $p(x)$ and $q(x)$. While before $+$ did not have meaning now it has in terms of the $+$ between numbers.\n\n$$\n\\begin{align}\\cdot : \\mathbb{R}\\times\\mathbb{P} &\\longrightarrow\\mathbb{V}\\\\(\\alpha,p)&\\longmapsto (\\alpha\\cdot p)(x):=\\alpha\\cdot p(x)\\end{align}\n$$\n\nThe multiplication between a real number and the function $p$ is understood in therms of the multiplication between the real numbers $\\alpha$ and $p(x)$.\n\nAll the rules above can be checked for this example.\n\nThe natural basis for this vector space is: $\\{1,x,x^2\\}$\n\n::: callout-note\n## Commentary\n\nThis example is provided to show that the elements of vectors spaces need not be column vectors!\n:::\n\n## Example 2 of a vector space\n\nConsider the set ordered pairs:\n\n$\\mathbb{V} = \\{(x,y)\\,\\,|\\,\\, x,y\\in \\mathbb{R}\\}$\n\nAnd the maps:\n\n$$\n\\begin{align}\n+ : \\mathbb{V}\\times\\mathbb{V} &\\longrightarrow\\mathbb{V}\\\\\n(u,v)&\\longmapsto u+v := (u_x,u_y)+(v_x,v_y)=(u_x+v_x,u_y+v_y)\n\\end{align}\n$$\n\nand\n\n$$\n\\begin{align}\\cdot : \\mathbb{R}\\times\\mathbb{V} &\\longrightarrow\\mathbb{V}\\\\(\\alpha,u)&\\longmapsto \\alpha\\cdot u:=(\\alpha u_x,\\alpha u_y)\\end{align}\n$$\n\nOnce again the summation of vectors manifests as a entry-wise summation of numbers and the multiplication of a vector by a real number means the multiplication of its entries (real numbers) by the real number $\\alpha$. While before $\\cdot$ procedure was not specified, now it is, through the multiplication $\\cdot$ between numbers.\n\nNotice that when I write $(u_x,u_y)+(v_x,v_y)$ one may wonder what it means, we know it is another element of $\\mathbb{V}$, but which one? To answer that we have to tell how the operation $+$ between two elements of $\\mathbb{V}$ actually maps. Whatever the way it maps, it must be consistent with the rules defined above. It turn out that to define this $+$ as doing $(u_x+v_x,u_y+v_y)$ (a plus between reals) is a good and useful choice.\n\n# Linear functions\n\n::: {.callout-tip appearance=\"simple\"}\n## Video\n\n[How to check if a function is linear?](https://youtu.be/RuVKcV56CbM)\n:::\n\nConsider two generic vector spaces $\\mathbb{V}$ and $\\mathbb{W}$.\n\nA linear function are the set of links between elements of both vectors spaces, these links (i.e. pairs $(v,w)$) are constructed as follows:\n\n::: {#def-linear_func style=\"color:gray\"}\n$$\n\\begin{align}\nf:\\mathbb{V}&\\overset{\\sim}{\\longrightarrow} \\mathbb{W}\\\\\nv&\\longmapsto f(v)\n\\end{align}\n$$\n\nthe word linear, describes a property that $f$ must have: $f(a u+b v) = a f (u) + b f(v)$ for all $a,b \\in \\mathbb{R}$ and $u,v\\in \\mathbb{V}$.\n\n*Terminology:* The vector space $\\mathbb{V}$ is the domain of the function $f$, while $\\mathbb{W}$ is the codomain (of $f$)\n\nWhen referring to a linear function we will use $\\sim$ and write: $f:\\mathbb{V}\\overset{\\sim}{\\longrightarrow} \\mathbb{W}$.\n:::\n\n\\[*Commentary:* Many function which are useful in practice have this property. Hence the definition.\\] Here are some examples of $f$'s and the tests for linearity property.\n\n## Example 1\n\nStart with the vectors spaces of polynomials of degree $2$ and $1$. The procedure $\\frac{d}{dx}$ defines the following map:\n\n$$\n\\begin{align}f:\\mathbb{P}_2&\\longrightarrow \\mathbb{P}_1,\\qquad  a_2x^2+a_1x+a_0\\longmapsto f(a_2x^2+a_1x+a_0):=\\frac{d}{dx}(a_2x^2+a_1x+a_0)=2a_2x+a_1\\end{align}\n$$\n\nwhich is clearly linear.\n\n## Example 2\n\nStill the same vector spaces but this time the map is:\n\n$$\n\\begin{align}f:\\mathbb{P}_2&\\longrightarrow \\mathbb{P}_1,\\qquad  a_2x^2+a_1x+a_0\\longmapsto f(a_2x^2+a_1x+a_0):=(a_2-a_1)x+(a_1+a_0)\\end{align}\n$$\n\nIt is linear as well!\n\n## Example 4\n\nNow a function from $\\mathbb{R}$ into $\\mathbb{R}^2$, which maps as follows:\n\n$$ \\begin{align}g:\\mathbb{R}&\\longrightarrow \\mathbb{R}^2\\\\ x&\\longmapsto g(x):=(2x,x)\\end{align} $$\n\nThe procedure on which the map is based is to each $x$ compute $2x$ and then write $(2x,x)$. Is this procedure linear?\n\n$$ g(c_1 x_1+ c_2 x_2)=(2(c_1 x_1+ c_2 x_2),c_1 x_1+ c_2 x_2) = c_1(2x_1+,x_1)+c_2(2x_2,x_2)=c_1g(x_1)+c_2g(x_2) $$\n\nBecause our choices where arbitrary, yes, it is linear.\n\n## Example 5:\n\nThe projection function procedure consists in picking out the $x$ component of the vector $(x,y)$:\n\n$$ \\begin{align}\\pi:\\mathbb{R}^2&\\longrightarrow \\mathbb{R}\\\\ (x,y)&\\longmapsto \\pi(x,y):=x\\end{align} $$\n\nIs it linear?\n\n$$ \\pi(c_1(x_1,y_1)+c_2(x_2,y_2)) = \\pi (c_1x_1+c_2x_2,c_1y_1+c_2y_2)=c_1x_1+c_2x_2 =c_1 \\pi(x_1,y_1) + c_2\\pi(x_2,y_2) $$\n\nA l.c. of inputs $(x_1,y_1)$ and $(x_2,y_2)$ yields the same l.c. of outputs $\\pi(x_1,y_1)$ and $\\pi(x_2,y_2)$.\n\n## Example 6:\n\nFrom $\\mathbb{R}^2$ to $\\mathbb{R}^2$:\n\n$$ \\begin{align}\\Phi:\\mathbb{R}^2&\\longrightarrow \\mathbb{R}^2\\\\ (x,y)&\\longmapsto \\Phi(x,y):=(2x-y,x+y)\\end{align} $$\n\nIs it?\n\n$$ \\begin{align} \\Phi(c_1(x_1,y_1)+c_2(x_2,y_2)) &= \\Phi(c_1x_1+c_2x_2,c_1y_1+c_2y_2)\\\\  &=(2(c_1x_1+c_2x_2)-(c_1y_1+c_2y_2),c_1x_1+c_2x_2+c_1y_1+c_2y_2)\\\\ &=c_1(2x_1-y_1,x_1+y_1)+c_2(2x_2-y_2,x_2+y_2)\\\\ &=c_1\\Phi(x_1,y_1)+c_2\\Phi(x_2,y_2) \\end{align} $$\n\nYes.\n\n::: callout-note\n## Commentary\n\nIt is usual to give a more break down version of the rule $f(a u+b v) = a f (u) + b f(v)$:\n\n-   $f(u+v)=f(u)+f(v)$\n\n-   $f(b u)=b f(v)$\n\nBecause, if you know these then $f(a u+b v) = a f (u) + b f(v)$ comes as natural consequence. So just checking for these two suffices.\n:::\n\n# The Matrix representation of a linear function\n\n::: {.callout-tip appearance=\"simple\"}\n## Video\n\n[From linear function a matrix is born](https://youtu.be/ZLeRc4rjUZ0)\n:::\n\nGiven a function $f$, for example $f(x,y)=(2x-y,x+y)$, what is its matrix representation?\n\n**How:**\n\n1.  Choose a basis for the domain and a basis for the codomain;\n2.  Compute the output of $f$ when it act on the basis of the domain and then write the output in terms of the basis of the codomain.\n3.  With 2. we can now act with the matrix on any vector of the domain, which is written wrt to that basis.\n\nLets follow these steps.\n\n## Step 1: Choose a basis for the domain and codomain\n\nAs said above we have to chose a basis for $\\mathbb{V}$, let it be:\n\n$$\n\\mathbf{B}=(v_1, v_2, .. , v_n)\n$$ {#eq-basis_of_domain}\n\nThe next step is to choose a basis for the codomain $\\mathbb{W}$.\n\n$$\n\\mathbf{C}=(w_1,w_1,..,w_m)\n$$ {#eq-basis_of_codomain}\n\nChoosing a basis allow us to express any element of these spaces in terms of the basis, therefore:\n\n-   If $v\\in\\mathbb{V}$ it has the form:\n\n$$ v=\\sum_{j=1}^n x_j v_j = \\mathbf{B}X,\\qquad X=\\begin{pmatrix}x_1\\\\\\vdots\\\\x_n\\end{pmatrix} $$ {#eq-v_repre}\n\n-   If $w\\in\\mathbb{W}$ it has the form:\n\n$$ w=\\sum_{i=1}^m y_i w_i = \\mathbf{C}Y,\\qquad Y=\\begin{pmatrix}x_1\\\\\\vdots\\\\x_m\\end{pmatrix} $$ {#eq-w_repre}\n\n### Example\n\nFor the function $f:\\mathbb{R}^2\\longrightarrow \\mathbb{R}^2$ given by $f(x,y)=(2x-y,x+y)$ we have $\\mathbb{V}=\\mathbb{W}=\\mathbb{R}^2$ . Choose the basis:\n\n$$\nv_1:=(1,-4),v_2:=(0,1)\\qquad w_1:=(1,0),w_2:=(1,1)\n$$ {#eq-chosen_bases}\n\nI would like to stress the fact that this is my choice, we can make other choices (as we shall see) of bases for $\\mathbb{V}$ and $\\mathbb{W}$.\n\nThus any vector $v$ of the domain is expressed as:\n\n$$\nv=\\begin{pmatrix}v_1 & v_2 \\end{pmatrix}\\begin{pmatrix}x_1\\\\x_2\\end{pmatrix}\n$$ {#eq-repre_in_V}\n\nand any $w$ from the codomain can be written as:\n\n$$\nw=\\begin{pmatrix}w_1 & w_2 \\end{pmatrix}\\begin{pmatrix}y_1\\\\y_2\\end{pmatrix}\n$$ {#eq-repre_in_W}\n\nHow to compute $x_1, x_2, y_1, y_2$ for given $v,w\\in\\mathbb{R}^2$ ?\n\nSubstitute into the lhs of @eq-repre_in_V our $v=(\\alpha,\\beta)$ and on the rhs our choice of $v_1$ and $v_2$ from @eq-chosen_bases:\n\n$$\n\\begin{pmatrix}\\alpha\\\\\\beta\\end{pmatrix}=\\begin{pmatrix}1 & 0\\\\-4 & 1 \\end{pmatrix}\\begin{pmatrix}x_1\\\\x_2\\end{pmatrix}\n$$ {#eq-repre_in_V_II}\n\nthen solve for $x_1$ and $x_2$.\n\nSimilarly, given $w=(\\mu,\\nu)^\\intercal$, @eq-repre_in_W becomes:\n\n$$\n\\begin{pmatrix}\\mu\\\\\\nu\\end{pmatrix}=\\begin{pmatrix}1 & 1\\\\0 & 1 \\end{pmatrix}\\begin{pmatrix}y_1\\\\y_2\\end{pmatrix}\n$$ {#eq-repre_in_W_II}\n\n## Step 2: Compute how $f$ acts on the domain's basis\n\nAct with $f$ on the chosen basis vectors $v_j$, obtaining $f(v_j)$.\n\nFor the particular example we are considering we find:\n\n$$\nf(v_1)=f(1,-4)=(2\\cdot 1-(-4),1-4)=(6,-3)\\qquad f(v_2)=f(0,1)=(2\\cdot 0-1,0+1)=(-1,1)\n$$\n\nBut since these outputs live in $\\mathbb{W}=\\mathbb{R}^2$ they can be written as an appropriate l.c. of the basis $w_1$ and $w_2$, see @eq-repre_in_W_II:\n\n$$\n\\begin{align}\nf(v_1)&=(6,-3)=9(1,0)-3(1,1)=9w_1-3w_2\\\\\nf(v_2)&=(-1,1)=-2(1,0)+1(1,1)=-2w_1+w_2\n\\end{align}\n$$ {#eq-f_onbasis}\n\nThe rhs of these formulas have the form:\n\n$$\nf(v_j) = \\sum_{i=1}^m A_{ij}\\,\\,w_i\n$$ {#eq-repre_of_outputs}\n\nthe coefficients are conveniently named as $A_{ij}$. See @eq-repre_of_outputs as a special case of the second equation in @eq-repre_in_W for the basis vectors.\n\nComparing @eq-repre_of_outputs and @eq-f_onbasis we see that:\n\n$$\n\\begin{matrix}\nA_{11} = 9 & A_{12}=-2\\\\\nA_{21} = -3 & A_{22} = 1\n\\end{matrix}\n$$\n\nThe numbers $A_{ij}$ tell us how much $f(v_j)$ looks like $w_i$ just as the $y_i$'s did in @eq-repre_in_W. Of course, unlike the $y_i$'s which were specific for the $w$ vector, the $A_{ij}$'s have an additional index $j$ to remind us that we are currently expressing $f(v_j)$. As a result $i=1,2,..,m$ and $j=1,2,..,n$, where $m=\\dim \\mathbb{W}$ is the dimension of the codomain and $n=\\dim \\mathbb{V}$ is that of the domain. In the present example both $n=m=2$.\n\nWe can write the $n$ results in @eq-repre_of_outputs in a compact manner as:\n\n$$ (f(v_1),..,f(v_n))=:f(\\mathbf{B})=\\mathbf{C}A $$ {#eq-fv}\n\nwhere the $A$ is a matrix with entries $A_{ij}$.\n\n@eq-fv for our particular case turns out to be:\n\n$$\n(f(v_1),f(v_2))=:f(\\mathbf{B})=\\begin{pmatrix}w_1 & w_2 \\end{pmatrix}\\begin{pmatrix}9 & -2\\\\-3 & 1\\end{pmatrix}=\\begin{pmatrix} 9w_1-3w_2 & -2w_1+w_2\\end{pmatrix}\n$$ {#eq-map_of_basis}\n\n## Step 3: Mapping a generic vector\n\nWe can act with $f(x,y)=(2x-y,x+y)$ on any vector of $\\mathbb{V}=\\mathbb{R}^2$, no problem, for example, acting with $f$ on $v=(1,3)$ gives us:\n\n$$\nf(1,3)=(2\\cdot 1-3,1+3)=(-1,4)\n$$ {#eq-eg_of_calc}\n\nHowever, what we want is to do something quiet different from the calculation @eq-eg_of_calc, which is to represent how $f$ acts on any element of $\\mathbb{V}$ expressed in terms of $\\mathbf{B}$ and return the output expressed in terms of the chosen $\\mathbf{C}$. Step 1 and 2 allow us to do that.\n\nWe express $v$ in terms of $\\mathbf{B}$ which is:\n\n$$\nv=\\begin{pmatrix}v_1 & v_2 \\end{pmatrix} \\begin{pmatrix}x_1\\\\x_2\\end{pmatrix}\\qquad\\text{with}\\qquad x_1=1\\,\\,\\, x_2=7\n$$\n\nAct with $f$ on it:\n\n$$  f\\left(\\begin{pmatrix}v_1 & v_2 \\end{pmatrix}\\begin{pmatrix}1\\\\7\\end{pmatrix}\\right)=f\\begin{pmatrix}v_1 & v_2 \\end{pmatrix}\\begin{pmatrix}1\\\\7\\end{pmatrix}=\\begin{pmatrix}w_1 & w_2 \\end{pmatrix}\\begin{pmatrix}9 & -2\\\\-3 & 1\\end{pmatrix}\\begin{pmatrix}1\\\\7\\end{pmatrix} = \\begin{pmatrix}w_1 & w_2 \\end{pmatrix}\\begin{pmatrix}-5\\\\4\\end{pmatrix} $$ {#eq-action_of_f}\n\nwhere in the first step we used linearity and in the second step we used how $f$ acts on the basis, see @eq-map_of_basis.\n\nThe vector $\\mathbf{B}\\begin{pmatrix}1\\\\7\\end{pmatrix}$ is mapped into the vector $\\mathbf{C}\\begin{pmatrix}-5\\\\4\\end{pmatrix}$ by the function $f$. As we can check, the output is the $(-1,4)$ we had in @eq-eg_og_calc is just the rhs of @eq-action_of_f:\n\n$$\n\\begin{pmatrix}w_1 & w_2 \\end{pmatrix}\\begin{pmatrix}-5\\\\4\\end{pmatrix} =-5w_1+4w_2 =  -5\\begin{pmatrix}1\\\\0 \\end{pmatrix}+4 \\begin{pmatrix}1\\\\1 \\end{pmatrix}=\\begin{pmatrix}-1\\\\4 \\end{pmatrix}\n$$\n\nIn general formula @eq-action_of_f can be compactly written as:\n\n$$\nf(v)=f(\\mathbf{B}X)=f(\\mathbf{B})X=\\mathbf{C}AX\n$$ {#eq-CAX}\n\n::: {.callout-important collapse=\"True\" appearance=\"minimal\"}\n## Details\n\nThe @eq-CAX is an abbreviation for:\n\n$$\n\\begin{align}f(v)&=f(\\sum_{j=1}^m x_jv_j)\\\\&=\\sum_{j=1}^m x_jf(v_j)\\\\&=\\sum_{j=1}^m x_j\\sum_{i=1}^n A_{ij}\\,\\,w_i\\\\&=\\sum_{i=1}^n\\left(\\sum_{j=1}^m A_{ij}x_j\\right)w_i\\end{align}\n$$\n\nwhere in the last line is the expanded version of $\\mathbf{C} AX$.\n:::\n\n# What does steps 1. 2. 3. above have to do with $Ax=b$?\n\nStarting from @eq-CAX, with further calculation, we can see the connection.\n\nSince $f(v)$ is in the codomain $\\mathbb{W}$, then it is some l.c. of the $w$'s, recall @eq-w_repre. Therefore:\n\n$$\nf(v)=\\sum_{i=1}^n y_i w_i =\\mathbf{C}Y, \\qquad Y=\\begin{pmatrix}y_1\\\\\\vdots\\\\y_n\\end{pmatrix}\n$$ {#eq-general_repre}\n\nComparing @eq-general_map with @eq-CAX:\n\n$$\n\\mathbf{C}Y = \\mathbf{C}A X\n$$ {#eq-almost_Axequaly}\n\nWhich implies:\n\n$$\nY=AX\n$$\n\nWhich is our $A\\mathbf{x} =\\mathbf{b}$ we have dealing with.\n\n::: {.callout-important collapse=\"True\" appearance=\"minimal\"}\n## Details\n\n@eq-almost_Axequaly is:\n\n$$\n\\begin{align}\n\\sum_{i=1}^m y_iw_i=\\sum_{i=1}^m\\left(\\sum_{j=1}^n A_{ij}x_j\\right)w_i\\\\\n\\implies y_i=\\sum_{j=1}^n A_{ij}x_j\n\\end{align}\n$$\n:::\n\nThis calculation tell us something important: from the coefficients of $v$ on the chosen basis of the domain of $f$, they give us the coefficients of $f(u)$ in the chosen basis of the codomain. Lets read better the meaning of this formula, the $x$'s tell us how much the input vector $v$ looks like to each one of the basis vectors $v$. The numbers $A_{ij}$ quantify how much a basis vector $f(v_j)$ look like the basis vector $w_j$. This means that the calculation $\\sum_{j=1}^n A_{ij}x_j$ can be understood like:\n\n**\\[**how much $f(v)$ looks like $w_i$**\\]** = **\\[**how much $f(v_1)$ looks like $w_i$**\\]** $\\times$ **\\[**how much $v$ looks like $v_1$**\\]** $+$ \\[how much $f(v_2)$ looks like $w_i$**\\]** $\\times$ **\\[**how much $\\mathbf{u}$ looks like $v_2$**\\]** $+\\dots +$ **\\[**how much $f(v_n)$ looks like $w_i$**\\]** $\\times$ **\\[**how much $v$ looks like $v_n$**\\]**\n\nIts a weighted sum of the representation of $v$!\n\nThis understanding is clear because we know exactly what basis we are speaking off, after all, the first step was to make the choice of bases.\n\n::: callout-note\n## Commentary\n\nI these sections we have started with a specific function $f$, specific sets $\\mathbb{V}$ and $\\mathbb{W}$ and bases. Notice however that the only essential ingredients for the whole derivation is to know how $f$ acting on the basis of the domain is expressed in the basis of the codomain! Knowing this together with the property $f(au+\\mu bv) = a f (u) + b f(v)$ allowed us to recast the procedures $f$ in a matrix form. These three steps are based on the idea that if we know how a linear function $f$ maps the chosen basis of the domain, we know how it maps any vector of the domain. And this is possible because $f$ has the linear property.\n:::\n\n# Isomorphism and stuff\n\nThe hypervector $\\mathbf{B}$ (and $\\mathbf{C}$) allow us to make a bijective correspondence between vectors in $\\mathbb{R}^n$ (and $\\mathbb{R}^m$) and vectors in $\\mathbb{V}$ (and $\\mathbb{W}$). A vector $X\\in\\mathbb{R}^n$ is transformed into $v\\in\\mathbb{V}$ by dotting the column vector $X$ with the hypervector $\\mathbf{B}$:\n\n$$\nv=\\mathbf{B}X\n$$\n\nsame thing with $Y$ and $w$:\n\n$$\nw=\\mathbf{C}Y\n$$\n\nWe picture this correspondence by the vertical arrows in the following picture:\n\n![](images/clipboard-690894529.png){width=\"402\"}\n\nSubspaces in $\\mathbb{R}^n$ are translated into subspaces of $\\mathbb{V}$, this has important consequences. If $\\mathbb{R}^n=\\alpha\\oplus\\beta$ is decomposed by a direct sum of subspaces $\\alpha$ and $\\beta$, then multiplying the vectors in the them by the hypervector $\\mathbf{B}$ yields vector spaces in $\\mathbb{V}=(\\mathbf{B}\\alpha) \\oplus(\\mathbf{B}\\beta)$. \\[Warning: if $\\beta=\\alpha^\\perp$ it may happen that it is *not* the case that$(\\mathbf{B}\\beta)=(\\mathbf{B}\\alpha)^\\perp$.\\]\n\nNow, introduce a map $f$ between elements of $\\mathbb{V}$ and $\\mathbb{W}$, as was our $f(x,y)=(2x-y,x+y)$, this is represented by the bottom arrow. Since the elements $v$ are being mapped into $w$ and both can be translated uniquely into $X$ and $Y$, we naturally expect that there is also map, called $A$, that directly connects $X$ with $Y$.\n\nIf $f$ links $\\mathbf{B}X$ with $\\mathbf{C}Y$, then $A$ links $X$ with $Y$, how? Answer: $Y=AX$.\n\nSee the diagram below:\n\n![](images/clipboard-3851920532.png){width=\"321\"}\n\nWe have been talking about maps of individual vectors, but we can also do it for subspaces. A subspace $V\\subset_{vec}\\mathbb{V}$ is mapped by $f$ into the subspace$f(V)\\subset_{vec} \\mathbb{W}$. Here is an intuitive justification: if $e_1$ and $e_2$ are a basis for $V$, then any l.c. of them gives us a vector in $V$, this is expected, after all $V$ is a vector space. But is it true that any l.c. of $f(e_1)$ and $f(e_2)$ is also a vector of $f(V)$? Well, a generic vector in $f(V)$ must have the form $f(ae_1+be_2)$, but since $f$ is linear this in turn means $af(e_1)+bf(e_2)$, but this just tells us that any element of $f(V)$ is a l.c. of $f(e_1)$ and $f(e_2)$.\n\n**Thus:** *a linear function* $f$ *maps subspaces of* $\\mathbb{V}$ *into subspaces of* $\\mathbb{W}$*.*\n\nAnd since subspaces of $\\mathbb{V}$ and $\\mathbb{W}$ in turn have a version in $\\mathbb{R}^n$ and $\\mathbb{R}^m$, the matrix $A$ maps those as well.\n\n**Thus:** *a linear function* $f$ *maps subspaces of* $\\mathbb{V}$ *into subspaces of* $\\mathbb{W}$*. And correspondingly, its matrix representation* $A$ *maps subspaces of* $\\mathbb{R}^n$ *into the corresponding subspaces of* $\\mathbb{R}^m$*.*\n\n## Revisiting Two important subspaces\n\nIn the domain and codomain of a map $f:\\mathbb{V}\\overset{\\sim}{\\longrightarrow}\\mathbb{W}$ we find two important subspaces.\n\n### Kernel of a linear function\n\n::: {#def-kernel style=\"color:gray\"}\nLet $f:\\mathbb{V}\\overset{\\sim}{\\longrightarrow} \\mathbb{W}$, then the kernel of $f$ is the subset of the domain $\\mathbb{V}$:\n\n$$ \\ker f = \\{v_N\\in\\mathbb{V}\\,\\,|\\,\\,f(v_N)=0\\} $$\n\nMoreover, it is in fact a subspace.\n:::\n\nWith basis introduced in both spaces we can write this set as:\n\n$$ \\ker f = \\{\\mathbf{B}X_N\\in\\mathbb{V}\\,\\,|\\,\\,\\mathbf{C}AX_N=\\mathbf{C}0\\} $$\n\nAfter solving $AX_N=0$ we multiply the solutions by $\\mathbf{B}$, giving us $\\ker f$.\n\nNotice that the solutions of $AX_N=0$ constitute the nullspace of the matrix $A$. The difference between $N(A)$ and $\\ker f$ is a multiplication by $\\mathbf{B}$\n\n$$ \\ker f = \\mathbf{B} N(A) $$\n\nSee the diagram above.\n\n### Image of the domain (range of linear functions)\n\n::: {#def-range style=\"color:gray\"}\nLet $f:\\mathbb{V}\\overset{\\sim}{\\longrightarrow} \\mathbb{W}$, then the range (image) of $f$ is the following subset of the domain $\\mathbb{W}$:\n\n$$ f(\\mathbb{V}) = \\{f(u)\\in\\mathbb{W}\\,\\,|\\,\\,u\\in\\mathbb{V}\\} $$\n\nMoreover, it is also a subspace.\n:::\n\nUpon introducing a basis, this problem can be reformulated as:\n\n$$ f(\\mathbb{V}) = \\{\\mathbf{C}AX\\in\\mathbb{W}\\,\\,|\\,\\,X\\in\\mathbb{R}^{\\dim \\mathbb{V}}\\} $$\n\nTo compute this vector space we find first the column space of $A$ and then multiply it by $\\mathbf{C}$ get us the image!\n\n$$ f(\\mathbb{V})=\\mathbf{C} C(A) $$\n\nSee the diagram above.\n\n::: callout-note\n## Commentary\n\nWe know that:\n\n-   $\\mathbb{R}^n=C(A^\\intercal) \\oplus N(A)$ which translate into $\\mathbb{V}=(\\mathbf{B}C(A^\\intercal)) \\oplus (\\mathbf{B}N(A))$\n\n-   $\\mathbb{R}^m=C(A) \\oplus N(A^\\intercal)$ which translates into $\\mathbb{W}=(\\mathbf{C}C(A)) \\oplus (\\mathbf{C}N(A^\\intercal))$\n\n-   $C(A^\\intercal) \\perp N(A)$ and that $C(A) \\perp N(A)^\\intercal$, always! But the same is not true after multiplying by $\\mathbf{B}$ and $\\mathbf{C}$, unless the chosen basis $\\mathbf{B}$ and $\\mathbf{C}$ are orthogonal (eg. canonical basis).\n:::\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"linear_functions_v4.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.32","editor":"visual","comments":{"hypothesis":true},"preview":{"browser":"chrome"},"theme":{"light":"minty"},"fontcolor":"rgb(190,190,190)","backgroundcolor":"rgb(32,31,30)","margin-left":"rgb(32,31,30)","navbar-color":"rgba(216, 6, 33 .65)","title":"Linear functions"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}