{"title":"Linear functions","markdown":{"yaml":{"title":"Linear functions","execute":{"echo":false}},"headingText":"The concept of a vector space","containsRefs":false,"markdown":"\n\nUp to now, we learned a more practical part of the course:\n\n-   How to compute the solution of $A\\mathbf{x}=\\mathbf{b}$?\n\n-   How to compute a basis for the $4$ subspaces associated with $A$?\n\n-   How to compute the inverse $A^{-1}$?\n\n-   How to compute the determinant?\n\nIt always *how to compute*. Now we enter a more conceptual part of the course and organize ideas.\n\n\n*Suspend what you know:* We have been speaking about a vector space as being closed under linear combinations. However if we stop to think about it, we can only compute a linear combination provided we know how to multiply vectors by numbers and how to add vectors, see below:\n\n$$\n\\overbrace{\\alpha u}^\\text{number times a vector} + \\overbrace{\\beta v}^\\text{number times a vector}=\\underbrace{u'+v'}_\\text{sum of vectors}\n$$\n\nFor the moment we will consider these two operations in separate and also for the moment suspend how they are done in practice, thus, when we think about:\n\n$$\n(1,3)+(4,7)=?\\qquad 10(2,5)=?\n$$\n\nWe do not know how to do these operations.\n\nNow lets return to the idea of a vector space.\n\n*Today:* A set of things is just a list. Let us label those things as\n\n$$\n\\mathbb{V}=\\{v_1,v_2,\\dots,v_n\\}\n$$\n\nWhat can we do with the things in this set?\n\nWe can define a map, a map is a rule or sequence of operations that tell me how to assigns to each pair of elements in $\\mathbb{V}$ a new element in $\\mathbb{V}$. Right now, we are not thinking about a specific map, you can even imagine (if it helps you) that aliens choose the map, but did not told us what it is (except for that fact that they chose one.)\n\nAnother kind of map they decide to introduce, for which we do not know yet how it operates consists in assigning to each pairing of a real number and an element of $\\mathbb{V}$, another element of $\\mathbb{V}$.\n\nThe notion of map as we just described in words is described in mathematical notation as:\n\n$$\n\\begin{align}\n+ : \\mathbb{V}\\times\\mathbb{V} &\\longrightarrow\\mathbb{V}\\\\\n(u,v)&\\longmapsto u\\tilde{+}v\n\\end{align}\n$$\n\nand\n\n$$\n\\begin{align}\\cdot : \\mathbb{R}\\times\\mathbb{V} &\\longrightarrow\\mathbb{V}\\\\(\\alpha,v)&\\longmapsto \\alpha\\cdot v\\end{align}\n$$\n\nThe $u+v$ is the symbol for the element in $\\mathbb{V}$ that corresponds to the pair $(u,v)$. And $\\alpha\\cdot v$ is the symbol for the element assigned to $(\\alpha,v)$.\n\nNow they disclose to us a few properties that their maps have:\n\n-   Commutative: $u+v=v+u$. \\[*You can flip the order*\\]\n\n-   Associative: $(u+v)+w=u+(v+w)$. \\[*You can shift the brackets*\\]\n\n-   Neutral element: There is a neutral element wrt to $+$: There is an element in $\\mathbf{V}$, call it $o$ so that for all $u\\in \\mathbb{V}$ we find $u+o=u$. \\[*This is not* $0$ *from the real numbers, it is the zero of* $\\mathbb{V}$*. To avoid confusion you can call* $o$ *as circle rather than zero.*\\]\n\n-   Inverse element: For every element $u$ there an element, which we denote by $-u$, that guarantees $u+(-u)=o$. \\[ $u-v$ is just a short notation for $u+(-v)$\\]\n\nThe next list of rules describe how the operations $+$ and $\\cdot$ interact when both present in a calculation, two of this rules are distributive rules:\n\n-   Associative: $\\alpha(\\beta u)=(\\alpha\\beta)v$\n\n-   Distributive I: $(\\alpha+\\beta)u = \\alpha u +\\beta u$\n\n-   Distributive II: $\\alpha (u +v) = \\alpha u + \\alpha v$\n\n-   Unitary: $1 u = u$\n\nWe do not know specifically how the assignments are made, the only thing we know is: whatever they are, they obey the rules above.\n\nA vector space is a triplet $(\\mathbb{V},+,\\cdot)$\n\nIts like describing someone: you can list all its characteristics, or just list the essential one. Here essential depends for what the description is useful for.\n\n::: callout-note\n## Commentary\n\nWhy is maps and these rules? Because on them fit many useful maps in practice.\n\nThese rules all boil down to $\\mathbb{V}$ is closed under l.c. where the l.c. is realized in practice the natural and expected way. The rules above are the rules of common sense for l.c.\n:::\n\nWhat specific sets and $+$ and $\\cdot$ operations fit the profile above? Here are some examples:\n\n## Example 1 of a vector space\n\nThe set of all polynomials:\n\n$$\n\\mathbb{P}=\\{\\dots+a_n x^n+a_{n-1} x^{n-1} +\\dots +a_1 x +a_0\\,\\,|\\,\\, a_i \\text{'s are real}\\}\n$$\n\nTogether with:\n\n$$\n\\begin{align}+ : \\mathbb{P}\\times\\mathbb{P} &\\longrightarrow\\mathbb{V}\\\\(p,q)&\\longmapsto (p+q)(x)=p(x)+q(x)\\end{align}\n$$\n\nHere, the $+$ sign between functions $p$ and $q$ manifests in practice as the summation $+$ of real numbers $p(x)$ and $q(x)$.\n\n$$\n\\begin{align}\\cdot : \\mathbb{R}\\times\\mathbb{P} &\\longrightarrow\\mathbb{V}\\\\(\\alpha,p)&\\longmapsto (\\alpha\\cdot p)(x):=\\alpha\\cdot p(x)\\end{align}\n$$\n\nThe multiplication between a real number and the function $p$ is understood in therms of the multiplication between the real number $\\alpha$ and $p(x)$.\n\nAll the rules above can be checked for this example.\n\nThe natural basis for this vector space is: $\\{1,x,x^2,\\dots\\}$\n\n## Example 2 of a vector space\n\nConsider the set:\n\n$\\mathbb{V} = \\{(x,y)\\,\\,|\\,\\, x,y\\in \\mathbb{R}\\}$\n\nAnd the maps:\n\n$$\n\\begin{align}\n+ : \\mathbb{V}\\times\\mathbb{V} &\\longrightarrow\\mathbb{V}\\\\\n(\\mathbf{u},\\mathbf{v})&\\longmapsto \\mathbf{u}+\\mathbf{v} := (u_x,u_y)+(v_x,v_y)=(u_x+v_x,u_y+v_y)\n\\end{align}\n$$\n\nand\n\n$$\n\\begin{align}\\cdot : \\mathbb{R}\\times\\mathbb{V} &\\longrightarrow\\mathbb{V}\\\\(\\alpha,\\mathbf{u})&\\longmapsto \\alpha\\cdot \\mathbf{u}:=(\\alpha u_x,\\alpha u_y)\\end{align}\n$$\n\nOnce again the summation of vectors is in practice the summation of numbers and the multiplication of a vector by a real number means the multiplication of its entries (real numbers) by the real number $\\alpha$.\n\nNotice that when I write $(u_x,u_y)+(v_x,v_y)$ one may wonder what it means, we know it is another element of $\\mathbf{V}$, but which one? To answer that we have to tell how the operation $+$ between two elements of $\\mathbb{V}$ actually maps. Whatever the way it maps, it must be consistent with the rules defined above. It turn out that to define this $+$ as doing $(u_x+v_x,u_y+v_y)$ (a plus between reals) is a good and useful choice.\n\nThe natural choice of basis for this vector space is: $\\{(1,0),(0,1)\\}$\n\n# Intuitive idea of structure of a vs (Retirar?)\n\nFor example let us define the summation and multiplication by a number.\n\nFor example take $\\mathbb{R}^2$, we know that:\n\n$$\n(1,3)+(0,1) = (1,4)\n$$\n\nTherefore, the vectors $(1,3)$ and $(0,1)$ are connected to $(1,4)$.\n\nWhen I multiply a vector by number:\n\n$$\n3(1,9) = (3,27)\n$$\n\nand as a result the vector $(1,9)$ is connected with $(3,27)$.\n\nA generic l.c. $\\alpha(1,3)+\\beta(0,1) = (\\alpha,3\\alpha+\\beta)$. Connects $\\alpha(1,3)$ and $\\beta(0,1)$ with $(\\alpha,3\\alpha+\\beta)$.\n\nThe operation l.c. creates this web of connectivity between elements of a vector space.\n\n# Linear functions (START HERE)\n\nConsider two vector spaces $\\mathbb{R}^n$ and $\\mathbb{R}^m$.\n\nA linear function is a bridge between elements of both vectors spaces defined as follows:\n\n::: {#def-linear_func style=\"color:gray\"}\n$$\n\\begin{align}\nf:\\mathbb{R}^n&\\longrightarrow \\mathbb{R}^m\\\\\n\\bf{u}&\\longmapsto f(\\bf{u})\n\\end{align}\n$$\n\nsuch that the following rule is obeyed: $f(\\lambda \\bf{u}+\\mu \\bf{v}) = \\lambda f (\\bf{u}) + \\mu f(\\bf{v})$\n\nfor all $\\lambda,\\mu \\in \\mathbb{R}$ and $\\bf{u},\\bf{v}\\in \\mathbb{R}^n$.\n\n*Terminology:* The vector space $\\mathbb{R}^n$ is the domain of the function $f$, while $\\mathbb{R}^m$ is the codomain (of $f$)\n\nWhen referring to a linear function we will use $\\sim$ and write: $f:\\mathbb{R}^n\\overset{\\sim}{\\longrightarrow} \\mathbb{R}^m$.\n:::\n\nWe do know how $f$ maps between the spaces, but whatever procedure $f$ is, it must have the property $f(\\lambda \\bf{u}+\\mu \\bf{v}) = \\lambda f (\\bf{u}) + \\mu f(\\bf{v})$. In fact, this property, together with the choice of a basis for the domain and codomain, we can write a concrete formula (procedure) for $f$.\n\n**Goal:** Choose the natural basis for the domain and the natural basis for the codomain and then write a formula for the linear map $f$ .\n\n**Idea:** If we know how a linear function $f$ maps the chosen basis of the domain, we know how it maps any vector of the domain.\n\nTo put this idea in practice we follow three steps:\n\n## Step 1: Choose a basis for the domain and codomain\n\nAs said above we have to chose a basis for $\\mathbb{R}^n$, let it be:\n\n$$\n\\mathbf{e}_1:=\\begin{pmatrix}1\\\\0\\\\ \\vdots\\\\0\\end{pmatrix}\\qquad \\mathbf{e}_2:=\\begin{pmatrix}0\\\\1\\\\ \\vdots\\\\0\\end{pmatrix}\\qquad\\dots\\qquad \\mathbf{e}_n:=\\begin{pmatrix}0\\\\0\\\\ \\vdots\\\\1\\end{pmatrix}\n$$\n\nEach vector is $n\\times 1$ and there are $n$ of them.\n\nThis basis is called the canonical basis of $\\mathbb{R}^n$, it is the natural basis for that space.\n\nThe next step is to choose a basis for the codomain $\\mathbb{R}^m$. Let us choose its canonical basis:\n\n$$\n\\mathbf{b}_1:=\\begin{pmatrix}1\\\\0\\\\ \\vdots\\\\0\\end{pmatrix}\\qquad \\mathbf{b}_2:=\\begin{pmatrix}0\\\\1\\\\ \\vdots\\\\0\\end{pmatrix}\\qquad\\dots\\qquad \\mathbf{b}_m:=\\begin{pmatrix}0\\\\0\\\\ \\vdots\\\\1\\end{pmatrix}\n$$ {#eq-basis_of_codomain}\n\nWhere this time each vector is $m\\times 1$.\n\n### Example\n\nLet $n=2$ and $m=3$. The canonical basis for the domain $\\mathbb{R}^2$ is:\n\n$$\n\\mathbf{e}_1=\\begin{pmatrix}1\\\\0\\end{pmatrix}\\qquad \\mathbf{e}_2=\\begin{pmatrix}0\\\\1\\end{pmatrix}\n$$\n\nThe canonical basis for the codomain $\\mathbb{R}^3$ is:\n\n$$\n\\mathbf{b}_1=\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}\\qquad \\mathbf{b}_2=\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}\\qquad\\mathbf{b}_3=\\begin{pmatrix}0\\\\0\\\\1\\end{pmatrix}\n$$\n\n## Step 2: Decide how $f$ acts on the domain's basis\n\nWhen $f$ acts on $\\mathbf{e}_1$, we obtain $f(\\mathbf{e}_j)$, but what is $f(\\mathbf{e}_j)$ specifically? We can say what it is specifically by saying what this vector (living in $\\mathbb{R}^m$) looks like wrt to its basis. Thus $f(\\mathbf{e}_j)$ can be written by an appropriate l.c. of the $\\mathbf{b}$'s:\n\n$$\nf(\\mathbf{e}_j) = \\sum_{i=1}^m A_{ij}\\,\\,\\mathbf{b}_i\n$$ {#eq-repre_of_output}\n\nThe numbers $A_{ij}$ are chosen by us and they tell us how much we want $f(\\mathbf{e}_j)$ to look like $\\mathbf{b}_i$. Of course $A_{ij}$ has two indices $i$ and $j$, one for the current $\\mathbf{e}_j$ being mapped and another for the basis vector $\\mathbf{e}_j$ to which it is being compared, as a result $i=1,2,..,m$ and $j=1,2,..,n$.\n\n### Example\n\nI choose the $A_{ij}$ to be like this:\n\n$$\n\\begin{align}\nf(\\mathbf{e}_1)&=A_{11}\\mathbf{b}_1+A_{21}\\mathbf{b}_2+A_{31}\\mathbf{b}_3=2\\mathbf{b}_1+2\\mathbf{b}_2+2\\mathbf{b}_3\\\\\nf(\\mathbf{e}_2)&=A_{12}\\mathbf{b}_1+A_{22}\\mathbf{b}_2+A_{32}\\mathbf{b}_3=-\\mathbf{b}_1+\\mathbf{b}_2+ \\mathbf{b}_3\n\\end{align}\n$$ {#eq-choice_of_As}\n\n## Step 3: Mapping a generic vector of the domain\n\nArmed with @eq-repre_of_output we can compute the output $f(\\mathbf{u})$ for any $\\mathbf{u}$ of the domain $\\mathbb{R}^2$, i.e., for any $\\mathbf{u}$ of the form:\n\n$$\n\\mathbf{u}=c_1\\mathbf{e}_1+c_2\\mathbf{e}_2\n$$\n\nLet us do the calculation:\n\n$$\n\\begin{align}\nf(\\mathbf{u}) &= f(c_1\\mathbf{e}_1+c_2\\mathbf{e}_2)\\\\\n&=c_1f(\\mathbf{e}_1)+c_2f(\\mathbf{e}_2)\\\\\n&=c_1(A_{11}\\mathbf{b}_1+A_{21}\\mathbf{b}_2+A_{31}\\mathbf{b}_3)+c_2(A_{12}\\mathbf{b}_1+A_{22}\\mathbf{b}_2+A_{32}\\mathbf{b}_3)\\\\\n&=(A_{11}c_1+A_{12}c_2)\\mathbf{b}_1+(A_{21}c_1+A_{22}c_2)\\mathbf{b}_2+(A_{31}c_1+A_{32}c_2)\\mathbf{b}_3\n\\end{align}\n$$\n\nNow we substitute the numbers $A_{ij}$ we chose in @eq-choice_of_As into:\n\n$$\nf(c_1\\mathbf{e}_1+c_2\\mathbf{e}_2)=(A_{11}c_1+A_{12}c_2)\\mathbf{b}_1+(A_{21}c_1+A_{22}c_2)\\mathbf{b}_2+(A_{31}c_1+A_{32}c_2)\\mathbf{b}_3\n$$ {#eq-general_map}\n\nwhich gives us:\n\n$$\nf(c_1\\mathbf{e}_1+c_2\\mathbf{e}_2)=(2c_1-c_2)\\mathbf{b}_1+(2c_1+c_2)\\mathbf{b}_2+(c_1+c_2)\\mathbf{b}_3\n$$\n\nFor example, if I want to map $3\\mathbf{e}_1+7\\mathbf{e}_2$, we obtain:\n\n$$\nf(3\\mathbf{e}_1+7\\mathbf{e}_2)=(2\\cdot3-7)\\mathbf{b}_1+(2\\cdot 3+7)\\mathbf{b}_2+(3+7)\\mathbf{b}_3=-\\mathbf{b}_1+13\\mathbf{b}_2+10\\mathbf{b}_3\n$$\n\n**Important observation:** Since $f(\\mathbf{u})$ is in the codomain $\\mathbb{R}^3$, then it is some l.c. of the $\\mathbf{b}$'s as we did for the $f(\\mathbf{e}_1)$ and $f(\\mathbf{e}_2)$ in @eq-choice_of_As. Therefore:\n\n$$\nf(\\mathbf{u})=d_1\\mathbf{b}_1+d_2\\mathbf{b}_2+d_3\\mathbf{b}_3\n$$ {#eq-general_repre}\n\nComparing @eq-general_map with @eq-general_repre:\n\n$$\nd_1\\mathbf{b}_1+d_2\\mathbf{b}_2+d_3\\mathbf{b}_3=(A_{11}c_1+A_{12}c_2)\\mathbf{b}_1+(A_{21}c_1+A_{22}c_2)\\mathbf{b}_2+(A_{31}c_1+A_{32}c_2)\\mathbf{b}_3\n$$\n\nwe conclude that the coefficients $d$ are in fact the calculations:\n\n$$\n\\begin{align}\nd_1&=A_{11}c_1+A_{12}c_2\\\\\nd_2 &= A_{21}c_1+A_{22}c_2\\\\\nd_3 &=A_{31}c_1+A_{32}c_2\n\\end{align}\n$$\n\n## A formula for $f$:\n\nFrom the three steps above we can write the image of $\\mathbf{u}=c_1\\mathbf{e}_1+\\dots+c_m\\mathbf{e}_m$ under $f$ as:\n\n$$\n\\begin{align}\nf(\\mathbf{u})&=f(\\sum_{j=1}^m c_j\\mathbf{e}_j)\\\\\n&=\\sum_{j=1}^n c_jf(\\mathbf{e}_j)\\\\\n&=\\sum_{j=1}^n c_j\\sum_{i=1}^m A_{ij}\\,\\,\\mathbf{b}_i\\\\\n&=\\sum_{i=1}^m\\left(\\sum_{j=1}^n A_{ij}c_j\\right)\\mathbf{b}_i\n\\end{align}\n$$ {#eq-gen_map}\n\nBut since $f(\\mathbf{u})=\\sum_{i=1}^m d_i\\,\\,\\mathbf{b}_i$, substituting in @eq-gen_map:\n\n$$\n\\sum_{i=1}^m d_i\\mathbf{b}_i=f(\\mathbf{u})=\\sum_{i=1}^m\\left(\\sum_{j=1}^n A_{ij}c_j\\right)\\mathbf{b}_i\n$$\n\nSince the coefficients must be the same we conclude that:\n\n$$\nd_i=\\sum_{j=1}^n A_{ij}c_j\n$$ {#eq-matrix_mult_in_comp}\n\nThis calculation tell us something important: from the coefficients of $\\mathbf{u}$ on the chosen basis of the domain of $f$, they give us the coefficients of $f(\\mathbf{u})$ in the chosen basis of the codomain. Lets read better the meaning of this formula, the $c$'s tell us how much the input vector $\\mathbf{u}$ looks like to each one of the basis vectors $\\mathbf{e}$. The numbers $A_{ij}$ quantify how much a basis vector $f(\\mathbf{e}_j)$ look like the basis vector $\\mathbf{b}_j$. This means that the calculation $\\sum_{j=1}^n A_{ij}c_j$ can be understood like:\n\n**\\[**how much $f(\\mathbf{u})$ looks like $\\mathbf{b}_i$**\\]** = **\\[**how much $f(\\mathbf{e}_1)$ looks like $\\mathbf{b}_i$**\\]** $\\times$ **\\[**how much $\\mathbf{u}$ looks like $\\mathbf{e}_1$**\\]** $+$ \\[how much $f(\\mathbf{e}_2)$ looks like $\\mathbf{b}_i$**\\]** $\\times$ **\\[**how much $\\mathbf{u}$ looks like $\\mathbf{e}_2$**\\]** $+\\dots +$ **\\[**how much $f(\\mathbf{e}_n)$ looks like $\\mathbf{b}_i$**\\]** $\\times$ **\\[**how much $\\mathbf{u}$ looks like $\\mathbf{e}_n$**\\]**\n\nIts a weighted sum of the representation of $\\mathbf{u}$!\n\nThis understanding is clear because we know exactly what basis we are speaking off, after all, the first step was to make the choice of bases.\n\nNow, we can arrange these numbers (if you wish) in a more geometrically pleasing form as:\n\n$$\n\\left(\\begin{matrix}d_1\\\\d_2\\\\\\vdots\\\\d_n\\end{matrix}\\right)=\n\\left(\\begin{matrix}A_{11} & A_{12} & \\dots & A_{1m}\\\\A_{21} & A_{22} & \\dots & A_{2m} \\\\\\vdots & \\vdots &\\ddots & \\vdots\\\\A_{n1} & A_{n2} &\\dots & A_{nm} \\end{matrix}\\right)\n\\left(\\begin{matrix}c_1\\\\c_2\\\\\\vdots\\\\c_m\\end{matrix}\\right)\n$$ {#eq-matrix_form}\n\nwhich is just the $A\\mathbf{u}$. Always in the back of our minds, we need to remember that @eq-matrix_form is just @eq-matrix_mult_in_comp, and @eq-matrix_mult_in_comp comes from @eq-gen_map, where is the meaning of these coefficients $d$, $A$'s and $c$'s are clear (because right next to them we find the basis vectors they multiply)\n\nAs a final conclusion we should note that: if the bases chosen where canonical bases, then we can rewrite the formula:\n\n$$\nf(\\mathbf{u})=\\sum_{i=1}^m\\left(\\sum_{j=1}^n A_{ij}c_j\\right)\\mathbf{b}_i\n$$ {#eq-one}\n\nby substituting in it the $\\mathbf{b}$'s in @eq-basis_of_codomain, the then @eq-one turn into:\n\n$$\nf(\\mathbf{u}) = A\\mathbf{u}\n$$ {#eq-two}\n\nNow we have the context where operations $A$ times a vector $\\mathbf{u}$ emerges. They are the concrete realization of linear maps between vector spaces after a basis for each is chosen.\n\n*Notice:* @eq-one is useful to understand the meaning of the numbers, while @eq-two is useful to do calculations, like column space and nullspace as we shall see.\n\nThe matrix we have been considering is:\n\n$$\n\\left(\\begin{matrix}d_1\\\\d_2\\\\d_3\\end{matrix}\\right)=\\left(\\begin{matrix}2 & -1\\\\2 & 1 \\\\1 & 1  \\end{matrix}\\right)\\left(\\begin{matrix}c_1\\\\c_2\\end{matrix}\\right)\n$$\n\n::: callout-note\n## Commentary\n\nI wish you could follow the steps:\n\nStep 1: Choose a basis for the domain and codomain\n\nStep 2: Decide how $f$ acts on the domain's basis\n\nStep 3: Mapping a generic vector of the domain\n\nand reach @eq-matrix_mult_in_comp and then the matrix.\n\nAnd also go backwards!\n:::\n\n# Revisiting Two important subspaces\n\nThe domain and codomain of a map $f:\\mathbb{R}^n\\overset{\\sim}{\\longrightarrow}\\mathbb{R}^m$ are defined by us when we introduce $f$ to the world. But within $\\mathbb{R}^n$ and $\\mathbb{R}^m$ there are subspaces of great importance and which are determined by the type of operation $f$ we choose.\n\nThese two spaces are the range of $f$ also known as the *image of the domain under* $f$ and the *kernel* (or nullspace) *of* $f$*,* \\[*Comment:* the nullspace of $f$ are the zeros of the function, just you have seen in high-school.\\]\n\n## Kernel of a linear function\n\n::: {#def-kernel style=\"color:gray\"}\nLet $f:\\mathbb{R}^n\\overset{\\sim}{\\longrightarrow} \\mathbb{R}^m$, then the kernel of $f$ is the subset of the domain $\\mathbb{R}^n$:\n\n$$ \\ker f = \\{\\mathbf{u}\\in\\mathbb{R}^n\\,\\,|\\,\\,f(\\bf{u})=\\bf{0}\\} $$\n\nMoreover, it is in fact a subspace.\n:::\n\n## Image of the domain (range of linear functions)\n\n::: {#def-range style=\"color:gray\"}\nLet $f:\\mathbb{R}^n\\overset{\\sim}{\\longrightarrow} \\mathbb{R}^m$, then the range of $f$ is the subset of the domain $\\mathbb{R}^m$:\n\n$$ f(\\mathbb{R}^n) = \\{f(\\mathbf{u})\\in\\mathbb{R}^m\\,\\,|\\,\\,\\mathbf{u}\\in\\mathbb{R}^n\\} $$\n\nMoreover, it is also a subspace.\n:::\n\n## Example 1\n\nLets follow the three steps again.\n\nWe intend to define a function $f:\\mathbb{R}^3\\overset{\\sim}{\\longrightarrow} \\mathbb{R}$. To do that we new first a basis for $\\mathbb{R}^3$ and another for $\\mathbb{R}$:\n\n$$\n\\mathbf{e}_1=\\begin{pmatrix}1\\\\0\\\\0 \\end{pmatrix}\\qquad \\mathbf{e}_2=\\begin{pmatrix}0\\\\1\\\\0 \\end{pmatrix} \\qquad \\mathbf{e}_3=\\begin{pmatrix}0\\\\0\\\\1 \\end{pmatrix}\\qquad\n\\mathbf{b}_1=1\n$$ {#eq-bases_of_ex1}\n\nTo write a formula for $f$ given these bases we at step two decide how $f$ acts on the chosencanonical bases:\n\n$$\nf(\\mathbf{e}_1)=f(\\mathbf{e}_3)=2\\mathbf{b}_1\\qquad f(\\mathbf{e}_2)=0\\mathbf{b}_1\n$$\n\nAt step three we now write the action of $f$ on an arbitrary vector $\\mathbf{x}\\in\\mathbb{R}^3$ is:\n\n$$\n\\begin{align}\nf(x_1\\mathbf{e}_1+x_2\\mathbf{e}_2+x_3\\mathbf{e}_3)&=x_1f(\\mathbf{e}_1)+x_2f(\\mathbf{e}_2)+x_3f(\\mathbf{e}_3)\\\\\n&=(2x_1+2x_3)\\mathbf{b}_1\n\\end{align}\n$$ {#eq-eq1}\n\nand then decompose $f(x_1\\mathbf{e}_1+x_2\\mathbf{e}_2+x_3\\mathbf{e}_3)$ on the basis of $\\mathbb{R}$:\n\n$$\nf(x_1\\mathbf{e}_1+x_2\\mathbf{e}_2+x_3\\mathbf{e}_3)=d_1\\mathbf{b}_1\n$$ {#eq-eq2}\n\nWhat is $d_1$ in terms of the $x_1$, $x_2$ and $x_3$? Comparing @eq-eq1 and @eq-eq2 we arrive at:\n\n$$\nd_1=2x_1+2x_3\n$$\n\nThis equation is analogous to @eq-matrix_mult_in_comp, from this we identify the matrix $A=(2,0,2)$:\n\n$$\nd_1=(2,0,2)\\begin{pmatrix}x_1\\\\x_2\\\\x_3\\end{pmatrix} \n$$\n\nThis formula is exactly @eq-matrix_form!\n\nWith the information above, the function\n\n$$\n\\begin{align} f:\\mathbb{R}^3&\\longrightarrow \\mathbb{R}\\\\ \\mathbf{x}&\\longmapsto f(\\mathbf{x}):=2(x_1+x_3)\\mathbf{b}_1 \\end{align}\n$$\n\nSubstituting $\\mathbf{b}_1=1$ and the above can be rewritten as:\n\n$$\n\\begin{align} f:\\mathbb{R}^3&\\longrightarrow \\mathbb{R}\\\\ \\mathbf{x}&\\longmapsto f(\\mathbf{x}):=A\\mathbf{x} =2(x+z)  \\end{align}\n$$\n\n### The 4 spaces of example 1:\n\nWhat is the kernel? It is the nullspace of $2(1,0,1)$ which is the set of vectors perpendicular to it:\n\n$$\n2(2,0,2)\\begin{pmatrix}x_{1N}\\\\x_{2N}\\\\x_{3N}\\end{pmatrix}=0\n$$\n\nSince we have one pivot on the first column and two dependent columns, we expect two dimensions.\n\nIsolating $Col_2$ by setting $y_N=1$ and $z_N=0$, the only answer is $x_N=0$, thus $(0,1,0)$ is in the nullspace.\n\nIsolating $Col_3$ by setting $y_N=0$ and $z_N=1$, we find $x_N=-1$, the second basis vector is $(-1,0,1)$.\n\nAny l.c. of these solutions is also mapped to $0$, thus:\n\n$$\n\\ker f = span \\{(0,1,0)^\\intercal,(-1,0,1)^\\intercal\\}\n$$\n\nThe image is one dimensional since there is only one pivot:\n\n$$\nf(\\mathbb{R}^3)=\\mathbb{R}\n$$\n\nFrom the matrix $A$ we can define the row-space as\n\n$$\nC(A^\\intercal)=span\\{(1,0,1)^\\intercal\\}\n$$\n\nWhile the left-nullspace is just $N(A^\\intercal)=\\{0\\}$.\n\nThe rowspace and null space break the domain $\\mathbb{R}^3$ into two subspaces, while the column space and codomain are equal:\n\n$$\n\\mathbb{R}^3 = span\\{(1,0,1)^\\intercal\\}\\oplus span \\{(0,1,0)^\\intercal,(-1,0,1)^\\intercal\\}\\qquad \\mathbb{R} = \\mathbb{R}\\oplus\\{0\\}\n$$\n\nwhich can be rewritten using @eq-bases_of_ex1 as:\n\n$$\n\\mathbb{R}^3 = span\\{\\mathbf{e}_1+\\mathbf{e}_2\\}\\oplus span \\{\\mathbf{e}_2,-\\mathbf{e}_1+\\mathbf{e}_2\\}\\qquad \\mathbb{R} = span\\{\\mathbf{b}_1\\}\\oplus\\{\\mathbf{0}\\}\n$$\n\nFrom this, the following picture tells us how one vector $\\mathbf{u}$ in $\\mathbb{R}^3$ is mapped into $f(\\mathbf{u})\\in\\mathbb{R}$\n\n![](figs/maps_yellow_vector.png)\n\nGiven these subspaces of the domain and codomain we can now focus on describing the kind of mapping that $f$ does. On the picture we see a vector $\\mathbf{x}$ being decomposed into its parts, one in the nullspace and another in the row space:\n\n$$\n\\mathbf{x}=\\mathbf{x}_{R}+\\mathbf{x}_{N}\n$$\n\nThe $\\mathbf{x}_N$ is mapped by this $f$ into $0$, and $\\mathbf{x}_R$ is mapped into $f(\\mathbf{x})$, thus we can write $f(\\mathbf{x})=f(\\mathbf{x}_R)$ for all $\\mathbf{x}\\in \\mathbb{R}^3$.\n\nFor each $\\mathbf{x}_R$ in the rowspace we can any add any vector of the null space, the image is always $f(\\mathbf{x}_R)$. Therefore this is not a 1-1 function.\n\nBut is it onto? For any element $d_1$ of $\\mathbb{R}$ we can find a $\\mathbf{x}$, such that $f(\\mathbf{x})=d_1$ i.e.\n\n$$\nd_1=2x+2z\n$$\n\nis always solvable. This $f$ is onto.\n\nAs a consequence we conclude that any equation\n\n$$\nf(\\mathbf{x})=b\n$$\n\n(given a $b$) always has solution, in fact has an infinite number of solutions, thanks to the fact that the nullspace is not just $\\mathbf{0}$.\n\nAnother way to put this, this $f$ has no inverse, but a fibre can be defined:\n\n$$\nf^{-1}(b)=\\{\\mathbf{x}\\in\\mathbb{R}^3\\,\\,|\\,\\,f(\\mathbf{x})=b\\}\n$$\n\nThis set is the solution of $A\\mathbf{x}=\\mathbf{b}$!\n\n## Example 2\n\nConsider two vector spaces $\\mathbb{R}^4$ and $\\mathbb{R}^3$ and theys canonical basis $\\mathbf{e}_i$ and $\\mathbf{b}_j$ and a map $f$ such that:\n\n$$\n\\begin{align}\nf(\\mathbf{e}_1)&=\\mathbf{b}_1+2\\mathbf{b}_2+3\\mathbf{b}_3\\\\\nf(\\mathbf{e}_2)&=2\\mathbf{b}_1+4\\mathbf{b}_2+6\\mathbf{b}_3\\\\\nf(\\mathbf{e}_3)&=2\\mathbf{b}_1+6\\mathbf{b}_2+8\\mathbf{b}_3\\\\\nf(\\mathbf{e}_4)&=2\\mathbf{b}_1+8\\mathbf{b}_2+10\\mathbf{b}_3\n\\end{align}\n$$ {#eq-f_on_basis}\n\nThe action of $f$ on some generic:\n\n$$\n\\mathbf{x}=x\\mathbf{e}_1+y\\mathbf{e}_2+z\\mathbf{e}_3+w\\mathbf{e}_w=\\begin{pmatrix}x\\\\y\\\\z\\\\w\\end{pmatrix}\n$$\n\ngives us:\n\n$$\nf(\\mathbf{x})=xf(\\mathbf{e}_1)+yf(\\mathbf{e}_2)+zf(\\mathbf{e}_3)+wf(\\mathbf{e}_w)=A\\mathbf{x}\n$$\n\nwhere we inserted @eq-f_on_basis at the second step, here $A$ is an already familiar matrix:\n\n$$ A=\\begin{pmatrix}1 & 2 & 2 & 2 \\\\2 & 4 & 6 & 8 \\\\3 & 6 & 8 & 10 \\end{pmatrix} $$\n\nNote: the three entries of $f(\\mathbf{x})$ is $d_1\\mathbf{b}_1+d_2\\mathbf{b}_2+d_2\\mathbf{b}_2$, thus\n\n$$\nf(\\mathbf{x})=\\begin{pmatrix}d_1\\\\d_2\\\\d_3\\end{pmatrix}\n$$\n\nWith the three steps done, we define this function as:\n\n$$ \\begin{align} f:\\mathbb{R}^4&\\longrightarrow \\mathbb{R}^3\\\\ \\mathbf{x}&\\longmapsto f(\\mathbf{x}):=A\\mathbf{x}  \\end{align} $$\n\nIts shape tell us all, $4$ columns ready to multiply the $4$ coefficients in $(x,y,z,w)$ returning a $3$ entry vector $A\\mathbf{x}$.\n\nFrom the matrix multiplying rules we know$A(\\alpha \\mathbf{x} +\\beta \\mathbf{y})=\\alpha A\\mathbf{x} +\\beta A \\mathbf{y}$, which in another notation mean $f(\\alpha \\mathbb{x} +\\beta \\mathbf{y})=\\alpha f(\\mathbf{x}) +\\beta f (\\mathbf{y})$. Thus the procedure on which $f$ was defined is said to be linear and thus we call this function a linear function.\n\nThe kernel of $f$ is:\n\n$$\n\\ker f = \\{\\mathbf{u}\\in\\mathbb{R}^4\\,\\,|\\,\\,A\\mathbf{u}=\\mathbf{0}\\}=span\\{\\begin{pmatrix}-2\\\\1\\\\0\\\\0 \\end{pmatrix},\\begin{pmatrix}2\\\\0\\\\-2\\\\1 \\end{pmatrix}\\}\n$$\n\nwhich is the familiar $N(A)$ we computed before.\n\nThe image of $f$ is:\n\n$$\nf(\\mathbb{R}^4) = \\{A\\mathbf{u}\\in\\mathbb{R}^3\\,\\,|\\,\\,\\mathbf{u}\\in\\mathbb{R}^4\\} =span\\{\\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix},\\begin{pmatrix}2\\\\6\\\\8\\end{pmatrix}\\}\n$$\n\nBut this is just what we called the column space of $A$.\n\nRecall the rowspace and left nullspace are orthogonal to these and have the following dimensions:\n\n$$\nC(A^\\intercal)=span\\{(1,2,2,2),(2,4,6,8)\\} \\qquad N(A^\\intercal)=span\\{(-1,-1,1)\\}\n$$\n\n![](figs/maps_fav_matrix.png)\n\nLooking at the diagram any $\\mathbf{x}$, it can broken into two parts:\n\n$$\n\\mathbf{x}=\\mathbf{x}_R+\\mathbf{x}_N\n$$\n\nAny $\\mathbf{b}$ can also be broken into:\n\n$$\n\\mathbf{b}=\\mathbf{b}_C+\\mathbf{b}_{LN}\n$$\n\nThe function $f$ maps any $\\mathbf{x}$ into the $C(A)$, the $N(A^\\intercal)$ is never reached. This means that for any $\\mathbf{b}$ with components in the left null space the equation:\n\n$$\nf(\\mathbf{x})=\\mathbf{b} \\iff A\\mathbf{x}=\\mathbf{b}\n$$\n\nhas no solution.\n\nAny $\\mathbf{b}$ living exclusively in the column space has a fiber of elements of the form $\\mathbf{x}_R+\\mathbf{x}_N$, for a fixed $\\mathbf{x}_R$ and $\\mathbf{x}_N$ ranging over the entire two dimensional $N(A)$.\n\nEvidently, this function is neither 1-1, nor onto, and no inverse (except in the sense of fiber) exists.\n\n## Example 3\n\nThe map:\n\n$$\n\\begin{align} f:\\mathbb{R}^3&\\longrightarrow \\mathbb{R}^3\\\\ \\mathbf{x}&\\longmapsto f(\\mathbf{x}):=A\\mathbf{x}  \\end{align}\n$$\n\nwith:\n\n$$ A=\\begin{pmatrix}1 & 1 & -1 \\\\2 & -1& 2 \\\\1 & 2 & -1\\end{pmatrix} $$\n\nHere, the vector $\\mathbf{x}$ is understood wrt the canonical basis of $\\mathbb{R}^3$:\n\n$$\n\\mathbf{x}=x\\mathbf{e}_1+y\\mathbf{e}_2+z\\mathbf{e}_3 =\\begin{pmatrix}x\\\\y\\\\z\\end{pmatrix}\n$$\n\nWhile its image:\n\n$$\nf(\\mathbf{x})=d_1\\mathbf{b}_1+d_2\\mathbf{b}_2+d_3\\mathbf{b}_3=\\begin{pmatrix}d_1\\\\d_2\\\\d_3\\end{pmatrix}=A\\mathbf{x}\n$$\n\nThis matrix has full rank and it maps as follows:\n\n![](figs/map_fc_full_rank.png){width=\"435\"}\n\nThere is only one solution. It is 1-1 and surjective. There is inverse.\n\n## Example 4:\n\n$$\n\\begin{align} f:\\mathbb{R}^3&\\longrightarrow \\mathbb{R}^3\\\\ \\mathbf{x}&\\longmapsto f(\\mathbf{x}):=A\\mathbf{x}  \\end{align}\n$$\n\nwith\n\n$$\nA=\\begin{pmatrix}2 & -1 \\\\2 & 1 \\\\2 & 1\\end{pmatrix} \n$$\n\n![](figs/map_no_sol.png){width=\"411\"}\n\nIf $b$ vector is outside colspace them there is no solution! Not surjective.\n\n## (Mathematical) Motivation: Linear Maps are Structure preserving maps. Whaaaat?\n\nWhy are these linear functions important?\n\nWe will answer this question in two ways: a zoom in version and a zoom out version (later)\n\n**Zoom in version:** The rule says something important about the map $f$, on the lhs we see an l.c. of elements of $\\mathbb{R}^n$, i.e., $\\nu\\bf{u}$ and $\\lambda \\bf{v}$ are mapped into $\\nu\\bf{u}+\\lambda\\bf{v}\\in \\mathbb{R}^n$. This vector in turn is mapped, now under $f$, into the element $f(\\nu\\bf{u}+\\lambda\\bf{v})\\in\\mathbb{R}^m$. On the rhs we see a l.c. of elements of $\\mathbb{R}^m$: $f (\\bf{u})$ is being combined with $f (\\bf{v})$ with the coefficients $\\nu$ and $\\lambda$.\n\n![](figs/linear_func.png)\n\nOn the picture we see a l.c. diagram-$(\\nu,\\lambda)$ on the left and another on the right whose coefficients are the same. These diagrams and all the other are what we can the connectivity structure of the vector space. Notice, the elements being combined are different - on the left we have $\\bf{u}$ with $n$ dimensions while $\\bf{u}'$ has $m$ dimensions.\n\nThe equality $f(\\nu \\bf{u}+\\lambda \\bf{v}) = \\nu f (\\bf{u}) + \\lambda f(\\bf{v})$ says the three points of diagram on the left are \"connected\" to a diagram on the right involving the same coefficients. As a result, the connection on the left is preserved because it is reproduced on the right. Its image under $f$ does not change.\n\n## Commentary\n\nAn example of a non-preserving function was the $\\square$ function whose behavior is diagrammatically akin to this:\n\n![](figs/not_linear.png)\n\nThe image of the l.c. diagram on the left does not have a corresponding l.c. diagram (with the same coefficients) on the right. Thus we say $\\square$ does not preserve the diagram during is mapping action.\n\nIf we linearly combine two vectors the following map is observed:\n\n![](figs/map_of_lc.png){width=\"378\"}\n\nHowever if we add the nullspace to each one of these we have the same map\n\n![](figs/planes_map.png)\n\nIn other words the structure on the domain is replicated in the codomain. The structure is preserved.\n\nEssentially, what we want with the introduction of this $f$ is to name an idea already present in $A\\mathbf{x}=\\mathbf{b}$, the idea that the $A\\mathbf{x}$ part of the equation is a function that maps $\\mathbf{x}$ into the vector $A\\mathbf{x}$. With this idea in mind, the system of equations encoded $A\\mathbf{x}=\\mathbf{b}$ is like asking what $\\mathbf{x}$ in the domain of $f$ is mapped into the fixed $\\mathbf{b}$ in the codomain.\n\n$$ A\\mathbf{x}=\\mathbf{b} \\iff f(\\mathbf{x})=\\mathbf{b} $$\n\n# Def f through action on the basis\n\n# Composition\n\n# Inverse.\n\n# Change of Basis\n\n## Is it linear or not?\n\n### Example 1:\n\nConsider the function:\n\n$$\n\\begin{align}f:\\mathbb{R}&\\longrightarrow \\mathbb{R}\\\\ x&\\longmapsto f(x):=10x\\end{align}\n$$\n\nIn this definition we specified clearly how $f$ maps elements of $\\mathbb{R}$ into $\\mathbb{R}$ - when $f$ acts on the element $x$ it assigns $f(x)$ which specifically is the element $10x$ in $\\mathbb{R}$. This $f$ in this case is the ten-x function. But does ithe procedure $10x$ have the so called linearity property?\n\nTo check, we choose two generic elements of the domain $x_1$ and $x_2$ and l.c. them using a generic coefficients $c_1$ and $c_2$:\n\n$$\nf(c_1 x_1+ c_2 x_2) = 10 ( c_1 x_1+ c_2 x_2) = c_1 10x_1+ c_2 10x_2 = c_1f(x_1)+c_2f(x_2)\n$$\n\nIndeed it is.\n\n### Example 2:\n\nNow a function from $\\mathbb{R}$ into $\\mathbb{R}^2$, which maps as follows:\n\n$$\n\\begin{align}g:\\mathbb{R}&\\longrightarrow \\mathbb{R}^2\\\\ x&\\longmapsto g(x):=(2x,x)\\end{align}\n$$\n\nThe procedure on which the map is based is to each $x$ compute $2x$ and construct which these numbers the vector $(2x,x)$. Is this procedure linear?\n\n$$\ng(c_1 x_1+ c_2 x_2)=(2(c_1 x_1+ c_2 x_2),c_1 x_1+ c_2 x_2) = c_1(2x_1+,x_1)+c_2(2x_2,x_2)=c_1g(x_1)+c_2g(x_2)\n$$\n\nBecause our choices where arbitrary, yes, this it is linear.\n\n### Example 3:\n\nThe projection function procedure consists in picks out the $x$ component of the vector $(x,y)$:\n\n$$\n\\begin{align}\\pi:\\mathbb{R}^2&\\longrightarrow \\mathbb{R}\\\\ (x,y)&\\longmapsto \\pi(x,y):=x\\end{align}\n$$\n\nIs it linear?\n\n$$\n\\pi(c_1(x_1,y_1)+c_2(x_2,y_2)) = \\pi (c_1x_1+c_2x_2,c_1y_1+c_2y_2)=c_1x_1+c_2x_2 =c_1 \\pi(x_1,y_1) + c_2\\pi(x_2,y_2)\n$$\n\nA l.c. of inputs - $(x_1,y_1)$ and $(x_2,y_2)$ - yields the same l.c. of outputs - $\\pi(x_1,y_1)$ and $\\pi(x_2,y_2)$.\n\n### Example 4:\n\nFrom $\\mathbb{R}^2$ to $\\mathbb{R}^2$:\n\n$$\n\\begin{align}\\Phi:\\mathbb{R}^2&\\longrightarrow \\mathbb{R}^2\\\\ (x,y)&\\longmapsto \\Phi(x,y):=(2x-y,x+y)\\end{align}\n$$\n\nIs it?\n\n$$\n\\begin{align}\n\\Phi(c_1(x_1,y_1)+c_2(x_2,y_2)) &= \\Phi(c_1x_1+c_2x_2,c_1y_1+c_2y_2)\\\\ \n&=(2(c_1x_1+c_2x_2)-(c_1y_1+c_2y_2),c_1x_1+c_2x_2+c_1y_1+c_2y_2)\\\\\n&=c_1(2x_1-y_1,x_1+y_1)+c_2(2x_2-y_2,x_2+y_2)\\\\\n&=c_1\\Phi(x_1,y_1)+c_2\\Phi(x_2,y_2)\n\\end{align}\n$$\n\nYes.\n\n### Example 5:\n\nFrom $\\mathbb{R}^2$ to $\\mathbb{R}^2$ once more:\n\n$$ \\begin{align}\\Xi:\\mathbb{R}^2&\\longrightarrow \\mathbb{R}^2\\\\ (x,y)&\\longmapsto \\Xi(x,y):=(2x-2y,x-y)\\end{align} $$\n\n?\n\n$$\n\\begin{align}\n\\Xi(c_1(x_1,y_1)+c_2(x_2,y_2)) &= \\Xi(c_1x_1+c_2x_2,c_1y_1+c_2y_2)\\\\\n &=(2(c_1x_1+c_2x_2)-2(c_1y_1+c_2y_2),c_1x_1+c_2x_2-c_1y_1-c_2y_2)\\\\\n&=c_1(2x_1-2y_1,x_1-y_1)+c_2(2x_2-2y_2,x_2-y_2)\\\\\n&=c_1\\Xi(x_1,y_1)+c_2\\Xi(x_2,y_2)\n\\end{align}\n$$\n\nIt is.\n\n### Example 6: (NON-linear)\n\nAgain, from $\\mathbb{R}^2$ to $\\mathbb{R}^2$, but this time we have a square involved, hence the name square function:\n\n$$\n\\begin{align}\\square:\\mathbb{R}^2&\\longrightarrow \\mathbb{R}^2\\\\ (x,y)&\\longmapsto \\square(x,y):=(2x^2,y)\\end{align}\n$$\n\nChecking as usual:\n\n$$\n\\begin{align}\n\\square(c_1(x_1,y_1)+c_2(x_2,y_2))&=\\square(c_1x_1+c_2x_2,c_1y_1+c_2y_2)\\\\\n&=(2(c_1x_1+c_2x_2)^2,c_1y_1+c_2y_2)\\\\\n&=(2((c_1x_1)^2+2c_1c_2x_1x_2+(c_2x_2)^2),c_1y_1+c_2y_2)\\\\\n&\\not=c_1\\square(x_1,y_1)+c_2\\square(x_2,y_2)\n\\end{align}\n$$\n\nThe range is $\\square(\\mathbb{R}^2)=[0,\\infty)\\times\\mathbb{R}$.\n\n::: callout-note\n## Commentary\n\n-   It is usual to give a more break down $f(\\nu \\bf{u}+\\lambda \\bf{v}) = \\nu f (\\bf{u}) + \\lambda f(\\bf{v})$ into two rules:\n\n    -   $f(\\bf{u}+\\bf{v})=f(\\bf{u})+f(\\bf{v})$\n\n    -   $f(\\lambda \\bf{u})=\\lambda f(\\bf{u})$\n\n    Because, if you know these then $f(\\nu \\bf{u}+\\lambda \\bf{v}) = \\nu f (\\bf{u}) + \\lambda f(\\bf{v})$ comes as natural consequence. So just checking for these two suffices.\n\n-   Notice the summation and multiplication are defined in distinct vector spaces.\n:::\n\n## Example 7: Matrices define procedures that define linear functions\n\n**Idea:** Consider two vector spaces $\\mathbb{R}^4$ and $\\mathbb{R}^3$. The vectors that live in $\\mathbb{R}^4$ have the form $\\mathbf{x}=(x,y,z,w)$ while those in $\\mathbb{R}^2$ are $\\mathbf{u}=(u,v,t)$.\n\nA function $f$ that maps vectors from $\\mathbb{R}^4$ and $\\mathbb{R}^3$ can be constructed from the procedure $A\\mathbf{x}$.\n\nWe define this function as:\n\n$$ \\begin{align} f:\\mathbb{R}^4&\\longrightarrow \\mathbb{R}^3\\\\ \\mathbf{x}&\\longmapsto f(\\mathbf{x}):=A\\mathbf{x}  \\end{align} $$\n\nwhere $A$ is for example (an already familiar matrix):\n\n$$ A=\\begin{pmatrix}1 & 2 & 2 & 2 \\\\2 & 4 & 6 & 8 \\\\3 & 6 & 8 & 10 \\end{pmatrix} $$\n\nIts shape tell us all, $4$ columns ready to multiply the $4$ coefficients in $(x,y,z,w)$ returning a $3$ entry vector $A\\mathbf{x}$.\n\nFrom the rules we seen about matrix multiplying vectors we know$A(\\alpha \\mathbb{x} +\\beta \\mathbf{y})=\\alpha A\\mathbf{x} +\\beta A \\mathbf{y}$, which in another notation mean $f(\\alpha \\mathbb{x} +\\beta \\mathbf{y})=\\alpha f(\\mathbf{x}) +\\beta f (\\mathbf{y})$. Thus the procedure on which $f$ was defined is said to be linear and thus we call this function a linear function.\n\nEssentially, what we want with the introduction of this $f$ is to name an idea already present in $A\\mathbf{x}=\\mathbf{b}$, the idea that the $A\\mathbf{x}$ part of the equation is a function that maps $\\mathbf{x}$ into the vector $A\\mathbf{x}$. With this idea in mind, the system of equations encoded $A\\mathbf{x}=\\mathbf{b}$ is like asking what $\\mathbf{x}$ in the domain of $f$ is mapped into the fixed $\\mathbf{b}$ in the codomain.\n\n$$ A\\mathbf{x}=\\mathbf{b} \\iff f(\\mathbf{x})=\\mathbf{b} $$\n\n## (Mathematical) Motivation\n\nWhy are these linear functions important?\n\nWe will answer this question in two ways: a zoom in version and a zoom out version (later)\n\n**Zoom in version:** The rule says something important about the map $f$, on the lhs we see an l.c. of elements of $\\mathbb{R}^n$, i.e., $\\nu\\bf{u}$ and $\\lambda \\bf{v}$ are mapped into $\\nu\\bf{u}+\\lambda\\bf{v}\\in \\mathbb{R}^n$. This vector in turn is mapped, now under $f$, into the element $f(\\nu\\bf{u}+\\lambda\\bf{v})\\in\\mathbb{R}^m$. On the rhs we see a l.c. of elements of $\\mathbb{R}^m$: $f (\\bf{u})$ is being combined with $f (\\bf{v})$ with the coefficients $\\nu$ and $\\lambda$.\n\n![](figs/linear_func.png)\n\nOn the picture we see a l.c. diagram-$(\\nu,\\lambda)$ on the left and another on the right whose coefficients are the same. These diagrams and all the other are what we can the connectivity structure of the vector space. Notice, the elements being combined are different - on the left we have $\\bf{u}$ with $n$ dimensions while $\\bf{u}'$ has $m$ dimensions.\n\nThe equality $f(\\nu \\bf{u}+\\lambda \\bf{v}) = \\nu f (\\bf{u}) + \\lambda f(\\bf{v})$ says the three points of diagram on the left are \"connected\" to a diagram on the right involving the same coefficients. As a result, the connection on the left is preserved because it is reproduced on the right. Its image under $f$ does not change.\n\n::: callout-note\n## Commentary\n\nAn example of a non-preserving function was the $\\square$ function whose behavior is diagrammatically akin to this:\n\n![](figs/not_linear.png)\n\nThe image of the l.c. diagram on the left does not have a corresponding l.c. diagram (with the same coefficients) on the right. Thus we say $\\square$ does not preserve the diagram during is mapping action.\n:::\n\nIf we linearly combine two vectors the following map is observed:\n\n![](figs/map_of_lc.png){width=\"378\"}\n\nHowever if we add the nullspace to each one of these we have the same map\n\n![](figs/planes_map.png)\n\nIn other words the structure on the domain is replicated in the codomain. The structure is preserved.\n\nEssentially, what we want with the introduction of this $f$ is to name an idea already present in $A\\mathbf{x}=\\mathbf{b}$, the idea that the $A\\mathbf{x}$ part of the equation is a function that maps $\\mathbf{x}$ into the vector $A\\mathbf{x}$. With this idea in mind, the system of equations encoded $A\\mathbf{x}=\\mathbf{b}$ is like asking what $\\mathbf{x}$ in the domain of $f$ is mapped into the fixed $\\mathbf{b}$ in the codomain.\n\n$$ A\\mathbf{x}=\\mathbf{b} \\iff f(\\mathbf{x})=\\mathbf{b} $$\n\n# Two important subspaces\n\nThe domain and codomain of a map $f:\\mathbb{R}^n\\overset{\\sim}{\\longrightarrow}\\mathbb{R}^m$ are defined by us when we introduce $f$ to the world. But within $\\mathbb{R}^n$ and $\\mathbb{R}^m$ there are subspaces of great importance and which are determined not by us diretly, but indirectly by the operation $f$ we choose.\n\nThese two spaces are the range of $f$ also known as the image of the domain under $f$ and the kernel or subspace of $f$, also known as the zeros of the function - from high-school.\n\n## Kernel of a linear function\n\n::: {style=\"color:gray\"}\nLet $f:\\mathbb{R}^n\\overset{\\sim}{\\longrightarrow} \\mathbb{R}^m$, then the kernel or nullspace of $f$ is the subset of the domain $\\mathbb{R}^n$:\n\n$$\n\\ker f = \\{\\bf{u}\\in\\mathbb{R}^n\\,\\,|\\,\\,f(\\bf{u})=\\bf{0}\\}\n$$\n\nMoreover, it is in fact a subspace.\n:::\n\n### Example 1 (cont):\n\nThe kernel of $f(x)=ax$ is the zeros of the function:\n\n$$\n\\ker f = \\{x\\in \\mathbb{R}\\,\\,|\\,\\,ax =0\\}=\\{0\\}\n$$\n\nIt is just one point in the domain, geometrically where the line intercept the x-axis.\n\n### Example 2 (cont):\n\nReturning to $g(x)=(2x,x)$, its nullspace is the subspace:\n\n$$\n\\ker g = \\{x\\in \\mathbb{R}\\,\\,|\\,\\,(2x,x) =(0,0)\\}=\\{0\\}\n$$\n\nAgain, just one vector.\n\n### Example 3 (cont):\n\n$$\n\\ker \\pi = \\{(x,y)\\in \\mathbb{R}\\,\\,|\\,\\,\\pi(x,y)=x=0\\}=\\{(0,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y\\in \\mathbb{R}\\}\n$$\n\n![](figs/ker_pi.png){width=\"326\"}\n\nAny vector along the subspace **\\[0y axis\\]** of the domain $\\mathbb{R}^2$ is always projected in the $0$ of the codomain. This vertical axis is the kernel of $\\pi$ (determined by $\\pi$ itself)\n\n### Example 4 (cont):\n\nThe kernel of $\\Phi(x,y)=(2x-y,x+y)$ is:\n\n$$\n\\ker \\Phi = \\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\,(2x-y,x+y)=(0,0)\\}\n$$\n\nWe have to solve the system of equations:\n\n$$\n\\begin{cases}\n2x-y=0\\\\\nx+y=0\n\\end{cases}\n\\leftrightsquigarrow\n\\begin{pmatrix}\n2 & -1\\\\\n1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx\\\\y\n\\end{pmatrix}\n=\\begin{pmatrix}\n0\\\\0\n\\end{pmatrix}\n\\leftrightsquigarrow\n\\begin{pmatrix}2 & -1 &\\bigm|0\\\\1 & 1 &\\bigm| 0\\end{pmatrix}\n$$\n\nBy inspection, the columns are independent and thus the solution is $(0,0)$.\n\n### Example 5 (cont):\n\n$$\n\\ker \\Xi = \\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\,(2x-2y,x-y)=(0,0)\\}\n$$\n\nThe corresponding system to solve is\n\n$$\n\\begin{pmatrix}2 & -2 &\\bigm|0\\\\1 & -1 &\\bigm| 0\\end{pmatrix}\n$$\n\nwhose solution is $c(1,1)$. The kernel is the whole line along $(1,1)$.\n\n## Image of the domain (range of linear functions)\n\n::: {style=\"color:gray\"}\nLet $f:\\mathbb{R}^n\\overset{\\sim}{\\longrightarrow} \\mathbb{R}^m$, then the range of $f$ is the subset of the domain $\\mathbb{R}^m$:\n\n$$\nf(\\mathbb{R}^n) = \\{f(\\bf{u})\\in\\mathbb{R}^m\\,\\,|\\,\\,\\bf{u}\\in\\mathbb{R}^n\\}\n$$\n\nMoreover, it is also a subspace.\n:::\n\nThe range of $f$ can is hard to compute, the concept is what is most important here.\n\nHow do we compute the image of the domain?\n\n**Example 1 (cont):**\n\nIs it possible to indentify any element $y$ of the codomain $\\mathbb{R}$ by some label $x$ of the domain $\\mathbb{R}$? In other words, can we solve for $x$, the equation:\n\n$$\nax = y\n$$\n\ngiven an arbitrary $y$?\n\nAnswer is yes, the form of the element of the domain assigned to $y$ is $y/a$.\n\nWe conclude $f(\\mathbb{R})=\\mathbb{R}$.\n\n**Example 2 (cont):**\n\nIs there for every $(u,v)\\in \\mathbb{R}^2$ a corresponding element $x$ in the domain $\\mathbb{R}$? We answer by solving for $x$:\n\n$$\n(2x,x)=(u,v) \\implies \\begin{cases}2x=u\\\\x=v\\end{cases}\\implies \\begin{cases}x=u/2\\\\x=v\\end{cases}\n$$\n\nThe answer is no. We cannot choose $u,v$ arbitrarily and find an $x$, the system only has solution $x$ for specific pairs of $u,v$, namely those that lie along the line $y=x/2$.\n\nComputing the image of the domain is $f(\\mathbb{R})=[\\text{line with equation}\\,\\, y=x/2]$ we can see that it is a subset of the codomain $\\mathbb{R}^2$ and thus not everyone of its elements (those outside the line in question) are not assigned to some real $x$. The function is therefore not surjective.\n\n**Example 3 (cont):**\n\nThe codomain is $\\mathbb{R}$ is it true or false that every one of its elements has a corresponding $(x,y)$ living in the domain $\\mathbb{R}^2$.\n\nLets solve for $(x,y)$ the equation:\n\n$$\n\\pi(x,y) = u\\implies x=u\n$$\n\nThere is indeed a solution for this equation, in fact many of them, of the form $(u,y)$ where $y$ is any real number.\n\nThe image of the domain under $\\pi$ is the same set as the codomain - $f(\\mathbb{R}^2)=\\mathbb{R}$ - and thus the function is surjective.\n\n**Example 4 (cont):**\n\nWe seek, given any $(u,y)$ of the codomain, the corresponding $(x,y)$ in the domain:\n\n$$\n\\Phi(x,y)=(u,v)\\implies \\begin{cases}2x-y=u\\\\x+y=v\\end{cases}\\implies\\begin{cases}x=(u+v)/3\\\\y=(2v-u)/3\\end{cases}\n$$\n\nAnother way to check this is to compute the image of the domain: $\\Phi(\\mathbb{R}^2)=\\mathbb{R}^2$, because the column of the matrix are independent. The $\\Phi$ function is surjective.\n\n**Example 5 (cont):**\n\nThe function $\\Xi$ maps $(x,y)\\in\\mathbb{R}^2$ into $(u,v)\\in\\mathbb{R}^2$. Is it true that every $(u,y)$ comes from some $(x,y)$? Lets solve the adequate equation:\n\n$$\n\\Xi(x,y)=(u,v) \\implies \\begin{cases}2x-2y=u\\\\x-y=v\\end{cases}\\implies \\begin{cases}x=u/2+y\\\\v=2u\\end{cases}\n$$\n\nThe equation $l_2$ tells us that we cannot choose any pair $u,v$ hence not every element of the codomain $\\mathbb{R}^2$ is assigned to some $(x,y)$.\n\nAnother way is to compute the image of the domain, notice as $x,y$ ranges in $\\mathbb{R}$, then $x-y$ ranges in $\\mathbb{R}$ as well, and thus the $\\Xi$ function is just like $\\Xi(\\spadesuit) = (2\\spadesuit,\\spadesuit)$, see example 2.\n\nThus\n\n$\\Xi(\\mathbb{R}^2) = [\\text{line with equation}\\,\\, y=x/2]$ again. And the function is not surjective.\n\n# Describing the type of mapping $f$\n\n1-1, sobrejective, bijective, etc, this is more important than the range. From Kernel we can evaluate the type of connectivity\n\nSince a function $f$ assigns elements $x$ to $y$ we can view it as analogous to assigning labels $y$ to the fruits $x$, hence $f$ allows us to distinguish elements in $X$ through the labels in $Y$. But how well were these labels assigned to the fruits? The ideal situation occurs when each $x$ has a unique label $y$, under this scenario we can retrieve the fruit from the label without any issue as they are unique; the worst case occurs when we have the same label for all $x$'s, its impossible to retrieve any fruits, under $f$ they are all the same. These are the two extremes of a spectrum of labeling, in between we have the intermediate cases where some fruits might have some common labels, examples of these three scenarios are shown in Fig.\n\n![](figs/onetoone_or_not.png)\n\nIt is precisely to evaluate the quality of the $f$ labeling that we introduce the words surjective, injective and bijective.\n\n::: {#def-onto_1to1 style=\"color:gray\"}\nLet $f:\\mathbb{R}^n\\overset{\\sim}{\\longrightarrow} \\mathbb{R}^m$ be a function, then we say:\n\n-   $[f\\,\\, \\text{is onto}]:= [\\forall \\mathbf{y} \\in \\mathbb{R}^m :\\exists\\mathbf{x}\\in\\mathbb{R}^n : f(\\mathbf{x})=\\mathbf{y}] = [\\text{Im}\\,\\,f =f(\\mathbb{R}^n)]$\n\n    Comment: every element in $\\mathbb{R}^m$ has a corresponding element in $\\mathbb{R}^n$.\n\n-   $[f\\,\\, \\text{is 1-1}]:=[\\forall \\mathbf{x},\\mathbf{x}' \\in \\mathbb{R}^n:\\mathbf{x}\\not =\\mathbf{x}' \\implies f(\\mathbf{x}_1)\\not= f(\\mathbf{x}')]=[\\ker f =\\{\\mathbf{0}_{\\mathbb{R}^n}\\}]$\n\n    Comment: in other words, to say $f(\\mathbf{x})=f(\\mathbf{x}')$ implies $\\mathbf{x}=\\mathbf{x}'$\n\n-   $[f\\,\\, \\text{is bijective}]:=[f\\,\\, \\text{is 1-1 and onto}]$\n:::\n\nSummarizing, a surjective function makes use of all available labels, but doesn't guarantee that the labels are used one time only. Thus it might happen that apples and strawberries are given the same label. On the other hand, a 1-1 function guarantees that the labels only appear once, but does not guarantee that all available labels are used. A bijective function makes use of all labels one time.\n\n# Dimension Theorem\n\n::: {#thm-dimension style=\"color: gray\"}\nLet $f:\\mathbb{R}^n\\overset{\\sim}{\\longrightarrow} \\mathbb{R}^m$, then:\n\n$$\n\\dim \\mathbb{R}^n=n = \\dim \\ker f + \\dim \\text{Im}\\, f\n$$\n:::\n\n**Example 1 (cont):** $1 = \\dim \\{0\\} + \\dim \\{y/a\\,\\,|\\,\\, y\\in\\mathbb{R}\\}=0+1$ Note: only one vectors is needed to expand the image.\n\n**Example 2 (cont):** $1 = \\dim \\{0\\} + \\dim [\\text{line with equation}\\,\\, y=x/2]=0+1$ Note: A line only needs one basis vector!\n\n**Example 3 (cont):** $2 = \\dim\\{(0,y)\\in\\mathbb{R}^2\\,\\,|\\,\\, y\\in\\mathbb{R}\\}+\\dim \\mathbb{R} = 1+1$\n\n**Example 4 (cont):** $2 = \\dim \\{(0,0)\\} +\\dim \\mathbb{R}^2=0+2$\n\n**Example 5 (cont):** $2 = \\dim \\{c(1,1)\\,\\,|\\,\\,c\\in\\mathbb{R}\\} + \\dim [\\text{line with equation} \\,\\, y=x/2]=1+1$\n\n![](images/clipboard-688989313.png)\n\nThis picture helps us also understand the following better:\n\n-   *A system of equations* $A\\mathbf{x}=\\mathbf{b}$ *have one solution provided* $r=r^*=m$. Why? $m-r=0$ and thus there is no nullspace, the whole $\\mathbb{R}^m$ is the row space. And the $\\mathbf{b}$ lives inside the column space. The vector $\\mathbf{x}$ is in the row space and we are computing ................???????\n\n-   *A system of equations* $A\\mathbf{x}=\\mathbf{b}$ *have infinite solutions provided* $r=r^*<n$.\n\n-   *A system of equations* $A\\mathbf{x}=\\mathbf{b}$ *have no solutions provided* $r<r^*$.\n\n## Maps involved in matrix multiplication (composition)\n\n![](figs/matrix%20_product_maps.png)\n\nWHEN IS THER INERVES; EXPLAIN\n","srcMarkdownNoYaml":"\n\nUp to now, we learned a more practical part of the course:\n\n-   How to compute the solution of $A\\mathbf{x}=\\mathbf{b}$?\n\n-   How to compute a basis for the $4$ subspaces associated with $A$?\n\n-   How to compute the inverse $A^{-1}$?\n\n-   How to compute the determinant?\n\nIt always *how to compute*. Now we enter a more conceptual part of the course and organize ideas.\n\n# The concept of a vector space\n\n*Suspend what you know:* We have been speaking about a vector space as being closed under linear combinations. However if we stop to think about it, we can only compute a linear combination provided we know how to multiply vectors by numbers and how to add vectors, see below:\n\n$$\n\\overbrace{\\alpha u}^\\text{number times a vector} + \\overbrace{\\beta v}^\\text{number times a vector}=\\underbrace{u'+v'}_\\text{sum of vectors}\n$$\n\nFor the moment we will consider these two operations in separate and also for the moment suspend how they are done in practice, thus, when we think about:\n\n$$\n(1,3)+(4,7)=?\\qquad 10(2,5)=?\n$$\n\nWe do not know how to do these operations.\n\nNow lets return to the idea of a vector space.\n\n*Today:* A set of things is just a list. Let us label those things as\n\n$$\n\\mathbb{V}=\\{v_1,v_2,\\dots,v_n\\}\n$$\n\nWhat can we do with the things in this set?\n\nWe can define a map, a map is a rule or sequence of operations that tell me how to assigns to each pair of elements in $\\mathbb{V}$ a new element in $\\mathbb{V}$. Right now, we are not thinking about a specific map, you can even imagine (if it helps you) that aliens choose the map, but did not told us what it is (except for that fact that they chose one.)\n\nAnother kind of map they decide to introduce, for which we do not know yet how it operates consists in assigning to each pairing of a real number and an element of $\\mathbb{V}$, another element of $\\mathbb{V}$.\n\nThe notion of map as we just described in words is described in mathematical notation as:\n\n$$\n\\begin{align}\n+ : \\mathbb{V}\\times\\mathbb{V} &\\longrightarrow\\mathbb{V}\\\\\n(u,v)&\\longmapsto u\\tilde{+}v\n\\end{align}\n$$\n\nand\n\n$$\n\\begin{align}\\cdot : \\mathbb{R}\\times\\mathbb{V} &\\longrightarrow\\mathbb{V}\\\\(\\alpha,v)&\\longmapsto \\alpha\\cdot v\\end{align}\n$$\n\nThe $u+v$ is the symbol for the element in $\\mathbb{V}$ that corresponds to the pair $(u,v)$. And $\\alpha\\cdot v$ is the symbol for the element assigned to $(\\alpha,v)$.\n\nNow they disclose to us a few properties that their maps have:\n\n-   Commutative: $u+v=v+u$. \\[*You can flip the order*\\]\n\n-   Associative: $(u+v)+w=u+(v+w)$. \\[*You can shift the brackets*\\]\n\n-   Neutral element: There is a neutral element wrt to $+$: There is an element in $\\mathbf{V}$, call it $o$ so that for all $u\\in \\mathbb{V}$ we find $u+o=u$. \\[*This is not* $0$ *from the real numbers, it is the zero of* $\\mathbb{V}$*. To avoid confusion you can call* $o$ *as circle rather than zero.*\\]\n\n-   Inverse element: For every element $u$ there an element, which we denote by $-u$, that guarantees $u+(-u)=o$. \\[ $u-v$ is just a short notation for $u+(-v)$\\]\n\nThe next list of rules describe how the operations $+$ and $\\cdot$ interact when both present in a calculation, two of this rules are distributive rules:\n\n-   Associative: $\\alpha(\\beta u)=(\\alpha\\beta)v$\n\n-   Distributive I: $(\\alpha+\\beta)u = \\alpha u +\\beta u$\n\n-   Distributive II: $\\alpha (u +v) = \\alpha u + \\alpha v$\n\n-   Unitary: $1 u = u$\n\nWe do not know specifically how the assignments are made, the only thing we know is: whatever they are, they obey the rules above.\n\nA vector space is a triplet $(\\mathbb{V},+,\\cdot)$\n\nIts like describing someone: you can list all its characteristics, or just list the essential one. Here essential depends for what the description is useful for.\n\n::: callout-note\n## Commentary\n\nWhy is maps and these rules? Because on them fit many useful maps in practice.\n\nThese rules all boil down to $\\mathbb{V}$ is closed under l.c. where the l.c. is realized in practice the natural and expected way. The rules above are the rules of common sense for l.c.\n:::\n\nWhat specific sets and $+$ and $\\cdot$ operations fit the profile above? Here are some examples:\n\n## Example 1 of a vector space\n\nThe set of all polynomials:\n\n$$\n\\mathbb{P}=\\{\\dots+a_n x^n+a_{n-1} x^{n-1} +\\dots +a_1 x +a_0\\,\\,|\\,\\, a_i \\text{'s are real}\\}\n$$\n\nTogether with:\n\n$$\n\\begin{align}+ : \\mathbb{P}\\times\\mathbb{P} &\\longrightarrow\\mathbb{V}\\\\(p,q)&\\longmapsto (p+q)(x)=p(x)+q(x)\\end{align}\n$$\n\nHere, the $+$ sign between functions $p$ and $q$ manifests in practice as the summation $+$ of real numbers $p(x)$ and $q(x)$.\n\n$$\n\\begin{align}\\cdot : \\mathbb{R}\\times\\mathbb{P} &\\longrightarrow\\mathbb{V}\\\\(\\alpha,p)&\\longmapsto (\\alpha\\cdot p)(x):=\\alpha\\cdot p(x)\\end{align}\n$$\n\nThe multiplication between a real number and the function $p$ is understood in therms of the multiplication between the real number $\\alpha$ and $p(x)$.\n\nAll the rules above can be checked for this example.\n\nThe natural basis for this vector space is: $\\{1,x,x^2,\\dots\\}$\n\n## Example 2 of a vector space\n\nConsider the set:\n\n$\\mathbb{V} = \\{(x,y)\\,\\,|\\,\\, x,y\\in \\mathbb{R}\\}$\n\nAnd the maps:\n\n$$\n\\begin{align}\n+ : \\mathbb{V}\\times\\mathbb{V} &\\longrightarrow\\mathbb{V}\\\\\n(\\mathbf{u},\\mathbf{v})&\\longmapsto \\mathbf{u}+\\mathbf{v} := (u_x,u_y)+(v_x,v_y)=(u_x+v_x,u_y+v_y)\n\\end{align}\n$$\n\nand\n\n$$\n\\begin{align}\\cdot : \\mathbb{R}\\times\\mathbb{V} &\\longrightarrow\\mathbb{V}\\\\(\\alpha,\\mathbf{u})&\\longmapsto \\alpha\\cdot \\mathbf{u}:=(\\alpha u_x,\\alpha u_y)\\end{align}\n$$\n\nOnce again the summation of vectors is in practice the summation of numbers and the multiplication of a vector by a real number means the multiplication of its entries (real numbers) by the real number $\\alpha$.\n\nNotice that when I write $(u_x,u_y)+(v_x,v_y)$ one may wonder what it means, we know it is another element of $\\mathbf{V}$, but which one? To answer that we have to tell how the operation $+$ between two elements of $\\mathbb{V}$ actually maps. Whatever the way it maps, it must be consistent with the rules defined above. It turn out that to define this $+$ as doing $(u_x+v_x,u_y+v_y)$ (a plus between reals) is a good and useful choice.\n\nThe natural choice of basis for this vector space is: $\\{(1,0),(0,1)\\}$\n\n# Intuitive idea of structure of a vs (Retirar?)\n\nFor example let us define the summation and multiplication by a number.\n\nFor example take $\\mathbb{R}^2$, we know that:\n\n$$\n(1,3)+(0,1) = (1,4)\n$$\n\nTherefore, the vectors $(1,3)$ and $(0,1)$ are connected to $(1,4)$.\n\nWhen I multiply a vector by number:\n\n$$\n3(1,9) = (3,27)\n$$\n\nand as a result the vector $(1,9)$ is connected with $(3,27)$.\n\nA generic l.c. $\\alpha(1,3)+\\beta(0,1) = (\\alpha,3\\alpha+\\beta)$. Connects $\\alpha(1,3)$ and $\\beta(0,1)$ with $(\\alpha,3\\alpha+\\beta)$.\n\nThe operation l.c. creates this web of connectivity between elements of a vector space.\n\n# Linear functions (START HERE)\n\nConsider two vector spaces $\\mathbb{R}^n$ and $\\mathbb{R}^m$.\n\nA linear function is a bridge between elements of both vectors spaces defined as follows:\n\n::: {#def-linear_func style=\"color:gray\"}\n$$\n\\begin{align}\nf:\\mathbb{R}^n&\\longrightarrow \\mathbb{R}^m\\\\\n\\bf{u}&\\longmapsto f(\\bf{u})\n\\end{align}\n$$\n\nsuch that the following rule is obeyed: $f(\\lambda \\bf{u}+\\mu \\bf{v}) = \\lambda f (\\bf{u}) + \\mu f(\\bf{v})$\n\nfor all $\\lambda,\\mu \\in \\mathbb{R}$ and $\\bf{u},\\bf{v}\\in \\mathbb{R}^n$.\n\n*Terminology:* The vector space $\\mathbb{R}^n$ is the domain of the function $f$, while $\\mathbb{R}^m$ is the codomain (of $f$)\n\nWhen referring to a linear function we will use $\\sim$ and write: $f:\\mathbb{R}^n\\overset{\\sim}{\\longrightarrow} \\mathbb{R}^m$.\n:::\n\nWe do know how $f$ maps between the spaces, but whatever procedure $f$ is, it must have the property $f(\\lambda \\bf{u}+\\mu \\bf{v}) = \\lambda f (\\bf{u}) + \\mu f(\\bf{v})$. In fact, this property, together with the choice of a basis for the domain and codomain, we can write a concrete formula (procedure) for $f$.\n\n**Goal:** Choose the natural basis for the domain and the natural basis for the codomain and then write a formula for the linear map $f$ .\n\n**Idea:** If we know how a linear function $f$ maps the chosen basis of the domain, we know how it maps any vector of the domain.\n\nTo put this idea in practice we follow three steps:\n\n## Step 1: Choose a basis for the domain and codomain\n\nAs said above we have to chose a basis for $\\mathbb{R}^n$, let it be:\n\n$$\n\\mathbf{e}_1:=\\begin{pmatrix}1\\\\0\\\\ \\vdots\\\\0\\end{pmatrix}\\qquad \\mathbf{e}_2:=\\begin{pmatrix}0\\\\1\\\\ \\vdots\\\\0\\end{pmatrix}\\qquad\\dots\\qquad \\mathbf{e}_n:=\\begin{pmatrix}0\\\\0\\\\ \\vdots\\\\1\\end{pmatrix}\n$$\n\nEach vector is $n\\times 1$ and there are $n$ of them.\n\nThis basis is called the canonical basis of $\\mathbb{R}^n$, it is the natural basis for that space.\n\nThe next step is to choose a basis for the codomain $\\mathbb{R}^m$. Let us choose its canonical basis:\n\n$$\n\\mathbf{b}_1:=\\begin{pmatrix}1\\\\0\\\\ \\vdots\\\\0\\end{pmatrix}\\qquad \\mathbf{b}_2:=\\begin{pmatrix}0\\\\1\\\\ \\vdots\\\\0\\end{pmatrix}\\qquad\\dots\\qquad \\mathbf{b}_m:=\\begin{pmatrix}0\\\\0\\\\ \\vdots\\\\1\\end{pmatrix}\n$$ {#eq-basis_of_codomain}\n\nWhere this time each vector is $m\\times 1$.\n\n### Example\n\nLet $n=2$ and $m=3$. The canonical basis for the domain $\\mathbb{R}^2$ is:\n\n$$\n\\mathbf{e}_1=\\begin{pmatrix}1\\\\0\\end{pmatrix}\\qquad \\mathbf{e}_2=\\begin{pmatrix}0\\\\1\\end{pmatrix}\n$$\n\nThe canonical basis for the codomain $\\mathbb{R}^3$ is:\n\n$$\n\\mathbf{b}_1=\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}\\qquad \\mathbf{b}_2=\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}\\qquad\\mathbf{b}_3=\\begin{pmatrix}0\\\\0\\\\1\\end{pmatrix}\n$$\n\n## Step 2: Decide how $f$ acts on the domain's basis\n\nWhen $f$ acts on $\\mathbf{e}_1$, we obtain $f(\\mathbf{e}_j)$, but what is $f(\\mathbf{e}_j)$ specifically? We can say what it is specifically by saying what this vector (living in $\\mathbb{R}^m$) looks like wrt to its basis. Thus $f(\\mathbf{e}_j)$ can be written by an appropriate l.c. of the $\\mathbf{b}$'s:\n\n$$\nf(\\mathbf{e}_j) = \\sum_{i=1}^m A_{ij}\\,\\,\\mathbf{b}_i\n$$ {#eq-repre_of_output}\n\nThe numbers $A_{ij}$ are chosen by us and they tell us how much we want $f(\\mathbf{e}_j)$ to look like $\\mathbf{b}_i$. Of course $A_{ij}$ has two indices $i$ and $j$, one for the current $\\mathbf{e}_j$ being mapped and another for the basis vector $\\mathbf{e}_j$ to which it is being compared, as a result $i=1,2,..,m$ and $j=1,2,..,n$.\n\n### Example\n\nI choose the $A_{ij}$ to be like this:\n\n$$\n\\begin{align}\nf(\\mathbf{e}_1)&=A_{11}\\mathbf{b}_1+A_{21}\\mathbf{b}_2+A_{31}\\mathbf{b}_3=2\\mathbf{b}_1+2\\mathbf{b}_2+2\\mathbf{b}_3\\\\\nf(\\mathbf{e}_2)&=A_{12}\\mathbf{b}_1+A_{22}\\mathbf{b}_2+A_{32}\\mathbf{b}_3=-\\mathbf{b}_1+\\mathbf{b}_2+ \\mathbf{b}_3\n\\end{align}\n$$ {#eq-choice_of_As}\n\n## Step 3: Mapping a generic vector of the domain\n\nArmed with @eq-repre_of_output we can compute the output $f(\\mathbf{u})$ for any $\\mathbf{u}$ of the domain $\\mathbb{R}^2$, i.e., for any $\\mathbf{u}$ of the form:\n\n$$\n\\mathbf{u}=c_1\\mathbf{e}_1+c_2\\mathbf{e}_2\n$$\n\nLet us do the calculation:\n\n$$\n\\begin{align}\nf(\\mathbf{u}) &= f(c_1\\mathbf{e}_1+c_2\\mathbf{e}_2)\\\\\n&=c_1f(\\mathbf{e}_1)+c_2f(\\mathbf{e}_2)\\\\\n&=c_1(A_{11}\\mathbf{b}_1+A_{21}\\mathbf{b}_2+A_{31}\\mathbf{b}_3)+c_2(A_{12}\\mathbf{b}_1+A_{22}\\mathbf{b}_2+A_{32}\\mathbf{b}_3)\\\\\n&=(A_{11}c_1+A_{12}c_2)\\mathbf{b}_1+(A_{21}c_1+A_{22}c_2)\\mathbf{b}_2+(A_{31}c_1+A_{32}c_2)\\mathbf{b}_3\n\\end{align}\n$$\n\nNow we substitute the numbers $A_{ij}$ we chose in @eq-choice_of_As into:\n\n$$\nf(c_1\\mathbf{e}_1+c_2\\mathbf{e}_2)=(A_{11}c_1+A_{12}c_2)\\mathbf{b}_1+(A_{21}c_1+A_{22}c_2)\\mathbf{b}_2+(A_{31}c_1+A_{32}c_2)\\mathbf{b}_3\n$$ {#eq-general_map}\n\nwhich gives us:\n\n$$\nf(c_1\\mathbf{e}_1+c_2\\mathbf{e}_2)=(2c_1-c_2)\\mathbf{b}_1+(2c_1+c_2)\\mathbf{b}_2+(c_1+c_2)\\mathbf{b}_3\n$$\n\nFor example, if I want to map $3\\mathbf{e}_1+7\\mathbf{e}_2$, we obtain:\n\n$$\nf(3\\mathbf{e}_1+7\\mathbf{e}_2)=(2\\cdot3-7)\\mathbf{b}_1+(2\\cdot 3+7)\\mathbf{b}_2+(3+7)\\mathbf{b}_3=-\\mathbf{b}_1+13\\mathbf{b}_2+10\\mathbf{b}_3\n$$\n\n**Important observation:** Since $f(\\mathbf{u})$ is in the codomain $\\mathbb{R}^3$, then it is some l.c. of the $\\mathbf{b}$'s as we did for the $f(\\mathbf{e}_1)$ and $f(\\mathbf{e}_2)$ in @eq-choice_of_As. Therefore:\n\n$$\nf(\\mathbf{u})=d_1\\mathbf{b}_1+d_2\\mathbf{b}_2+d_3\\mathbf{b}_3\n$$ {#eq-general_repre}\n\nComparing @eq-general_map with @eq-general_repre:\n\n$$\nd_1\\mathbf{b}_1+d_2\\mathbf{b}_2+d_3\\mathbf{b}_3=(A_{11}c_1+A_{12}c_2)\\mathbf{b}_1+(A_{21}c_1+A_{22}c_2)\\mathbf{b}_2+(A_{31}c_1+A_{32}c_2)\\mathbf{b}_3\n$$\n\nwe conclude that the coefficients $d$ are in fact the calculations:\n\n$$\n\\begin{align}\nd_1&=A_{11}c_1+A_{12}c_2\\\\\nd_2 &= A_{21}c_1+A_{22}c_2\\\\\nd_3 &=A_{31}c_1+A_{32}c_2\n\\end{align}\n$$\n\n## A formula for $f$:\n\nFrom the three steps above we can write the image of $\\mathbf{u}=c_1\\mathbf{e}_1+\\dots+c_m\\mathbf{e}_m$ under $f$ as:\n\n$$\n\\begin{align}\nf(\\mathbf{u})&=f(\\sum_{j=1}^m c_j\\mathbf{e}_j)\\\\\n&=\\sum_{j=1}^n c_jf(\\mathbf{e}_j)\\\\\n&=\\sum_{j=1}^n c_j\\sum_{i=1}^m A_{ij}\\,\\,\\mathbf{b}_i\\\\\n&=\\sum_{i=1}^m\\left(\\sum_{j=1}^n A_{ij}c_j\\right)\\mathbf{b}_i\n\\end{align}\n$$ {#eq-gen_map}\n\nBut since $f(\\mathbf{u})=\\sum_{i=1}^m d_i\\,\\,\\mathbf{b}_i$, substituting in @eq-gen_map:\n\n$$\n\\sum_{i=1}^m d_i\\mathbf{b}_i=f(\\mathbf{u})=\\sum_{i=1}^m\\left(\\sum_{j=1}^n A_{ij}c_j\\right)\\mathbf{b}_i\n$$\n\nSince the coefficients must be the same we conclude that:\n\n$$\nd_i=\\sum_{j=1}^n A_{ij}c_j\n$$ {#eq-matrix_mult_in_comp}\n\nThis calculation tell us something important: from the coefficients of $\\mathbf{u}$ on the chosen basis of the domain of $f$, they give us the coefficients of $f(\\mathbf{u})$ in the chosen basis of the codomain. Lets read better the meaning of this formula, the $c$'s tell us how much the input vector $\\mathbf{u}$ looks like to each one of the basis vectors $\\mathbf{e}$. The numbers $A_{ij}$ quantify how much a basis vector $f(\\mathbf{e}_j)$ look like the basis vector $\\mathbf{b}_j$. This means that the calculation $\\sum_{j=1}^n A_{ij}c_j$ can be understood like:\n\n**\\[**how much $f(\\mathbf{u})$ looks like $\\mathbf{b}_i$**\\]** = **\\[**how much $f(\\mathbf{e}_1)$ looks like $\\mathbf{b}_i$**\\]** $\\times$ **\\[**how much $\\mathbf{u}$ looks like $\\mathbf{e}_1$**\\]** $+$ \\[how much $f(\\mathbf{e}_2)$ looks like $\\mathbf{b}_i$**\\]** $\\times$ **\\[**how much $\\mathbf{u}$ looks like $\\mathbf{e}_2$**\\]** $+\\dots +$ **\\[**how much $f(\\mathbf{e}_n)$ looks like $\\mathbf{b}_i$**\\]** $\\times$ **\\[**how much $\\mathbf{u}$ looks like $\\mathbf{e}_n$**\\]**\n\nIts a weighted sum of the representation of $\\mathbf{u}$!\n\nThis understanding is clear because we know exactly what basis we are speaking off, after all, the first step was to make the choice of bases.\n\nNow, we can arrange these numbers (if you wish) in a more geometrically pleasing form as:\n\n$$\n\\left(\\begin{matrix}d_1\\\\d_2\\\\\\vdots\\\\d_n\\end{matrix}\\right)=\n\\left(\\begin{matrix}A_{11} & A_{12} & \\dots & A_{1m}\\\\A_{21} & A_{22} & \\dots & A_{2m} \\\\\\vdots & \\vdots &\\ddots & \\vdots\\\\A_{n1} & A_{n2} &\\dots & A_{nm} \\end{matrix}\\right)\n\\left(\\begin{matrix}c_1\\\\c_2\\\\\\vdots\\\\c_m\\end{matrix}\\right)\n$$ {#eq-matrix_form}\n\nwhich is just the $A\\mathbf{u}$. Always in the back of our minds, we need to remember that @eq-matrix_form is just @eq-matrix_mult_in_comp, and @eq-matrix_mult_in_comp comes from @eq-gen_map, where is the meaning of these coefficients $d$, $A$'s and $c$'s are clear (because right next to them we find the basis vectors they multiply)\n\nAs a final conclusion we should note that: if the bases chosen where canonical bases, then we can rewrite the formula:\n\n$$\nf(\\mathbf{u})=\\sum_{i=1}^m\\left(\\sum_{j=1}^n A_{ij}c_j\\right)\\mathbf{b}_i\n$$ {#eq-one}\n\nby substituting in it the $\\mathbf{b}$'s in @eq-basis_of_codomain, the then @eq-one turn into:\n\n$$\nf(\\mathbf{u}) = A\\mathbf{u}\n$$ {#eq-two}\n\nNow we have the context where operations $A$ times a vector $\\mathbf{u}$ emerges. They are the concrete realization of linear maps between vector spaces after a basis for each is chosen.\n\n*Notice:* @eq-one is useful to understand the meaning of the numbers, while @eq-two is useful to do calculations, like column space and nullspace as we shall see.\n\nThe matrix we have been considering is:\n\n$$\n\\left(\\begin{matrix}d_1\\\\d_2\\\\d_3\\end{matrix}\\right)=\\left(\\begin{matrix}2 & -1\\\\2 & 1 \\\\1 & 1  \\end{matrix}\\right)\\left(\\begin{matrix}c_1\\\\c_2\\end{matrix}\\right)\n$$\n\n::: callout-note\n## Commentary\n\nI wish you could follow the steps:\n\nStep 1: Choose a basis for the domain and codomain\n\nStep 2: Decide how $f$ acts on the domain's basis\n\nStep 3: Mapping a generic vector of the domain\n\nand reach @eq-matrix_mult_in_comp and then the matrix.\n\nAnd also go backwards!\n:::\n\n# Revisiting Two important subspaces\n\nThe domain and codomain of a map $f:\\mathbb{R}^n\\overset{\\sim}{\\longrightarrow}\\mathbb{R}^m$ are defined by us when we introduce $f$ to the world. But within $\\mathbb{R}^n$ and $\\mathbb{R}^m$ there are subspaces of great importance and which are determined by the type of operation $f$ we choose.\n\nThese two spaces are the range of $f$ also known as the *image of the domain under* $f$ and the *kernel* (or nullspace) *of* $f$*,* \\[*Comment:* the nullspace of $f$ are the zeros of the function, just you have seen in high-school.\\]\n\n## Kernel of a linear function\n\n::: {#def-kernel style=\"color:gray\"}\nLet $f:\\mathbb{R}^n\\overset{\\sim}{\\longrightarrow} \\mathbb{R}^m$, then the kernel of $f$ is the subset of the domain $\\mathbb{R}^n$:\n\n$$ \\ker f = \\{\\mathbf{u}\\in\\mathbb{R}^n\\,\\,|\\,\\,f(\\bf{u})=\\bf{0}\\} $$\n\nMoreover, it is in fact a subspace.\n:::\n\n## Image of the domain (range of linear functions)\n\n::: {#def-range style=\"color:gray\"}\nLet $f:\\mathbb{R}^n\\overset{\\sim}{\\longrightarrow} \\mathbb{R}^m$, then the range of $f$ is the subset of the domain $\\mathbb{R}^m$:\n\n$$ f(\\mathbb{R}^n) = \\{f(\\mathbf{u})\\in\\mathbb{R}^m\\,\\,|\\,\\,\\mathbf{u}\\in\\mathbb{R}^n\\} $$\n\nMoreover, it is also a subspace.\n:::\n\n## Example 1\n\nLets follow the three steps again.\n\nWe intend to define a function $f:\\mathbb{R}^3\\overset{\\sim}{\\longrightarrow} \\mathbb{R}$. To do that we new first a basis for $\\mathbb{R}^3$ and another for $\\mathbb{R}$:\n\n$$\n\\mathbf{e}_1=\\begin{pmatrix}1\\\\0\\\\0 \\end{pmatrix}\\qquad \\mathbf{e}_2=\\begin{pmatrix}0\\\\1\\\\0 \\end{pmatrix} \\qquad \\mathbf{e}_3=\\begin{pmatrix}0\\\\0\\\\1 \\end{pmatrix}\\qquad\n\\mathbf{b}_1=1\n$$ {#eq-bases_of_ex1}\n\nTo write a formula for $f$ given these bases we at step two decide how $f$ acts on the chosencanonical bases:\n\n$$\nf(\\mathbf{e}_1)=f(\\mathbf{e}_3)=2\\mathbf{b}_1\\qquad f(\\mathbf{e}_2)=0\\mathbf{b}_1\n$$\n\nAt step three we now write the action of $f$ on an arbitrary vector $\\mathbf{x}\\in\\mathbb{R}^3$ is:\n\n$$\n\\begin{align}\nf(x_1\\mathbf{e}_1+x_2\\mathbf{e}_2+x_3\\mathbf{e}_3)&=x_1f(\\mathbf{e}_1)+x_2f(\\mathbf{e}_2)+x_3f(\\mathbf{e}_3)\\\\\n&=(2x_1+2x_3)\\mathbf{b}_1\n\\end{align}\n$$ {#eq-eq1}\n\nand then decompose $f(x_1\\mathbf{e}_1+x_2\\mathbf{e}_2+x_3\\mathbf{e}_3)$ on the basis of $\\mathbb{R}$:\n\n$$\nf(x_1\\mathbf{e}_1+x_2\\mathbf{e}_2+x_3\\mathbf{e}_3)=d_1\\mathbf{b}_1\n$$ {#eq-eq2}\n\nWhat is $d_1$ in terms of the $x_1$, $x_2$ and $x_3$? Comparing @eq-eq1 and @eq-eq2 we arrive at:\n\n$$\nd_1=2x_1+2x_3\n$$\n\nThis equation is analogous to @eq-matrix_mult_in_comp, from this we identify the matrix $A=(2,0,2)$:\n\n$$\nd_1=(2,0,2)\\begin{pmatrix}x_1\\\\x_2\\\\x_3\\end{pmatrix} \n$$\n\nThis formula is exactly @eq-matrix_form!\n\nWith the information above, the function\n\n$$\n\\begin{align} f:\\mathbb{R}^3&\\longrightarrow \\mathbb{R}\\\\ \\mathbf{x}&\\longmapsto f(\\mathbf{x}):=2(x_1+x_3)\\mathbf{b}_1 \\end{align}\n$$\n\nSubstituting $\\mathbf{b}_1=1$ and the above can be rewritten as:\n\n$$\n\\begin{align} f:\\mathbb{R}^3&\\longrightarrow \\mathbb{R}\\\\ \\mathbf{x}&\\longmapsto f(\\mathbf{x}):=A\\mathbf{x} =2(x+z)  \\end{align}\n$$\n\n### The 4 spaces of example 1:\n\nWhat is the kernel? It is the nullspace of $2(1,0,1)$ which is the set of vectors perpendicular to it:\n\n$$\n2(2,0,2)\\begin{pmatrix}x_{1N}\\\\x_{2N}\\\\x_{3N}\\end{pmatrix}=0\n$$\n\nSince we have one pivot on the first column and two dependent columns, we expect two dimensions.\n\nIsolating $Col_2$ by setting $y_N=1$ and $z_N=0$, the only answer is $x_N=0$, thus $(0,1,0)$ is in the nullspace.\n\nIsolating $Col_3$ by setting $y_N=0$ and $z_N=1$, we find $x_N=-1$, the second basis vector is $(-1,0,1)$.\n\nAny l.c. of these solutions is also mapped to $0$, thus:\n\n$$\n\\ker f = span \\{(0,1,0)^\\intercal,(-1,0,1)^\\intercal\\}\n$$\n\nThe image is one dimensional since there is only one pivot:\n\n$$\nf(\\mathbb{R}^3)=\\mathbb{R}\n$$\n\nFrom the matrix $A$ we can define the row-space as\n\n$$\nC(A^\\intercal)=span\\{(1,0,1)^\\intercal\\}\n$$\n\nWhile the left-nullspace is just $N(A^\\intercal)=\\{0\\}$.\n\nThe rowspace and null space break the domain $\\mathbb{R}^3$ into two subspaces, while the column space and codomain are equal:\n\n$$\n\\mathbb{R}^3 = span\\{(1,0,1)^\\intercal\\}\\oplus span \\{(0,1,0)^\\intercal,(-1,0,1)^\\intercal\\}\\qquad \\mathbb{R} = \\mathbb{R}\\oplus\\{0\\}\n$$\n\nwhich can be rewritten using @eq-bases_of_ex1 as:\n\n$$\n\\mathbb{R}^3 = span\\{\\mathbf{e}_1+\\mathbf{e}_2\\}\\oplus span \\{\\mathbf{e}_2,-\\mathbf{e}_1+\\mathbf{e}_2\\}\\qquad \\mathbb{R} = span\\{\\mathbf{b}_1\\}\\oplus\\{\\mathbf{0}\\}\n$$\n\nFrom this, the following picture tells us how one vector $\\mathbf{u}$ in $\\mathbb{R}^3$ is mapped into $f(\\mathbf{u})\\in\\mathbb{R}$\n\n![](figs/maps_yellow_vector.png)\n\nGiven these subspaces of the domain and codomain we can now focus on describing the kind of mapping that $f$ does. On the picture we see a vector $\\mathbf{x}$ being decomposed into its parts, one in the nullspace and another in the row space:\n\n$$\n\\mathbf{x}=\\mathbf{x}_{R}+\\mathbf{x}_{N}\n$$\n\nThe $\\mathbf{x}_N$ is mapped by this $f$ into $0$, and $\\mathbf{x}_R$ is mapped into $f(\\mathbf{x})$, thus we can write $f(\\mathbf{x})=f(\\mathbf{x}_R)$ for all $\\mathbf{x}\\in \\mathbb{R}^3$.\n\nFor each $\\mathbf{x}_R$ in the rowspace we can any add any vector of the null space, the image is always $f(\\mathbf{x}_R)$. Therefore this is not a 1-1 function.\n\nBut is it onto? For any element $d_1$ of $\\mathbb{R}$ we can find a $\\mathbf{x}$, such that $f(\\mathbf{x})=d_1$ i.e.\n\n$$\nd_1=2x+2z\n$$\n\nis always solvable. This $f$ is onto.\n\nAs a consequence we conclude that any equation\n\n$$\nf(\\mathbf{x})=b\n$$\n\n(given a $b$) always has solution, in fact has an infinite number of solutions, thanks to the fact that the nullspace is not just $\\mathbf{0}$.\n\nAnother way to put this, this $f$ has no inverse, but a fibre can be defined:\n\n$$\nf^{-1}(b)=\\{\\mathbf{x}\\in\\mathbb{R}^3\\,\\,|\\,\\,f(\\mathbf{x})=b\\}\n$$\n\nThis set is the solution of $A\\mathbf{x}=\\mathbf{b}$!\n\n## Example 2\n\nConsider two vector spaces $\\mathbb{R}^4$ and $\\mathbb{R}^3$ and theys canonical basis $\\mathbf{e}_i$ and $\\mathbf{b}_j$ and a map $f$ such that:\n\n$$\n\\begin{align}\nf(\\mathbf{e}_1)&=\\mathbf{b}_1+2\\mathbf{b}_2+3\\mathbf{b}_3\\\\\nf(\\mathbf{e}_2)&=2\\mathbf{b}_1+4\\mathbf{b}_2+6\\mathbf{b}_3\\\\\nf(\\mathbf{e}_3)&=2\\mathbf{b}_1+6\\mathbf{b}_2+8\\mathbf{b}_3\\\\\nf(\\mathbf{e}_4)&=2\\mathbf{b}_1+8\\mathbf{b}_2+10\\mathbf{b}_3\n\\end{align}\n$$ {#eq-f_on_basis}\n\nThe action of $f$ on some generic:\n\n$$\n\\mathbf{x}=x\\mathbf{e}_1+y\\mathbf{e}_2+z\\mathbf{e}_3+w\\mathbf{e}_w=\\begin{pmatrix}x\\\\y\\\\z\\\\w\\end{pmatrix}\n$$\n\ngives us:\n\n$$\nf(\\mathbf{x})=xf(\\mathbf{e}_1)+yf(\\mathbf{e}_2)+zf(\\mathbf{e}_3)+wf(\\mathbf{e}_w)=A\\mathbf{x}\n$$\n\nwhere we inserted @eq-f_on_basis at the second step, here $A$ is an already familiar matrix:\n\n$$ A=\\begin{pmatrix}1 & 2 & 2 & 2 \\\\2 & 4 & 6 & 8 \\\\3 & 6 & 8 & 10 \\end{pmatrix} $$\n\nNote: the three entries of $f(\\mathbf{x})$ is $d_1\\mathbf{b}_1+d_2\\mathbf{b}_2+d_2\\mathbf{b}_2$, thus\n\n$$\nf(\\mathbf{x})=\\begin{pmatrix}d_1\\\\d_2\\\\d_3\\end{pmatrix}\n$$\n\nWith the three steps done, we define this function as:\n\n$$ \\begin{align} f:\\mathbb{R}^4&\\longrightarrow \\mathbb{R}^3\\\\ \\mathbf{x}&\\longmapsto f(\\mathbf{x}):=A\\mathbf{x}  \\end{align} $$\n\nIts shape tell us all, $4$ columns ready to multiply the $4$ coefficients in $(x,y,z,w)$ returning a $3$ entry vector $A\\mathbf{x}$.\n\nFrom the matrix multiplying rules we know$A(\\alpha \\mathbf{x} +\\beta \\mathbf{y})=\\alpha A\\mathbf{x} +\\beta A \\mathbf{y}$, which in another notation mean $f(\\alpha \\mathbb{x} +\\beta \\mathbf{y})=\\alpha f(\\mathbf{x}) +\\beta f (\\mathbf{y})$. Thus the procedure on which $f$ was defined is said to be linear and thus we call this function a linear function.\n\nThe kernel of $f$ is:\n\n$$\n\\ker f = \\{\\mathbf{u}\\in\\mathbb{R}^4\\,\\,|\\,\\,A\\mathbf{u}=\\mathbf{0}\\}=span\\{\\begin{pmatrix}-2\\\\1\\\\0\\\\0 \\end{pmatrix},\\begin{pmatrix}2\\\\0\\\\-2\\\\1 \\end{pmatrix}\\}\n$$\n\nwhich is the familiar $N(A)$ we computed before.\n\nThe image of $f$ is:\n\n$$\nf(\\mathbb{R}^4) = \\{A\\mathbf{u}\\in\\mathbb{R}^3\\,\\,|\\,\\,\\mathbf{u}\\in\\mathbb{R}^4\\} =span\\{\\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix},\\begin{pmatrix}2\\\\6\\\\8\\end{pmatrix}\\}\n$$\n\nBut this is just what we called the column space of $A$.\n\nRecall the rowspace and left nullspace are orthogonal to these and have the following dimensions:\n\n$$\nC(A^\\intercal)=span\\{(1,2,2,2),(2,4,6,8)\\} \\qquad N(A^\\intercal)=span\\{(-1,-1,1)\\}\n$$\n\n![](figs/maps_fav_matrix.png)\n\nLooking at the diagram any $\\mathbf{x}$, it can broken into two parts:\n\n$$\n\\mathbf{x}=\\mathbf{x}_R+\\mathbf{x}_N\n$$\n\nAny $\\mathbf{b}$ can also be broken into:\n\n$$\n\\mathbf{b}=\\mathbf{b}_C+\\mathbf{b}_{LN}\n$$\n\nThe function $f$ maps any $\\mathbf{x}$ into the $C(A)$, the $N(A^\\intercal)$ is never reached. This means that for any $\\mathbf{b}$ with components in the left null space the equation:\n\n$$\nf(\\mathbf{x})=\\mathbf{b} \\iff A\\mathbf{x}=\\mathbf{b}\n$$\n\nhas no solution.\n\nAny $\\mathbf{b}$ living exclusively in the column space has a fiber of elements of the form $\\mathbf{x}_R+\\mathbf{x}_N$, for a fixed $\\mathbf{x}_R$ and $\\mathbf{x}_N$ ranging over the entire two dimensional $N(A)$.\n\nEvidently, this function is neither 1-1, nor onto, and no inverse (except in the sense of fiber) exists.\n\n## Example 3\n\nThe map:\n\n$$\n\\begin{align} f:\\mathbb{R}^3&\\longrightarrow \\mathbb{R}^3\\\\ \\mathbf{x}&\\longmapsto f(\\mathbf{x}):=A\\mathbf{x}  \\end{align}\n$$\n\nwith:\n\n$$ A=\\begin{pmatrix}1 & 1 & -1 \\\\2 & -1& 2 \\\\1 & 2 & -1\\end{pmatrix} $$\n\nHere, the vector $\\mathbf{x}$ is understood wrt the canonical basis of $\\mathbb{R}^3$:\n\n$$\n\\mathbf{x}=x\\mathbf{e}_1+y\\mathbf{e}_2+z\\mathbf{e}_3 =\\begin{pmatrix}x\\\\y\\\\z\\end{pmatrix}\n$$\n\nWhile its image:\n\n$$\nf(\\mathbf{x})=d_1\\mathbf{b}_1+d_2\\mathbf{b}_2+d_3\\mathbf{b}_3=\\begin{pmatrix}d_1\\\\d_2\\\\d_3\\end{pmatrix}=A\\mathbf{x}\n$$\n\nThis matrix has full rank and it maps as follows:\n\n![](figs/map_fc_full_rank.png){width=\"435\"}\n\nThere is only one solution. It is 1-1 and surjective. There is inverse.\n\n## Example 4:\n\n$$\n\\begin{align} f:\\mathbb{R}^3&\\longrightarrow \\mathbb{R}^3\\\\ \\mathbf{x}&\\longmapsto f(\\mathbf{x}):=A\\mathbf{x}  \\end{align}\n$$\n\nwith\n\n$$\nA=\\begin{pmatrix}2 & -1 \\\\2 & 1 \\\\2 & 1\\end{pmatrix} \n$$\n\n![](figs/map_no_sol.png){width=\"411\"}\n\nIf $b$ vector is outside colspace them there is no solution! Not surjective.\n\n## (Mathematical) Motivation: Linear Maps are Structure preserving maps. Whaaaat?\n\nWhy are these linear functions important?\n\nWe will answer this question in two ways: a zoom in version and a zoom out version (later)\n\n**Zoom in version:** The rule says something important about the map $f$, on the lhs we see an l.c. of elements of $\\mathbb{R}^n$, i.e., $\\nu\\bf{u}$ and $\\lambda \\bf{v}$ are mapped into $\\nu\\bf{u}+\\lambda\\bf{v}\\in \\mathbb{R}^n$. This vector in turn is mapped, now under $f$, into the element $f(\\nu\\bf{u}+\\lambda\\bf{v})\\in\\mathbb{R}^m$. On the rhs we see a l.c. of elements of $\\mathbb{R}^m$: $f (\\bf{u})$ is being combined with $f (\\bf{v})$ with the coefficients $\\nu$ and $\\lambda$.\n\n![](figs/linear_func.png)\n\nOn the picture we see a l.c. diagram-$(\\nu,\\lambda)$ on the left and another on the right whose coefficients are the same. These diagrams and all the other are what we can the connectivity structure of the vector space. Notice, the elements being combined are different - on the left we have $\\bf{u}$ with $n$ dimensions while $\\bf{u}'$ has $m$ dimensions.\n\nThe equality $f(\\nu \\bf{u}+\\lambda \\bf{v}) = \\nu f (\\bf{u}) + \\lambda f(\\bf{v})$ says the three points of diagram on the left are \"connected\" to a diagram on the right involving the same coefficients. As a result, the connection on the left is preserved because it is reproduced on the right. Its image under $f$ does not change.\n\n## Commentary\n\nAn example of a non-preserving function was the $\\square$ function whose behavior is diagrammatically akin to this:\n\n![](figs/not_linear.png)\n\nThe image of the l.c. diagram on the left does not have a corresponding l.c. diagram (with the same coefficients) on the right. Thus we say $\\square$ does not preserve the diagram during is mapping action.\n\nIf we linearly combine two vectors the following map is observed:\n\n![](figs/map_of_lc.png){width=\"378\"}\n\nHowever if we add the nullspace to each one of these we have the same map\n\n![](figs/planes_map.png)\n\nIn other words the structure on the domain is replicated in the codomain. The structure is preserved.\n\nEssentially, what we want with the introduction of this $f$ is to name an idea already present in $A\\mathbf{x}=\\mathbf{b}$, the idea that the $A\\mathbf{x}$ part of the equation is a function that maps $\\mathbf{x}$ into the vector $A\\mathbf{x}$. With this idea in mind, the system of equations encoded $A\\mathbf{x}=\\mathbf{b}$ is like asking what $\\mathbf{x}$ in the domain of $f$ is mapped into the fixed $\\mathbf{b}$ in the codomain.\n\n$$ A\\mathbf{x}=\\mathbf{b} \\iff f(\\mathbf{x})=\\mathbf{b} $$\n\n# Def f through action on the basis\n\n# Composition\n\n# Inverse.\n\n# Change of Basis\n\n## Is it linear or not?\n\n### Example 1:\n\nConsider the function:\n\n$$\n\\begin{align}f:\\mathbb{R}&\\longrightarrow \\mathbb{R}\\\\ x&\\longmapsto f(x):=10x\\end{align}\n$$\n\nIn this definition we specified clearly how $f$ maps elements of $\\mathbb{R}$ into $\\mathbb{R}$ - when $f$ acts on the element $x$ it assigns $f(x)$ which specifically is the element $10x$ in $\\mathbb{R}$. This $f$ in this case is the ten-x function. But does ithe procedure $10x$ have the so called linearity property?\n\nTo check, we choose two generic elements of the domain $x_1$ and $x_2$ and l.c. them using a generic coefficients $c_1$ and $c_2$:\n\n$$\nf(c_1 x_1+ c_2 x_2) = 10 ( c_1 x_1+ c_2 x_2) = c_1 10x_1+ c_2 10x_2 = c_1f(x_1)+c_2f(x_2)\n$$\n\nIndeed it is.\n\n### Example 2:\n\nNow a function from $\\mathbb{R}$ into $\\mathbb{R}^2$, which maps as follows:\n\n$$\n\\begin{align}g:\\mathbb{R}&\\longrightarrow \\mathbb{R}^2\\\\ x&\\longmapsto g(x):=(2x,x)\\end{align}\n$$\n\nThe procedure on which the map is based is to each $x$ compute $2x$ and construct which these numbers the vector $(2x,x)$. Is this procedure linear?\n\n$$\ng(c_1 x_1+ c_2 x_2)=(2(c_1 x_1+ c_2 x_2),c_1 x_1+ c_2 x_2) = c_1(2x_1+,x_1)+c_2(2x_2,x_2)=c_1g(x_1)+c_2g(x_2)\n$$\n\nBecause our choices where arbitrary, yes, this it is linear.\n\n### Example 3:\n\nThe projection function procedure consists in picks out the $x$ component of the vector $(x,y)$:\n\n$$\n\\begin{align}\\pi:\\mathbb{R}^2&\\longrightarrow \\mathbb{R}\\\\ (x,y)&\\longmapsto \\pi(x,y):=x\\end{align}\n$$\n\nIs it linear?\n\n$$\n\\pi(c_1(x_1,y_1)+c_2(x_2,y_2)) = \\pi (c_1x_1+c_2x_2,c_1y_1+c_2y_2)=c_1x_1+c_2x_2 =c_1 \\pi(x_1,y_1) + c_2\\pi(x_2,y_2)\n$$\n\nA l.c. of inputs - $(x_1,y_1)$ and $(x_2,y_2)$ - yields the same l.c. of outputs - $\\pi(x_1,y_1)$ and $\\pi(x_2,y_2)$.\n\n### Example 4:\n\nFrom $\\mathbb{R}^2$ to $\\mathbb{R}^2$:\n\n$$\n\\begin{align}\\Phi:\\mathbb{R}^2&\\longrightarrow \\mathbb{R}^2\\\\ (x,y)&\\longmapsto \\Phi(x,y):=(2x-y,x+y)\\end{align}\n$$\n\nIs it?\n\n$$\n\\begin{align}\n\\Phi(c_1(x_1,y_1)+c_2(x_2,y_2)) &= \\Phi(c_1x_1+c_2x_2,c_1y_1+c_2y_2)\\\\ \n&=(2(c_1x_1+c_2x_2)-(c_1y_1+c_2y_2),c_1x_1+c_2x_2+c_1y_1+c_2y_2)\\\\\n&=c_1(2x_1-y_1,x_1+y_1)+c_2(2x_2-y_2,x_2+y_2)\\\\\n&=c_1\\Phi(x_1,y_1)+c_2\\Phi(x_2,y_2)\n\\end{align}\n$$\n\nYes.\n\n### Example 5:\n\nFrom $\\mathbb{R}^2$ to $\\mathbb{R}^2$ once more:\n\n$$ \\begin{align}\\Xi:\\mathbb{R}^2&\\longrightarrow \\mathbb{R}^2\\\\ (x,y)&\\longmapsto \\Xi(x,y):=(2x-2y,x-y)\\end{align} $$\n\n?\n\n$$\n\\begin{align}\n\\Xi(c_1(x_1,y_1)+c_2(x_2,y_2)) &= \\Xi(c_1x_1+c_2x_2,c_1y_1+c_2y_2)\\\\\n &=(2(c_1x_1+c_2x_2)-2(c_1y_1+c_2y_2),c_1x_1+c_2x_2-c_1y_1-c_2y_2)\\\\\n&=c_1(2x_1-2y_1,x_1-y_1)+c_2(2x_2-2y_2,x_2-y_2)\\\\\n&=c_1\\Xi(x_1,y_1)+c_2\\Xi(x_2,y_2)\n\\end{align}\n$$\n\nIt is.\n\n### Example 6: (NON-linear)\n\nAgain, from $\\mathbb{R}^2$ to $\\mathbb{R}^2$, but this time we have a square involved, hence the name square function:\n\n$$\n\\begin{align}\\square:\\mathbb{R}^2&\\longrightarrow \\mathbb{R}^2\\\\ (x,y)&\\longmapsto \\square(x,y):=(2x^2,y)\\end{align}\n$$\n\nChecking as usual:\n\n$$\n\\begin{align}\n\\square(c_1(x_1,y_1)+c_2(x_2,y_2))&=\\square(c_1x_1+c_2x_2,c_1y_1+c_2y_2)\\\\\n&=(2(c_1x_1+c_2x_2)^2,c_1y_1+c_2y_2)\\\\\n&=(2((c_1x_1)^2+2c_1c_2x_1x_2+(c_2x_2)^2),c_1y_1+c_2y_2)\\\\\n&\\not=c_1\\square(x_1,y_1)+c_2\\square(x_2,y_2)\n\\end{align}\n$$\n\nThe range is $\\square(\\mathbb{R}^2)=[0,\\infty)\\times\\mathbb{R}$.\n\n::: callout-note\n## Commentary\n\n-   It is usual to give a more break down $f(\\nu \\bf{u}+\\lambda \\bf{v}) = \\nu f (\\bf{u}) + \\lambda f(\\bf{v})$ into two rules:\n\n    -   $f(\\bf{u}+\\bf{v})=f(\\bf{u})+f(\\bf{v})$\n\n    -   $f(\\lambda \\bf{u})=\\lambda f(\\bf{u})$\n\n    Because, if you know these then $f(\\nu \\bf{u}+\\lambda \\bf{v}) = \\nu f (\\bf{u}) + \\lambda f(\\bf{v})$ comes as natural consequence. So just checking for these two suffices.\n\n-   Notice the summation and multiplication are defined in distinct vector spaces.\n:::\n\n## Example 7: Matrices define procedures that define linear functions\n\n**Idea:** Consider two vector spaces $\\mathbb{R}^4$ and $\\mathbb{R}^3$. The vectors that live in $\\mathbb{R}^4$ have the form $\\mathbf{x}=(x,y,z,w)$ while those in $\\mathbb{R}^2$ are $\\mathbf{u}=(u,v,t)$.\n\nA function $f$ that maps vectors from $\\mathbb{R}^4$ and $\\mathbb{R}^3$ can be constructed from the procedure $A\\mathbf{x}$.\n\nWe define this function as:\n\n$$ \\begin{align} f:\\mathbb{R}^4&\\longrightarrow \\mathbb{R}^3\\\\ \\mathbf{x}&\\longmapsto f(\\mathbf{x}):=A\\mathbf{x}  \\end{align} $$\n\nwhere $A$ is for example (an already familiar matrix):\n\n$$ A=\\begin{pmatrix}1 & 2 & 2 & 2 \\\\2 & 4 & 6 & 8 \\\\3 & 6 & 8 & 10 \\end{pmatrix} $$\n\nIts shape tell us all, $4$ columns ready to multiply the $4$ coefficients in $(x,y,z,w)$ returning a $3$ entry vector $A\\mathbf{x}$.\n\nFrom the rules we seen about matrix multiplying vectors we know$A(\\alpha \\mathbb{x} +\\beta \\mathbf{y})=\\alpha A\\mathbf{x} +\\beta A \\mathbf{y}$, which in another notation mean $f(\\alpha \\mathbb{x} +\\beta \\mathbf{y})=\\alpha f(\\mathbf{x}) +\\beta f (\\mathbf{y})$. Thus the procedure on which $f$ was defined is said to be linear and thus we call this function a linear function.\n\nEssentially, what we want with the introduction of this $f$ is to name an idea already present in $A\\mathbf{x}=\\mathbf{b}$, the idea that the $A\\mathbf{x}$ part of the equation is a function that maps $\\mathbf{x}$ into the vector $A\\mathbf{x}$. With this idea in mind, the system of equations encoded $A\\mathbf{x}=\\mathbf{b}$ is like asking what $\\mathbf{x}$ in the domain of $f$ is mapped into the fixed $\\mathbf{b}$ in the codomain.\n\n$$ A\\mathbf{x}=\\mathbf{b} \\iff f(\\mathbf{x})=\\mathbf{b} $$\n\n## (Mathematical) Motivation\n\nWhy are these linear functions important?\n\nWe will answer this question in two ways: a zoom in version and a zoom out version (later)\n\n**Zoom in version:** The rule says something important about the map $f$, on the lhs we see an l.c. of elements of $\\mathbb{R}^n$, i.e., $\\nu\\bf{u}$ and $\\lambda \\bf{v}$ are mapped into $\\nu\\bf{u}+\\lambda\\bf{v}\\in \\mathbb{R}^n$. This vector in turn is mapped, now under $f$, into the element $f(\\nu\\bf{u}+\\lambda\\bf{v})\\in\\mathbb{R}^m$. On the rhs we see a l.c. of elements of $\\mathbb{R}^m$: $f (\\bf{u})$ is being combined with $f (\\bf{v})$ with the coefficients $\\nu$ and $\\lambda$.\n\n![](figs/linear_func.png)\n\nOn the picture we see a l.c. diagram-$(\\nu,\\lambda)$ on the left and another on the right whose coefficients are the same. These diagrams and all the other are what we can the connectivity structure of the vector space. Notice, the elements being combined are different - on the left we have $\\bf{u}$ with $n$ dimensions while $\\bf{u}'$ has $m$ dimensions.\n\nThe equality $f(\\nu \\bf{u}+\\lambda \\bf{v}) = \\nu f (\\bf{u}) + \\lambda f(\\bf{v})$ says the three points of diagram on the left are \"connected\" to a diagram on the right involving the same coefficients. As a result, the connection on the left is preserved because it is reproduced on the right. Its image under $f$ does not change.\n\n::: callout-note\n## Commentary\n\nAn example of a non-preserving function was the $\\square$ function whose behavior is diagrammatically akin to this:\n\n![](figs/not_linear.png)\n\nThe image of the l.c. diagram on the left does not have a corresponding l.c. diagram (with the same coefficients) on the right. Thus we say $\\square$ does not preserve the diagram during is mapping action.\n:::\n\nIf we linearly combine two vectors the following map is observed:\n\n![](figs/map_of_lc.png){width=\"378\"}\n\nHowever if we add the nullspace to each one of these we have the same map\n\n![](figs/planes_map.png)\n\nIn other words the structure on the domain is replicated in the codomain. The structure is preserved.\n\nEssentially, what we want with the introduction of this $f$ is to name an idea already present in $A\\mathbf{x}=\\mathbf{b}$, the idea that the $A\\mathbf{x}$ part of the equation is a function that maps $\\mathbf{x}$ into the vector $A\\mathbf{x}$. With this idea in mind, the system of equations encoded $A\\mathbf{x}=\\mathbf{b}$ is like asking what $\\mathbf{x}$ in the domain of $f$ is mapped into the fixed $\\mathbf{b}$ in the codomain.\n\n$$ A\\mathbf{x}=\\mathbf{b} \\iff f(\\mathbf{x})=\\mathbf{b} $$\n\n# Two important subspaces\n\nThe domain and codomain of a map $f:\\mathbb{R}^n\\overset{\\sim}{\\longrightarrow}\\mathbb{R}^m$ are defined by us when we introduce $f$ to the world. But within $\\mathbb{R}^n$ and $\\mathbb{R}^m$ there are subspaces of great importance and which are determined not by us diretly, but indirectly by the operation $f$ we choose.\n\nThese two spaces are the range of $f$ also known as the image of the domain under $f$ and the kernel or subspace of $f$, also known as the zeros of the function - from high-school.\n\n## Kernel of a linear function\n\n::: {style=\"color:gray\"}\nLet $f:\\mathbb{R}^n\\overset{\\sim}{\\longrightarrow} \\mathbb{R}^m$, then the kernel or nullspace of $f$ is the subset of the domain $\\mathbb{R}^n$:\n\n$$\n\\ker f = \\{\\bf{u}\\in\\mathbb{R}^n\\,\\,|\\,\\,f(\\bf{u})=\\bf{0}\\}\n$$\n\nMoreover, it is in fact a subspace.\n:::\n\n### Example 1 (cont):\n\nThe kernel of $f(x)=ax$ is the zeros of the function:\n\n$$\n\\ker f = \\{x\\in \\mathbb{R}\\,\\,|\\,\\,ax =0\\}=\\{0\\}\n$$\n\nIt is just one point in the domain, geometrically where the line intercept the x-axis.\n\n### Example 2 (cont):\n\nReturning to $g(x)=(2x,x)$, its nullspace is the subspace:\n\n$$\n\\ker g = \\{x\\in \\mathbb{R}\\,\\,|\\,\\,(2x,x) =(0,0)\\}=\\{0\\}\n$$\n\nAgain, just one vector.\n\n### Example 3 (cont):\n\n$$\n\\ker \\pi = \\{(x,y)\\in \\mathbb{R}\\,\\,|\\,\\,\\pi(x,y)=x=0\\}=\\{(0,y)\\in \\mathbb{R}^2\\,\\,|\\,\\, y\\in \\mathbb{R}\\}\n$$\n\n![](figs/ker_pi.png){width=\"326\"}\n\nAny vector along the subspace **\\[0y axis\\]** of the domain $\\mathbb{R}^2$ is always projected in the $0$ of the codomain. This vertical axis is the kernel of $\\pi$ (determined by $\\pi$ itself)\n\n### Example 4 (cont):\n\nThe kernel of $\\Phi(x,y)=(2x-y,x+y)$ is:\n\n$$\n\\ker \\Phi = \\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\,(2x-y,x+y)=(0,0)\\}\n$$\n\nWe have to solve the system of equations:\n\n$$\n\\begin{cases}\n2x-y=0\\\\\nx+y=0\n\\end{cases}\n\\leftrightsquigarrow\n\\begin{pmatrix}\n2 & -1\\\\\n1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx\\\\y\n\\end{pmatrix}\n=\\begin{pmatrix}\n0\\\\0\n\\end{pmatrix}\n\\leftrightsquigarrow\n\\begin{pmatrix}2 & -1 &\\bigm|0\\\\1 & 1 &\\bigm| 0\\end{pmatrix}\n$$\n\nBy inspection, the columns are independent and thus the solution is $(0,0)$.\n\n### Example 5 (cont):\n\n$$\n\\ker \\Xi = \\{(x,y)\\in \\mathbb{R}^2\\,\\,|\\,\\,(2x-2y,x-y)=(0,0)\\}\n$$\n\nThe corresponding system to solve is\n\n$$\n\\begin{pmatrix}2 & -2 &\\bigm|0\\\\1 & -1 &\\bigm| 0\\end{pmatrix}\n$$\n\nwhose solution is $c(1,1)$. The kernel is the whole line along $(1,1)$.\n\n## Image of the domain (range of linear functions)\n\n::: {style=\"color:gray\"}\nLet $f:\\mathbb{R}^n\\overset{\\sim}{\\longrightarrow} \\mathbb{R}^m$, then the range of $f$ is the subset of the domain $\\mathbb{R}^m$:\n\n$$\nf(\\mathbb{R}^n) = \\{f(\\bf{u})\\in\\mathbb{R}^m\\,\\,|\\,\\,\\bf{u}\\in\\mathbb{R}^n\\}\n$$\n\nMoreover, it is also a subspace.\n:::\n\nThe range of $f$ can is hard to compute, the concept is what is most important here.\n\nHow do we compute the image of the domain?\n\n**Example 1 (cont):**\n\nIs it possible to indentify any element $y$ of the codomain $\\mathbb{R}$ by some label $x$ of the domain $\\mathbb{R}$? In other words, can we solve for $x$, the equation:\n\n$$\nax = y\n$$\n\ngiven an arbitrary $y$?\n\nAnswer is yes, the form of the element of the domain assigned to $y$ is $y/a$.\n\nWe conclude $f(\\mathbb{R})=\\mathbb{R}$.\n\n**Example 2 (cont):**\n\nIs there for every $(u,v)\\in \\mathbb{R}^2$ a corresponding element $x$ in the domain $\\mathbb{R}$? We answer by solving for $x$:\n\n$$\n(2x,x)=(u,v) \\implies \\begin{cases}2x=u\\\\x=v\\end{cases}\\implies \\begin{cases}x=u/2\\\\x=v\\end{cases}\n$$\n\nThe answer is no. We cannot choose $u,v$ arbitrarily and find an $x$, the system only has solution $x$ for specific pairs of $u,v$, namely those that lie along the line $y=x/2$.\n\nComputing the image of the domain is $f(\\mathbb{R})=[\\text{line with equation}\\,\\, y=x/2]$ we can see that it is a subset of the codomain $\\mathbb{R}^2$ and thus not everyone of its elements (those outside the line in question) are not assigned to some real $x$. The function is therefore not surjective.\n\n**Example 3 (cont):**\n\nThe codomain is $\\mathbb{R}$ is it true or false that every one of its elements has a corresponding $(x,y)$ living in the domain $\\mathbb{R}^2$.\n\nLets solve for $(x,y)$ the equation:\n\n$$\n\\pi(x,y) = u\\implies x=u\n$$\n\nThere is indeed a solution for this equation, in fact many of them, of the form $(u,y)$ where $y$ is any real number.\n\nThe image of the domain under $\\pi$ is the same set as the codomain - $f(\\mathbb{R}^2)=\\mathbb{R}$ - and thus the function is surjective.\n\n**Example 4 (cont):**\n\nWe seek, given any $(u,y)$ of the codomain, the corresponding $(x,y)$ in the domain:\n\n$$\n\\Phi(x,y)=(u,v)\\implies \\begin{cases}2x-y=u\\\\x+y=v\\end{cases}\\implies\\begin{cases}x=(u+v)/3\\\\y=(2v-u)/3\\end{cases}\n$$\n\nAnother way to check this is to compute the image of the domain: $\\Phi(\\mathbb{R}^2)=\\mathbb{R}^2$, because the column of the matrix are independent. The $\\Phi$ function is surjective.\n\n**Example 5 (cont):**\n\nThe function $\\Xi$ maps $(x,y)\\in\\mathbb{R}^2$ into $(u,v)\\in\\mathbb{R}^2$. Is it true that every $(u,y)$ comes from some $(x,y)$? Lets solve the adequate equation:\n\n$$\n\\Xi(x,y)=(u,v) \\implies \\begin{cases}2x-2y=u\\\\x-y=v\\end{cases}\\implies \\begin{cases}x=u/2+y\\\\v=2u\\end{cases}\n$$\n\nThe equation $l_2$ tells us that we cannot choose any pair $u,v$ hence not every element of the codomain $\\mathbb{R}^2$ is assigned to some $(x,y)$.\n\nAnother way is to compute the image of the domain, notice as $x,y$ ranges in $\\mathbb{R}$, then $x-y$ ranges in $\\mathbb{R}$ as well, and thus the $\\Xi$ function is just like $\\Xi(\\spadesuit) = (2\\spadesuit,\\spadesuit)$, see example 2.\n\nThus\n\n$\\Xi(\\mathbb{R}^2) = [\\text{line with equation}\\,\\, y=x/2]$ again. And the function is not surjective.\n\n# Describing the type of mapping $f$\n\n1-1, sobrejective, bijective, etc, this is more important than the range. From Kernel we can evaluate the type of connectivity\n\nSince a function $f$ assigns elements $x$ to $y$ we can view it as analogous to assigning labels $y$ to the fruits $x$, hence $f$ allows us to distinguish elements in $X$ through the labels in $Y$. But how well were these labels assigned to the fruits? The ideal situation occurs when each $x$ has a unique label $y$, under this scenario we can retrieve the fruit from the label without any issue as they are unique; the worst case occurs when we have the same label for all $x$'s, its impossible to retrieve any fruits, under $f$ they are all the same. These are the two extremes of a spectrum of labeling, in between we have the intermediate cases where some fruits might have some common labels, examples of these three scenarios are shown in Fig.\n\n![](figs/onetoone_or_not.png)\n\nIt is precisely to evaluate the quality of the $f$ labeling that we introduce the words surjective, injective and bijective.\n\n::: {#def-onto_1to1 style=\"color:gray\"}\nLet $f:\\mathbb{R}^n\\overset{\\sim}{\\longrightarrow} \\mathbb{R}^m$ be a function, then we say:\n\n-   $[f\\,\\, \\text{is onto}]:= [\\forall \\mathbf{y} \\in \\mathbb{R}^m :\\exists\\mathbf{x}\\in\\mathbb{R}^n : f(\\mathbf{x})=\\mathbf{y}] = [\\text{Im}\\,\\,f =f(\\mathbb{R}^n)]$\n\n    Comment: every element in $\\mathbb{R}^m$ has a corresponding element in $\\mathbb{R}^n$.\n\n-   $[f\\,\\, \\text{is 1-1}]:=[\\forall \\mathbf{x},\\mathbf{x}' \\in \\mathbb{R}^n:\\mathbf{x}\\not =\\mathbf{x}' \\implies f(\\mathbf{x}_1)\\not= f(\\mathbf{x}')]=[\\ker f =\\{\\mathbf{0}_{\\mathbb{R}^n}\\}]$\n\n    Comment: in other words, to say $f(\\mathbf{x})=f(\\mathbf{x}')$ implies $\\mathbf{x}=\\mathbf{x}'$\n\n-   $[f\\,\\, \\text{is bijective}]:=[f\\,\\, \\text{is 1-1 and onto}]$\n:::\n\nSummarizing, a surjective function makes use of all available labels, but doesn't guarantee that the labels are used one time only. Thus it might happen that apples and strawberries are given the same label. On the other hand, a 1-1 function guarantees that the labels only appear once, but does not guarantee that all available labels are used. A bijective function makes use of all labels one time.\n\n# Dimension Theorem\n\n::: {#thm-dimension style=\"color: gray\"}\nLet $f:\\mathbb{R}^n\\overset{\\sim}{\\longrightarrow} \\mathbb{R}^m$, then:\n\n$$\n\\dim \\mathbb{R}^n=n = \\dim \\ker f + \\dim \\text{Im}\\, f\n$$\n:::\n\n**Example 1 (cont):** $1 = \\dim \\{0\\} + \\dim \\{y/a\\,\\,|\\,\\, y\\in\\mathbb{R}\\}=0+1$ Note: only one vectors is needed to expand the image.\n\n**Example 2 (cont):** $1 = \\dim \\{0\\} + \\dim [\\text{line with equation}\\,\\, y=x/2]=0+1$ Note: A line only needs one basis vector!\n\n**Example 3 (cont):** $2 = \\dim\\{(0,y)\\in\\mathbb{R}^2\\,\\,|\\,\\, y\\in\\mathbb{R}\\}+\\dim \\mathbb{R} = 1+1$\n\n**Example 4 (cont):** $2 = \\dim \\{(0,0)\\} +\\dim \\mathbb{R}^2=0+2$\n\n**Example 5 (cont):** $2 = \\dim \\{c(1,1)\\,\\,|\\,\\,c\\in\\mathbb{R}\\} + \\dim [\\text{line with equation} \\,\\, y=x/2]=1+1$\n\n![](images/clipboard-688989313.png)\n\nThis picture helps us also understand the following better:\n\n-   *A system of equations* $A\\mathbf{x}=\\mathbf{b}$ *have one solution provided* $r=r^*=m$. Why? $m-r=0$ and thus there is no nullspace, the whole $\\mathbb{R}^m$ is the row space. And the $\\mathbf{b}$ lives inside the column space. The vector $\\mathbf{x}$ is in the row space and we are computing ................???????\n\n-   *A system of equations* $A\\mathbf{x}=\\mathbf{b}$ *have infinite solutions provided* $r=r^*<n$.\n\n-   *A system of equations* $A\\mathbf{x}=\\mathbf{b}$ *have no solutions provided* $r<r^*$.\n\n## Maps involved in matrix multiplication (composition)\n\n![](figs/matrix%20_product_maps.png)\n\nWHEN IS THER INERVES; EXPLAIN\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"linear_functions_v2.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.32","editor":"visual","comments":{"hypothesis":true},"preview":{"browser":"chrome"},"theme":{"light":"minty"},"fontcolor":"rgb(190,190,190)","backgroundcolor":"rgb(32,31,30)","margin-left":"rgb(32,31,30)","navbar-color":"rgba(216, 6, 33 .65)","title":"Linear functions"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}