{"title":"Eigenvalues and eigenvectors","markdown":{"yaml":{"title":"Eigenvalues and eigenvectors"},"headingText":"How to compute the eigenvalues and eigenvectors of $f$?","containsRefs":false,"markdown":"\n\nIn the following section we will, again, focus our attention on linear functions that map vectors of $\\mathbb{V}$ into vectors of $\\mathbb{V}$:\n\n$$\n\\begin{align}f:\\mathbb{V}&\\overset{\\sim}{\\longrightarrow}    \\mathbb{V}\\\\ v&\\longmapsto f(v)\\end{align}\n$$\n\nTo each element $v$ of $\\mathbb{V}$ the function $f$ assigns a new vector of $\\mathbb{V}$.\n\nIt may happen, as we shall see, that within $\\mathbb{V}$ we find special subspaces, special because, any of its elements under $f$ is assigned to some vector within that subspace. If $\\mathbb{G}$ is such a subspace we can write in mathematical language that the image of $\\mathbb{G}$ is a subset, and thus a subspace of $\\mathbb{G}$:\n\n$$\nf(\\mathbb{G})\\subseteq_{\\text{vec}} \\mathbb{G}\n$$\n\nBecause $\\mathbb{G}\\subseteq_\\text{vec} \\mathbb{V}$, the dimension $k$ of $\\mathbb{G}$ is less or equal to the dimension $n$ of $\\mathbb{V}$. This observation provides us with a good idea for the choice of the $n$ basis vectors required to span $\\mathbb{V}$. We may choose the $k$ basis vectors of $\\mathbb{G}$ as the first $k$ basis vectors of $\\mathbb{V}$ and pick the remaining $n-k$ vectors from the complementary space. Thus:\n\n$$\n\\mathbf{B}=\\begin{pmatrix}g_1 & \\cdots & g_k &; & w_{k+1} & \\cdots & w_n\\end{pmatrix}\n$$\n\nHence $\\mathbb{V}=\\mathbb{G}\\oplus\\mathbb{W}$ where $\\mathbb{W}$ is the complementary (not necessarily orthogonal) space of $\\mathbb{G}$.\n\nIf a choice such as this is made, then the matrix representation of $f$ becomes like this:\n\n$$\nf(\\mathbf{B}) = \\mathbf{B}A\\qquad A=\\begin{pmatrix}\\alpha & \\beta\\\\0 &\\delta \\end{pmatrix} \n$$\n\nHere the $\\alpha$ is a matrix blocks of shape $k\\times k$ whose columns tell us the components of $f(g_1)$ through $f(g_k)$ wrt the basis $\\begin{pmatrix}g_1 &\\cdots & g_k\\end{pmatrix}$. If the second part of the chosen basis, $\\begin{pmatrix}w_{k+1} &\\cdots & w_n\\end{pmatrix}$ does *not* span another invariant subspace, the images of $f(w_{k+1})$ through $f(w_n)$ have a part that belongs to $\\mathbb{W}$, hence the non-zero block $\\beta$, and another part lives on the complementary space of $\\mathbb{G}$, requiring the components found in the $\\delta$ matrix block. The shapes of $\\beta$ and $\\delta$ are $k\\times(n-k)$ and $(n-k)\\times (n-k)$, respectively. The block $\\begin{pmatrix}\\beta\\\\ \\delta \\end{pmatrix}$ has shape $n\\times (n-k)$ and describes the components of $f(w_{k+1})$ through $f(w_n)$, again, wrt whole $\\mathbf{B}$.\n\n*Observe:* If we imagine that the complementary space $\\mathbb{W}$ is another invariant subspace of $f$, then the representation of $f(w_{k+1})$ through $f(w_n)$ only requires the basis vectors $\\begin{pmatrix}w_{k+1} &\\cdots & w_n\\end{pmatrix}$, as a result only the $\\delta$ is needed and thus $\\beta$ is a block of zeros. The matrix $A$ becomes in this case:\n\n$$\n\\begin{pmatrix}\\alpha & 0\\\\0 &\\delta \\end{pmatrix} \n$$\n\nDecomposing a vector space into two invariant subspaces introduces many zeros into $A$ and thus easier calculation can be performed with this representation.\n\nHow do we find these invariant subspaces?\n\n*Answer:* A simple example is to compute the eigenvectors of $f$ for example $s_1$, $s_2$ and $s_3$. Then $\\text{span}\\{s_1\\}$, $\\text{span}\\{s_2\\}$ and $\\text{span}\\{s_3\\}$ each constitute invariant subspaces each dimension $1$.\n\n\n::: {#def-vaps_veps}\nLet $f:\\mathbb{V}\\overset{\\sim}{\\longrightarrow} \\mathbb{V}$ then a vector $v$ different from zero is an eigenvector of $f$ with eigenvalue $\\lambda$ if\n\n$$\nf(v)=\\lambda v\n$$\n\nThe number $\\lambda$ can be a complex number. But in these notes we'll only consider examples where $\\lambda\\in\\mathbb{R}$.\n:::\n\nHow to we solve this problem? To solve it we need to do calculation, and to do calculation we need to choose a basis for $\\mathbb{V}$, any basis!\n\nIf we have chosen:\n\n$$\n\\mathbf{B}=\\begin{pmatrix}v_1 & v_2 & \\cdots & v_n\\end{pmatrix}\n$$\n\nthen our problem becomes:\n\n$$\n\\mathbf{B}A X =\\lambda\\mathbf{B}  X\\implies AX=\\lambda X\\qquad v=\\mathbf{B}X\\qquad X=\\begin{pmatrix}x_1\\\\ \\vdots\\\\ x_n\\end{pmatrix}\n$$\n\nIn practice, what we have to compute are the solutions of:\\\n\n$$\nAX=\\lambda X\n$$\n\nHow do we compute these?\n\n1.  Rearrange into $(A-\\lambda)X=0$.\n2.  We have $n$ equations and $n+1$ unknowns (the $\\lambda$ being the $+1$ unknown). For this system to be solvable we need to provide a new equation which involves the unknown $\\lambda$.\n3.  The extra equation to add could be for example something as simples as $\\lambda=1$, however, noticing that we really want solutions $X\\not = 0$, that extra equation (involving $\\lambda$) better be: $|A-\\lambda|=0$. Why? The pool of $\\lambda$'s provided by solving $|A-\\lambda|=0$ guarantees that $A-\\lambda$ matrix has dependent columns and in return this guarantees that $X\\not = 0$. Something which is not safe guarded if we simply choose $\\lambda=1$ or make any other choice, for that matter.\n4.  Compute the $\\lambda$'s that satisfy $|A-\\lambda|=0$.\n5.  Substitute these into $(A-\\lambda)X=0$, and since we have now dependent columns we can compute the corresponding eigenvector(s) $X$, these constitute the $N(A-\\lambda)$ and are the eigenvectors of the matrix $A$.\n6.  The eigenvectors of $f$ are obtained by dotting these $X$'s with the hypervector $\\mathbf{B}$.\n\n## Example 1\n\nWe start with the endomorphism:\n\n$$\n\\begin{align}f:\\mathbb{R}^2&\\overset{\\sim}{\\longrightarrow}    \\mathbb{R}^2\\\\ (x,y)&\\longmapsto f(x,y):=(2x-y,2y-x)\\end{align}\n$$\n\nWe want to solve the problem:\n\n$$\nf(v)=\\lambda v \\qquad v=(x,y)\n$$\n\nTo do that we introduce a basis in $\\mathbb{R}^2$, any basis works, but we opt for the simplest one:\n\n$$\n\\mathbf{B}=\\begin{pmatrix}v_1 & v_2\\end{pmatrix}\\qquad v_1=(1,0) \\qquad v_2=(0,1)\n$$\n\nTherefore, any element $v$ of $\\mathbb{R}^2$ can be written as dotting $\\mathbf{B}$ with the column vector $X$ of coefficients:\n\n$$\n\\mathbb{R}^2\\ni v= \\mathbf{B} X \\qquad X=\\begin{pmatrix}x_1\\\\x_2 \\end{pmatrix}\n$$\n\nwhile the column vectors $X$ belong to the vector space of column vectors denote, also, by $\\mathbf{R}^2$. (Not to be confused the $\\mathbb{R}^2$ whose elements are of the form $(x,y)$.\n\nThe action of $f$ on the basis is:\n\n$$\nf(\\mathbf{B})=\\mathbf{B} A \\qquad A=\\begin{pmatrix}2 & -1\\\\-1 & 2\\end{pmatrix}\n$$\n\nSubstituting into $f(v)=\\lambda v$ yields:\n\n$$\nAX=\\lambda X \\leftrightsquigarrow \\begin{pmatrix}2 & -1\\\\-1 & 2\\end{pmatrix}\\begin{pmatrix}x_1\\\\x_2 \\end{pmatrix}=\\lambda \\begin{pmatrix}x_1\\\\x_2 \\end{pmatrix}\n$$\n\nTo find nonzero solutions of this equation is to solve the following problem:\n\n$$\n(A-\\lambda) X=0 \\leftrightsquigarrow \\begin{pmatrix}2-\\lambda & -1\\\\-1 & 2-\\lambda\\end{pmatrix}\\begin{pmatrix}x_1\\\\x_2 \\end{pmatrix}=\\begin{pmatrix}0\\\\0 \\end{pmatrix}\n$$\n\nprovided we choose $\\lambda$'s that make the matrix have dependent columns, i.e., we must choose the $\\lambda$'s that obey:\n\n$$\n|A-\\lambda|=0\n$$\n\nLets solve this equation first, to get our pool of eigenvalues, the $\\lambda$'s that ensure $A-\\lambda$ have dependent columns:\n\n$$\n\\begin{vmatrix} 2-\\lambda & -1\\\\-1 & 2-\\lambda\\end{vmatrix}=0 \\implies (\\lambda-2)^2-1=0 \\implies \\lambda = 2\\pm1\n$$\n\nWe found two eigenvalues, lets now substitute one at a time and compute the corresponding eigenvector:\n\n$$\n\\begin{align}\n&\\lambda=2+1 \\implies \\begin{pmatrix}-1 & -1\\\\-1 & -1\\end{pmatrix}\\begin{pmatrix}x_1\\\\x_2 \\end{pmatrix}=\\begin{pmatrix}0\\\\0 \\end{pmatrix} \\implies \\begin{pmatrix}x_1\\\\x_2 \\end{pmatrix}=\\begin{pmatrix}-1\\\\1 \\end{pmatrix}\\\\\n&\\lambda=2-1 \\implies \\begin{pmatrix}1 & -1\\\\-1 & 1\\end{pmatrix}\\begin{pmatrix}x_1\\\\x_2 \\end{pmatrix}=\\begin{pmatrix}0\\\\0 \\end{pmatrix} \\implies \\begin{pmatrix}x_1\\\\x_2 \\end{pmatrix}=\\begin{pmatrix}1\\\\1 \\end{pmatrix}\n\\end{align}\n$$\n\n\\[*Commentary:* above we just compute the nullspaces of two matrices, which are so simple, we did it by guess work. Of course the elimination method can be employed as usual.\\]\n\nThe solution set $V_{3}$ of eigenvectors of $A$ with eigenvalue $3$ is:\n\n$$\nV_{3}=N(A-3)=span\\{\\begin{pmatrix}-1\\\\1 \\end{pmatrix}\\}\n$$\n\nwhile the solution set $V_1$ is:\n\n$$\nV_{1}=N(A-1)=span\\{\\begin{pmatrix}1\\\\1 \\end{pmatrix}\\}\n$$\n\nBoth constitute subspaces of the vector space of column vectors $\\mathbf{R}^2$.\n\nSince the basis are independent we can also say that:\n\n$$\n\\mathbf{R}^2=V_3\\oplus V_1\n$$\n\n## Example 2\n\nThis time our linear function is:\n\n$$\n\\begin{align}f:\\mathbb{R}^3&\\overset{\\sim}{\\longrightarrow}    \\mathbb{R}^3\\\\ (x,y)&\\longmapsto f(x,y,z):=(3x-y,y,3z-y)\\end{align}\n$$\n\nIn the canonical basis:\n\n$$\n\\mathbf{B}=\\begin{pmatrix}e_1 & e_2 & e_3\\end{pmatrix}\\qquad e_1=(1,0,0)\\qquad e_2=(0,1,0)\\qquad e_3=(0,0,1)\n$$\n\nOur function is represented as:\n\n$$\nf(v) = \\mathbf{B} A X \\qquad A=\\begin{pmatrix} 3 & -1 & 0\\\\0 & 1 & 0 \\\\ 0 & -1 & 3\\end{pmatrix} \\qquad X=\\begin{pmatrix}x_1\\\\x_2\\\\x_3\\end{pmatrix}\n$$\n\nTo compute its eigenvalues and eigenvectors we solve first:\n\n$$\n\\begin{vmatrix} 3-\\lambda & -1 & 0\\\\0 & 1-\\lambda & 0 \\\\ 0 & -1 & 3-\\lambda\\end{vmatrix}=0\n$$\n\nWhich gives us:\n\n$$\n(3-\\lambda)(3-\\lambda)(1-\\lambda)=0\n$$The eigenvalues are $3$, again $3$ and $1$.\n\nThe corresponding eigenvectors are:\n\n$$\n\\begin{align}\n&V_3=span\\{\\begin{pmatrix}1& 0&0\\end{pmatrix}^\\intercal,\\begin{pmatrix}2 & 0 & 1\\end{pmatrix}^\\intercal\\}\\\\\n&V_1=span\\{\\begin{pmatrix}1&2&1\\end{pmatrix}^\\intercal\\}\n\\end{align}\n$$\n\nThus the eigenvectors of $f$ are:\n\n$$\n\\begin{align}\n&\\mathbb{V}_3=span\\{\\mathbf{B}\\begin{pmatrix}1& 0&0\\end{pmatrix}^\\intercal,\\mathbf{B}\\begin{pmatrix}2 & 0 & 1\\end{pmatrix}^\\intercal\\}\\\\\n&\\mathbb{V}_1=span\\{\\mathbf{B}\\begin{pmatrix}1&2&1\\end{pmatrix}^\\intercal\\}\n\\end{align}\n$$\n\n## Example 3\n\nThe function:\n\n$$\n\\begin{align}f:\\mathbb{R}^3&\\overset{\\sim}{\\longrightarrow}    \\mathbb{R}^3\\\\ (x,y)&\\longmapsto f(x,y,z):=(3x+y,3y,2z)\\end{align}\n$$\n\nacting on vectors represented wrt canonical basis as:\n\n$$\nf(v) = \\mathbf{B} A X \\qquad A=\\begin{pmatrix} 3 & 1 & 0\\\\0 & 3 & 0 \\\\ 0 & 0 & 2\\end{pmatrix} \\qquad X=\\begin{pmatrix}x_1\\\\x_2\\\\x_3\\end{pmatrix}\n$$\n\nComputing the eigenvalues as usual\n\n$$\n\\begin{vmatrix} 3-\\lambda & 1 & 0\\\\0 & 3-\\lambda & 0 \\\\ 0 & 0 & 2-\\lambda\\end{vmatrix}=0\n$$\n\nSolving $(3-\\lambda)^2(2-\\lambda)=0$ gives us: $\\lambda=3,2$.\n\nFor $\\lambda=3$, the eigenvector of $A$ and $f$ are respectively:\n\n$$\nc\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix},\\qquad c\\mathbf{B}\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}\n$$\n\nJust one! Despite the fact that the eigenvalue $3$ appear twice in that determinant equation.\n\nFor $\\lambda=2$:\n\n$$\nc\\begin{pmatrix}0\\\\0\\\\1\\end{pmatrix},\\qquad c\\mathbf{B}\\begin{pmatrix}0\\\\0\\\\1\\end{pmatrix}\n$$\n\nIn this example we have a shortage on eigenvectors, not enough to span the $\\mathbf{R}^3$ they are embedded in.\n\n# Representing $f$ wrt basis of its eigenvectors (Diagonalization)\n\nIn the previous three examples seen how to compute the eigenvalues and eigenvectors of a linear function represented wrt the canonical basis.\n\nWhat is the matrix representing those $f$'s wrt to the eigenvector basis? Is this always possible?\n\nTo get the $A'$ representation of $f$ we can follow either one of two paths that we seen before:\n\n1.  Act with $f$ on the eigenvectors we computed at express the output wrt those as well.\n2.  Compute $A$ wrt the canonical basis, compute the change of basis $P$, that related $\\mathbf{B}'=\\mathbf{B}P$, and compute $P^{-1} A P =: A'$. We will call the matrix $P$ as $S$ when dealing with eigenvectors.\n\nWe will only focus on the 2. approach.\n\n## Example 1 (cont.)\n\nSince the eigenvectors:\n\n$$\nv_1=\\mathbf{B}\\begin{pmatrix}-1\\\\1\\end{pmatrix} \\qquad v_2=\\mathbf{B}\\begin{pmatrix}1\\\\1\\end{pmatrix}\\qquad \\mathbf{B}=\\begin{pmatrix}e_1 & e_2\\end{pmatrix}\n$$ {#eq-1}\n\nare independent and they span the vector space $\\mathbb{V}$. Lets promote them to a basis:\n\n$$\n\\mathbf{B}':=\\begin{pmatrix}v_1 & v_2\\end{pmatrix}\n$$ {#eq-2}\n\nThere is a connection $s$ (or $S$) between both bases, substituting @eq-1 into @eq-2 we obtain:\n\n$$\n\\mathbf{B}'=\\mathbf{B} S=: s(\\mathbf{B}) \\qquad S=\\begin{pmatrix}-1 & 1\\\\1 & 1\\end{pmatrix}\n$$\n\nwhere the matrix we call $S$ (before we called it $P$) is a special matrix, whose columns are the eigenvectors of $A$. And the function $s$ (previously called $p$) is a linear function that transforms the canonical basis into the eigenvector basis. This means that the components $X$ of $v$ wrt the canonical basis are transformed into the components $X'$ of $v$ wrt the eigenvector basis as:\n\n$$\nX=SX'\n$$\n\nSince $\\mathbf{B} Y =:f(v)=\\mathbf{B}AX$ then:\n\n$$\n\\mathbf{B}'Y' =:f(v)=\\mathbf{B}'S^{-1} A SX'\n$$\n\nThus: $Y'=S^{-1}AS$ and $A'=S^{-1}AS$. Substituting the matrices above we find:\n\n$$\nA'=\\begin{pmatrix}-1 & 1\\\\1 & 1\\end{pmatrix}^{-1}\\begin{pmatrix}2 & -1\\\\-1 & 2\\end{pmatrix}\\begin{pmatrix}-1 & 1\\\\1 & 1\\end{pmatrix}=\\begin{pmatrix}3 & 0\\\\0 & 1\\end{pmatrix}\n$$ {#eq-Aprime}\n\nComputing these matrix product gave us a diagonal matrix whose entries are the eigenvalues.\n\nWhy a diagonal matrix?\n\n*Answer:* Equation @eq-Aprime is closely related to the equation $AS=\\Lambda S$:\n\n$$\n\\begin{pmatrix}2 & -1\\\\-1 & 2\\end{pmatrix}\\begin{pmatrix}-1 & 1\\\\1 & 1\\end{pmatrix}=\\begin{pmatrix}3 & 0\\\\0 & 1\\end{pmatrix}\\begin{pmatrix}-1 & 1\\\\1 & 1\\end{pmatrix}\n$$\n\nSince $S$ is invertible (the eigenvectors are independent) multiplying both sides from the left by $S^{-1}$ tell us $\\Lambda=S^{-1}AS$. Thus the $A'$ in @eq-Aprime above is in fact the $\\Lambda$.\n\nThus, $f$ act on vectors represented wrt to the eigenvector basis as:\n\n$$\nf(v)=\\mathbf{B}'\\Lambda X'\\qquad v=\\mathbf{B}'X' \\qquad \\Lambda = \\begin{pmatrix}3 & 0\\\\0 & 1\\end{pmatrix}\n$$\n\nmuch easier and simple to compute maps from now on, the eigenvectors are indeed special.\n\n## Example 2 (cont.)\n\nCalculations are similar to example 1. Putting the eigenvectors of $A$ as columns of matrix we get our special matrix:\n\n$$\nS=\\begin{pmatrix}1 & 2 & 1\\\\0 & 1 & 2\\\\0 & 2 & 1\\end{pmatrix}\n$$ {#eq-Smatrix_eg_2}\n\nNotice in this case that col 1 and col 2 of $S$ could be any other l.c. of $\\begin{pmatrix}1 & 0 & 0\\end{pmatrix}^\\intercal$ and $\\begin{pmatrix}2 & 1 & 2\\end{pmatrix}^\\intercal$ since $V_3$ is two dimensional and thus many other basis could be used, similarly, any multiple of col 3 could also have been used when defining $S$ above, since $V_1$ is one dimensional and any multiple of $\\begin{pmatrix}1 & 2 & 1\\end{pmatrix}^\\intercal$ constitutes a basis.\n\nSticking with our @eq-Smatrix_eg_2, we know that:\n\n$$\nAS=\\Lambda S \\leftrightsquigarrow \\begin{pmatrix} 3 & -1 & 0\\\\0 & 1 & 0 \\\\ 0 & -1 & 3\\end{pmatrix}\\begin{pmatrix}1 & 2 & 1\\\\0 & 1 & 2\\\\0 & 2 & 1\\end{pmatrix}=\\begin{pmatrix}3 & 0 & 0\\\\0 & 3 & 0\\\\0 & 0 & 1\\end{pmatrix}\\begin{pmatrix}1 & 2 & 1\\\\0 & 1 & 2\\\\0 & 2 & 1\\end{pmatrix}\n$$\n\nAnd therefore:\n\n$$\nf(v)=\\mathbf{B}'\\Lambda X'\\qquad v=\\mathbf{B}'X' \\qquad \\Lambda = \\begin{pmatrix}3 & 0 & 0\\\\0 & 3 & 0\\\\0 & 0 & 1\\end{pmatrix}\n$$\n\n## Example 3 (cont)\n\nIn this case, we only have two eigenvectors. We needed three to write our $S$. In this case it is not possible to represent our $f$ wrt to its eigenvector basis, simply because the eigenvectors for this specific $f$ do now constitute a basis, one vector is missing.\n","srcMarkdownNoYaml":"\n\nIn the following section we will, again, focus our attention on linear functions that map vectors of $\\mathbb{V}$ into vectors of $\\mathbb{V}$:\n\n$$\n\\begin{align}f:\\mathbb{V}&\\overset{\\sim}{\\longrightarrow}    \\mathbb{V}\\\\ v&\\longmapsto f(v)\\end{align}\n$$\n\nTo each element $v$ of $\\mathbb{V}$ the function $f$ assigns a new vector of $\\mathbb{V}$.\n\nIt may happen, as we shall see, that within $\\mathbb{V}$ we find special subspaces, special because, any of its elements under $f$ is assigned to some vector within that subspace. If $\\mathbb{G}$ is such a subspace we can write in mathematical language that the image of $\\mathbb{G}$ is a subset, and thus a subspace of $\\mathbb{G}$:\n\n$$\nf(\\mathbb{G})\\subseteq_{\\text{vec}} \\mathbb{G}\n$$\n\nBecause $\\mathbb{G}\\subseteq_\\text{vec} \\mathbb{V}$, the dimension $k$ of $\\mathbb{G}$ is less or equal to the dimension $n$ of $\\mathbb{V}$. This observation provides us with a good idea for the choice of the $n$ basis vectors required to span $\\mathbb{V}$. We may choose the $k$ basis vectors of $\\mathbb{G}$ as the first $k$ basis vectors of $\\mathbb{V}$ and pick the remaining $n-k$ vectors from the complementary space. Thus:\n\n$$\n\\mathbf{B}=\\begin{pmatrix}g_1 & \\cdots & g_k &; & w_{k+1} & \\cdots & w_n\\end{pmatrix}\n$$\n\nHence $\\mathbb{V}=\\mathbb{G}\\oplus\\mathbb{W}$ where $\\mathbb{W}$ is the complementary (not necessarily orthogonal) space of $\\mathbb{G}$.\n\nIf a choice such as this is made, then the matrix representation of $f$ becomes like this:\n\n$$\nf(\\mathbf{B}) = \\mathbf{B}A\\qquad A=\\begin{pmatrix}\\alpha & \\beta\\\\0 &\\delta \\end{pmatrix} \n$$\n\nHere the $\\alpha$ is a matrix blocks of shape $k\\times k$ whose columns tell us the components of $f(g_1)$ through $f(g_k)$ wrt the basis $\\begin{pmatrix}g_1 &\\cdots & g_k\\end{pmatrix}$. If the second part of the chosen basis, $\\begin{pmatrix}w_{k+1} &\\cdots & w_n\\end{pmatrix}$ does *not* span another invariant subspace, the images of $f(w_{k+1})$ through $f(w_n)$ have a part that belongs to $\\mathbb{W}$, hence the non-zero block $\\beta$, and another part lives on the complementary space of $\\mathbb{G}$, requiring the components found in the $\\delta$ matrix block. The shapes of $\\beta$ and $\\delta$ are $k\\times(n-k)$ and $(n-k)\\times (n-k)$, respectively. The block $\\begin{pmatrix}\\beta\\\\ \\delta \\end{pmatrix}$ has shape $n\\times (n-k)$ and describes the components of $f(w_{k+1})$ through $f(w_n)$, again, wrt whole $\\mathbf{B}$.\n\n*Observe:* If we imagine that the complementary space $\\mathbb{W}$ is another invariant subspace of $f$, then the representation of $f(w_{k+1})$ through $f(w_n)$ only requires the basis vectors $\\begin{pmatrix}w_{k+1} &\\cdots & w_n\\end{pmatrix}$, as a result only the $\\delta$ is needed and thus $\\beta$ is a block of zeros. The matrix $A$ becomes in this case:\n\n$$\n\\begin{pmatrix}\\alpha & 0\\\\0 &\\delta \\end{pmatrix} \n$$\n\nDecomposing a vector space into two invariant subspaces introduces many zeros into $A$ and thus easier calculation can be performed with this representation.\n\nHow do we find these invariant subspaces?\n\n*Answer:* A simple example is to compute the eigenvectors of $f$ for example $s_1$, $s_2$ and $s_3$. Then $\\text{span}\\{s_1\\}$, $\\text{span}\\{s_2\\}$ and $\\text{span}\\{s_3\\}$ each constitute invariant subspaces each dimension $1$.\n\n# How to compute the eigenvalues and eigenvectors of $f$?\n\n::: {#def-vaps_veps}\nLet $f:\\mathbb{V}\\overset{\\sim}{\\longrightarrow} \\mathbb{V}$ then a vector $v$ different from zero is an eigenvector of $f$ with eigenvalue $\\lambda$ if\n\n$$\nf(v)=\\lambda v\n$$\n\nThe number $\\lambda$ can be a complex number. But in these notes we'll only consider examples where $\\lambda\\in\\mathbb{R}$.\n:::\n\nHow to we solve this problem? To solve it we need to do calculation, and to do calculation we need to choose a basis for $\\mathbb{V}$, any basis!\n\nIf we have chosen:\n\n$$\n\\mathbf{B}=\\begin{pmatrix}v_1 & v_2 & \\cdots & v_n\\end{pmatrix}\n$$\n\nthen our problem becomes:\n\n$$\n\\mathbf{B}A X =\\lambda\\mathbf{B}  X\\implies AX=\\lambda X\\qquad v=\\mathbf{B}X\\qquad X=\\begin{pmatrix}x_1\\\\ \\vdots\\\\ x_n\\end{pmatrix}\n$$\n\nIn practice, what we have to compute are the solutions of:\\\n\n$$\nAX=\\lambda X\n$$\n\nHow do we compute these?\n\n1.  Rearrange into $(A-\\lambda)X=0$.\n2.  We have $n$ equations and $n+1$ unknowns (the $\\lambda$ being the $+1$ unknown). For this system to be solvable we need to provide a new equation which involves the unknown $\\lambda$.\n3.  The extra equation to add could be for example something as simples as $\\lambda=1$, however, noticing that we really want solutions $X\\not = 0$, that extra equation (involving $\\lambda$) better be: $|A-\\lambda|=0$. Why? The pool of $\\lambda$'s provided by solving $|A-\\lambda|=0$ guarantees that $A-\\lambda$ matrix has dependent columns and in return this guarantees that $X\\not = 0$. Something which is not safe guarded if we simply choose $\\lambda=1$ or make any other choice, for that matter.\n4.  Compute the $\\lambda$'s that satisfy $|A-\\lambda|=0$.\n5.  Substitute these into $(A-\\lambda)X=0$, and since we have now dependent columns we can compute the corresponding eigenvector(s) $X$, these constitute the $N(A-\\lambda)$ and are the eigenvectors of the matrix $A$.\n6.  The eigenvectors of $f$ are obtained by dotting these $X$'s with the hypervector $\\mathbf{B}$.\n\n## Example 1\n\nWe start with the endomorphism:\n\n$$\n\\begin{align}f:\\mathbb{R}^2&\\overset{\\sim}{\\longrightarrow}    \\mathbb{R}^2\\\\ (x,y)&\\longmapsto f(x,y):=(2x-y,2y-x)\\end{align}\n$$\n\nWe want to solve the problem:\n\n$$\nf(v)=\\lambda v \\qquad v=(x,y)\n$$\n\nTo do that we introduce a basis in $\\mathbb{R}^2$, any basis works, but we opt for the simplest one:\n\n$$\n\\mathbf{B}=\\begin{pmatrix}v_1 & v_2\\end{pmatrix}\\qquad v_1=(1,0) \\qquad v_2=(0,1)\n$$\n\nTherefore, any element $v$ of $\\mathbb{R}^2$ can be written as dotting $\\mathbf{B}$ with the column vector $X$ of coefficients:\n\n$$\n\\mathbb{R}^2\\ni v= \\mathbf{B} X \\qquad X=\\begin{pmatrix}x_1\\\\x_2 \\end{pmatrix}\n$$\n\nwhile the column vectors $X$ belong to the vector space of column vectors denote, also, by $\\mathbf{R}^2$. (Not to be confused the $\\mathbb{R}^2$ whose elements are of the form $(x,y)$.\n\nThe action of $f$ on the basis is:\n\n$$\nf(\\mathbf{B})=\\mathbf{B} A \\qquad A=\\begin{pmatrix}2 & -1\\\\-1 & 2\\end{pmatrix}\n$$\n\nSubstituting into $f(v)=\\lambda v$ yields:\n\n$$\nAX=\\lambda X \\leftrightsquigarrow \\begin{pmatrix}2 & -1\\\\-1 & 2\\end{pmatrix}\\begin{pmatrix}x_1\\\\x_2 \\end{pmatrix}=\\lambda \\begin{pmatrix}x_1\\\\x_2 \\end{pmatrix}\n$$\n\nTo find nonzero solutions of this equation is to solve the following problem:\n\n$$\n(A-\\lambda) X=0 \\leftrightsquigarrow \\begin{pmatrix}2-\\lambda & -1\\\\-1 & 2-\\lambda\\end{pmatrix}\\begin{pmatrix}x_1\\\\x_2 \\end{pmatrix}=\\begin{pmatrix}0\\\\0 \\end{pmatrix}\n$$\n\nprovided we choose $\\lambda$'s that make the matrix have dependent columns, i.e., we must choose the $\\lambda$'s that obey:\n\n$$\n|A-\\lambda|=0\n$$\n\nLets solve this equation first, to get our pool of eigenvalues, the $\\lambda$'s that ensure $A-\\lambda$ have dependent columns:\n\n$$\n\\begin{vmatrix} 2-\\lambda & -1\\\\-1 & 2-\\lambda\\end{vmatrix}=0 \\implies (\\lambda-2)^2-1=0 \\implies \\lambda = 2\\pm1\n$$\n\nWe found two eigenvalues, lets now substitute one at a time and compute the corresponding eigenvector:\n\n$$\n\\begin{align}\n&\\lambda=2+1 \\implies \\begin{pmatrix}-1 & -1\\\\-1 & -1\\end{pmatrix}\\begin{pmatrix}x_1\\\\x_2 \\end{pmatrix}=\\begin{pmatrix}0\\\\0 \\end{pmatrix} \\implies \\begin{pmatrix}x_1\\\\x_2 \\end{pmatrix}=\\begin{pmatrix}-1\\\\1 \\end{pmatrix}\\\\\n&\\lambda=2-1 \\implies \\begin{pmatrix}1 & -1\\\\-1 & 1\\end{pmatrix}\\begin{pmatrix}x_1\\\\x_2 \\end{pmatrix}=\\begin{pmatrix}0\\\\0 \\end{pmatrix} \\implies \\begin{pmatrix}x_1\\\\x_2 \\end{pmatrix}=\\begin{pmatrix}1\\\\1 \\end{pmatrix}\n\\end{align}\n$$\n\n\\[*Commentary:* above we just compute the nullspaces of two matrices, which are so simple, we did it by guess work. Of course the elimination method can be employed as usual.\\]\n\nThe solution set $V_{3}$ of eigenvectors of $A$ with eigenvalue $3$ is:\n\n$$\nV_{3}=N(A-3)=span\\{\\begin{pmatrix}-1\\\\1 \\end{pmatrix}\\}\n$$\n\nwhile the solution set $V_1$ is:\n\n$$\nV_{1}=N(A-1)=span\\{\\begin{pmatrix}1\\\\1 \\end{pmatrix}\\}\n$$\n\nBoth constitute subspaces of the vector space of column vectors $\\mathbf{R}^2$.\n\nSince the basis are independent we can also say that:\n\n$$\n\\mathbf{R}^2=V_3\\oplus V_1\n$$\n\n## Example 2\n\nThis time our linear function is:\n\n$$\n\\begin{align}f:\\mathbb{R}^3&\\overset{\\sim}{\\longrightarrow}    \\mathbb{R}^3\\\\ (x,y)&\\longmapsto f(x,y,z):=(3x-y,y,3z-y)\\end{align}\n$$\n\nIn the canonical basis:\n\n$$\n\\mathbf{B}=\\begin{pmatrix}e_1 & e_2 & e_3\\end{pmatrix}\\qquad e_1=(1,0,0)\\qquad e_2=(0,1,0)\\qquad e_3=(0,0,1)\n$$\n\nOur function is represented as:\n\n$$\nf(v) = \\mathbf{B} A X \\qquad A=\\begin{pmatrix} 3 & -1 & 0\\\\0 & 1 & 0 \\\\ 0 & -1 & 3\\end{pmatrix} \\qquad X=\\begin{pmatrix}x_1\\\\x_2\\\\x_3\\end{pmatrix}\n$$\n\nTo compute its eigenvalues and eigenvectors we solve first:\n\n$$\n\\begin{vmatrix} 3-\\lambda & -1 & 0\\\\0 & 1-\\lambda & 0 \\\\ 0 & -1 & 3-\\lambda\\end{vmatrix}=0\n$$\n\nWhich gives us:\n\n$$\n(3-\\lambda)(3-\\lambda)(1-\\lambda)=0\n$$The eigenvalues are $3$, again $3$ and $1$.\n\nThe corresponding eigenvectors are:\n\n$$\n\\begin{align}\n&V_3=span\\{\\begin{pmatrix}1& 0&0\\end{pmatrix}^\\intercal,\\begin{pmatrix}2 & 0 & 1\\end{pmatrix}^\\intercal\\}\\\\\n&V_1=span\\{\\begin{pmatrix}1&2&1\\end{pmatrix}^\\intercal\\}\n\\end{align}\n$$\n\nThus the eigenvectors of $f$ are:\n\n$$\n\\begin{align}\n&\\mathbb{V}_3=span\\{\\mathbf{B}\\begin{pmatrix}1& 0&0\\end{pmatrix}^\\intercal,\\mathbf{B}\\begin{pmatrix}2 & 0 & 1\\end{pmatrix}^\\intercal\\}\\\\\n&\\mathbb{V}_1=span\\{\\mathbf{B}\\begin{pmatrix}1&2&1\\end{pmatrix}^\\intercal\\}\n\\end{align}\n$$\n\n## Example 3\n\nThe function:\n\n$$\n\\begin{align}f:\\mathbb{R}^3&\\overset{\\sim}{\\longrightarrow}    \\mathbb{R}^3\\\\ (x,y)&\\longmapsto f(x,y,z):=(3x+y,3y,2z)\\end{align}\n$$\n\nacting on vectors represented wrt canonical basis as:\n\n$$\nf(v) = \\mathbf{B} A X \\qquad A=\\begin{pmatrix} 3 & 1 & 0\\\\0 & 3 & 0 \\\\ 0 & 0 & 2\\end{pmatrix} \\qquad X=\\begin{pmatrix}x_1\\\\x_2\\\\x_3\\end{pmatrix}\n$$\n\nComputing the eigenvalues as usual\n\n$$\n\\begin{vmatrix} 3-\\lambda & 1 & 0\\\\0 & 3-\\lambda & 0 \\\\ 0 & 0 & 2-\\lambda\\end{vmatrix}=0\n$$\n\nSolving $(3-\\lambda)^2(2-\\lambda)=0$ gives us: $\\lambda=3,2$.\n\nFor $\\lambda=3$, the eigenvector of $A$ and $f$ are respectively:\n\n$$\nc\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix},\\qquad c\\mathbf{B}\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}\n$$\n\nJust one! Despite the fact that the eigenvalue $3$ appear twice in that determinant equation.\n\nFor $\\lambda=2$:\n\n$$\nc\\begin{pmatrix}0\\\\0\\\\1\\end{pmatrix},\\qquad c\\mathbf{B}\\begin{pmatrix}0\\\\0\\\\1\\end{pmatrix}\n$$\n\nIn this example we have a shortage on eigenvectors, not enough to span the $\\mathbf{R}^3$ they are embedded in.\n\n# Representing $f$ wrt basis of its eigenvectors (Diagonalization)\n\nIn the previous three examples seen how to compute the eigenvalues and eigenvectors of a linear function represented wrt the canonical basis.\n\nWhat is the matrix representing those $f$'s wrt to the eigenvector basis? Is this always possible?\n\nTo get the $A'$ representation of $f$ we can follow either one of two paths that we seen before:\n\n1.  Act with $f$ on the eigenvectors we computed at express the output wrt those as well.\n2.  Compute $A$ wrt the canonical basis, compute the change of basis $P$, that related $\\mathbf{B}'=\\mathbf{B}P$, and compute $P^{-1} A P =: A'$. We will call the matrix $P$ as $S$ when dealing with eigenvectors.\n\nWe will only focus on the 2. approach.\n\n## Example 1 (cont.)\n\nSince the eigenvectors:\n\n$$\nv_1=\\mathbf{B}\\begin{pmatrix}-1\\\\1\\end{pmatrix} \\qquad v_2=\\mathbf{B}\\begin{pmatrix}1\\\\1\\end{pmatrix}\\qquad \\mathbf{B}=\\begin{pmatrix}e_1 & e_2\\end{pmatrix}\n$$ {#eq-1}\n\nare independent and they span the vector space $\\mathbb{V}$. Lets promote them to a basis:\n\n$$\n\\mathbf{B}':=\\begin{pmatrix}v_1 & v_2\\end{pmatrix}\n$$ {#eq-2}\n\nThere is a connection $s$ (or $S$) between both bases, substituting @eq-1 into @eq-2 we obtain:\n\n$$\n\\mathbf{B}'=\\mathbf{B} S=: s(\\mathbf{B}) \\qquad S=\\begin{pmatrix}-1 & 1\\\\1 & 1\\end{pmatrix}\n$$\n\nwhere the matrix we call $S$ (before we called it $P$) is a special matrix, whose columns are the eigenvectors of $A$. And the function $s$ (previously called $p$) is a linear function that transforms the canonical basis into the eigenvector basis. This means that the components $X$ of $v$ wrt the canonical basis are transformed into the components $X'$ of $v$ wrt the eigenvector basis as:\n\n$$\nX=SX'\n$$\n\nSince $\\mathbf{B} Y =:f(v)=\\mathbf{B}AX$ then:\n\n$$\n\\mathbf{B}'Y' =:f(v)=\\mathbf{B}'S^{-1} A SX'\n$$\n\nThus: $Y'=S^{-1}AS$ and $A'=S^{-1}AS$. Substituting the matrices above we find:\n\n$$\nA'=\\begin{pmatrix}-1 & 1\\\\1 & 1\\end{pmatrix}^{-1}\\begin{pmatrix}2 & -1\\\\-1 & 2\\end{pmatrix}\\begin{pmatrix}-1 & 1\\\\1 & 1\\end{pmatrix}=\\begin{pmatrix}3 & 0\\\\0 & 1\\end{pmatrix}\n$$ {#eq-Aprime}\n\nComputing these matrix product gave us a diagonal matrix whose entries are the eigenvalues.\n\nWhy a diagonal matrix?\n\n*Answer:* Equation @eq-Aprime is closely related to the equation $AS=\\Lambda S$:\n\n$$\n\\begin{pmatrix}2 & -1\\\\-1 & 2\\end{pmatrix}\\begin{pmatrix}-1 & 1\\\\1 & 1\\end{pmatrix}=\\begin{pmatrix}3 & 0\\\\0 & 1\\end{pmatrix}\\begin{pmatrix}-1 & 1\\\\1 & 1\\end{pmatrix}\n$$\n\nSince $S$ is invertible (the eigenvectors are independent) multiplying both sides from the left by $S^{-1}$ tell us $\\Lambda=S^{-1}AS$. Thus the $A'$ in @eq-Aprime above is in fact the $\\Lambda$.\n\nThus, $f$ act on vectors represented wrt to the eigenvector basis as:\n\n$$\nf(v)=\\mathbf{B}'\\Lambda X'\\qquad v=\\mathbf{B}'X' \\qquad \\Lambda = \\begin{pmatrix}3 & 0\\\\0 & 1\\end{pmatrix}\n$$\n\nmuch easier and simple to compute maps from now on, the eigenvectors are indeed special.\n\n## Example 2 (cont.)\n\nCalculations are similar to example 1. Putting the eigenvectors of $A$ as columns of matrix we get our special matrix:\n\n$$\nS=\\begin{pmatrix}1 & 2 & 1\\\\0 & 1 & 2\\\\0 & 2 & 1\\end{pmatrix}\n$$ {#eq-Smatrix_eg_2}\n\nNotice in this case that col 1 and col 2 of $S$ could be any other l.c. of $\\begin{pmatrix}1 & 0 & 0\\end{pmatrix}^\\intercal$ and $\\begin{pmatrix}2 & 1 & 2\\end{pmatrix}^\\intercal$ since $V_3$ is two dimensional and thus many other basis could be used, similarly, any multiple of col 3 could also have been used when defining $S$ above, since $V_1$ is one dimensional and any multiple of $\\begin{pmatrix}1 & 2 & 1\\end{pmatrix}^\\intercal$ constitutes a basis.\n\nSticking with our @eq-Smatrix_eg_2, we know that:\n\n$$\nAS=\\Lambda S \\leftrightsquigarrow \\begin{pmatrix} 3 & -1 & 0\\\\0 & 1 & 0 \\\\ 0 & -1 & 3\\end{pmatrix}\\begin{pmatrix}1 & 2 & 1\\\\0 & 1 & 2\\\\0 & 2 & 1\\end{pmatrix}=\\begin{pmatrix}3 & 0 & 0\\\\0 & 3 & 0\\\\0 & 0 & 1\\end{pmatrix}\\begin{pmatrix}1 & 2 & 1\\\\0 & 1 & 2\\\\0 & 2 & 1\\end{pmatrix}\n$$\n\nAnd therefore:\n\n$$\nf(v)=\\mathbf{B}'\\Lambda X'\\qquad v=\\mathbf{B}'X' \\qquad \\Lambda = \\begin{pmatrix}3 & 0 & 0\\\\0 & 3 & 0\\\\0 & 0 & 1\\end{pmatrix}\n$$\n\n## Example 3 (cont)\n\nIn this case, we only have two eigenvectors. We needed three to write our $S$. In this case it is not possible to represent our $f$ wrt to its eigenvector basis, simply because the eigenvectors for this specific $f$ do now constitute a basis, one vector is missing.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"eigen.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.32","editor":"visual","comments":{"hypothesis":true},"preview":{"browser":"chrome"},"theme":{"light":"minty"},"fontcolor":"rgb(190,190,190)","backgroundcolor":"rgb(32,31,30)","margin-left":"rgb(32,31,30)","navbar-color":"rgba(216, 6, 33 .65)","title":"Eigenvalues and eigenvectors"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}